<!doctype html><html lang=en class=no-js> <head><!-- Code from google optimiser --><script src="https://www.googleoptimize.com/optimize.js?id=OPT-M98W6LW"></script><!-- Code from Mouseflow --><script type=text/javascript>
      window._mfq = window._mfq || [];
      (function() {
        var mf = document.createElement("script");
        mf.type = "text/javascript"; mf.defer = true;
        mf.src = "//cdn.mouseflow.com/projects/7b4494c9-7974-49f0-9777-d14450db37e6.js";
        document.getElementsByTagName("head")[0].appendChild(mf);
      })();
    </script><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Harsha's notes on data science"><meta name=author content="Harsha Achyuthuni"><link href=https://www.harshaash.com/Python/Linear%20regression%20usecase%20mpg%20prediction/ rel=canonical><link rel=icon href=../../assets/images/logo.jpg><meta name=generator content="mkdocs-1.5.0, mkdocs-material-7.2.4"><title>Lasso and Ridge regression (Python) - Data Science with Harsha</title><link rel=stylesheet href=../../assets/stylesheets/main.f7f47774.min.css><link rel=stylesheet href=../../assets/stylesheets/palette.3f5d1f46.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback"><style>:root{--md-text-font-family:"Roboto";--md-code-font-family:"Roboto Mono"}</style><link rel=stylesheet href=../../overrides/assets/stylesheets/user_defined.css><script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-65034507-2","auto"),ga("set","anonymizeIp",!0),ga("send","pageview"),document.addEventListener("DOMContentLoaded",function(){document.forms.search&&document.forms.search.query.addEventListener("blur",function(){var e;this.value&&(e=document.location.pathname,ga("send","pageview",e+"?q="+this.value))}),"undefined"!=typeof location$&&location$.subscribe(function(e){ga("send","pageview",e.pathname)})})</script><script async src=https://www.google-analytics.com/analytics.js></script></head> <body dir=ltr data-md-color-scheme data-md-color-primary data-md-color-accent> <script>function __prefix(e){return new URL("../..",location).pathname+"."+e}function __get(e,t=localStorage){return JSON.parse(t.getItem(__prefix(e)))}</script> <script>var palette=__get("__palette");if(null!==palette&&"object"==typeof palette.color)for(var key in palette.color)document.body.setAttribute("data-md-color-"+key,palette.color[key])</script> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#auto-mpg-linear-regression-analysis class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../.. title="Data Science with Harsha" class="md-header__button md-logo" aria-label="Data Science with Harsha" data-md-component=logo> <img src=../../assets/images/logo.jpg alt=logo> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Data Science with Harsha </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Lasso and Ridge regression (Python) </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme data-md-color-primary data-md-color-accent aria-hidden=true type=radio name=__palette id=__palette_1> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_2> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M7 10a2 2 0 0 1 2 2 2 2 0 0 1-2 2 2 2 0 0 1-2-2 2 2 0 0 1 2-2m10-3a5 5 0 0 1 5 5 5 5 0 0 1-5 5H7a5 5 0 0 1-5-5 5 5 0 0 1 5-5h10M7 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3h10a3 3 0 0 0 3-3 3 3 0 0 0-3-3H7z"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme data-md-color-primary data-md-color-accent aria-hidden=true type=radio name=__palette id=__palette_3> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=red data-md-color-accent=red aria-label="Switch to light mode" type=radio name=__palette id=__palette_4> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_3 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3z"/></svg> </label> </form> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7 0-.24-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91 1.61 0 2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08z"/></svg> </a> <button type=reset class="md-search__icon md-icon" aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list></ol> </div> </div> </div> </div> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../.. title="Data Science with Harsha" class="md-nav__button md-logo" aria-label="Data Science with Harsha" data-md-component=logo> <img src=../../assets/images/logo.jpg alt=logo> </a> Data Science with Harsha </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../.. class=md-nav__link> Home </a> </li> <li class=md-nav__item> <a href=../../resume/ class=md-nav__link> Resume </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3 type=checkbox id=__nav_3 checked> <label class=md-nav__link for=__nav_3> Blog <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Blog data-md-level=1> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Blog </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Others/Table_of_Contents/ class=md-nav__link> Table of Contents </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_2 type=checkbox id=__nav_3_2> <label class=md-nav__link for=__nav_3_2> Visualization <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Visualization data-md-level=2> <label class=md-nav__title for=__nav_3_2> <span class="md-nav__icon md-icon"></span> Visualization </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../Vizualisation%20using%20python%20Part%201/ class=md-nav__link> Vizualizing tabular data (Python) </a> </li> <li class=md-nav__item> <a href=../Visualization%20for%20predictive%20analytics/ class=md-nav__link> Vizualising for predictive analytics (Python) </a> </li> <li class=md-nav__item> <a href=../../R/Univariate-analysis/ class=md-nav__link> Univariate Analysis (R) </a> </li> <li class=md-nav__item> <a href=../../R/multivariateAnalysis/ class=md-nav__link> Multivariate Analysis (R) </a> </li> <li class=md-nav__item> <a href=../../R/multicollinearity/ class=md-nav__link> Multicollinearity (R) </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_3 type=checkbox id=__nav_3_3> <label class=md-nav__link for=__nav_3_3> Statistics basics <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Statistics basics" data-md-level=2> <label class=md-nav__title for=__nav_3_3> <span class="md-nav__icon md-icon"></span> Statistics basics </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../Statistics%20basics/ class=md-nav__link> Statistics Basics (Python) </a> </li> <li class=md-nav__item> <a href=../../R/Probability/ class=md-nav__link> Probability (R) </a> </li> <li class=md-nav__item> <a href=../../R/vectors/ class=md-nav__link> Vectors (R) </a> </li> <li class=md-nav__item> <a href=../../R/matrices/ class=md-nav__link> Matrices (R) </a> </li> <li class=md-nav__item> <a href=../Call%20center%20distributions/ class=md-nav__link> Call center distributions (Python) </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_4 type=checkbox id=__nav_3_4> <label class=md-nav__link for=__nav_3_4> Hypothesis Testing <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Hypothesis Testing" data-md-level=2> <label class=md-nav__title for=__nav_3_4> <span class="md-nav__icon md-icon"></span> Hypothesis Testing </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../R/attendance_t_test/ class=md-nav__link> z-test and t-test (R) </a> </li> <li class=md-nav__item> <a href=../../R/anova/ class=md-nav__link> ANOVA Test (R) </a> </li> <li class=md-nav__item> <a href=../../R/chi-sq-goodness-of-fit/ class=md-nav__link> Chi-Square Goodness of fit (R) </a> </li> <li class=md-nav__item> <a href=../../R/chi-sq-test-of-independence/ class=md-nav__link> Chi-Square test of independence (R) </a> </li> <li class=md-nav__item> <a href=../Hypothesis_Testing_Python/ class=md-nav__link> Hypothesis testing using Python </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_5 type=checkbox id=__nav_3_5> <label class=md-nav__link for=__nav_3_5> Factor Analysis <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Factor Analysis" data-md-level=2> <label class=md-nav__title for=__nav_3_5> <span class="md-nav__icon md-icon"></span> Factor Analysis </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../R/Curse-of-Dimensionality/ class=md-nav__link> Curse of dimensionality </a> </li> <li class=md-nav__item> <a href=../../R/EFA/ class=md-nav__link> Exploratory factor analysis (R) </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_6 type=checkbox id=__nav_3_6 checked> <label class=md-nav__link for=__nav_3_6> Prediction algorithms <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Prediction algorithms" data-md-level=2> <label class=md-nav__title for=__nav_3_6> <span class="md-nav__icon md-icon"></span> Prediction algorithms </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_6_1 type=checkbox id=__nav_3_6_1 checked> <label class=md-nav__link for=__nav_3_6_1> Classification Algorithms <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Classification Algorithms" data-md-level=3> <label class=md-nav__title for=__nav_3_6_1> <span class="md-nav__icon md-icon"></span> Classification Algorithms </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../R/logistic-regression/ class=md-nav__link> Logistic Regression (R) </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" data-md-toggle=toc type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> Lasso and Ridge regression (Python) <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> Lasso and Ridge regression (Python) </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#dataset class=md-nav__link> Dataset </a> </li> <li class=md-nav__item> <a href=#use-case-description class=md-nav__link> Use Case Description </a> </li> <li class=md-nav__item> <a href=#feature-engineering-and-cleaning class=md-nav__link> Feature Engineering and Cleaning </a> <nav class=md-nav aria-label="Feature Engineering and Cleaning"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#categorical-encoding class=md-nav__link> Categorical encoding </a> </li> <li class=md-nav__item> <a href=#train-test-split class=md-nav__link> Train test split </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#baseline-linear-regression-model class=md-nav__link> Baseline Linear Regression model </a> <nav class=md-nav aria-label="Baseline Linear Regression model"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#accuracy class=md-nav__link> Accuracy </a> </li> <li class=md-nav__item> <a href=#detailed-statistical-summary class=md-nav__link> Detailed statistical summary </a> <nav class=md-nav aria-label="Detailed statistical summary"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#t-test-for-individual-coefficients class=md-nav__link> t-test for Individual Coefficients </a> </li> <li class=md-nav__item> <a href=#f-test-for-overall-model-significance class=md-nav__link> F-test for Overall Model Significance </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#feature-significance-summary class=md-nav__link> Feature Significance Summary </a> <nav class=md-nav aria-label="Feature Significance Summary"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#statistically-significant-features class=md-nav__link> Statistically Significant Features </a> </li> <li class=md-nav__item> <a href=#non-significant-features class=md-nav__link> Non-Significant Features </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#overall-model-significance class=md-nav__link> Overall Model Significance </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#assumptions-of-linear-regression class=md-nav__link> Assumptions of Linear Regression </a> </li> <li class=md-nav__item> <a href=#testing-residuals class=md-nav__link> Testing residuals </a> </li> <li class=md-nav__item> <a href=#multicollinearity class=md-nav__link> Multicollinearity </a> </li> <li class=md-nav__item> <a href=#outlier-detection class=md-nav__link> Outlier detection </a> </li> <li class=md-nav__item> <a href=#cross-validation-accuracy-scores class=md-nav__link> Cross-validation accuracy scores </a> </li> <li class=md-nav__item> <a href=#building-a-better-model class=md-nav__link> Building a better model </a> <nav class=md-nav aria-label="Building a better model"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#visualizations class=md-nav__link> Visualizations </a> </li> <li class=md-nav__item> <a href=#correlation-matrix class=md-nav__link> Correlation matrix </a> </li> <li class=md-nav__item> <a href=#feature-engineering class=md-nav__link> Feature engineering </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#feature-selection class=md-nav__link> Feature selection </a> <nav class=md-nav aria-label="Feature selection"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#forward-and-backward-selection class=md-nav__link> Forward and backward selection </a> </li> <li class=md-nav__item> <a href=#lasso-and-ridge-regularization class=md-nav__link> Lasso and Ridge Regularization </a> </li> <li class=md-nav__item> <a href=#cross-validation-and-grid-search class=md-nav__link> Cross validation and Grid search </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#references class=md-nav__link> References </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../R/CHAID/ class=md-nav__link> CHAID Decision Trees (R) </a> </li> <li class=md-nav__item> <a href=../../R/CART-Classification/ class=md-nav__link> CART Classification (R) </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_6_2 type=checkbox id=__nav_3_6_2> <label class=md-nav__link for=__nav_3_6_2> Regression Algorithms <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Regression Algorithms" data-md-level=3> <label class=md-nav__title for=__nav_3_6_2> <span class="md-nav__icon md-icon"></span> Regression Algorithms </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../R/part-and-partial-corr/ class=md-nav__link> Part and partial correlation </a> </li> <li class=md-nav__item> <a href=../../R/Linear-regression/ class=md-nav__link> Linear Regression (R) </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_7 type=checkbox id=__nav_3_7> <label class=md-nav__link for=__nav_3_7> Preprocessing data <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Preprocessing data" data-md-level=2> <label class=md-nav__title for=__nav_3_7> <span class="md-nav__icon md-icon"></span> Preprocessing data </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../R/KNN_Imputation/ class=md-nav__link> Null Value Imputation (R) </a> </li> <li class=md-nav__item> <a href=../Machine%20Learning%20Part%201/ class=md-nav__link> Feature engineering (Python) </a> </li> <li class=md-nav__item> <a href=../../R/Handling-Imbalanced-classes/ class=md-nav__link> Handling Imbalanced Classes </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_8 type=checkbox id=__nav_3_8> <label class=md-nav__link for=__nav_3_8> Machine Learning <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Machine Learning" data-md-level=2> <label class=md-nav__title for=__nav_3_8> <span class="md-nav__icon md-icon"></span> Machine Learning </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=https://harshaachyuthuni.shinyapps.io/Machine_Learning/ class=md-nav__link> Interactive Machine Learning (RShiny) </a> </li> <li class=md-nav__item> <a href=../ML%20using%20scikit-learn/ class=md-nav__link> Multi models (Python) </a> </li> <li class=md-nav__item> <a href=../Explainable%20models%20using%20Heart%20Failure/ class=md-nav__link> Explainable models (Python) </a> </li> <li class=md-nav__item> <a href=../Demonstrating%20online%20learning/ class=md-nav__link> Streaming Machine Learning (Python) </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_9 type=checkbox id=__nav_3_9> <label class=md-nav__link for=__nav_3_9> Time Series forecasting <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Time Series forecasting" data-md-level=2> <label class=md-nav__title for=__nav_3_9> <span class="md-nav__icon md-icon"></span> Time Series forecasting </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../R/time-series/ class=md-nav__link> Introduction to stationarity (R) </a> </li> <li class=md-nav__item> <a href=../../R/Stationarity-tests/ class=md-nav__link> Stationary Tests (R) </a> </li> <li class=md-nav__item> <a href=../../R/ARIMA/ class=md-nav__link> ARIMA in R </a> </li> <li class=md-nav__item> <a href=../ARIMA%20Forecasting/ class=md-nav__link> ARIMA in Python </a> </li> <li class=md-nav__item> <a href=../../R/Seasonal-Time-Series/ class=md-nav__link> Seasonal time series (R) </a> </li> <li class=md-nav__item> <a href=../../R/VAR-models/ class=md-nav__link> VAR Models (R) </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_10 type=checkbox id=__nav_3_10> <label class=md-nav__link for=__nav_3_10> Deep learning <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Deep learning" data-md-level=2> <label class=md-nav__title for=__nav_3_10> <span class="md-nav__icon md-icon"></span> Deep learning </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../ANN-1/ class=md-nav__link> Perceptron </a> </li> <li class=md-nav__item> <a href=../Backpropagation/ class=md-nav__link> Backpropagation </a> </li> <li class=md-nav__item> <a href=../DNN/ class=md-nav__link> Tensorflow and Keras </a> </li> <li class=md-nav__item> <a href=../Time%20series%20deep%20learning/ class=md-nav__link> Time series (python) </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_11 type=checkbox id=__nav_3_11> <label class=md-nav__link for=__nav_3_11> Generative AI <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Generative AI" data-md-level=2> <label class=md-nav__title for=__nav_3_11> <span class="md-nav__icon md-icon"></span> Generative AI </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../LLM%20Tokenizers/ class=md-nav__link> LLM Tokenizers </a> </li> <li class=md-nav__item> <a href=../Agentic%20AI/ class=md-nav__link> Agentic AI </a> </li> <li class=md-nav__item> <a href=../RAG/ class=md-nav__link> RAG </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_12 type=checkbox id=__nav_3_12> <label class=md-nav__link for=__nav_3_12> Prescriptive Analytics <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Prescriptive Analytics" data-md-level=2> <label class=md-nav__title for=__nav_3_12> <span class="md-nav__icon md-icon"></span> Prescriptive Analytics </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../R/Linear-programming/ class=md-nav__link> Linear Programming (R) </a> </li> <li class=md-nav__item> <a href=../../R/adoption_of_new_product/ class=md-nav__link> Adoption of new product (R) </a> </li> <li class=md-nav__item> <a href=../Diffusion%20on%20networks/ class=md-nav__link> Bass Forecasting model (Python) </a> </li> <li class=md-nav__item> <a href=../../Others/AHP/ class=md-nav__link> Analytic Hierarchy Process </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_13 type=checkbox id=__nav_3_13> <label class=md-nav__link for=__nav_3_13> Clustering <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Clustering data-md-level=2> <label class=md-nav__title for=__nav_3_13> <span class="md-nav__icon md-icon"></span> Clustering </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../R/hierarchical_clustering/ class=md-nav__link> Hierarchical Clustering </a> </li> <li class=md-nav__item> <a href=../../R/kMeansClustering/ class=md-nav__link> K-Means Clustering </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_14 type=checkbox id=__nav_3_14> <label class=md-nav__link for=__nav_3_14> Reinforcement Learning <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Reinforcement Learning" data-md-level=2> <label class=md-nav__title for=__nav_3_14> <span class="md-nav__icon md-icon"></span> Reinforcement Learning </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../R/CustomerLifetimeValue/ class=md-nav__link> Customer Lifetime Value </a> </li> <li class=md-nav__item> <a href=../../R/recommendation-systems/ class=md-nav__link> Recommendation Systems (R) </a> </li> <li class=md-nav__item> <a href=../Neural_collaborative_filtering/ class=md-nav__link> Collaborative Filtering (Python) </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_15 type=checkbox id=__nav_3_15> <label class=md-nav__link for=__nav_3_15> Networks <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Networks data-md-level=2> <label class=md-nav__title for=__nav_3_15> <span class="md-nav__icon md-icon"></span> Networks </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../Introduction%20to%20Networkx/ class=md-nav__link> Introduction to NetworkX (Python) </a> </li> <li class=md-nav__item> <a href=../Network%20Science/ class=md-nav__link> Network Science (Python) </a> </li> <li class=md-nav__item> <a href=../Network%20centrality/ class=md-nav__link> Network Centrality (Python) </a> </li> <li class=md-nav__item> <a href=../Shortest%20path%20problems/ class=md-nav__link> Shortest path using integer programming (Python) </a> </li> <li class=md-nav__item> <a href=../Network%20Flow%20problems/ class=md-nav__link> Network flow problems (Python) </a> </li> <li class=md-nav__item> <a href=../Community%20detection/ class=md-nav__link> Community detection (Python) </a> </li> <li class=md-nav__item> <a href=../Bipartite%20matching/ class=md-nav__link> Bipartite matching (Python) </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_16 type=checkbox id=__nav_3_16> <label class=md-nav__link for=__nav_3_16> Deployment <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Deployment data-md-level=2> <label class=md-nav__title for=__nav_3_16> <span class="md-nav__icon md-icon"></span> Deployment </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../Machine%20learning%20as%20HTTP%20Request/ class=md-nav__link> ML deployment in Flask (Python) </a> </li> <li class=md-nav__item> <a href=../Saving%20predictions%20in%20database/ class=md-nav__link> Handling databases using python </a> </li> <li class=md-nav__item> <a href=../ORM/ class=md-nav__link> ORM (Python) </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_17 type=checkbox id=__nav_3_17> <label class=md-nav__link for=__nav_3_17> Higher education review <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Higher education review" data-md-level=2> <label class=md-nav__title for=__nav_3_17> <span class="md-nav__icon md-icon"></span> Higher education review </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Others/IIMB%20BAI/ class=md-nav__link> IIMB BAI </a> </li> <li class=md-nav__item> <a href=../../Others/part%20time%20data%20science%20masters/ class=md-nav__link> Part-time DS masters </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_18 type=checkbox id=__nav_3_18> <label class=md-nav__link for=__nav_3_18> External blogs <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="External blogs" data-md-level=2> <label class=md-nav__title for=__nav_3_18> <span class="md-nav__icon md-icon"></span> External blogs </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Others/Imperial%20College%20London/ class=md-nav__link> Imperial college London </a> </li> <li class=md-nav__item> <a href=../../Others/Rolls%20Royce/ class=md-nav__link> Rolls Royce </a> </li> <li class=md-nav__item> <a href=../../Others/Publications/ class=md-nav__link> Publications/conferences </a> </li> <li class=md-nav__item> <a href=../../Others/Deployed%20apps/ class=md-nav__link> Deployed apps </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_19 type=checkbox id=__nav_3_19> <label class=md-nav__link for=__nav_3_19> Projects <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Projects data-md-level=2> <label class=md-nav__title for=__nav_3_19> <span class="md-nav__icon md-icon"></span> Projects </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Others/preventive_maintainence/ class=md-nav__link> Preventive maintainence </a> </li> <li class=md-nav__item> <a href=../../Others/Contract%20Intelligence%20tool/ class=md-nav__link> Contract Intelligence </a> </li> <li class=md-nav__item> <a href=../../Others/competitor%20intelligence/ class=md-nav__link> Competitor intelligence </a> </li> <li class=md-nav__item> <a href=../../Others/Scarecrow/ class=md-nav__link> Intelligent annotation </a> </li> <li class=md-nav__item> <a href=../../Others/Demand%20Forecasting/ class=md-nav__link> Demand Forecasting </a> </li> <li class=md-nav__item> <a href=../../Others/bid%20allocation%20model/ class=md-nav__link> Bid Allocation </a> </li> <li class=md-nav__item> <a href=../../Others/IIMB%20project/ class=md-nav__link> Reward and Recognition contests </a> </li> <li class=md-nav__item> <a href=../../Others/supply%20chain%20analytics/ class=md-nav__link> Supply chain analytics </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#dataset class=md-nav__link> Dataset </a> </li> <li class=md-nav__item> <a href=#use-case-description class=md-nav__link> Use Case Description </a> </li> <li class=md-nav__item> <a href=#feature-engineering-and-cleaning class=md-nav__link> Feature Engineering and Cleaning </a> <nav class=md-nav aria-label="Feature Engineering and Cleaning"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#categorical-encoding class=md-nav__link> Categorical encoding </a> </li> <li class=md-nav__item> <a href=#train-test-split class=md-nav__link> Train test split </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#baseline-linear-regression-model class=md-nav__link> Baseline Linear Regression model </a> <nav class=md-nav aria-label="Baseline Linear Regression model"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#accuracy class=md-nav__link> Accuracy </a> </li> <li class=md-nav__item> <a href=#detailed-statistical-summary class=md-nav__link> Detailed statistical summary </a> <nav class=md-nav aria-label="Detailed statistical summary"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#t-test-for-individual-coefficients class=md-nav__link> t-test for Individual Coefficients </a> </li> <li class=md-nav__item> <a href=#f-test-for-overall-model-significance class=md-nav__link> F-test for Overall Model Significance </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#feature-significance-summary class=md-nav__link> Feature Significance Summary </a> <nav class=md-nav aria-label="Feature Significance Summary"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#statistically-significant-features class=md-nav__link> Statistically Significant Features </a> </li> <li class=md-nav__item> <a href=#non-significant-features class=md-nav__link> Non-Significant Features </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#overall-model-significance class=md-nav__link> Overall Model Significance </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#assumptions-of-linear-regression class=md-nav__link> Assumptions of Linear Regression </a> </li> <li class=md-nav__item> <a href=#testing-residuals class=md-nav__link> Testing residuals </a> </li> <li class=md-nav__item> <a href=#multicollinearity class=md-nav__link> Multicollinearity </a> </li> <li class=md-nav__item> <a href=#outlier-detection class=md-nav__link> Outlier detection </a> </li> <li class=md-nav__item> <a href=#cross-validation-accuracy-scores class=md-nav__link> Cross-validation accuracy scores </a> </li> <li class=md-nav__item> <a href=#building-a-better-model class=md-nav__link> Building a better model </a> <nav class=md-nav aria-label="Building a better model"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#visualizations class=md-nav__link> Visualizations </a> </li> <li class=md-nav__item> <a href=#correlation-matrix class=md-nav__link> Correlation matrix </a> </li> <li class=md-nav__item> <a href=#feature-engineering class=md-nav__link> Feature engineering </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#feature-selection class=md-nav__link> Feature selection </a> <nav class=md-nav aria-label="Feature selection"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#forward-and-backward-selection class=md-nav__link> Forward and backward selection </a> </li> <li class=md-nav__item> <a href=#lasso-and-ridge-regularization class=md-nav__link> Lasso and Ridge Regularization </a> </li> <li class=md-nav__item> <a href=#cross-validation-and-grid-search class=md-nav__link> Cross validation and Grid search </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#references class=md-nav__link> References </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <script type=text/javascript src=https://cdn.mathjax.org/mathjax/latest/MathJax.js>
MathJax.Hub.Config({
 extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js"],
 jax: ["input/TeX", "output/HTML-CSS"],
 tex2jax: {
     inlineMath: [ ['$','$'], ["\\(","\\)"] ],
     displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
 },
 "HTML-CSS": { availableFonts: ["TeX"] }
});
</script> <h1 id=auto-mpg-linear-regression-analysis>Auto MPG Linear Regression Analysis<a class=headerlink href=#auto-mpg-linear-regression-analysis title="Permanent link">&para;</a></h1> <p>This notebook performs a complete linear regression analysis on the Auto MPG dataset. The goal is to predict the miles per gallon (mpg) of vehicles based on various features such as engine size, weight, and model year.</p> <h2 id=dataset>Dataset<a class=headerlink href=#dataset title="Permanent link">&para;</a></h2> <p>The <a href=https://archive.ics.uci.edu/dataset/9/auto+mpg>dataset</a> is designed to predict city-cycle fuel consumption in miles per gallon (MPG) based on various vehicle attributes. Originally from the StatLib library maintained at Carnegie Mellon University, originally used in the 1983 American Statistical Association Exposition. Predicting fuel efficiency helps manufacturers design better vehicles, assists consumers in making informed decisions, and supports environmental policy planning.</p> <h2 id=use-case-description>Use Case Description<a class=headerlink href=#use-case-description title="Permanent link">&para;</a></h2> <p>Our objective is to predict the <em>mpg</em> using linear regression techniques. This involves: - Exploring relationships between car attributes and fuel consumption - Identifying statistically significant predictors - Validating model assumptions and performance - Diagnosing issues such as multicollinearity, autocorrelation, and outliers - Evaluating model robustness using cross-validation and residual analysis</p> <div class=highlight><pre><span></span><code><span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
<span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
<span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
<span class=kn>import</span> <span class=nn>seaborn</span> <span class=k>as</span> <span class=nn>sns</span>
<span class=kn>from</span> <span class=nn>sklearn.model_selection</span> <span class=kn>import</span> <span class=n>train_test_split</span><span class=p>,</span> <span class=n>cross_val_score</span><span class=p>,</span> <span class=n>KFold</span>
<span class=kn>from</span> <span class=nn>sklearn.linear_model</span> <span class=kn>import</span> <span class=n>LinearRegression</span>
<span class=kn>from</span> <span class=nn>sklearn.metrics</span> <span class=kn>import</span> <span class=n>mean_squared_error</span><span class=p>,</span> <span class=n>r2_score</span><span class=p>,</span> <span class=n>mean_absolute_percentage_error</span>
<span class=kn>from</span> <span class=nn>sklearn.preprocessing</span> <span class=kn>import</span> <span class=n>StandardScaler</span>
<span class=kn>from</span> <span class=nn>sklearn.feature_selection</span> <span class=kn>import</span> <span class=n>SequentialFeatureSelector</span>
<span class=kn>from</span> <span class=nn>statsmodels.api</span> <span class=kn>import</span> <span class=n>OLS</span><span class=p>,</span> <span class=n>add_constant</span>
<span class=kn>from</span> <span class=nn>statsmodels.stats.outliers_influence</span> <span class=kn>import</span> <span class=n>variance_inflation_factor</span>
<span class=kn>from</span> <span class=nn>statsmodels.stats.stattools</span> <span class=kn>import</span> <span class=n>durbin_watson</span>
<span class=kn>from</span> <span class=nn>scipy</span> <span class=kn>import</span> <span class=n>stats</span>
<span class=kn>import</span> <span class=nn>warnings</span>
<span class=n>warnings</span><span class=o>.</span><span class=n>filterwarnings</span><span class=p>(</span><span class=s1>&#39;ignore&#39;</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code><span class=n>df</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>read_csv</span><span class=p>(</span><span class=s1>&#39;../Data/auto-mpg.csv&#39;</span><span class=p>)</span>
<span class=n>df</span><span class=o>.</span><span class=n>dropna</span><span class=p>(</span><span class=n>inplace</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
<span class=n>df</span>
</code></pre></div> <div> <style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style> <table border=1 class=dataframe> <thead> <tr style="text-align: right;"> <th></th> <th>mpg</th> <th>cylinders</th> <th>displacement</th> <th>horsepower</th> <th>weight</th> <th>acceleration</th> <th>model-year</th> </tr> </thead> <tbody> <tr> <th>0</th> <td>18.0</td> <td>8</td> <td>307.0</td> <td>130.0</td> <td>3504</td> <td>12.0</td> <td>70</td> </tr> <tr> <th>1</th> <td>15.0</td> <td>8</td> <td>350.0</td> <td>165.0</td> <td>3693</td> <td>11.5</td> <td>70</td> </tr> <tr> <th>2</th> <td>18.0</td> <td>8</td> <td>318.0</td> <td>150.0</td> <td>3436</td> <td>11.0</td> <td>70</td> </tr> <tr> <th>3</th> <td>16.0</td> <td>8</td> <td>304.0</td> <td>150.0</td> <td>3433</td> <td>12.0</td> <td>70</td> </tr> <tr> <th>4</th> <td>17.0</td> <td>8</td> <td>302.0</td> <td>140.0</td> <td>3449</td> <td>10.5</td> <td>70</td> </tr> <tr> <th>...</th> <td>...</td> <td>...</td> <td>...</td> <td>...</td> <td>...</td> <td>...</td> <td>...</td> </tr> <tr> <th>393</th> <td>27.0</td> <td>4</td> <td>140.0</td> <td>86.0</td> <td>2790</td> <td>15.6</td> <td>82</td> </tr> <tr> <th>394</th> <td>44.0</td> <td>4</td> <td>97.0</td> <td>52.0</td> <td>2130</td> <td>24.6</td> <td>82</td> </tr> <tr> <th>395</th> <td>32.0</td> <td>4</td> <td>135.0</td> <td>84.0</td> <td>2295</td> <td>11.6</td> <td>82</td> </tr> <tr> <th>396</th> <td>28.0</td> <td>4</td> <td>120.0</td> <td>79.0</td> <td>2625</td> <td>18.6</td> <td>82</td> </tr> <tr> <th>397</th> <td>31.0</td> <td>4</td> <td>119.0</td> <td>82.0</td> <td>2720</td> <td>19.4</td> <td>82</td> </tr> </tbody> </table> <p>396 rows × 7 columns</p> </div> <h2 id=feature-engineering-and-cleaning>Feature Engineering and Cleaning<a class=headerlink href=#feature-engineering-and-cleaning title="Permanent link">&para;</a></h2> <p>Data attributes and description</p> <table> <thead> <tr> <th><strong>Feature</strong></th> <th><strong>Type</strong></th> <th><strong>Description</strong></th> </tr> </thead> <tbody> <tr> <td>mpg</td> <td>Continuous</td> <td>Miles per gallon (target variable)</td> </tr> <tr> <td>cylinders</td> <td>Multi-valued discrete</td> <td>Number of cylinders</td> </tr> <tr> <td>displacement</td> <td>Continuous</td> <td>Engine displacement</td> </tr> <tr> <td>horsepower</td> <td>Continuous</td> <td>Engine horsepower</td> </tr> <tr> <td>weight</td> <td>Continuous</td> <td>Vehicle weight</td> </tr> <tr> <td>acceleration</td> <td>Continuous</td> <td>Time to accelerate from 0 to 60 mph</td> </tr> <tr> <td>model year</td> <td>Multi-valued discrete</td> <td>Year of manufacture</td> </tr> </tbody> </table> <div class=highlight><pre><span></span><code><span class=n>df</span><span class=o>.</span><span class=n>describe</span><span class=p>()</span>
</code></pre></div> <div> <style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style> <table border=1 class=dataframe> <thead> <tr style="text-align: right;"> <th></th> <th>mpg</th> <th>cylinders</th> <th>displacement</th> <th>horsepower</th> <th>weight</th> <th>acceleration</th> <th>model-year</th> </tr> </thead> <tbody> <tr> <th>count</th> <td>396.000000</td> <td>396.000000</td> <td>396.000000</td> <td>396.000000</td> <td>396.000000</td> <td>396.000000</td> <td>396.000000</td> </tr> <tr> <th>mean</th> <td>23.517172</td> <td>5.457071</td> <td>193.650253</td> <td>104.189394</td> <td>2973.000000</td> <td>15.555808</td> <td>76.027778</td> </tr> <tr> <th>std</th> <td>7.834368</td> <td>1.703511</td> <td>104.422387</td> <td>38.402030</td> <td>847.690354</td> <td>2.758295</td> <td>3.696969</td> </tr> <tr> <th>min</th> <td>9.000000</td> <td>3.000000</td> <td>68.000000</td> <td>46.000000</td> <td>1613.000000</td> <td>8.000000</td> <td>70.000000</td> </tr> <tr> <th>25%</th> <td>17.375000</td> <td>4.000000</td> <td>104.750000</td> <td>75.000000</td> <td>2225.250000</td> <td>13.800000</td> <td>73.000000</td> </tr> <tr> <th>50%</th> <td>23.000000</td> <td>4.000000</td> <td>148.500000</td> <td>92.000000</td> <td>2803.500000</td> <td>15.500000</td> <td>76.000000</td> </tr> <tr> <th>75%</th> <td>29.000000</td> <td>8.000000</td> <td>263.250000</td> <td>125.000000</td> <td>3610.000000</td> <td>17.125000</td> <td>79.000000</td> </tr> <tr> <th>max</th> <td>46.600000</td> <td>8.000000</td> <td>455.000000</td> <td>230.000000</td> <td>5140.000000</td> <td>24.800000</td> <td>82.000000</td> </tr> </tbody> </table> </div> <h3 id=categorical-encoding>Categorical encoding<a class=headerlink href=#categorical-encoding title="Permanent link">&para;</a></h3> <p>To use categorical variables in machine learning models, they must be converted into numerical form through encoding. Common types include:<br> - Label Encoding: Assigns a unique integer to each category. Simple but may imply order where none exists. - One-Hot Encoding: Creates a binary column for each category. Prevents ordinal assumptions. - Dummy Encoding: A variant of one-hot encoding that drops one column to avoid multicollinearity (dummy variable trap).</p> <p>For example, if the feature 'cylinders' has categories 3,4,5,6,8 dummy encoding would create four columns: cylinders_4, cylinders_5, cylinders_6, cylinders_8 with cylinders_3 implied when all are zero.</p> <div class=highlight><pre><span></span><code><span class=n>df</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>get_dummies</span><span class=p>(</span><span class=n>df</span><span class=p>,</span> <span class=n>columns</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;cylinders&#39;</span><span class=p>,</span> <span class=s1>&#39;model-year&#39;</span><span class=p>],</span> <span class=n>drop_first</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
<span class=n>df</span><span class=o>.</span><span class=n>columns</span>
</code></pre></div> <div class=highlight><pre><span></span><code>Index([&#39;mpg&#39;, &#39;displacement&#39;, &#39;horsepower&#39;, &#39;weight&#39;, &#39;acceleration&#39;,
       &#39;cylinders_4&#39;, &#39;cylinders_5&#39;, &#39;cylinders_6&#39;, &#39;cylinders_8&#39;,
       &#39;model-year_71&#39;, &#39;model-year_72&#39;, &#39;model-year_73&#39;, &#39;model-year_74&#39;,
       &#39;model-year_75&#39;, &#39;model-year_76&#39;, &#39;model-year_77&#39;, &#39;model-year_78&#39;,
       &#39;model-year_79&#39;, &#39;model-year_80&#39;, &#39;model-year_81&#39;, &#39;model-year_82&#39;],
      dtype=&#39;object&#39;)
</code></pre></div> <h3 id=train-test-split>Train test split<a class=headerlink href=#train-test-split title="Permanent link">&para;</a></h3> <p>We separate the dataset into independent variables (features) and the dependent variable (target). The independent variables (X) are the predictors that influence the outcome, while the dependent variable (Y) is what we aim to predict (in this case, the car's miles per gallon (MPG))</p> <div class=highlight><pre><span></span><code><span class=n>X</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>drop</span><span class=p>(</span><span class=n>columns</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;mpg&#39;</span><span class=p>])</span>
<span class=n>y</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=s1>&#39;mpg&#39;</span><span class=p>]</span>
</code></pre></div> <p>We divide our data into two parts: training data (80%) and testing data (20%). The training set is used to fit the model, while the test set evaluates how well the model generalizes to unseen data. This helps prevent overfitting and gives us a realistic estimate of model performance.</p> <div class=highlight><pre><span></span><code><span class=n>X_train</span><span class=p>,</span> <span class=n>X_test</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>y_test</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>test_size</span><span class=o>=</span><span class=mf>0.2</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
</code></pre></div> <h2 id=baseline-linear-regression-model>Baseline Linear Regression model<a class=headerlink href=#baseline-linear-regression-model title="Permanent link">&para;</a></h2> <p>We fit a baseline linear regression on the training data to establish reference performance. The model assumes a linear relationship between features and MPG, with no hyperparameter tuning. We evaluate RMSE and R² on the test set.</p> <div class=highlight><pre><span></span><code><span class=n>lr</span> <span class=o>=</span> <span class=n>LinearRegression</span><span class=p>()</span>
<span class=n>lr</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
<span class=n>y_pred</span> <span class=o>=</span> <span class=n>lr</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span>
</code></pre></div> <h3 id=accuracy>Accuracy<a class=headerlink href=#accuracy title="Permanent link">&para;</a></h3> <p>After training, we evaluate the model using: - RMSE (Root Mean Squared Error): quantifies the average prediction error in MPG (lower is better).<br> <span class=arithmatex>\(\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}\)</span><br> - R² (Coefficient of Determination): Indicates how much variance in the target is explained by the model (higher is better).<br> <span class=arithmatex>\(R^2 = 1 - \frac{SS_{res}}{SS_{tot}}\)</span><br> - Adjusted R²: Adjusts R² for the number of predictors, penalizing unnecessary complexity.<br> <span class=arithmatex>\(\text{Adjusted } R^2 = 1 - \left( \frac{(1 - R^2)(n - 1)}{n - p - 1} \right)\)</span><br> - MAPE (Mean Absolute Percentage Error): Expresses prediction error as a percentage. It’s useful for understanding the error relative to the actual values. (lower is better)<br> <span class=arithmatex>\(\text{MAPE} = \frac{100\%}{n} \sum_{i=1}^{n} \left| \frac{y_i - \hat{y}_i}{y_i} \right|\)</span> </p> <p>These metrics help us understand both accuracy and model fit.</p> <div class=highlight><pre><span></span><code><span class=k>def</span> <span class=nf>calculate_adjusted_r_squared</span><span class=p>(</span><span class=n>X_test</span><span class=p>,</span> <span class=n>r2</span><span class=p>):</span>
    <span class=n>n</span> <span class=o>=</span> <span class=n>X_test</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
    <span class=n>p</span> <span class=o>=</span> <span class=n>X_test</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span>
    <span class=n>adj_r2</span> <span class=o>=</span> <span class=mi>1</span> <span class=o>-</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>r2</span><span class=p>)</span> <span class=o>*</span> <span class=p>(</span><span class=n>n</span> <span class=o>-</span> <span class=mi>1</span><span class=p>)</span> <span class=o>/</span> <span class=p>(</span><span class=n>n</span> <span class=o>-</span> <span class=n>p</span> <span class=o>-</span> <span class=mi>1</span><span class=p>)</span>
    <span class=k>return</span> <span class=n>adj_r2</span>

<span class=n>rmse</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>mean_squared_error</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>))</span>
<span class=n>r2</span> <span class=o>=</span> <span class=n>r2_score</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>)</span>
<span class=n>mape</span> <span class=o>=</span> <span class=n>mean_absolute_percentage_error</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>)</span> <span class=o>*</span> <span class=mi>100</span>

<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;RMSE: </span><span class=si>{</span><span class=n>rmse</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;R²: </span><span class=si>{</span><span class=n>r2</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Adjusted R-squared: </span><span class=si>{0}</span><span class=s1>&#39;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=nb>round</span><span class=p>(</span><span class=n>calculate_adjusted_r_squared</span><span class=p>(</span><span class=n>X_test</span><span class=p>,</span> <span class=n>r2</span><span class=p>),</span> <span class=mi>3</span><span class=p>)))</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;MAPE: </span><span class=si>{</span><span class=n>mape</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>%&quot;</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>RMSE: 2.552
R²: 0.874
Adjusted R-squared: 0.832
MAPE: 9.25%
</code></pre></div> <p>MAPE is 9% (lower is better). The model explains 87% of the variance in MPG (R² = 0.87). We next examine coefficient significance and overall model. These statistical tests help assess the reliability and explanatory power of the regression.</p> <h3 id=detailed-statistical-summary>Detailed statistical summary<a class=headerlink href=#detailed-statistical-summary title="Permanent link">&para;</a></h3> <h4 id=t-test-for-individual-coefficients>t-test for Individual Coefficients<a class=headerlink href=#t-test-for-individual-coefficients title="Permanent link">&para;</a></h4> <p>Used to determine whether each independent variable significantly contributes to the model. - Null Hypothesis (H₀): The coefficient of the predictor variable is equal to zero (i.e., the variable has no effect).<br> $$ H_0: \beta_i = 0$$ - Alternative Hypothesis (H₁): The coefficient of the predictor variable is not equal to zero (i.e., the variable has a significant effect).<br> $$ H_1: \beta_i \ne 0 $$</p> <h4 id=f-test-for-overall-model-significance>F-test for Overall Model Significance<a class=headerlink href=#f-test-for-overall-model-significance title="Permanent link">&para;</a></h4> <p>Used to test whether the regression model as a whole is statistically significant. - Null Hypothesis (H₀): All regression coefficients are equal to zero (i.e., the model has no explanatory power). $$ H_0: \beta_1 = \beta_2 = \dots = \beta_k = 0$$ - Alternative Hypothesis (H₁): At least one regression coefficient is not zero (i.e., the model explains some variability in the response). $$ \text{At least one } \beta_i \ne 0 $$ Using Ordinary Least Squares (OLS) from statsmodels, we get a detailed statistical summary: - p-values: Test the significance of each feature. A low p-value (&lt; 0.05) suggests the feature is statistically significant. - t-statistics: Measure how many standard deviations the coefficient is from zero. - F-statistic: Tests whether the overall regression model is a good fit. - Confidence Intervals: Provide a range in which the true coefficient likely falls. This analysis helps us interpret the model beyond just accuracy.</p> <div class=highlight><pre><span></span><code><span class=n>X_const</span> <span class=o>=</span> <span class=n>add_constant</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>OLS</span><span class=p>(</span><span class=n>y</span><span class=p>,</span> <span class=n>X_const</span><span class=p>)</span><span class=o>.</span><span class=n>fit</span><span class=p>()</span>
<span class=nb>print</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>summary</span><span class=p>())</span>
</code></pre></div> <div class=highlight><pre><span></span><code>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                    mpg   R-squared:                       0.864
Model:                            OLS   Adj. R-squared:                  0.857
Method:                 Least Squares   F-statistic:                     119.5
Date:                Sun, 26 Oct 2025   Prob (F-statistic):          5.23e-149
Time:                        15:42:54   Log-Likelihood:                -980.93
No. Observations:                 396   AIC:                             2004.
Df Residuals:                     375   BIC:                             2087.
Df Model:                          20                                         
Covariance Type:            nonrobust                                         
=================================================================================
                    coef    std err          t      P&gt;|t|      [0.025      0.975]
---------------------------------------------------------------------------------
const            33.1314      2.397     13.822      0.000      28.418      37.845
displacement      0.0008      0.007      0.129      0.898      -0.012       0.014
horsepower       -0.0236      0.013     -1.788      0.075      -0.050       0.002
weight           -0.0057      0.001     -9.036      0.000      -0.007      -0.004
acceleration      0.0326      0.089      0.365      0.715      -0.143       0.208
cylinders_4       6.5792      1.573      4.184      0.000       3.487       9.671
cylinders_5       7.3758      2.368      3.115      0.002       2.719      12.032
cylinders_6       4.1555      1.760      2.361      0.019       0.695       7.616
cylinders_8       7.0737      2.032      3.480      0.001       3.077      11.070
model-year_71     1.0667      0.844      1.264      0.207      -0.592       2.726
model-year_72    -0.4355      0.831     -0.524      0.601      -2.070       1.199
model-year_73    -0.3859      0.749     -0.516      0.606      -1.858       1.086
model-year_74     1.4282      0.883      1.618      0.107      -0.308       3.164
model-year_75     1.1654      0.865      1.347      0.179      -0.536       2.867
model-year_76     1.6378      0.831      1.972      0.049       0.005       3.271
model-year_77     3.0367      0.845      3.592      0.000       1.374       4.699
model-year_78     3.0020      0.802      3.741      0.000       1.424       4.580
model-year_79     4.7447      0.850      5.579      0.000       3.073       6.417
model-year_80     9.3696      0.884     10.595      0.000       7.631      11.109
model-year_81     6.7887      0.874      7.767      0.000       5.070       8.507
model-year_82     7.4486      0.856      8.699      0.000       5.765       9.132
==============================================================================
Omnibus:                       30.461   Durbin-Watson:                   1.513
Prob(Omnibus):                  0.000   Jarque-Bera (JB):               57.549
Skew:                           0.460   Prob(JB):                     3.19e-13
Kurtosis:                       4.625   Cond. No.                     7.88e+04
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 7.88e+04. This might indicate that there are
strong multicollinearity or other numerical problems.
</code></pre></div> <h3 id=feature-significance-summary>Feature Significance Summary<a class=headerlink href=#feature-significance-summary title="Permanent link">&para;</a></h3> <h4 id=statistically-significant-features>Statistically Significant Features<a class=headerlink href=#statistically-significant-features title="Permanent link">&para;</a></h4> <p>These features have p-values less than 0.05, indicating they significantly contribute to predicting MPG: - weight (negative relationship with mpg) - cylinders_4, cylinders_5, cylinders_6, cylinders_8 - model-year_76 to 82 These variables are strong predictors of fuel efficiency in the dataset.</p> <h4 id=non-significant-features>Non-Significant Features<a class=headerlink href=#non-significant-features title="Permanent link">&para;</a></h4> <p>These features have p-values greater than 0.05, suggesting they do not significantly impact the target variable in this model: - displacement - horsepower - acceleration - model-year_71 to 75</p> <p>These may be candidates for removal or further investigation (e.g., multicollinearity or interaction effects).</p> <h3 id=overall-model-significance>Overall Model Significance<a class=headerlink href=#overall-model-significance title="Permanent link">&para;</a></h3> <ul> <li>R-squared = 0.864: The model explains 86.4% of the variance in MPG, which indicates a strong fit.</li> <li>F-statistic = 119, with a p-value &lt; 0.05: This means the overall regression model is highly statistically significant, and at least one predictor variable is meaningfully related to the target.</li> </ul> <h2 id=assumptions-of-linear-regression>Assumptions of Linear Regression<a class=headerlink href=#assumptions-of-linear-regression title="Permanent link">&para;</a></h2> <ol> <li><strong>Linearity</strong>: The relationship between predictors and response is linear.</li> <li><strong>Independence</strong>: Observations are independent.</li> <li><strong>Homoscedasticity</strong>: Constant variance of residuals.</li> <li><strong>Normality of residuals</strong>: Residuals are normally distributed.</li> <li><strong>No autocorrelation</strong>: Residuals are not autocorrelated. </li> <li><strong>No multicollinearity</strong>: Predictors are not highly correlated.</li> </ol> <h2 id=testing-residuals>Testing residuals<a class=headerlink href=#testing-residuals title="Permanent link">&para;</a></h2> <p>For a linear regression, we have the following assumptions on the residuals (errors): 1. Randomly scattered (no patterns): indicates linearity and homoscedasticity. 2. Normally distributed: checked using a Q-Q plot. 3. Mean of zero: ensures unbiased predictions. 4. No autocorrelation of residuals: ensures no lag or lead patterns in the data (using Durbin Watson test)</p> <p>We use residual plots to visually inspect these assumptions. Violations may suggest model misspecification or the need for transformation.</p> <div class=highlight><pre><span></span><code><span class=n>residuals</span> <span class=o>=</span> <span class=n>y_test</span> <span class=o>-</span> <span class=n>y_pred</span>
<span class=n>sns</span><span class=o>.</span><span class=n>histplot</span><span class=p>(</span><span class=n>residuals</span><span class=p>,</span> <span class=n>kde</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s2>&quot;Residuals Distribution&quot;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>

<span class=n>stats</span><span class=o>.</span><span class=n>probplot</span><span class=p>(</span><span class=n>residuals</span><span class=p>,</span> <span class=n>dist</span><span class=o>=</span><span class=s2>&quot;norm&quot;</span><span class=p>,</span> <span class=n>plot</span><span class=o>=</span><span class=n>plt</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s2>&quot;Q-Q Plot of Residuals&quot;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>

<span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>y_pred</span><span class=p>,</span> <span class=n>residuals</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>axhline</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=s1>&#39;red&#39;</span><span class=p>,</span> <span class=n>linestyle</span><span class=o>=</span><span class=s1>&#39;--&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s2>&quot;Predicted MPG&quot;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s2>&quot;Residuals&quot;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s2>&quot;Residuals vs Predicted Values&quot;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</code></pre></div> <p><img alt=png src=output_18_0.png></p> <p><img alt=png src=output_18_1.png></p> <p><img alt=png src=output_18_2.png></p> <p><strong>Results</strong> 1. Residuals vs. Predicted Values Plot - We observe residuals are scattered around the horizontal line at zero. - This suggests that the model captures the linear relationship reasonably well. - No clear pattern is visible, which supports the assumption of linearity and homoscedasticity (constant variance of residuals). - However, if there’s any slight funnel shape or curvature, it might hint at mild heteroscedasticity or non-linearity.</p> <ol> <li>Q-Q Plot of Residuals</li> <li>We observe most points lie close to the red reference line.</li> <li>This indicates that the residuals are approximately normally distributed, which satisfies one of the key assumptions of linear regression.</li> <li> <p>Minor deviations at the tails suggest a few outliers or slight skewness.</p> </li> <li> <p>Histogram of Residuals</p> </li> <li>We observe the distribution is roughly symmetric but slightly right-skewed.</li> <li>Most residuals are centered around zero, which is good. The slight skew suggests a few larger positive errors, but overall, the distribution is close to normal.</li> </ol> <p><strong>Durbin-Watson statistic</strong></p> <p>The Durbin-Watson (DW) statistic tests for autocorrelation in residuals: - Value ≈ 2 → no autocorrelation - Value &lt; 2 → positive autocorrelation - Value &gt; 2 → negative autocorrelation </p> <p>DW test interpretation is limited for cross‑sectional data without a natural sequence, the test assumes a meaningful order within the data, so it might not be applicable for <em>mpg</em> dataset.</p> <div class=highlight><pre><span></span><code><span class=n>dw_stat</span> <span class=o>=</span> <span class=n>durbin_watson</span><span class=p>(</span><span class=n>residuals</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Durbin-Watson statistic:&#39;</span><span class=p>,</span> <span class=n>dw_stat</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>Durbin-Watson statistic: 2.2787422579479517
</code></pre></div> <p><strong>Conclusion</strong></p> <p>The residual diagnostics suggest that the model assumptions are mostly satisfied: - Linearity: Supported by the residuals vs. predicted plot. - Homoscedasticity: No strong evidence of non-constant variance. - Normality of residuals: Mostly satisfied with minor deviations. - Autocorrelation: No autocorrelation within the data</p> <h2 id=multicollinearity>Multicollinearity<a class=headerlink href=#multicollinearity title="Permanent link">&para;</a></h2> <p>One of the assumptions of linear regression is "no perfect multicollinearity": Linear regression assumes that no independent variable is a perfect linear function of another. VIF helps quantify and diagnose this issue. VIF measures how much the variance of a regression coefficient is inflated due to multicollinearity. Multicollinearity is when two or more independent variables are highly correlated. High multicollinearity can make coefficient estimates unstable and difficult to interpret.</p> <p>Multicollinearity violates one of the key assumptions of linear regression: that the independent variables are not highly correlated. It can lead to: - Inflated standard errors - Unreliable p-values - Difficulty in identifying the true effect of each predictor</p> <p>When multicollinearity is high, coefficients (<span class=arithmatex>\(\beta\)</span> values) and p‑values become unstable, even if predictions can remain reasonably accurate. The predicted results are not affected by multicollinearity.</p> <p>For each predictor <span class=arithmatex>\(X_i\)</span>, VIF is calculated as: $$ \text{VIF}_i = \frac{1}{1 - R_i^2} $$ Where <span class=arithmatex>\(R_i^2\)</span> is the R-squared value from regressing <span class=arithmatex>\(X_i\)</span> on all other predictors.</p> <p><strong>Interpreting VIF Values</strong></p> <table> <thead> <tr> <th><strong>VIF Value</strong></th> <th><strong>Interpretation</strong></th> </tr> </thead> <tbody> <tr> <td>1</td> <td>No multicollinearity</td> </tr> <tr> <td>1 – 5</td> <td>Moderate correlation (usually acceptable)</td> </tr> <tr> <td>&gt; 5</td> <td>High correlation (potential concern)</td> </tr> <tr> <td>&gt; 10</td> <td>Serious multicollinearity (consider action)</td> </tr> </tbody> </table> <div class=highlight><pre><span></span><code><span class=n>vif_data</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>()</span>
<span class=n>X_vif</span> <span class=o>=</span> <span class=n>add_constant</span><span class=p>(</span><span class=n>pd</span><span class=o>.</span><span class=n>get_dummies</span><span class=p>(</span><span class=n>df</span><span class=o>.</span><span class=n>drop</span><span class=p>(</span><span class=n>columns</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;mpg&#39;</span><span class=p>]),</span> <span class=n>drop_first</span><span class=o>=</span><span class=kc>True</span><span class=p>))</span>
<span class=n>vif_data</span><span class=p>[</span><span class=s1>&#39;feature&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>X_vif</span><span class=o>.</span><span class=n>columns</span>
<span class=n>vif_data</span><span class=p>[</span><span class=s1>&#39;VIF&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=p>[</span><span class=n>variance_inflation_factor</span><span class=p>(</span><span class=n>X_vif</span><span class=p>,</span> <span class=n>i</span><span class=p>)</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>X_vif</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>])]</span>
<span class=nb>print</span><span class=p>(</span><span class=n>vif_data</span><span class=o>.</span><span class=n>sort_values</span><span class=p>(</span><span class=s2>&quot;VIF&quot;</span><span class=p>,</span> <span class=n>ascending</span><span class=o>=</span><span class=kc>False</span><span class=p>))</span>
</code></pre></div> <div class=highlight><pre><span></span><code>          feature         VIF
0           const  259.596012
8     cylinders_8   35.914435
5     cylinders_4   27.913953
7     cylinders_6   23.177861
1    displacement   21.175481
3          weight   12.802439
2      horsepower   11.594109
4    acceleration    2.734647
14  model-year_76    2.446511
16  model-year_78    2.404265
18  model-year_80    2.398209
20  model-year_82    2.390149
13  model-year_75    2.368613
19  model-year_81    2.342316
11  model-year_73    2.298801
17  model-year_79    2.217575
12  model-year_74    2.159981
15  model-year_77    2.122018
10  model-year_72    2.051657
9   model-year_71    2.043253
6     cylinders_5    1.904919
</code></pre></div> <p>We can see that multiple features are highly correlated such as <em>cylinders</em>, <em>displacement</em>, <em>weight</em> and <em>horsepower</em>.</p> <h2 id=outlier-detection>Outlier detection<a class=headerlink href=#outlier-detection title="Permanent link">&para;</a></h2> <p>Outliers can distort model performance. We use z-scores to detect them <span class=arithmatex>\(z = \frac{(x - \mu)}{\sigma}\)</span><br> A z-score &gt; 3 or &lt; -3 typically indicates an outlier. Identifying and handling outliers ensures the model is not overly influenced by extreme values.</p> <div class=highlight><pre><span></span><code><span class=n>continuous</span> <span class=o>=</span> <span class=p>[</span><span class=s1>&#39;displacement&#39;</span><span class=p>,</span><span class=s1>&#39;horsepower&#39;</span><span class=p>,</span><span class=s1>&#39;weight&#39;</span><span class=p>,</span><span class=s1>&#39;acceleration&#39;</span><span class=p>]</span>
<span class=n>numeric_df</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=n>continuous</span><span class=p>]</span>
<span class=n>z_scores</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>abs</span><span class=p>(</span><span class=n>stats</span><span class=o>.</span><span class=n>zscore</span><span class=p>(</span><span class=n>numeric_df</span><span class=p>))</span>
<span class=n>outliers</span> <span class=o>=</span> <span class=p>(</span><span class=n>z_scores</span> <span class=o>&gt;</span> <span class=mi>3</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
<span class=n>num_outliers</span> <span class=o>=</span> <span class=p>(</span><span class=n>outliers</span> <span class=o>&gt;</span> <span class=mi>0</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span>
<span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Number of outliers detected using Z-score method:&#39;</span><span class=p>,</span> <span class=n>num_outliers</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>Number of outliers detected using Z-score method: 7
</code></pre></div> <h2 id=cross-validation-accuracy-scores>Cross-validation accuracy scores<a class=headerlink href=#cross-validation-accuracy-scores title="Permanent link">&para;</a></h2> <p>Cross-validation splits the data into multiple folds to train and test the model on different subsets. This gives a more robust estimate of model performance. We typically use k-fold cross-validation (e.g., k=5 or 10) to reduce variance and avoid overfitting.</p> <div class=highlight><pre><span></span><code><span class=n>cv</span> <span class=o>=</span> <span class=n>KFold</span><span class=p>(</span><span class=n>n_splits</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>shuffle</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
<span class=n>cv_scores</span> <span class=o>=</span> <span class=n>cross_val_score</span><span class=p>(</span><span class=n>lr</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>cv</span><span class=o>=</span><span class=n>cv</span><span class=p>,</span> <span class=n>scoring</span><span class=o>=</span><span class=s1>&#39;neg_root_mean_squared_error&#39;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=s2>&quot;CV RMSE (mean ± std):&quot;</span><span class=p>,</span> <span class=o>-</span><span class=n>cv_scores</span><span class=o>.</span><span class=n>mean</span><span class=p>(),</span> <span class=s2>&quot;±&quot;</span><span class=p>,</span> <span class=n>cv_scores</span><span class=o>.</span><span class=n>std</span><span class=p>())</span>
</code></pre></div> <div class=highlight><pre><span></span><code>CV RMSE (mean ± std): 3.094602349591984 ± 0.5105627986552046
</code></pre></div> <p>We can see that the cross validations scores is varying and this means that the model is not robust enough on unseen data. </p> <h2 id=building-a-better-model>Building a better model<a class=headerlink href=#building-a-better-model title="Permanent link">&para;</a></h2> <p>We improve the model through a multi‑step approach: addressing multicollinearity, feature engineering, and regularization with cross‑validated hyperparameters:<br> - Addressing multicollinearity using techniques like removing high-VIF features, combining correlated variables, or applying dimensionality reduction. - Enhance the model with feature engineering: create interaction terms, polynomial features, or log transformations based on bivariate visualizations. - To improve generalization and reduce overfitting, apply regularization techniques such as Lasso (for feature selection) and Ridge (for coefficient shrinkage), and tune their hyperparameters using Grid Search. - Validate the model using k-fold cross-validation to ensure stability and robustness across different data splits.</p> <h3 id=visualizations>Visualizations<a class=headerlink href=#visualizations title="Permanent link">&para;</a></h3> <div class=highlight><pre><span></span><code><span class=c1># Importing the necessary libraries</span>
<span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
<span class=kn>import</span> <span class=nn>seaborn</span> <span class=k>as</span> <span class=nn>sns</span>
<span class=o>%</span><span class=n>matplotlib</span> <span class=n>inline</span>

<span class=kn>from</span> <span class=nn>ml_plottings</span> <span class=kn>import</span> <span class=n>regression_bivariate_analysis</span><span class=p>,</span> <span class=n>plot_correlation_matrix</span>
</code></pre></div> <div class=highlight><pre><span></span><code><span class=n>df</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>read_csv</span><span class=p>(</span><span class=s1>&#39;../Data/auto-mpg.csv&#39;</span><span class=p>)</span>
<span class=n>df</span><span class=p>[</span><span class=s1>&#39;cylinders&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>cylinders</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=nb>str</span><span class=p>)</span>
<span class=n>df</span><span class=p>[</span><span class=s1>&#39;model-year&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=s1>&#39;model-year&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=nb>str</span><span class=p>)</span>
<span class=n>df</span><span class=o>.</span><span class=n>dropna</span><span class=p>(</span><span class=n>inplace</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
<span class=n>regression_bivariate_analysis</span><span class=p>(</span><span class=n>df</span><span class=p>,</span> <span class=s1>&#39;mpg&#39;</span><span class=p>)</span>
</code></pre></div> <p><img alt=png src=output_29_0.png></p> <p><img alt=png src=output_29_1.png></p> <p><img alt=png src=output_29_2.png></p> <p><img alt=png src=output_29_3.png></p> <p><img alt=png src=output_29_4.png></p> <p><img alt=png src=output_29_5.png></p> <p><strong>Observations</strong> 1. Displacement and horsepower show a non-linear relationship with mpg, indicating that linear regression may not capture their effects accurately without transformation. 2. MPG varies significantly by cylinder count — vehicles with 4 cylinders have the highest average mpg, followed by those with 5 cylinders, suggesting a strong categorical influence. 3. Model year has a noticeable impact on mpg, with vehicles from 1977 onwards showing a higher average and wider range of mpg values, likely due to improvements in fuel efficiency over time. </p> <h3 id=correlation-matrix>Correlation matrix<a class=headerlink href=#correlation-matrix title="Permanent link">&para;</a></h3> <p>A correlation matrix is a table that shows the pairwise correlation coefficients between features in a dataset. It helps identify linear relationships between variables, where values close to +1 or -1 indicate strong positive or negative correlations, respectively. This is used to detect multicollinearity.</p> <div class=highlight><pre><span></span><code><span class=n>col_for_corr</span> <span class=o>=</span> <span class=p>[</span><span class=s1>&#39;cylinders&#39;</span><span class=p>,</span> <span class=s1>&#39;displacement&#39;</span><span class=p>,</span> <span class=s1>&#39;horsepower&#39;</span><span class=p>,</span> <span class=s1>&#39;weight&#39;</span><span class=p>,</span>
       <span class=s1>&#39;acceleration&#39;</span><span class=p>,</span> <span class=s1>&#39;model-year&#39;</span><span class=p>]</span>

<span class=n>plot_correlation_matrix</span><span class=p>(</span><span class=n>df</span><span class=p>,</span> <span class=n>col_for_corr</span><span class=p>)</span>
</code></pre></div> <p><img alt=png src=output_31_0.png></p> <p>From the correlation matrix, we observe that displacement, horsepower, cylinders_8, and weight are strongly positively correlated with each other, indicating that these features tend to increase together. In contrast, cylinders_4 and acceleration show negative correlations with this group, suggesting that vehicles with fewer cylinders and higher acceleration typically have lower displacement and weight(they are more fuel-efficient). These relationships are further confirmed by the high VIF scores for the positively correlated features, all exceeding 10, which signals severe multicollinearity and potential redundancy in the model.</p> <h3 id=feature-engineering>Feature engineering<a class=headerlink href=#feature-engineering title="Permanent link">&para;</a></h3> <p>Feature engineering is done to address the non-linear relationships correlations and improve model performance: 1. The patterns suggest a hyperbolic (1/x) or logarithmic relationship. Therefore, we create <span class=arithmatex>\(1/horsepower\)</span>, <span class=arithmatex>\(1/weight\)</span> and <span class=arithmatex>\(log(\text{displacement})\)</span> 2. From domain knowledge, horsepower and weight jointly influence fuel efficiency. To capture this interaction, we create <span class=arithmatex>\(\frac{1}{(\text{horsepower} \times \text{weight})}\)</span>. This also helps in reducing VIF (multi-correlation) as we are combining highly correlated features</p> <div class=highlight><pre><span></span><code><span class=c1># Creating 1/var</span>
<span class=n>df</span><span class=p>[</span><span class=s1>&#39;1_hp&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=mi>1</span><span class=o>/</span><span class=n>df</span><span class=p>[</span><span class=s1>&#39;horsepower&#39;</span><span class=p>]</span>
<span class=n>df</span><span class=p>[</span><span class=s1>&#39;1_weight&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=mi>1</span><span class=o>/</span><span class=n>df</span><span class=p>[</span><span class=s1>&#39;weight&#39;</span><span class=p>]</span>
<span class=n>df</span><span class=p>[</span><span class=s1>&#39;1_disp&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=mi>1</span><span class=o>/</span><span class=n>df</span><span class=p>[</span><span class=s1>&#39;displacement&#39;</span><span class=p>]</span>

<span class=c1># Creating log(var)</span>
<span class=n>df</span><span class=p>[</span><span class=s1>&#39;log_disp&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=n>df</span><span class=o>.</span><span class=n>displacement</span><span class=p>)</span>
<span class=n>df</span><span class=p>[</span><span class=s1>&#39;log_hp&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=n>df</span><span class=o>.</span><span class=n>horsepower</span><span class=p>)</span>
<span class=n>df</span><span class=p>[</span><span class=s1>&#39;log_weight&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=n>df</span><span class=o>.</span><span class=n>weight</span><span class=p>)</span>

<span class=c1># Creating domain knowledge vars</span>
<span class=n>df</span><span class=p>[</span><span class=s1>&#39;1_hp_times_weight&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=mi>1</span><span class=o>/</span><span class=p>(</span><span class=n>df</span><span class=p>[</span><span class=s1>&#39;horsepower&#39;</span><span class=p>]</span><span class=o>*</span><span class=n>df</span><span class=p>[</span><span class=s1>&#39;weight&#39;</span><span class=p>])</span>

<span class=n>regression_bivariate_analysis</span><span class=p>(</span><span class=n>df</span><span class=p>[[</span><span class=s1>&#39;1_hp&#39;</span><span class=p>,</span> <span class=s1>&#39;1_weight&#39;</span><span class=p>,</span> <span class=s1>&#39;1_disp&#39;</span><span class=p>,</span> <span class=s1>&#39;1_hp_times_weight&#39;</span><span class=p>,</span> <span class=s1>&#39;log_hp&#39;</span><span class=p>,</span><span class=s1>&#39;log_weight&#39;</span><span class=p>,</span> <span class=s1>&#39;log_disp&#39;</span><span class=p>,</span> <span class=s1>&#39;mpg&#39;</span><span class=p>]],</span> <span class=s1>&#39;mpg&#39;</span><span class=p>)</span>
</code></pre></div> <p><img alt=png src=output_33_0.png></p> <p><img alt=png src=output_33_1.png></p> <p><img alt=png src=output_33_2.png></p> <p><img alt=png src=output_33_3.png></p> <p><img alt=png src=output_33_4.png></p> <p><img alt=png src=output_33_5.png></p> <p><img alt=png src=output_33_6.png></p> <p>We can see that most of the newly created variables have a linear relationship.<br> Instead of treating each <em>model year</em> as a separate category, we can group them into three meaningful segments based on mpg trends: 1. Part 1: 1970–1975 - Lower average mpg and narrower range - Represents older, less fuel-efficient vehicles 2. Part 2: 1976–1979 - Moderate increase in mpg - Transitional period with improving fuel standards 3. Part 3: 1980–1982 - Significant jump in average mpg and wider range - Reflects newer, more efficient models</p> <p>This can be due to regulatory changes or fuel efficiency improvements in these eras.</p> <div class=highlight><pre><span></span><code><span class=n>df</span><span class=p>[</span><span class=s1>&#39;model_year_int&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=s1>&#39;model-year&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=nb>int</span><span class=p>)</span>
<span class=n>bins</span> <span class=o>=</span> <span class=p>[</span><span class=mi>1969</span><span class=p>,</span> <span class=mi>1975</span><span class=p>,</span> <span class=mi>1979</span><span class=p>,</span> <span class=mi>1982</span><span class=p>]</span>
<span class=n>labels</span> <span class=o>=</span> <span class=p>[</span><span class=s1>&#39;1970–1975&#39;</span><span class=p>,</span><span class=s1>&#39;1976–1979&#39;</span><span class=p>,</span><span class=s1>&#39;1980–1982&#39;</span><span class=p>]</span>
<span class=n>df</span><span class=p>[</span><span class=s1>&#39;model_year_group&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>cut</span><span class=p>(</span><span class=mi>1900</span><span class=o>+</span><span class=n>df</span><span class=p>[</span><span class=s1>&#39;model_year_int&#39;</span><span class=p>],</span> <span class=n>bins</span><span class=o>=</span><span class=p>[</span><span class=mi>1969</span><span class=p>,</span><span class=mi>1975</span><span class=p>,</span><span class=mi>1979</span><span class=p>,</span><span class=mi>1982</span><span class=p>],</span> <span class=n>labels</span><span class=o>=</span><span class=n>labels</span><span class=p>)</span>

<span class=n>regression_bivariate_analysis</span><span class=p>(</span><span class=n>df</span><span class=p>[[</span><span class=s1>&#39;model_year_group&#39;</span><span class=p>,</span> <span class=s1>&#39;mpg&#39;</span><span class=p>]],</span> <span class=s1>&#39;mpg&#39;</span><span class=p>)</span>
</code></pre></div> <p><img alt=png src=output_35_0.png></p> <p>We can see that there is a significant difference between the groups.</p> <h2 id=feature-selection>Feature selection<a class=headerlink href=#feature-selection title="Permanent link">&para;</a></h2> <p>Since we have several variables that are highly correlated (as seen in the correlation matrix and VIF scores), not significant (from the p-values in statistical summary) and multiple new variables created, it's important to apply feature selection to reduce redundancy, improve model interpretability, and prevent overfitting. Two effective techniques are: 1. Forward and Backward Selection: These are stepwise selection methods. These methods help identify a subset of features that contribute most to predicting mpg 2. Regularization (Lasso and Ridge Regression): Regularization stabilizes estimates under multicollinearity and improves generalization. Ridge (L2) shrinks coefficients; Lasso (L1) can set some to zero (feature selection). We select the regularization strength via cross‑validated grid search in a leakage‑safe pipeline. </p> <h3 id=forward-and-backward-selection>Forward and backward selection<a class=headerlink href=#forward-and-backward-selection title="Permanent link">&para;</a></h3> <p>These are feature selection techniques: - Forward Selection: Start with no features, add one at a time based on performance improvement. - Backward Elimination: Start with all features, remove the least significant one iteratively. </p> <p>These methods help in building a simpler, more interpretable model by retaining only the most relevant predictors.</p> <div class=highlight><pre><span></span><code><span class=n>df_fb_selection</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>get_dummies</span><span class=p>(</span><span class=n>df</span><span class=p>,</span> <span class=n>columns</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;model_year_group&#39;</span><span class=p>,</span> <span class=s1>&#39;cylinders&#39;</span><span class=p>],</span> <span class=n>drop_first</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
<span class=n>X_fb_selection</span> <span class=o>=</span> <span class=n>df_fb_selection</span><span class=o>.</span><span class=n>drop</span><span class=p>([</span><span class=s1>&#39;mpg&#39;</span><span class=p>,</span> <span class=s1>&#39;model-year&#39;</span><span class=p>],</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>inplace</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
<span class=n>y_fb_selection</span> <span class=o>=</span> <span class=n>df_fb_selection</span><span class=o>.</span><span class=n>mpg</span>

<span class=n>cv5</span> <span class=o>=</span> <span class=n>KFold</span><span class=p>(</span><span class=n>n_splits</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>shuffle</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
<span class=n>sfs_forward</span> <span class=o>=</span> <span class=n>SequentialFeatureSelector</span><span class=p>(</span><span class=n>LinearRegression</span><span class=p>(),</span> <span class=n>n_features_to_select</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>direction</span><span class=o>=</span><span class=s1>&#39;forward&#39;</span><span class=p>,</span> <span class=n>cv</span><span class=o>=</span><span class=n>cv5</span><span class=p>)</span>
<span class=n>sfs_forward</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_fb_selection</span><span class=p>,</span> <span class=n>y_fb_selection</span><span class=p>)</span>

<span class=n>sfs_backward</span> <span class=o>=</span> <span class=n>SequentialFeatureSelector</span><span class=p>(</span><span class=n>LinearRegression</span><span class=p>(),</span> <span class=n>n_features_to_select</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>direction</span><span class=o>=</span><span class=s1>&#39;backward&#39;</span><span class=p>,</span> <span class=n>cv</span><span class=o>=</span><span class=n>cv5</span><span class=p>)</span>
<span class=n>sfs_backward</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_fb_selection</span><span class=p>,</span> <span class=n>y_fb_selection</span><span class=p>)</span>

<span class=n>forward_features</span> <span class=o>=</span> <span class=n>X_fb_selection</span><span class=o>.</span><span class=n>columns</span><span class=p>[</span><span class=n>sfs_forward</span><span class=o>.</span><span class=n>get_support</span><span class=p>()]</span><span class=o>.</span><span class=n>tolist</span><span class=p>()</span>
<span class=n>backward_features</span> <span class=o>=</span> <span class=n>X_fb_selection</span><span class=o>.</span><span class=n>columns</span><span class=p>[</span><span class=n>sfs_backward</span><span class=o>.</span><span class=n>get_support</span><span class=p>()]</span><span class=o>.</span><span class=n>tolist</span><span class=p>()</span>
<span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Forward selected features:&#39;</span><span class=p>,</span> <span class=n>forward_features</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Backward selected features:&#39;</span><span class=p>,</span> <span class=n>backward_features</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>Forward selected features: [&#39;weight&#39;, &#39;1_hp&#39;, &#39;log_weight&#39;, &#39;model_year_int&#39;, &#39;model_year_group_1980–1982&#39;]
Backward selected features: [&#39;1_hp&#39;, &#39;1_weight&#39;, &#39;model_year_int&#39;, &#39;model_year_group_1980–1982&#39;, &#39;cylinders_4&#39;]
</code></pre></div> <h3 id=lasso-and-ridge-regularization>Lasso and Ridge Regularization<a class=headerlink href=#lasso-and-ridge-regularization title="Permanent link">&para;</a></h3> <p>Regularization techniques shrink the magnitude of coefficients reducing multicollinearity.<br> - Lasso Regression adds an L1 penalty, which can shrink some coefficients to zero, effectively performing feature selection. L1 penalty adds a penalty equal to the absolute value of the coefficients. <span class=arithmatex>\(\text{Loss} = \text{RSS} + \alpha \sum |\beta_i|\)</span> - Ridge Regression adds an L2 penalty, which reduces the magnitude of coefficients but retains all features.L2 penalty adds a penalty equal to the square of the coefficients <span class=arithmatex>\(\text{Loss} = \text{RSS} + \alpha \sum \beta_i^2\)</span></p> <p>Because the penalty is on the magnitude of the <span class=arithmatex>\(\beta\)</span> values, the scale of the variables become important. First we need to bring all the variables to the same scale before applying Lasso and Ridge regularizations.</p> <div class=highlight><pre><span></span><code><span class=kn>from</span> <span class=nn>sklearn.pipeline</span> <span class=kn>import</span> <span class=n>Pipeline</span>
<span class=kn>from</span> <span class=nn>sklearn.compose</span> <span class=kn>import</span> <span class=n>ColumnTransformer</span>
<span class=kn>from</span> <span class=nn>sklearn.preprocessing</span> <span class=kn>import</span> <span class=n>MinMaxScaler</span><span class=p>,</span> <span class=n>OneHotEncoder</span>
<span class=kn>from</span> <span class=nn>sklearn.linear_model</span> <span class=kn>import</span> <span class=n>Lasso</span><span class=p>,</span> <span class=n>Ridge</span>
<span class=kn>from</span> <span class=nn>sklearn.model_selection</span> <span class=kn>import</span> <span class=n>train_test_split</span>

<span class=c1># list of features</span>
<span class=n>num_features</span> <span class=o>=</span> <span class=p>[</span><span class=s1>&#39;displacement&#39;</span><span class=p>,</span><span class=s1>&#39;horsepower&#39;</span><span class=p>,</span><span class=s1>&#39;weight&#39;</span><span class=p>,</span><span class=s1>&#39;acceleration&#39;</span><span class=p>,</span>
                <span class=s1>&#39;1_hp&#39;</span><span class=p>,</span><span class=s1>&#39;1_weight&#39;</span><span class=p>,</span><span class=s1>&#39;1_disp&#39;</span><span class=p>,</span><span class=s1>&#39;log_disp&#39;</span><span class=p>,</span><span class=s1>&#39;log_hp&#39;</span><span class=p>,</span><span class=s1>&#39;log_weight&#39;</span><span class=p>,</span>
                <span class=s1>&#39;1_hp_times_weight&#39;</span><span class=p>]</span>
<span class=n>cat_features</span> <span class=o>=</span> <span class=p>[</span><span class=s1>&#39;model_year_group&#39;</span><span class=p>,</span><span class=s1>&#39;cylinders&#39;</span><span class=p>]</span>

<span class=c1># Scaling and one hot encoding</span>
<span class=n>preprocess</span> <span class=o>=</span> <span class=n>ColumnTransformer</span><span class=p>([</span>
    <span class=p>(</span><span class=s1>&#39;num&#39;</span><span class=p>,</span> <span class=n>MinMaxScaler</span><span class=p>(),</span> <span class=n>num_features</span><span class=p>),</span>
    <span class=p>(</span><span class=s1>&#39;cat&#39;</span><span class=p>,</span> <span class=n>OneHotEncoder</span><span class=p>(</span><span class=n>handle_unknown</span><span class=o>=</span><span class=s1>&#39;ignore&#39;</span><span class=p>,</span> <span class=n>drop</span><span class=o>=</span><span class=s1>&#39;first&#39;</span><span class=p>),</span> <span class=n>cat_features</span><span class=p>)</span>
<span class=p>])</span>

<span class=c1># Test train split</span>
<span class=n>X_lasso</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=n>num_features</span> <span class=o>+</span> <span class=n>cat_features</span><span class=p>]</span>
<span class=n>y_lasso</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=s1>&#39;mpg&#39;</span><span class=p>]</span>
<span class=n>X_train</span><span class=p>,</span> <span class=n>X_test</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>y_test</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span><span class=n>X_lasso</span><span class=p>,</span> <span class=n>y_lasso</span><span class=p>,</span> <span class=n>test_size</span><span class=o>=</span><span class=mf>0.2</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>

<span class=c1># Creating pipelines</span>
<span class=n>lasso_pipe</span> <span class=o>=</span> <span class=n>Pipeline</span><span class=p>([(</span><span class=s1>&#39;preprocess&#39;</span><span class=p>,</span> <span class=n>preprocess</span><span class=p>),</span> <span class=p>(</span><span class=s1>&#39;reg&#39;</span><span class=p>,</span> <span class=n>Lasso</span><span class=p>(</span><span class=n>alpha</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span> <span class=n>max_iter</span><span class=o>=</span><span class=mi>10000</span><span class=p>))])</span>
<span class=n>ridge_pipe</span> <span class=o>=</span> <span class=n>Pipeline</span><span class=p>([(</span><span class=s1>&#39;preprocess&#39;</span><span class=p>,</span> <span class=n>preprocess</span><span class=p>),</span> <span class=p>(</span><span class=s1>&#39;reg&#39;</span><span class=p>,</span> <span class=n>Ridge</span><span class=p>(</span><span class=n>alpha</span><span class=o>=</span><span class=mf>1.0</span><span class=p>))])</span>

<span class=c1># Fitting the models</span>
<span class=n>lasso_pipe</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
<span class=n>ridge_pipe</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>


<span class=c1># Create DataFrame for coefficients</span>
<span class=n>ohe</span> <span class=o>=</span> <span class=n>lasso_pipe</span><span class=o>.</span><span class=n>named_steps</span><span class=p>[</span><span class=s1>&#39;preprocess&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>transformers_</span><span class=p>[</span><span class=mi>1</span><span class=p>][</span><span class=mi>1</span><span class=p>]</span>
<span class=n>cat_names</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=n>ohe</span><span class=o>.</span><span class=n>get_feature_names_out</span><span class=p>(</span><span class=n>cat_features</span><span class=p>))</span>
<span class=n>feat_names</span> <span class=o>=</span> <span class=n>num_features</span> <span class=o>+</span> <span class=n>cat_names</span>

<span class=n>coef_df</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>({</span>
    <span class=s1>&#39;Feature&#39;</span><span class=p>:</span> <span class=n>feat_names</span><span class=p>,</span>
    <span class=s1>&#39;Lasso Coefficient&#39;</span><span class=p>:</span> <span class=n>lasso_pipe</span><span class=o>.</span><span class=n>named_steps</span><span class=p>[</span><span class=s1>&#39;reg&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>coef_</span><span class=p>,</span>
    <span class=s1>&#39;Ridge Coefficient&#39;</span><span class=p>:</span> <span class=n>ridge_pipe</span><span class=o>.</span><span class=n>named_steps</span><span class=p>[</span><span class=s1>&#39;reg&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>coef_</span>
<span class=p>})</span>

<span class=c1># Plot</span>
<span class=n>coef_df</span><span class=o>.</span><span class=n>set_index</span><span class=p>(</span><span class=s1>&#39;Feature&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>kind</span><span class=o>=</span><span class=s1>&#39;bar&#39;</span><span class=p>,</span> <span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>6</span><span class=p>))</span>
<span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;Lasso vs Ridge Coefficients&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;Coefficient Value&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>();</span>
</code></pre></div> <p><img alt=png src=output_40_0.png></p> <p>These methods show how different approaches can be used to do feature selection.</p> <h3 id=cross-validation-and-grid-search>Cross validation and Grid search<a class=headerlink href=#cross-validation-and-grid-search title="Permanent link">&para;</a></h3> <p>We have looked at various approaches that provide different feature selections and different results. We have also observed that within each model, different parameters can be fine-tuned to get different results. We can use cross validation and grid search algorithms to identify the best parameters and models across all the combinations possible. They improve the robustness and performance of a base machine learning model. <br> - Cross-validation helps evaluate a model's generalizability by splitting the training data into multiple folds, ensuring that the model is tested on different subsets and not just a single train-test split. This reduces the risk of overfitting and gives a more reliable estimate of model performance. - GridSearchCV builds on this by systematically searching through a predefined set of hyperparameters (like alpha in regularized regressions) using cross-validation to find the combination that yields the best performance metric (e.g., R² or RMSE).</p> <p>Together, they help in selecting the most optimal model configuration, making the final model more accurate and stable when applied to unseen data.</p> <div class=highlight><pre><span></span><code><span class=kn>from</span> <span class=nn>sklearn.compose</span> <span class=kn>import</span> <span class=n>ColumnTransformer</span>
<span class=kn>from</span> <span class=nn>sklearn.preprocessing</span> <span class=kn>import</span> <span class=n>StandardScaler</span><span class=p>,</span> <span class=n>OneHotEncoder</span>
<span class=kn>from</span> <span class=nn>sklearn.pipeline</span> <span class=kn>import</span> <span class=n>Pipeline</span>
<span class=kn>from</span> <span class=nn>sklearn.linear_model</span> <span class=kn>import</span> <span class=n>LinearRegression</span><span class=p>,</span> <span class=n>Ridge</span><span class=p>,</span> <span class=n>Lasso</span><span class=p>,</span> <span class=n>ElasticNet</span>
<span class=kn>from</span> <span class=nn>sklearn.model_selection</span> <span class=kn>import</span> <span class=n>KFold</span><span class=p>,</span> <span class=n>GridSearchCV</span><span class=p>,</span> <span class=n>train_test_split</span>
<span class=kn>from</span> <span class=nn>sklearn.metrics</span> <span class=kn>import</span> <span class=n>r2_score</span><span class=p>,</span> <span class=n>mean_squared_error</span><span class=p>,</span> <span class=n>mean_absolute_error</span>


<span class=c1># Define models</span>
<span class=n>lr</span> <span class=o>=</span> <span class=n>LinearRegression</span><span class=p>()</span>
<span class=n>lasso</span> <span class=o>=</span> <span class=n>Lasso</span><span class=p>(</span><span class=n>max_iter</span><span class=o>=</span><span class=mi>10000</span><span class=p>)</span>
<span class=n>ridge</span> <span class=o>=</span> <span class=n>Ridge</span><span class=p>()</span>
<span class=n>elastic_net</span> <span class=o>=</span> <span class=n>ElasticNet</span><span class=p>(</span><span class=n>max_iter</span><span class=o>=</span><span class=mi>10000</span><span class=p>)</span>

<span class=c1># Features</span>
<span class=n>num_features</span> <span class=o>=</span> <span class=p>[</span><span class=s1>&#39;displacement&#39;</span><span class=p>,</span><span class=s1>&#39;horsepower&#39;</span><span class=p>,</span><span class=s1>&#39;weight&#39;</span><span class=p>,</span><span class=s1>&#39;acceleration&#39;</span><span class=p>,</span> <span class=s1>&#39;1_hp&#39;</span><span class=p>,</span><span class=s1>&#39;1_weight&#39;</span><span class=p>,</span><span class=s1>&#39;1_disp&#39;</span><span class=p>,</span>
                <span class=s1>&#39;log_disp&#39;</span><span class=p>,</span><span class=s1>&#39;log_hp&#39;</span><span class=p>,</span><span class=s1>&#39;log_weight&#39;</span><span class=p>,</span> <span class=s1>&#39;1_hp_times_weight&#39;</span><span class=p>]</span>
<span class=n>cat_features</span> <span class=o>=</span> <span class=p>[</span><span class=s1>&#39;model_year_group&#39;</span><span class=p>,</span><span class=s1>&#39;cylinders&#39;</span><span class=p>]</span>

<span class=c1># Ensure correct types</span>
<span class=n>df</span><span class=p>[</span><span class=s1>&#39;model_year_group&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=s1>&#39;model_year_group&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=s1>&#39;category&#39;</span><span class=p>)</span>
<span class=n>df</span><span class=p>[</span><span class=s1>&#39;cylinders&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=s1>&#39;cylinders&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=s1>&#39;category&#39;</span><span class=p>)</span>

<span class=c1># Create a pipeline</span>
<span class=n>preprocess</span> <span class=o>=</span> <span class=n>ColumnTransformer</span><span class=p>(</span>
    <span class=n>transformers</span><span class=o>=</span><span class=p>[</span>
        <span class=p>(</span><span class=s1>&#39;num&#39;</span><span class=p>,</span> <span class=n>StandardScaler</span><span class=p>(),</span> <span class=n>num_features</span><span class=p>),</span>
        <span class=p>(</span><span class=s1>&#39;cat&#39;</span><span class=p>,</span> <span class=n>OneHotEncoder</span><span class=p>(</span><span class=n>handle_unknown</span><span class=o>=</span><span class=s1>&#39;ignore&#39;</span><span class=p>,</span> <span class=n>drop</span><span class=o>=</span><span class=s1>&#39;first&#39;</span><span class=p>),</span> <span class=n>cat_features</span><span class=p>)</span>
    <span class=p>],</span>
    <span class=n>remainder</span><span class=o>=</span><span class=s1>&#39;drop&#39;</span>
<span class=p>)</span>

<span class=n>pipe</span> <span class=o>=</span> <span class=n>Pipeline</span><span class=p>([</span>
    <span class=p>(</span><span class=s1>&#39;preprocess&#39;</span><span class=p>,</span> <span class=n>preprocess</span><span class=p>),</span>
    <span class=p>(</span><span class=s1>&#39;regressor&#39;</span><span class=p>,</span> <span class=n>Lasso</span><span class=p>())</span>  <span class=c1># placeholder; will be overridden by grid</span>
<span class=p>])</span>

<span class=c1># Define hyperparameter grids</span>
<span class=n>param_lr</span> <span class=o>=</span> <span class=p>{</span><span class=s1>&#39;regressor&#39;</span><span class=p>:</span> <span class=p>[</span><span class=n>lr</span><span class=p>]}</span>
<span class=n>param_lasso</span> <span class=o>=</span> <span class=p>{</span><span class=s1>&#39;regressor&#39;</span><span class=p>:</span> <span class=p>[</span><span class=n>lasso</span><span class=p>],</span> <span class=s1>&#39;regressor__alpha&#39;</span><span class=p>:</span> <span class=p>[</span><span class=mf>0.01</span><span class=p>,</span> <span class=mf>0.1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>10</span><span class=p>]}</span>
<span class=n>param_ridge</span> <span class=o>=</span> <span class=p>{</span><span class=s1>&#39;regressor&#39;</span><span class=p>:</span> <span class=p>[</span><span class=n>ridge</span><span class=p>],</span> <span class=s1>&#39;regressor__alpha&#39;</span><span class=p>:</span> <span class=p>[</span><span class=mf>0.01</span><span class=p>,</span> <span class=mf>0.1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>10</span><span class=p>]}</span>
<span class=n>param_elastic_net</span> <span class=o>=</span> <span class=p>{</span><span class=s1>&#39;regressor&#39;</span><span class=p>:</span> <span class=p>[</span><span class=n>elastic_net</span><span class=p>],</span> <span class=s1>&#39;regressor__alpha&#39;</span><span class=p>:</span> <span class=p>[</span><span class=mf>0.01</span><span class=p>,</span> <span class=mf>0.1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>10</span><span class=p>],</span> <span class=s1>&#39;regressor__l1_ratio&#39;</span><span class=p>:[</span><span class=mf>0.25</span><span class=p>,</span> <span class=mf>0.5</span><span class=p>,</span> <span class=mf>0.75</span><span class=p>]}</span>

<span class=c1># Combine all</span>
<span class=n>param_grid</span> <span class=o>=</span> <span class=p>[</span><span class=n>param_lr</span><span class=p>,</span> <span class=n>param_lasso</span><span class=p>,</span> <span class=n>param_ridge</span><span class=p>,</span> <span class=n>param_elastic_net</span><span class=p>]</span>

<span class=c1># Train-test split</span>
<span class=n>X_train</span><span class=p>,</span> <span class=n>X_test</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>y_test</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span><span class=n>df</span><span class=p>[</span><span class=n>num_features</span><span class=o>+</span><span class=n>cat_features</span><span class=p>],</span> <span class=n>df</span><span class=o>.</span><span class=n>mpg</span><span class=p>,</span> <span class=n>test_size</span><span class=o>=</span><span class=mf>0.2</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>

<span class=c1># Grid Search CV</span>
<span class=n>grid_search</span> <span class=o>=</span> <span class=n>GridSearchCV</span><span class=p>(</span><span class=n>pipe</span><span class=p>,</span> <span class=n>param_grid</span><span class=p>,</span> <span class=n>cv</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>scoring</span><span class=o>=</span><span class=s1>&#39;neg_root_mean_squared_error&#39;</span><span class=p>)</span>
<span class=n>grid_search</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>

<span class=c1># Best model and coefficients</span>
<span class=n>best_model</span> <span class=o>=</span> <span class=n>grid_search</span><span class=o>.</span><span class=n>best_estimator_</span><span class=o>.</span><span class=n>named_steps</span><span class=p>[</span><span class=s1>&#39;regressor&#39;</span><span class=p>]</span>
<span class=n>feature_names</span> <span class=o>=</span> <span class=n>num_features</span> <span class=o>+</span> <span class=nb>list</span><span class=p>(</span><span class=n>grid_search</span><span class=o>.</span><span class=n>best_estimator_</span><span class=o>.</span><span class=n>named_steps</span><span class=p>[</span><span class=s1>&#39;preprocess&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>transformers_</span><span class=p>[</span><span class=mi>1</span><span class=p>][</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>get_feature_names_out</span><span class=p>(</span><span class=n>cat_features</span><span class=p>))</span>
<span class=n>coefficients</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>Series</span><span class=p>(</span><span class=n>best_model</span><span class=o>.</span><span class=n>coef_</span><span class=p>,</span> <span class=n>index</span><span class=o>=</span><span class=n>feature_names</span><span class=p>)</span>

<span class=c1># Output</span>
<span class=nb>print</span><span class=p>(</span><span class=s2>&quot;Best Parameters:&quot;</span><span class=p>,</span> <span class=n>grid_search</span><span class=o>.</span><span class=n>best_params_</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=s2>&quot;Best(least) RMSE Error:&quot;</span><span class=p>,</span> <span class=o>-</span><span class=n>grid_search</span><span class=o>.</span><span class=n>best_score_</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=s2>&quot;</span><span class=se>\n</span><span class=s2>Feature Coefficients (β values): (for scaled variables)&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=n>coefficients</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>Best Parameters: {&#39;regressor&#39;: Ridge(), &#39;regressor__alpha&#39;: 0.1}
Best(least) RMSE Error: 2.9181586103855244

Feature Coefficients (β values): (for scaled variables)
displacement                  0.849895
horsepower                   -0.421171
weight                        3.749970
acceleration                 -0.862817
1_hp                          9.801057
1_weight                      5.663238
1_disp                        3.234586
log_disp                      1.114903
log_hp                        2.405622
log_weight                   -4.632756
1_hp_times_weight            -8.933534
model_year_group_1976–1979    2.956526
model_year_group_1980–1982    7.097160
cylinders_4                   7.177942
cylinders_5                   8.560141
cylinders_6                   6.633527
cylinders_8                   7.123100
dtype: float64
</code></pre></div> <p>We can see that the cross-validation root mean square error reduces from 3.1 to 2.9. This indicates that linear regression—augmented with thoughtful features, preprocessing, and regularization produces a strong and interpretable MPG model on historical data, while diagnostics and CV guard against over‑confidence and reveal improvement avenues. </p> <h2 id=references>References<a class=headerlink href=#references title="Permanent link">&para;</a></h2> <p>Dataset: <a href=https://archive.ics.uci.edu/dataset/9/auto+mpg>Auto MPG Data Set</a>, UCI Machine Learning Repository (originally from StatLib / CMU; used in the 1983 ASA Exposition)</p> </article> </div> </div> <a href=# class="md-top md-icon" data-md-component=top data-md-state=hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"/></svg> Back to top </a> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../../R/logistic-regression/ class="md-footer__link md-footer__link--prev" aria-label="Previous: Logistic Regression (R)" rel=prev> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </div> <div class=md-footer__title> <div class=md-ellipsis> <span class=md-footer__direction> Previous </span> Logistic Regression (R) </div> </div> </a> <a href=../../R/CHAID/ class="md-footer__link md-footer__link--next" aria-label="Next: CHAID Decision Trees (R)" rel=next> <div class=md-footer__title> <div class=md-ellipsis> <span class=md-footer__direction> Next </span> CHAID Decision Trees (R) </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-footer-copyright> <div class=md-footer-copyright__highlight> <img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png> </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-footer-social> <a href=https://github.com/HarshaAsh target=_blank rel=noopener title=github.com class=md-footer-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> </a> <a href=https://www.linkedin.com/in/sri-harsha-achyuthuni/ target=_blank rel=noopener title=www.linkedin.com class=md-footer-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg> </a> <a href=https://www.instagram.com/harshaash_com/ target=_blank rel=noopener title=www.instagram.com class=md-footer-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><path d="M224.1 141c-63.6 0-114.9 51.3-114.9 114.9s51.3 114.9 114.9 114.9S339 319.5 339 255.9 287.7 141 224.1 141zm0 189.6c-41.1 0-74.7-33.5-74.7-74.7s33.5-74.7 74.7-74.7 74.7 33.5 74.7 74.7-33.6 74.7-74.7 74.7zm146.4-194.3c0 14.9-12 26.8-26.8 26.8-14.9 0-26.8-12-26.8-26.8s12-26.8 26.8-26.8 26.8 12 26.8 26.8zm76.1 27.2c-1.7-35.9-9.9-67.7-36.2-93.9-26.2-26.2-58-34.4-93.9-36.2-37-2.1-147.9-2.1-184.9 0-35.8 1.7-67.6 9.9-93.9 36.1s-34.4 58-36.2 93.9c-2.1 37-2.1 147.9 0 184.9 1.7 35.9 9.9 67.7 36.2 93.9s58 34.4 93.9 36.2c37 2.1 147.9 2.1 184.9 0 35.9-1.7 67.7-9.9 93.9-36.2 26.2-26.2 34.4-58 36.2-93.9 2.1-37 2.1-147.8 0-184.8zM398.8 388c-7.8 19.6-22.9 34.7-42.6 42.6-29.5 11.7-99.5 9-132.1 9s-102.7 2.6-132.1-9c-19.6-7.8-34.7-22.9-42.6-42.6-11.7-29.5-9-99.5-9-132.1s-2.6-102.7 9-132.1c7.8-19.6 22.9-34.7 42.6-42.6 29.5-11.7 99.5-9 132.1-9s102.7-2.6 132.1 9c19.6 7.8 34.7 22.9 42.6 42.6 11.7 29.5 9 99.5 9 132.1s2.7 102.7-9 132.1z"/></svg> </a> <a href=https://www.facebook.com/sri.harsha.achyuthuni/ target=_blank rel=noopener title=www.facebook.com class=md-footer-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><path d="M504 256C504 119 393 8 256 8S8 119 8 256c0 123.78 90.69 226.38 209.25 245V327.69h-63V256h63v-54.64c0-62.15 37-96.48 93.67-96.48 27.14 0 55.52 4.84 55.52 4.84v61h-31.28c-30.8 0-40.41 19.12-40.41 38.73V256h68.78l-11 71.69h-57.78V501C413.31 482.38 504 379.78 504 256z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../..", "features": ["content.code.annotate", "content.tabs.link", "header.autohide", "navigation.sections", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest"], "search": "../../assets/javascripts/workers/search.709b4209.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "version": null}</script> <script src=../../assets/javascripts/bundle.56838a2c.min.js></script> </body> </html>