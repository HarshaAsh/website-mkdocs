{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"this is a sample page","title":"Home"},{"location":"Others/Deployed%20apps/","text":"Websites and deployments \u00b6 This blog lists the ML apps and websites that I have built and are publicly accessible. Project-specific POC's \u00b6 OC And Gym Dashboard : Churn dashboard showing the effect of different decisions that the business can take. Contest Insights Dashboard : Dashboard for allocating different contests as part of a company's Rewards programme. Bid Allocation : App which allocates uploaded bids to suppliers minimising costs. RShiny Dashboards \u00b6 T-test : Demonstrating t-test using simulated data. Anova simulation : Demonstrating Anova three-class F-test. Linear Programming : Demonstrating the solution for a linear minimisation programme ( blog post ) Machine Learning : Demonstrating the concepts of experience, task and performance in ML. Python ML deployments \u00b6 Streamlit for deployment : Using Streamlit to deploy simple data science models and insights. Predicting PM25 pollution in Hyderabad using flask and SqlAlchemy : Using Flask, PostgreSQL, Github Actions and ElephantDB to make predictions using an ML model, store and extract these predictions, and display on a web app. (Blog posts 1 , 2 , 3 )","title":"Deployed apps"},{"location":"Others/Deployed%20apps/#websites-and-deployments","text":"This blog lists the ML apps and websites that I have built and are publicly accessible.","title":"Websites and deployments"},{"location":"Others/Deployed%20apps/#project-specific-pocs","text":"OC And Gym Dashboard : Churn dashboard showing the effect of different decisions that the business can take. Contest Insights Dashboard : Dashboard for allocating different contests as part of a company's Rewards programme. Bid Allocation : App which allocates uploaded bids to suppliers minimising costs.","title":"Project-specific POC's"},{"location":"Others/Deployed%20apps/#rshiny-dashboards","text":"T-test : Demonstrating t-test using simulated data. Anova simulation : Demonstrating Anova three-class F-test. Linear Programming : Demonstrating the solution for a linear minimisation programme ( blog post ) Machine Learning : Demonstrating the concepts of experience, task and performance in ML.","title":"RShiny Dashboards"},{"location":"Others/Deployed%20apps/#python-ml-deployments","text":"Streamlit for deployment : Using Streamlit to deploy simple data science models and insights. Predicting PM25 pollution in Hyderabad using flask and SqlAlchemy : Using Flask, PostgreSQL, Github Actions and ElephantDB to make predictions using an ML model, store and extract these predictions, and display on a web app. (Blog posts 1 , 2 , 3 )","title":"Python ML deployments"},{"location":"Others/Imperial%20College%20London/","text":"Imperial College London Student Blogs \u00b6 I was a student content creator at Imperial College London for 2020-21. I have published three blogs as part of the content creator at Imperial. Why you should study an online Master\u2019s in Business Analytics over a short course What to look for when choosing a part-time Master\u2019s programme Applying what I studied to my role as a Data Science Consultant Blogs as part of Data science club at Imperial (ICDSS) What I learnt from failures as a Data Science Consultant","title":"Imperial college London"},{"location":"Others/Imperial%20College%20London/#imperial-college-london-student-blogs","text":"I was a student content creator at Imperial College London for 2020-21. I have published three blogs as part of the content creator at Imperial. Why you should study an online Master\u2019s in Business Analytics over a short course What to look for when choosing a part-time Master\u2019s programme Applying what I studied to my role as a Data Science Consultant Blogs as part of Data science club at Imperial (ICDSS) What I learnt from failures as a Data Science Consultant","title":"Imperial College London Student Blogs"},{"location":"Others/Publications/","text":"Publications, whitepapers and presentations \u00b6 The following are the papers published in a journal or presented in a conference. Paper Publisher/Conference Author Status Date Subject Parametric Study of Cantelever Beams in Supersonic and Hypersonic flow IOC Physics Primary Jul 2017 Mechanical Engineering Personal analytics: Time management using Google Maps ICSADADS Conference Primary Feb 2020 Data Science Scarecrow - Intelligent Annotation platform for Engine Health Management AI-ML Systems / ACM Acknowledgement Oct 2021 Machine Learning","title":"Publications/conferences"},{"location":"Others/Publications/#publications-whitepapers-and-presentations","text":"The following are the papers published in a journal or presented in a conference. Paper Publisher/Conference Author Status Date Subject Parametric Study of Cantelever Beams in Supersonic and Hypersonic flow IOC Physics Primary Jul 2017 Mechanical Engineering Personal analytics: Time management using Google Maps ICSADADS Conference Primary Feb 2020 Data Science Scarecrow - Intelligent Annotation platform for Engine Health Management AI-ML Systems / ACM Acknowledgement Oct 2021 Machine Learning","title":"Publications, whitepapers and presentations"},{"location":"Others/Rolls%20Royce/","text":"Rolls Royce content (publicly available) \u00b6 While at Deloitte, I worked with Rolls Royce Data Labs as a data science consultant. This lists my publicly available work. Blog on Streaming Machine Learning at R2DL Library Published paper at ACM titled \"Scarecrow - Intelligent Annotation platform for Engine Health Management\"","title":"Rolls Royce"},{"location":"Others/Rolls%20Royce/#rolls-royce-content-publicly-available","text":"While at Deloitte, I worked with Rolls Royce Data Labs as a data science consultant. This lists my publicly available work. Blog on Streaming Machine Learning at R2DL Library Published paper at ACM titled \"Scarecrow - Intelligent Annotation platform for Engine Health Management\"","title":"Rolls Royce content (publicly available)"},{"location":"Others/resume/","text":"Achyuthuni Sri Harsha \u00b6 Data Scientist | Deloitte | Imperial College London |IIMB | www.harshaash.com Experience \u00b6 Deloitte \u00b6 Data science consultant | Bangalore | Jan 2021 - current As a data science consultant at Deloitte, I have deployed various data science solutions for various clients in manufacturing and supply chain domains. Key Projects: * Built a state-of-the-art streaming machine learning model for Rolls Royce data Labs (Open Source version: Scarecrow ) * Designed parts of predictive aircraft health management systems using machine learning at Rolls Royce with an impact of saving 1200 manhours * Developed an optimization engine that allocates suppliers which had an impact of 4 million dollars in a year Tesco \u00b6 Senior Business Analyst | Bangalore | April 2020 - Jan 2021 As Senior Business Analyst at Tesco Bengaluru, I am responsible in the domain of Space, Range, Display, Merchandising, and Promotions for the markets UK, Central Europe, and Ireland. I optimize store operations to ensure products are always available to the customers. I am also responsible for identifying and solving data science and analytics problems. Key Achievements: * Collaborated with business teams, display managers, and store managers to take various data-driven decisions * Led the team to identify potential problems that can be solved using data science and delivered three of them to different stakeholders * Analyzed 10+ ad-hoc requests to provide actionable insights and automated 5+ reports to facilitate better store efficiency * Created 5+ reusable codes and writeups as part of an organization level effort to enhance enterprise-level knowledge repository Mu-Sigma \u00b6 Decision Scientist | Bangalore | Oct 2017 - April 2020 As a consultant for Walmart, I have built various solutions using data science concepts. * Built and deployed end-to-end classification, regression, and optimization models for Walmart International supply chain with a cost-saving of 12 million dollars per month * Reduced out-of-stock scenarios in stores by identifying and quantifying different factors, risk analytics, forecasting, and optimizing supply chains in collaboration with technology and business partners from the USA, Mexico, and Argentina * Trained and mentored three batches of young analysts (5 in one batch) on demand forecasting and customer churn prediction to successfully onboard them. Education \u00b6 Imperial College London \u00b6 MSc in Business Analytics from Imperial College London Business School. Part-time, 2020-22 cohort Core modules: Data Structures and Algorithms, Machine Learning, Network Analytics, Statistics and Econometrics An active member of the Data Science Club. Student content creator Indian Institute of Management Bangalore \u00b6 Business Analytics and Intelligence part-time on-campus executive education program. 2019-20 Project: \u201c Optimisation of R&R contests for a life insurance company using predictive and prescriptive analytics \u201d (Awarded highly commended project) Amrita Vishwa Vidyapeetham \u00b6 Bachelors in Mechanical Engineering. Bangalore campus 2013-2017 Graduated with distinction and CGPA of 8.91 /10 Treasurer of SAE India club Publications \u00b6 Personal analytics: Time management using Google Maps : ICSADADS, Feb 2020 Scarecrow - Intelligent Annotation platform for Engine Health Management : AI ML Systems Conference, Oct 2021 Parametric Study of Cantilever Plates Exposed to Supersonic and Hypersonic Flows : IOP Conference Series, June 2017 Skills \u00b6 Analytics: R, Python, CPLEX Data handling: SQL, Alteryx Visualization: Tableau, HTML, Javascript Cloud: Google Cloud Platform, Azure Awards \u00b6 Highly commended project : IIMB for the capstone project Move the dot : Deloitte Spot Award : Mu-Sigma Certifications \u00b6 Data Engineering, Big Data, and Machine Learning on GCP - Google Cloud Data Science: Machine Learning - Harvard & edX Machine Learning A-Z: Hands-On Python & R In Data Science - Udemy Machine Learning & AI: Advanced Decision Trees - LinkedIn Contact Information \u00b6 Phone no: +919019413416 Website: www.harshaash.com Mail: sri.harsha.uni@gmail.com LinkedIn profile sri-harsha-achyuthuni Github: HarshaAsh Personal Information \u00b6 Linguistic Proficiency: Telugu, English, Hindi Interests and Hobbies: Reading, swimming, designing websites. Project details \u00b6 Scarecrow: ML for Aircraft Engine Management - Deloitte (Client: Rolls Royce) \u00b6 Technologies used : Python, streaming machine learning, Data bricks Timeline : Feb 2021 - Current Publications : Presented \"Scarecrow - Intelligent Annotation platform for Engine Health Management\" in AI ML Systems conference Impact : Preventive maintenance identified with 15% less false positivity(estimated) Team : 7-10 member team consisting of data engineers and data scientists Problem Statement : Assisting subject matter experts (SME) in identifying various performance issues in an engine Built a machine learning framework that continuously learns (streaming machine learning) by observing the decisions taken by SME's on a Tableau dashboard. SME's look at data from different sensors from aircraft engines in flight to identify the engines\\parts that may need maintenance or have low performance. These models are used to reduce the amount of data that SME's should review by identifying only the less confidence predictions (active learning). Bid Allocation tool - Deloitte (Client: Rolls Royce) \u00b6 Technologies used : Python, ORTools, Linear (Integer) Optimisation Timeline : Feb 2021 - Jun 2021 Impact : Estimated savings of 12 Million Pounds Problem Statement : Allocating parts in a manufacturing plant to suppliers for changing business constraints Suppliers bid on various parts that are to be procured in a manufacturing plant. Built the data science framework for a web-based tool that allocates parts to various suppliers across time. The tool uses OR Tools (Integer programming) to run a supplier allocation problem. Different business constraints were converted into integer-based constraints and added to the optimization function. These business constraints can be modified, added, or removed using the web-based UI and the optimal solution for each scenario is computed. Optimisation of R&R contests for a life insurance company using predictive and prescriptive analytics\u2014 IIMB capstone project \u00b6 Technologies used : R, Excel, Google Colab Timeline : Dec 2019 - April 2020 Awards : Awarded \"Highly commended project\" for the BAI batch of 2019-20 at IIM Bangalore. Problem Statement : Building optimal R&R contests for agents of a large life insurance company. Forecasting sales of an agent Built a regression model which can explain 97% variation in sales. Quantified the lift generated due to different contest parameters Clustering of agents based on their capacity Identifying the factors which affect the maximum capacity of sales of an agent and clustering the agents based on them Simulation and optimization of contest parameters Simulated the cumulative sales for different contest parameters in each cluster of agents Identified the most optimal parameters based on budget and other constraints Supply chain analytics \u2014 MuSigma (Client: Walmart) \u00b6 Technologies used : R, Python, SQL, CPLEX, Google Cloud Platform Timeline : 2018 - April 2020 Problem Statement : Reducing out-of-stock scenarios in stores by identifying and quantifying the different factors, predicting the failures due to various factors, and optimizing inventory based on them. Team : Collaborated with the technology and business units of Walmart Supply chain and market POC's in the US, Canada, Mexico, Argentina, and Chile. Worked end-to-end from ideation to POC development to production Quantified savings : The potential average cost savings from a reduction in inventory and out-of-stock costs would be $12 Million per month Quantify the reasons for under-stock scenarios Quantified the reasons causing under-stock scenarios in a store utilizing hypothesis testing and statistical modeling pinpointing the two main factors among 14 with the most significant impact (fillrate and lead time). Identify the risk of a supplier not delivering an order Designed classification model (gradient boosting) predicting the risk of a supplier not delivering an order in full (fillrate) with 75% accuracy and 50+% specificity Deployed the solution on the cloud and created workflows to predict the risk daily Forecasting inbound lead time of vendors Forecasted lead time applying a tree-based ensemble regression model (random forest) with 85% (SMAPE) accuracy Deployed the solution on the cloud and created workflows to predict lead time daily Optimizing inventory at store and warehouse Optimized EOQ and reorder point using an integer programming model Formulated and validated the approach under the Senior Director of Supply chain at Walmart Personal projects \u00b6 Technologies used : R, Python, HTML, CSS, JavaScript,Github Deployed various data science applications for personal use Blogs on data science topics for college as well as professionally","title":"Resume"},{"location":"Others/resume/#achyuthuni-sri-harsha","text":"Data Scientist | Deloitte | Imperial College London |IIMB | www.harshaash.com","title":"Achyuthuni Sri Harsha"},{"location":"Others/resume/#experience","text":"","title":"Experience"},{"location":"Others/resume/#deloitte","text":"Data science consultant | Bangalore | Jan 2021 - current As a data science consultant at Deloitte, I have deployed various data science solutions for various clients in manufacturing and supply chain domains. Key Projects: * Built a state-of-the-art streaming machine learning model for Rolls Royce data Labs (Open Source version: Scarecrow ) * Designed parts of predictive aircraft health management systems using machine learning at Rolls Royce with an impact of saving 1200 manhours * Developed an optimization engine that allocates suppliers which had an impact of 4 million dollars in a year","title":"Deloitte"},{"location":"Others/resume/#tesco","text":"Senior Business Analyst | Bangalore | April 2020 - Jan 2021 As Senior Business Analyst at Tesco Bengaluru, I am responsible in the domain of Space, Range, Display, Merchandising, and Promotions for the markets UK, Central Europe, and Ireland. I optimize store operations to ensure products are always available to the customers. I am also responsible for identifying and solving data science and analytics problems. Key Achievements: * Collaborated with business teams, display managers, and store managers to take various data-driven decisions * Led the team to identify potential problems that can be solved using data science and delivered three of them to different stakeholders * Analyzed 10+ ad-hoc requests to provide actionable insights and automated 5+ reports to facilitate better store efficiency * Created 5+ reusable codes and writeups as part of an organization level effort to enhance enterprise-level knowledge repository","title":"Tesco"},{"location":"Others/resume/#mu-sigma","text":"Decision Scientist | Bangalore | Oct 2017 - April 2020 As a consultant for Walmart, I have built various solutions using data science concepts. * Built and deployed end-to-end classification, regression, and optimization models for Walmart International supply chain with a cost-saving of 12 million dollars per month * Reduced out-of-stock scenarios in stores by identifying and quantifying different factors, risk analytics, forecasting, and optimizing supply chains in collaboration with technology and business partners from the USA, Mexico, and Argentina * Trained and mentored three batches of young analysts (5 in one batch) on demand forecasting and customer churn prediction to successfully onboard them.","title":"Mu-Sigma"},{"location":"Others/resume/#education","text":"","title":"Education"},{"location":"Others/resume/#imperial-college-london","text":"MSc in Business Analytics from Imperial College London Business School. Part-time, 2020-22 cohort Core modules: Data Structures and Algorithms, Machine Learning, Network Analytics, Statistics and Econometrics An active member of the Data Science Club. Student content creator","title":"Imperial College London"},{"location":"Others/resume/#indian-institute-of-management-bangalore","text":"Business Analytics and Intelligence part-time on-campus executive education program. 2019-20 Project: \u201c Optimisation of R&R contests for a life insurance company using predictive and prescriptive analytics \u201d (Awarded highly commended project)","title":"Indian Institute of Management Bangalore"},{"location":"Others/resume/#amrita-vishwa-vidyapeetham","text":"Bachelors in Mechanical Engineering. Bangalore campus 2013-2017 Graduated with distinction and CGPA of 8.91 /10 Treasurer of SAE India club","title":"Amrita Vishwa Vidyapeetham"},{"location":"Others/resume/#publications","text":"Personal analytics: Time management using Google Maps : ICSADADS, Feb 2020 Scarecrow - Intelligent Annotation platform for Engine Health Management : AI ML Systems Conference, Oct 2021 Parametric Study of Cantilever Plates Exposed to Supersonic and Hypersonic Flows : IOP Conference Series, June 2017","title":"Publications"},{"location":"Others/resume/#skills","text":"Analytics: R, Python, CPLEX Data handling: SQL, Alteryx Visualization: Tableau, HTML, Javascript Cloud: Google Cloud Platform, Azure","title":"Skills"},{"location":"Others/resume/#awards","text":"Highly commended project : IIMB for the capstone project Move the dot : Deloitte Spot Award : Mu-Sigma","title":"Awards"},{"location":"Others/resume/#certifications","text":"Data Engineering, Big Data, and Machine Learning on GCP - Google Cloud Data Science: Machine Learning - Harvard & edX Machine Learning A-Z: Hands-On Python & R In Data Science - Udemy Machine Learning & AI: Advanced Decision Trees - LinkedIn","title":"Certifications"},{"location":"Others/resume/#contact-information","text":"Phone no: +919019413416 Website: www.harshaash.com Mail: sri.harsha.uni@gmail.com LinkedIn profile sri-harsha-achyuthuni Github: HarshaAsh","title":"Contact Information"},{"location":"Others/resume/#personal-information","text":"Linguistic Proficiency: Telugu, English, Hindi Interests and Hobbies: Reading, swimming, designing websites.","title":"Personal Information"},{"location":"Others/resume/#project-details","text":"","title":"Project details"},{"location":"Others/resume/#scarecrow-ml-for-aircraft-engine-management-deloitte-client-rolls-royce","text":"Technologies used : Python, streaming machine learning, Data bricks Timeline : Feb 2021 - Current Publications : Presented \"Scarecrow - Intelligent Annotation platform for Engine Health Management\" in AI ML Systems conference Impact : Preventive maintenance identified with 15% less false positivity(estimated) Team : 7-10 member team consisting of data engineers and data scientists Problem Statement : Assisting subject matter experts (SME) in identifying various performance issues in an engine Built a machine learning framework that continuously learns (streaming machine learning) by observing the decisions taken by SME's on a Tableau dashboard. SME's look at data from different sensors from aircraft engines in flight to identify the engines\\parts that may need maintenance or have low performance. These models are used to reduce the amount of data that SME's should review by identifying only the less confidence predictions (active learning).","title":"Scarecrow: ML for Aircraft Engine Management - Deloitte (Client: Rolls Royce)"},{"location":"Others/resume/#bid-allocation-tool-deloitte-client-rolls-royce","text":"Technologies used : Python, ORTools, Linear (Integer) Optimisation Timeline : Feb 2021 - Jun 2021 Impact : Estimated savings of 12 Million Pounds Problem Statement : Allocating parts in a manufacturing plant to suppliers for changing business constraints Suppliers bid on various parts that are to be procured in a manufacturing plant. Built the data science framework for a web-based tool that allocates parts to various suppliers across time. The tool uses OR Tools (Integer programming) to run a supplier allocation problem. Different business constraints were converted into integer-based constraints and added to the optimization function. These business constraints can be modified, added, or removed using the web-based UI and the optimal solution for each scenario is computed.","title":"Bid Allocation tool - Deloitte (Client: Rolls Royce)"},{"location":"Others/resume/#optimisation-of-rr-contests-for-a-life-insurance-company-using-predictive-and-prescriptive-analytics-iimb-capstone-project","text":"Technologies used : R, Excel, Google Colab Timeline : Dec 2019 - April 2020 Awards : Awarded \"Highly commended project\" for the BAI batch of 2019-20 at IIM Bangalore. Problem Statement : Building optimal R&R contests for agents of a large life insurance company. Forecasting sales of an agent Built a regression model which can explain 97% variation in sales. Quantified the lift generated due to different contest parameters Clustering of agents based on their capacity Identifying the factors which affect the maximum capacity of sales of an agent and clustering the agents based on them Simulation and optimization of contest parameters Simulated the cumulative sales for different contest parameters in each cluster of agents Identified the most optimal parameters based on budget and other constraints","title":"Optimisation of R&amp;R contests for a life insurance company using predictive and prescriptive analytics\u2014 IIMB capstone project"},{"location":"Others/resume/#supply-chain-analytics-musigma-client-walmart","text":"Technologies used : R, Python, SQL, CPLEX, Google Cloud Platform Timeline : 2018 - April 2020 Problem Statement : Reducing out-of-stock scenarios in stores by identifying and quantifying the different factors, predicting the failures due to various factors, and optimizing inventory based on them. Team : Collaborated with the technology and business units of Walmart Supply chain and market POC's in the US, Canada, Mexico, Argentina, and Chile. Worked end-to-end from ideation to POC development to production Quantified savings : The potential average cost savings from a reduction in inventory and out-of-stock costs would be $12 Million per month Quantify the reasons for under-stock scenarios Quantified the reasons causing under-stock scenarios in a store utilizing hypothesis testing and statistical modeling pinpointing the two main factors among 14 with the most significant impact (fillrate and lead time). Identify the risk of a supplier not delivering an order Designed classification model (gradient boosting) predicting the risk of a supplier not delivering an order in full (fillrate) with 75% accuracy and 50+% specificity Deployed the solution on the cloud and created workflows to predict the risk daily Forecasting inbound lead time of vendors Forecasted lead time applying a tree-based ensemble regression model (random forest) with 85% (SMAPE) accuracy Deployed the solution on the cloud and created workflows to predict lead time daily Optimizing inventory at store and warehouse Optimized EOQ and reorder point using an integer programming model Formulated and validated the approach under the Senior Director of Supply chain at Walmart","title":"Supply chain analytics \u2014 MuSigma (Client: Walmart)"},{"location":"Others/resume/#personal-projects","text":"Technologies used : R, Python, HTML, CSS, JavaScript,Github Deployed various data science applications for personal use Blogs on data science topics for college as well as professionally","title":"Personal projects"},{"location":"Python/ARIMA%20Forecasting/","text":"MathJax.Hub.Config({ extensions: [\"tex2jax.js\",\"TeX/AMSmath.js\",\"TeX/AMSsymbols.js\"], jax: [\"input/TeX\", \"output/HTML-CSS\"], tex2jax: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ], }, \"HTML-CSS\": { availableFonts: [\"TeX\"] } }); Arima forecasting \u00b6 Air pollution is a major issue in Hyderabad where I live. I am using the data taken from aqicn.org on the PM25 pollutant near my house in Hyderabad, India. I am using this data to build a model that will predict the PM25 air quality near my home. The training data can be found at aqicn's api This model is deployed on a flask application at harshaash.pythonanywhere.com as a Rest API and you can find the past and current results at hydpm25.aharsha.com/ . The details on how to deploy the models can be found in the blog ML Deployment in Flask . To find the theory of ARIMA in detail, read the blogs on Stationarity , Tests for stationarity and ARIMA concept . import warnings warnings . filterwarnings ( 'ignore' , category = FutureWarning ) % matplotlib inline import pandas as pd import matplotlib.pyplot as plt import seaborn as sns data = pd . read_csv ( 'hyderabad-us consulate-air-quality.csv' , parse_dates = [ 'date' ]) Vizualising time series \u00b6 Historic data is present in the form of daily average since 2014 December. data = data . sort_values ( 'date' ) data . columns = [ 'date' , 'pm25' ] data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date pm25 2295 2014-12-10 172 2296 2014-12-11 166 2297 2014-12-12 159 2298 2014-12-13 164 2299 2014-12-14 166 ... ... ... 0 2021-11-01 155 1 2021-11-02 115 2 2021-11-03 67 3 2021-11-04 112 4 2021-11-05 115 2314 rows \u00d7 2 columns plt . figure ( figsize = ( 20 , 10 )) plt . plot ( data . date , data . pm25 , color = 'tab:red' ) plt . gca () . set ( title = 'Pollution at Hyderabad' , xlabel = 'Date' , ylabel = 'PM25' ) plt . show () From this plot, we can see that pollution is higher during winter months while its lower during summer months. This effect is observed every year indicating a seasonal pattern in the data. There seems to be no increasing or decreasing trend in the data. This can be better visualised by decomposing the data into three components: 1. Seasonal component: The component that varies with season 2. Trend: Increasing or decreasing pattern 3. Random component: Remaining component that has no pattern from statsmodels.tsa.seasonal import seasonal_decompose result = seasonal_decompose ( data . pm25 , model = 'additive' , period = 365 ) fs , axs = plt . subplots ( 3 , figsize = ( 20 , 10 )) plt . suptitle ( 'Pollution (PM25) at Hyderabad' , fontsize = 20 , y = 0.95 ) axs [ 0 ] . plot ( data . date , result . trend ) axs [ 1 ] . plot ( data . date , result . seasonal ) axs [ 2 ] . plot ( data . date , result . resid ) axs [ 0 ] . set_ylabel ( 'Trend' , fontsize = 15 ) axs [ 1 ] . set_ylabel ( 'Seasonality' , fontsize = 15 ) axs [ 2 ] . set_ylabel ( 'Random component' , fontsize = 15 ) plt . show () Looking at the trend, we can see how the pollution decreased during 2020 (probably due to covid) and is slowly rising as the country is getting back to its feet. data [ 'year' ] = data . date . dt . year data [ 'day' ] = data . date . dt . dayofyear plt . figure ( figsize = ( 16 , 12 ), dpi = 80 ) for i , y in enumerate ( data . year . unique ()): plt . plot ( 'day' , 'pm25' , data = data . loc [ data . year == y , :], label = y ) plt . title ( \"Seasonal Plot of Hyd AQ15\" , fontsize = 20 ) plt . legend ( loc = \"upper left\" ) Dickey Fuller unit root test \u00b6 To find out if a time series is stationary, we can use the Dickey Fuller test. As discussed in the previous blog[], unit root stochastic process is another name for Random walk process. A random walk process can be written as $$ Y_t=\\rho \\times Y_{t\u22121} + \\epsilon_t $$ Where \\(\\rho = 1\\) . If \\(|\\rho|<1\\) then the process represents Markov first order auto regressive model which is stationary. Only for \\(\\rho=1\\) we get non-stationary. The above equation can be alternatively written as $$ Y_t - Y_{t-1} = \\Delta Y_t = \\delta \\times Y_{t-1} + \\epsilon_t $$ where \\(\\delta = \\rho -1\\) . For non-stationarity, the condition now becomes \\(\\delta = 0\\) the alternative hypothesis being \\(\\delta < 0\\) . The null and alternate hypothesis are: \\(H_0: \\delta = 0\\) (Time series is non stationary) \\(H_1 : \\delta < 0\\) (Time series is stationary) Under this null hypothesis, \\(Y_{t-1}\\) does not follow a normal distribution(or t-distribution). Dickey and Fuller have shown that for the above null and alternate hypothesis, the the estimated test statistic follows the \\(\\tau\\) statistic. If the hypothesis that \\(\\delta=0\\) is rejected, that is if the series is stationary, then we can use the t-test for further analysis. For the data, the Dickey Fuller tests give the following results from statsmodels.tsa.stattools import adfuller , kpss result = adfuller ( data . pm25 ) print ( 'ADF Statistic: %f ' % result [ 0 ]) print ( 'p-value: %f ' % result [ 1 ]) print ( 'Critical Values:' ) for key , value in result [ 4 ] . items (): print ( ' \\t %s : %.3f ' % ( key , value )) ADF Statistic: -3.835594 p-value: 0.002563 Critical Values: 1%: -3.433 5%: -2.863 10%: -2.567 # KPSS Test result = kpss ( data . pm25 . values , regression = 'c' ) print ( ' \\n KPSS Statistic: %f ' % result [ 0 ]) print ( 'p-value: %f ' % result [ 1 ]) for key , value in result [ 3 ] . items (): print ( 'Critial Values:' ) print ( f ' { key } , { value } ' ) KPSS Statistic: 1.318795 p-value: 0.010000 Critial Values: 10%, 0.347 Critial Values: 5%, 0.463 Critial Values: 2.5%, 0.574 Critial Values: 1%, 0.739 As the p-value is less than the cutoff (5%), we reject the Null hypothesis. The time series is stationary. ACF and PACF plots \u00b6 from statsmodels.graphics.tsaplots import plot_acf , plot_pacf print ( plot_acf ( data . pm25 )) print ( plot_pacf ( data . pm25 )) This indicates that the data is stationary. Another way to verify this is using the pdarima library to identify the lag at which the data will be stationary using adf, kpss and pp tests. While adf and pp are consistant with the above kpss test indicates that the data is stationary at d=1. from pmdarima.arima.utils import ndiffs ## Adf Test print ( ndiffs ( data . pm25 , test = 'adf' )) # KPSS test print ( ndiffs ( data . pm25 , test = 'kpss' )) # PP test: print ( ndiffs ( data . pm25 , test = 'pp' )) 0 1 0 Stepwise ARIMA \u00b6 Performing stepwise arima in python to find the optimum p,d,q values. import pmdarima as pm # splitting into test and train split_time = len ( data ) - 365 * 2 # Latest two years is training, rest is test time_train = data . date [: split_time ] x_train = data . pm25 [: split_time ] time_valid = data . date [ split_time :] x_valid = data . pm25 [ split_time :] model = pm . auto_arima ( x_train , start_p = 1 , start_q = 1 , test = 'adf' , # use adftest to find optimal 'd' max_p = 3 , max_q = 3 , # maximum p and q m = 365 , # frequency of series d = None , # let model determine 'd' seasonal = False , # No Seasonality (as first trail) start_P = 0 , D = None , trace = True , error_action = 'ignore' , stepwise = True ) print ( model . summary ()) Performing stepwise search to minimize aic ARIMA(1,0,1)(0,0,0)[0] : AIC=13312.056, Time=0.26 sec ARIMA(0,0,0)(0,0,0)[0] : AIC=19910.319, Time=0.02 sec ARIMA(1,0,0)(0,0,0)[0] : AIC=inf, Time=0.02 sec ARIMA(0,0,1)(0,0,0)[0] : AIC=18004.728, Time=0.15 sec ARIMA(2,0,1)(0,0,0)[0] : AIC=13203.466, Time=0.48 sec ARIMA(2,0,0)(0,0,0)[0] : AIC=inf, Time=0.09 sec ARIMA(3,0,1)(0,0,0)[0] : AIC=13204.966, Time=0.45 sec ARIMA(2,0,2)(0,0,0)[0] : AIC=13204.925, Time=0.45 sec ARIMA(1,0,2)(0,0,0)[0] : AIC=13249.558, Time=0.30 sec ARIMA(3,0,0)(0,0,0)[0] : AIC=inf, Time=0.20 sec ARIMA(3,0,2)(0,0,0)[0] : AIC=13207.206, Time=0.35 sec ARIMA(2,0,1)(0,0,0)[0] intercept : AIC=13196.527, Time=1.07 sec ARIMA(1,0,1)(0,0,0)[0] intercept : AIC=13270.440, Time=0.36 sec ARIMA(2,0,0)(0,0,0)[0] intercept : AIC=13281.731, Time=0.17 sec ARIMA(3,0,1)(0,0,0)[0] intercept : AIC=13197.878, Time=1.48 sec ARIMA(2,0,2)(0,0,0)[0] intercept : AIC=13197.829, Time=1.33 sec ARIMA(1,0,0)(0,0,0)[0] intercept : AIC=13305.106, Time=0.06 sec ARIMA(1,0,2)(0,0,0)[0] intercept : AIC=13230.273, Time=0.55 sec ARIMA(3,0,0)(0,0,0)[0] intercept : AIC=13252.937, Time=0.20 sec ARIMA(3,0,2)(0,0,0)[0] intercept : AIC=13199.299, Time=1.87 sec Best model: ARIMA(2,0,1)(0,0,0)[0] intercept Total fit time: 9.881 seconds SARIMAX Results ============================================================================== Dep. Variable: y No. Observations: 1584 Model: SARIMAX(2, 0, 1) Log Likelihood -6593.264 Date: Tue, 28 Dec 2021 AIC 13196.527 Time: 21:42:13 BIC 13223.366 Sample: 0 HQIC 13206.498 - 1584 Covariance Type: opg ============================================================================== coef std err z P>|z| [0.025 0.975] ------------------------------------------------------------------------------ intercept 0.5080 0.235 2.162 0.031 0.047 0.969 ar.L1 1.5900 0.030 53.575 0.000 1.532 1.648 ar.L2 -0.5941 0.029 -20.557 0.000 -0.651 -0.537 ma.L1 -0.8732 0.023 -38.503 0.000 -0.918 -0.829 sigma2 241.1973 4.922 49.000 0.000 231.550 250.845 =================================================================================== Ljung-Box (L1) (Q): 0.17 Jarque-Bera (JB): 1164.77 Prob(Q): 0.68 Prob(JB): 0.00 Heteroskedasticity (H): 1.01 Skew: 0.02 Prob(H) (two-sided): 0.92 Kurtosis: 7.20 =================================================================================== The error metrics for the test data is: # Getting accuracy metrics on test data results_model = model . predict ( n_periods = 365 * 2 ) results_model from sklearn.metrics import mean_squared_error , mean_absolute_error from math import sqrt print ( 'RMSE is ' , sqrt ( mean_squared_error ( x_valid , results_model ))) print ( 'MAE is ' , mean_absolute_error ( x_valid , results_model )) RMSE is 50.53383627717328 MAE is 43.4421801512105 From stepwise ARIMA, we see that the most optimal is p=2, d=0, and q=1 values with no significant seasonal component.This makes sense as any air pollutant generally stays in the air for a maximum of two days for Hyderabad wind and climatic patterns. So the effect of any sudden increase or decrease in pollutants (MA Component) exists for a day in the future. Also the pollution today is effected by the baseline pollution in the last two days (AR component). # Predicting on the complete data (test + train) from statsmodels.tsa.arima_model import ARIMA model_arima = ARIMA ( data . pm25 , order = ( 2 , 0 , 1 )) results_AR = model_arima . fit ( disp =- 1 ) plt . figure ( figsize = ( 16 , 12 ), dpi = 80 ) plt . plot ( data . date , data . pm25 ) plt . plot ( data . date , results_AR . fittedvalues , color = 'red' , alpha = 0.9 ) plt . title ( \"Actual vs predicted for PM 25 in Hyderabad\" , fontsize = 20 ) plt . show () From the residuals we can see no patterns, indicating that we have a good prediction. print ( model . plot_diagnostics ( figsize = ( 20 , 10 ))) In the next blogs, we will implement deep learning (Like LSTM, RNN) and other methods on this data to deploy multiple models using various deployment methodologies.","title":"ARIMA in Python"},{"location":"Python/ARIMA%20Forecasting/#arima-forecasting","text":"Air pollution is a major issue in Hyderabad where I live. I am using the data taken from aqicn.org on the PM25 pollutant near my house in Hyderabad, India. I am using this data to build a model that will predict the PM25 air quality near my home. The training data can be found at aqicn's api This model is deployed on a flask application at harshaash.pythonanywhere.com as a Rest API and you can find the past and current results at hydpm25.aharsha.com/ . The details on how to deploy the models can be found in the blog ML Deployment in Flask . To find the theory of ARIMA in detail, read the blogs on Stationarity , Tests for stationarity and ARIMA concept . import warnings warnings . filterwarnings ( 'ignore' , category = FutureWarning ) % matplotlib inline import pandas as pd import matplotlib.pyplot as plt import seaborn as sns data = pd . read_csv ( 'hyderabad-us consulate-air-quality.csv' , parse_dates = [ 'date' ])","title":"Arima forecasting"},{"location":"Python/ARIMA%20Forecasting/#vizualising-time-series","text":"Historic data is present in the form of daily average since 2014 December. data = data . sort_values ( 'date' ) data . columns = [ 'date' , 'pm25' ] data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date pm25 2295 2014-12-10 172 2296 2014-12-11 166 2297 2014-12-12 159 2298 2014-12-13 164 2299 2014-12-14 166 ... ... ... 0 2021-11-01 155 1 2021-11-02 115 2 2021-11-03 67 3 2021-11-04 112 4 2021-11-05 115 2314 rows \u00d7 2 columns plt . figure ( figsize = ( 20 , 10 )) plt . plot ( data . date , data . pm25 , color = 'tab:red' ) plt . gca () . set ( title = 'Pollution at Hyderabad' , xlabel = 'Date' , ylabel = 'PM25' ) plt . show () From this plot, we can see that pollution is higher during winter months while its lower during summer months. This effect is observed every year indicating a seasonal pattern in the data. There seems to be no increasing or decreasing trend in the data. This can be better visualised by decomposing the data into three components: 1. Seasonal component: The component that varies with season 2. Trend: Increasing or decreasing pattern 3. Random component: Remaining component that has no pattern from statsmodels.tsa.seasonal import seasonal_decompose result = seasonal_decompose ( data . pm25 , model = 'additive' , period = 365 ) fs , axs = plt . subplots ( 3 , figsize = ( 20 , 10 )) plt . suptitle ( 'Pollution (PM25) at Hyderabad' , fontsize = 20 , y = 0.95 ) axs [ 0 ] . plot ( data . date , result . trend ) axs [ 1 ] . plot ( data . date , result . seasonal ) axs [ 2 ] . plot ( data . date , result . resid ) axs [ 0 ] . set_ylabel ( 'Trend' , fontsize = 15 ) axs [ 1 ] . set_ylabel ( 'Seasonality' , fontsize = 15 ) axs [ 2 ] . set_ylabel ( 'Random component' , fontsize = 15 ) plt . show () Looking at the trend, we can see how the pollution decreased during 2020 (probably due to covid) and is slowly rising as the country is getting back to its feet. data [ 'year' ] = data . date . dt . year data [ 'day' ] = data . date . dt . dayofyear plt . figure ( figsize = ( 16 , 12 ), dpi = 80 ) for i , y in enumerate ( data . year . unique ()): plt . plot ( 'day' , 'pm25' , data = data . loc [ data . year == y , :], label = y ) plt . title ( \"Seasonal Plot of Hyd AQ15\" , fontsize = 20 ) plt . legend ( loc = \"upper left\" )","title":"Vizualising time series"},{"location":"Python/ARIMA%20Forecasting/#dickey-fuller-unit-root-test","text":"To find out if a time series is stationary, we can use the Dickey Fuller test. As discussed in the previous blog[], unit root stochastic process is another name for Random walk process. A random walk process can be written as $$ Y_t=\\rho \\times Y_{t\u22121} + \\epsilon_t $$ Where \\(\\rho = 1\\) . If \\(|\\rho|<1\\) then the process represents Markov first order auto regressive model which is stationary. Only for \\(\\rho=1\\) we get non-stationary. The above equation can be alternatively written as $$ Y_t - Y_{t-1} = \\Delta Y_t = \\delta \\times Y_{t-1} + \\epsilon_t $$ where \\(\\delta = \\rho -1\\) . For non-stationarity, the condition now becomes \\(\\delta = 0\\) the alternative hypothesis being \\(\\delta < 0\\) . The null and alternate hypothesis are: \\(H_0: \\delta = 0\\) (Time series is non stationary) \\(H_1 : \\delta < 0\\) (Time series is stationary) Under this null hypothesis, \\(Y_{t-1}\\) does not follow a normal distribution(or t-distribution). Dickey and Fuller have shown that for the above null and alternate hypothesis, the the estimated test statistic follows the \\(\\tau\\) statistic. If the hypothesis that \\(\\delta=0\\) is rejected, that is if the series is stationary, then we can use the t-test for further analysis. For the data, the Dickey Fuller tests give the following results from statsmodels.tsa.stattools import adfuller , kpss result = adfuller ( data . pm25 ) print ( 'ADF Statistic: %f ' % result [ 0 ]) print ( 'p-value: %f ' % result [ 1 ]) print ( 'Critical Values:' ) for key , value in result [ 4 ] . items (): print ( ' \\t %s : %.3f ' % ( key , value )) ADF Statistic: -3.835594 p-value: 0.002563 Critical Values: 1%: -3.433 5%: -2.863 10%: -2.567 # KPSS Test result = kpss ( data . pm25 . values , regression = 'c' ) print ( ' \\n KPSS Statistic: %f ' % result [ 0 ]) print ( 'p-value: %f ' % result [ 1 ]) for key , value in result [ 3 ] . items (): print ( 'Critial Values:' ) print ( f ' { key } , { value } ' ) KPSS Statistic: 1.318795 p-value: 0.010000 Critial Values: 10%, 0.347 Critial Values: 5%, 0.463 Critial Values: 2.5%, 0.574 Critial Values: 1%, 0.739 As the p-value is less than the cutoff (5%), we reject the Null hypothesis. The time series is stationary.","title":"Dickey Fuller unit root test"},{"location":"Python/ARIMA%20Forecasting/#acf-and-pacf-plots","text":"from statsmodels.graphics.tsaplots import plot_acf , plot_pacf print ( plot_acf ( data . pm25 )) print ( plot_pacf ( data . pm25 )) This indicates that the data is stationary. Another way to verify this is using the pdarima library to identify the lag at which the data will be stationary using adf, kpss and pp tests. While adf and pp are consistant with the above kpss test indicates that the data is stationary at d=1. from pmdarima.arima.utils import ndiffs ## Adf Test print ( ndiffs ( data . pm25 , test = 'adf' )) # KPSS test print ( ndiffs ( data . pm25 , test = 'kpss' )) # PP test: print ( ndiffs ( data . pm25 , test = 'pp' )) 0 1 0","title":"ACF and PACF plots"},{"location":"Python/ARIMA%20Forecasting/#stepwise-arima","text":"Performing stepwise arima in python to find the optimum p,d,q values. import pmdarima as pm # splitting into test and train split_time = len ( data ) - 365 * 2 # Latest two years is training, rest is test time_train = data . date [: split_time ] x_train = data . pm25 [: split_time ] time_valid = data . date [ split_time :] x_valid = data . pm25 [ split_time :] model = pm . auto_arima ( x_train , start_p = 1 , start_q = 1 , test = 'adf' , # use adftest to find optimal 'd' max_p = 3 , max_q = 3 , # maximum p and q m = 365 , # frequency of series d = None , # let model determine 'd' seasonal = False , # No Seasonality (as first trail) start_P = 0 , D = None , trace = True , error_action = 'ignore' , stepwise = True ) print ( model . summary ()) Performing stepwise search to minimize aic ARIMA(1,0,1)(0,0,0)[0] : AIC=13312.056, Time=0.26 sec ARIMA(0,0,0)(0,0,0)[0] : AIC=19910.319, Time=0.02 sec ARIMA(1,0,0)(0,0,0)[0] : AIC=inf, Time=0.02 sec ARIMA(0,0,1)(0,0,0)[0] : AIC=18004.728, Time=0.15 sec ARIMA(2,0,1)(0,0,0)[0] : AIC=13203.466, Time=0.48 sec ARIMA(2,0,0)(0,0,0)[0] : AIC=inf, Time=0.09 sec ARIMA(3,0,1)(0,0,0)[0] : AIC=13204.966, Time=0.45 sec ARIMA(2,0,2)(0,0,0)[0] : AIC=13204.925, Time=0.45 sec ARIMA(1,0,2)(0,0,0)[0] : AIC=13249.558, Time=0.30 sec ARIMA(3,0,0)(0,0,0)[0] : AIC=inf, Time=0.20 sec ARIMA(3,0,2)(0,0,0)[0] : AIC=13207.206, Time=0.35 sec ARIMA(2,0,1)(0,0,0)[0] intercept : AIC=13196.527, Time=1.07 sec ARIMA(1,0,1)(0,0,0)[0] intercept : AIC=13270.440, Time=0.36 sec ARIMA(2,0,0)(0,0,0)[0] intercept : AIC=13281.731, Time=0.17 sec ARIMA(3,0,1)(0,0,0)[0] intercept : AIC=13197.878, Time=1.48 sec ARIMA(2,0,2)(0,0,0)[0] intercept : AIC=13197.829, Time=1.33 sec ARIMA(1,0,0)(0,0,0)[0] intercept : AIC=13305.106, Time=0.06 sec ARIMA(1,0,2)(0,0,0)[0] intercept : AIC=13230.273, Time=0.55 sec ARIMA(3,0,0)(0,0,0)[0] intercept : AIC=13252.937, Time=0.20 sec ARIMA(3,0,2)(0,0,0)[0] intercept : AIC=13199.299, Time=1.87 sec Best model: ARIMA(2,0,1)(0,0,0)[0] intercept Total fit time: 9.881 seconds SARIMAX Results ============================================================================== Dep. Variable: y No. Observations: 1584 Model: SARIMAX(2, 0, 1) Log Likelihood -6593.264 Date: Tue, 28 Dec 2021 AIC 13196.527 Time: 21:42:13 BIC 13223.366 Sample: 0 HQIC 13206.498 - 1584 Covariance Type: opg ============================================================================== coef std err z P>|z| [0.025 0.975] ------------------------------------------------------------------------------ intercept 0.5080 0.235 2.162 0.031 0.047 0.969 ar.L1 1.5900 0.030 53.575 0.000 1.532 1.648 ar.L2 -0.5941 0.029 -20.557 0.000 -0.651 -0.537 ma.L1 -0.8732 0.023 -38.503 0.000 -0.918 -0.829 sigma2 241.1973 4.922 49.000 0.000 231.550 250.845 =================================================================================== Ljung-Box (L1) (Q): 0.17 Jarque-Bera (JB): 1164.77 Prob(Q): 0.68 Prob(JB): 0.00 Heteroskedasticity (H): 1.01 Skew: 0.02 Prob(H) (two-sided): 0.92 Kurtosis: 7.20 =================================================================================== The error metrics for the test data is: # Getting accuracy metrics on test data results_model = model . predict ( n_periods = 365 * 2 ) results_model from sklearn.metrics import mean_squared_error , mean_absolute_error from math import sqrt print ( 'RMSE is ' , sqrt ( mean_squared_error ( x_valid , results_model ))) print ( 'MAE is ' , mean_absolute_error ( x_valid , results_model )) RMSE is 50.53383627717328 MAE is 43.4421801512105 From stepwise ARIMA, we see that the most optimal is p=2, d=0, and q=1 values with no significant seasonal component.This makes sense as any air pollutant generally stays in the air for a maximum of two days for Hyderabad wind and climatic patterns. So the effect of any sudden increase or decrease in pollutants (MA Component) exists for a day in the future. Also the pollution today is effected by the baseline pollution in the last two days (AR component). # Predicting on the complete data (test + train) from statsmodels.tsa.arima_model import ARIMA model_arima = ARIMA ( data . pm25 , order = ( 2 , 0 , 1 )) results_AR = model_arima . fit ( disp =- 1 ) plt . figure ( figsize = ( 16 , 12 ), dpi = 80 ) plt . plot ( data . date , data . pm25 ) plt . plot ( data . date , results_AR . fittedvalues , color = 'red' , alpha = 0.9 ) plt . title ( \"Actual vs predicted for PM 25 in Hyderabad\" , fontsize = 20 ) plt . show () From the residuals we can see no patterns, indicating that we have a good prediction. print ( model . plot_diagnostics ( figsize = ( 20 , 10 ))) In the next blogs, we will implement deep learning (Like LSTM, RNN) and other methods on this data to deploy multiple models using various deployment methodologies.","title":"Stepwise ARIMA"},{"location":"Python/Bipartite%20matching/","text":"Matching algorithms \u00b6 Author: Achyuthuni Sri Harsha Stock markets, housing and labor markets, dating and so forth are examples of matching tasks. Let us take suppliers an buyers as an example. in a matching problem, our job is to match the supliers to buyers so that both sides/ either side are satisfied. Matching problems can be considered as network problems. In network terms, a matching is a subset of edges where every node in one group goes through only one node in the other group. There should be only one edge from each node. # Import networkx library and rename it as nx. import networkx as nx # Other packages required import numpy as np import pandas as pd import matplotlib.pyplot as plt Unweighted bipartite mapping \u00b6 Let us take a simple example of mapping students and dorm rooms. In this problem, students give a list of rooms they are willing to stay at. We represent students on side as nodes of a bipartite graph and rooms on the other side as nodes, and we put an edge between students and rooms as per this list. We want to map students and rooms. We want to identify a subset where we match one student to exactly one other room (no roommates). The students give what rooms are acceptable, and many solutions are possible. Consider the below problem where we have students (A, B, C and D) and we want to match them to rooms (1,2,3,4). The list of acceptable rooms for each student is given below. edgelist_df = pd . DataFrame ({ 'node1' :[ 'A' , 'A' , 'B' , 'B' , 'C' , 'C' , 'D' , 'D' ], 'node2' :[ 1 , 2 , 2 , 3 , 3 , 4 , 4 , 1 ]}) edgelist_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } node1 node2 0 A 1 1 A 2 2 B 2 3 B 3 4 C 3 5 C 4 6 D 4 7 D 1 This can be represented as a bipartite graph as follows g = nx . Graph () for i , elrow in edgelist_df . iterrows (): g . add_edge ( elrow [ 0 ], elrow [ 1 ]) # Make two sets in bipartite and get positions for the same left , right = nx . bipartite . sets ( g ) pos = {} # Update position for node from each group for i , node in enumerate ( sorted ( list ( left ))): g . add_node ( node , pos = ( 0 , i )) for i , node in enumerate ( sorted ( list ( right ))): g . add_node ( node , pos = ( 1 , i )) nx . draw ( g , pos = nx . get_node_attributes ( g , 'pos' ), with_labels = True ) Visually, we can find a couple of solutions, for example: - A:1, B:2, C:3, D:4 - A:2, B:3, C:4, D:1 In large graphs visual analysis might be difficult, and in such situations Halls theorem is useful to identify if matching is possible. Halls theorem \u00b6 But before Halls theorem, let us look at constricted set. Constricted set : A constricted set is a subset of edges (on either side) whose neighbours are smaller than the subset. For examle, if we have two students (subset of students) who give only one room (same room) in the list, then the size of the students is 2 and the size of the rooms is one, and no matching can be done. Halls theorem states that for a mapping to exist, there should be no constricted set. Augmenting paths 1. Select any random matching of unmatched nodes. 2. Switch to the augumented paths if it exists. If it doesnt exist, then we have a constricted set and we cannot do matching. 3. Repeat until all left nodes are matched to one right node. This is implemented in NetworkX as follows: # Select random edges selected_edges = [] for left_node in left : # For a left node, select a random node in the right list_of_nodes = list ( g . edges ( left_node )) random_node = np . random . randint ( len ( list_of_nodes )) selected_edges . append ( list_of_nodes [ random_node ]) selected_edges [('D', 4), ('C', 4), ('A', 2), ('B', 3)] Selected edges are shown in green for edge in g . edges : if edge in selected_edges or ( edge [ 1 ], edge [ 0 ]) in selected_edges : g . add_edge ( edge [ 0 ], edge [ 1 ], color = 'g' , weight = 10 ) else : g . add_edge ( edge [ 0 ], edge [ 1 ], color = 'b' , weight = 0.1 ) nx . draw ( g , pos = nx . get_node_attributes ( g , 'pos' ), with_labels = True , edge_color = nx . get_edge_attributes ( g , 'color' ) . values ()) We can see that this is not matching as node C and D are mapped to 4. This can be resolved by moving through the augumented paths for the C-4 node. For the C-4 node, moving thru the augumented path selects the C-3 and D-4 node. Following the smae process with with B-3 and A-3 nodes, we get: selected_edges = {( 1 , 'A' ), ( 2 , 'B' ), ( 3 , 'C' ), ( 4 , 'D' )} for edge in g . edges : if edge in selected_edges or ( edge [ 1 ], edge [ 0 ]) in selected_edges : g . add_edge ( edge [ 0 ], edge [ 1 ], color = 'g' , weight = 10 ) else : g . add_edge ( edge [ 0 ], edge [ 1 ], color = 'b' , weight = 0.1 ) nx . draw ( g , pos = nx . get_node_attributes ( g , 'pos' ), with_labels = True , edge_color = nx . get_edge_attributes ( g , 'color' ) . values ()) The final mapping is shown in green nx . algorithms . bipartite . matching . hopcroft_karp_matching ( g , top_nodes = list ( set ( edgelist_df . node1 ))) {'D': 4, 'C': 3, 'B': 2, 'A': 1, 1: 'A', 2: 'B', 3: 'C', 4: 'D'} Weighted bipartite mapping \u00b6 In the previous problem, we tried to find a perfect maching in an unweighted graph. What if every edge in the graph has certain weight attached to it. The weights could be quality index (for student-dorm matching) or valuations in a market etc. Consider the suppliers-buyers example for a housing market as shown below. We have three suppliers, A, B and C and three buyers (x, y and z). The valuation for each of the sellers is also given. For example, buyer x values house A with 12, house B with 4 and house C with 2. sellers = [ 'A' , 'B' , 'C' ] buyers = [ 'x' , 'y' , 'z' ] valuations_for_buyers = [[ 12 , 4 , 2 ], [ 8 , 7 , 6 ], [ 7 , 5 , 2 ]] sellers_price = [ 0 , 0 , 0 ] g = nx . Graph () pos = {} # Update position for node from each group for i , node in enumerate ( sellers ): g . add_node ( node , pos = ( 0 , len ( sellers ) - i )) for i , node in enumerate ( buyers ): g . add_node ( node , pos = ( 1 , len ( buyers ) - i )) # Plot text for the buyers for i , buyer in enumerate ( buyers ): plt . text ( 1.1 , len ( buyers ) - i , s = valuations_for_buyers [ i ], horizontalalignment = 'left' ) # Plot text for the sellers for i , buyer in enumerate ( buyers ): plt . text ( - 0.1 , len ( buyers ) - i , s = sellers_price [ i ], horizontalalignment = 'right' ) nx . draw ( g , pos = nx . get_node_attributes ( g , 'pos' ), with_labels = True ) What we want to achieve is clearing of the market. Clearing happens when all houses are sold to one buyer, and every buyer bought one house. This can be done using an auction algorithm. 1. Sellers quote a price 2. Buyers calculate utility: Net valuation (payoff) = Gross Valuation - Price charged by seller 3. Buyers select the object that has highest payoff 4. If the market is not cleared, the sellers who have more than one offer (overdetermined) will increase the price by one unit, and the process is repeated. # Function to pick the supplier with the maximumm utility def match_to_maximum_utility ( sellers , buyers , valuation , price ): max_utility_sellers = {} for buyer_index in range ( len ( buyers )): max_utility = 0 for seller_index in range ( len ( sellers )): if ( max_utility < valuation [ buyer_index ][ seller_index ] - price [ seller_index ]): max_utility = valuation [ buyer_index ][ seller_index ] - price [ seller_index ] max_utility_sellers [ buyers [ buyer_index ]] = [ sellers [ seller_index ]] elif ( max_utility == valuation [ buyer_index ][ seller_index ] - price [ seller_index ]): max_utility_sellers [ buyers [ buyer_index ]] . append ( sellers [ seller_index ]) return max_utility_sellers Assuming that the initial price set by the seller is zero (scaled to zero - displayed beside the node), buyer x calculates the following utility: - For A: 12-0 = 12 - For B: 4-0 = 4 - For C: 2-0 = 2 As the utility of A is the highest, x will chose A. Similarly, B and C will also choose A. max_util = match_to_maximum_utility ( sellers , buyers , valuations_for_buyers , sellers_price ) max_util {'x': ['A'], 'y': ['A'], 'z': ['A']} Plotting the selection, we see that A is overdetermined. def plot_max_utility_graph ( sellers , buyers , valuations_for_buyers , sellers_price , max_util ): g = nx . Graph () pos = {} # Update position for node from each group for i , node in enumerate ( sellers ): g . add_node ( node , pos = ( 0 , len ( sellers ) - i )) for i , node in enumerate ( buyers ): g . add_node ( node , pos = ( 1 , len ( buyers ) - i )) # Make edges for key , values in max_util . items (): for value in values : g . add_edge ( key , value ) # Plot text for the buyers for i , buyer in enumerate ( buyers ): plt . text ( 1.1 , len ( buyers ) - i , s = valuations_for_buyers [ i ], horizontalalignment = 'left' ) # Plot text for the sellers for i , buyer in enumerate ( buyers ): plt . text ( - 0.1 , len ( buyers ) - i , s = sellers_price [ i ], horizontalalignment = 'right' ) nx . draw ( g , pos = nx . get_node_attributes ( g , 'pos' ), with_labels = True ) plt . show () plot_max_utility_graph ( sellers , buyers , valuations_for_buyers , sellers_price , max_util ) For the overdetermined edge A, we increase the price by one unit. def get_overdetermined_edge_and_increase_price ( sellers , sellers_price , max_util ): from collections import Counter counts = dict ( Counter ( sum ( max_util . values (), []))) over_determined_list = [] for key , value in counts . items (): if ( value > 1 ): over_determined_list . append ( key ) sellers_price [ sellers . index ( key )] += 1 print ( 'Nodes' , over_determined_list , 'are over determined. Added 1 to the price for the suppliers' ) return counts get_overdetermined_edge_and_increase_price ( sellers , sellers_price , max_util ) Nodes ['A'] are over determined. Added 1 to the price for the suppliers {'A': 3} We then contiue this process until the market is cleared. no_of_sellers_selected = 0 while ( no_of_sellers_selected != len ( sellers )): max_util = match_to_maximum_utility ( sellers , buyers , valuations_for_buyers , sellers_price ) plot_max_utility_graph ( sellers , buyers , valuations_for_buyers , sellers_price , max_util ) counts = get_overdetermined_edge_and_increase_price ( sellers , sellers_price , max_util ) no_of_sellers_selected = len ( counts ) # need not always be the case, check Nodes ['A'] are over determined. Added 1 to the price for the suppliers Nodes ['A', 'B'] are over determined. Added 1 to the price for the suppliers Nodes ['A', 'B'] are over determined. Added 1 to the price for the suppliers We can see that for costs (A:3, B:1, C:0), the market can be cleared with buyer x choosing A, y choosing C and z choosing B. This is the maximum weight perfect matching. Matching with preferences \u00b6 In the previous scenario, we had weights on the edges which indicated the utility. In this case we will look at matching where we have preferences in a ranked order. This is more natural way in many scenarios, like students preference to universities/universities selecting students, dating scenarios etc. This was originally implemented by Al Roth for matching hospitals and residency. Our goal is to clear the market, but also have a stable matching. So, what is a stable matching? Stable matching : Stability is an eqvilibrium when no pair on ether side has an incentive to devaite from the mapping. Let us understand this using an example. Take the dating scenario for example. On the left hand side we have men and on the right hand side we have women. All men rank women in strict order and all women rank men in strict order. In a stable matching, no pair of nodes (male-female) prefers each other to their curently assigned partners. Gale Shapley Algorithm \u00b6 Let us say that are n players on both sides with males(m) on one side and women(w) on another side. The algorithm is as follows: 1. Every unmatched male (m) proposses to their first preference available. 2. If the proposed women (w) is unmatched, w accepts. If the women is already matched and the m has higher preference for w, w switches. Else, previous mapping remains. 3. This process continues until there is stability Lets take an example with three males and three females. The preferences are mentioned at the side of the node in a list. For example, m2 has a preference w2, followed by w3 and then w1. Similarly w2 has a preference of m3, followed by m1 and then m2. males = [ 'm1' , 'm2' , 'm3' ] females = [ 'w1' , 'w2' , 'w3' ] male_preferences = [[ 'w1' , 'w2' , 'w3' ], [ 'w2' , 'w3' , 'w1' ], [ 'w2' , 'w3' , 'w1' ]] female_preferences = [[ 'm2' , 'm3' , 'm1' ], [ 'm3' , 'm1' , 'm2' ], [ 'm1' , 'm2' , 'm3' ]] def match_next_male ( male_index , males , females , male_preferences , female_preferences ): for female in male_preferences [ male_index ]: if ( female not in current_mapping . values ()): current_mapping [ males [ male_index ]] = female return current_mapping elif ( female in current_mapping . values ()): current_mapping_inverse = dict ( zip ( current_mapping . values (), current_mapping . keys ())) current_male_for_the_female = current_mapping_inverse [ female ] if ( female_preferences [ males . index ( current_male_for_the_female )] > female_preferences [ male_index ]): current_mapping [ males [ male_index ]] = female current_mapping . pop ( current_male_for_the_female ) return current_mapping def plot_max_utility_graph ( males , females , male_preferences , female_preferences , current_mapping ): g = nx . Graph () pos = {} # Update position for node from each group for i , node in enumerate ( males ): g . add_node ( node , pos = ( 0 , len ( males ) - i )) for i , node in enumerate ( females ): g . add_node ( node , pos = ( 1 , len ( females ) - i )) # Make edges for key , value in current_mapping . items (): g . add_edge ( key , value ) # Plot text for the males for i , male in enumerate ( males ): plt . text ( - 0.1 , len ( males ) - i , s = male_preferences [ i ], horizontalalignment = 'right' ) # Plot text for the females for i , female in enumerate ( females ): plt . text ( 1.1 , len ( females ) - i , s = female_preferences [ i ], horizontalalignment = 'left' ) nx . draw ( g , pos = nx . get_node_attributes ( g , 'pos' ), with_labels = True ) plt . show () print ( '_____________________________________________________________________________' ) current_mapping = {} while ( len ( current_mapping ) != len ( males )): for male_index in range ( len ( males )): if ( current_mapping . get ( males [ male_index ]) is None ): current_mapping = match_next_male ( male_index , males , females , male_preferences , female_preferences ) plot_max_utility_graph ( males , females , male_preferences , female_preferences , current_mapping ) _____________________________________________________________________________ _____________________________________________________________________________ _____________________________________________________________________________ _____________________________________________________________________________ The process goes on as follows: 1. m1 proposes to w1 as w1 has maximum rank and as w1 is unselected, w1 accepts. We create an edge between them. 2. m2 proposes to w2 as w2 has maximum rank, and as w2 is unselected, w2 accepts. We crreate a edge between them. 3. m3 also proposes to w2. As w2 is already selected, it checks the preference of the current selection (m2) to m3. As m3 has better preference, w2 switches from m2 to m3. The edge between m2 and w2 is reoved and a new edge between m3 and w2 is created. 4. m2 is currently unmapped, and selects the next best preference, which is w3. As w3 is unselected, w3 accepts. 5. This clears the market and the process stops. This is stable mapping (from the men's side). These are the common matching techniques that exist.","title":"Bipartite matching (Python)"},{"location":"Python/Bipartite%20matching/#matching-algorithms","text":"Author: Achyuthuni Sri Harsha Stock markets, housing and labor markets, dating and so forth are examples of matching tasks. Let us take suppliers an buyers as an example. in a matching problem, our job is to match the supliers to buyers so that both sides/ either side are satisfied. Matching problems can be considered as network problems. In network terms, a matching is a subset of edges where every node in one group goes through only one node in the other group. There should be only one edge from each node. # Import networkx library and rename it as nx. import networkx as nx # Other packages required import numpy as np import pandas as pd import matplotlib.pyplot as plt","title":"Matching algorithms"},{"location":"Python/Bipartite%20matching/#unweighted-bipartite-mapping","text":"Let us take a simple example of mapping students and dorm rooms. In this problem, students give a list of rooms they are willing to stay at. We represent students on side as nodes of a bipartite graph and rooms on the other side as nodes, and we put an edge between students and rooms as per this list. We want to map students and rooms. We want to identify a subset where we match one student to exactly one other room (no roommates). The students give what rooms are acceptable, and many solutions are possible. Consider the below problem where we have students (A, B, C and D) and we want to match them to rooms (1,2,3,4). The list of acceptable rooms for each student is given below. edgelist_df = pd . DataFrame ({ 'node1' :[ 'A' , 'A' , 'B' , 'B' , 'C' , 'C' , 'D' , 'D' ], 'node2' :[ 1 , 2 , 2 , 3 , 3 , 4 , 4 , 1 ]}) edgelist_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } node1 node2 0 A 1 1 A 2 2 B 2 3 B 3 4 C 3 5 C 4 6 D 4 7 D 1 This can be represented as a bipartite graph as follows g = nx . Graph () for i , elrow in edgelist_df . iterrows (): g . add_edge ( elrow [ 0 ], elrow [ 1 ]) # Make two sets in bipartite and get positions for the same left , right = nx . bipartite . sets ( g ) pos = {} # Update position for node from each group for i , node in enumerate ( sorted ( list ( left ))): g . add_node ( node , pos = ( 0 , i )) for i , node in enumerate ( sorted ( list ( right ))): g . add_node ( node , pos = ( 1 , i )) nx . draw ( g , pos = nx . get_node_attributes ( g , 'pos' ), with_labels = True ) Visually, we can find a couple of solutions, for example: - A:1, B:2, C:3, D:4 - A:2, B:3, C:4, D:1 In large graphs visual analysis might be difficult, and in such situations Halls theorem is useful to identify if matching is possible.","title":"Unweighted bipartite mapping"},{"location":"Python/Bipartite%20matching/#halls-theorem","text":"But before Halls theorem, let us look at constricted set. Constricted set : A constricted set is a subset of edges (on either side) whose neighbours are smaller than the subset. For examle, if we have two students (subset of students) who give only one room (same room) in the list, then the size of the students is 2 and the size of the rooms is one, and no matching can be done. Halls theorem states that for a mapping to exist, there should be no constricted set. Augmenting paths 1. Select any random matching of unmatched nodes. 2. Switch to the augumented paths if it exists. If it doesnt exist, then we have a constricted set and we cannot do matching. 3. Repeat until all left nodes are matched to one right node. This is implemented in NetworkX as follows: # Select random edges selected_edges = [] for left_node in left : # For a left node, select a random node in the right list_of_nodes = list ( g . edges ( left_node )) random_node = np . random . randint ( len ( list_of_nodes )) selected_edges . append ( list_of_nodes [ random_node ]) selected_edges [('D', 4), ('C', 4), ('A', 2), ('B', 3)] Selected edges are shown in green for edge in g . edges : if edge in selected_edges or ( edge [ 1 ], edge [ 0 ]) in selected_edges : g . add_edge ( edge [ 0 ], edge [ 1 ], color = 'g' , weight = 10 ) else : g . add_edge ( edge [ 0 ], edge [ 1 ], color = 'b' , weight = 0.1 ) nx . draw ( g , pos = nx . get_node_attributes ( g , 'pos' ), with_labels = True , edge_color = nx . get_edge_attributes ( g , 'color' ) . values ()) We can see that this is not matching as node C and D are mapped to 4. This can be resolved by moving through the augumented paths for the C-4 node. For the C-4 node, moving thru the augumented path selects the C-3 and D-4 node. Following the smae process with with B-3 and A-3 nodes, we get: selected_edges = {( 1 , 'A' ), ( 2 , 'B' ), ( 3 , 'C' ), ( 4 , 'D' )} for edge in g . edges : if edge in selected_edges or ( edge [ 1 ], edge [ 0 ]) in selected_edges : g . add_edge ( edge [ 0 ], edge [ 1 ], color = 'g' , weight = 10 ) else : g . add_edge ( edge [ 0 ], edge [ 1 ], color = 'b' , weight = 0.1 ) nx . draw ( g , pos = nx . get_node_attributes ( g , 'pos' ), with_labels = True , edge_color = nx . get_edge_attributes ( g , 'color' ) . values ()) The final mapping is shown in green nx . algorithms . bipartite . matching . hopcroft_karp_matching ( g , top_nodes = list ( set ( edgelist_df . node1 ))) {'D': 4, 'C': 3, 'B': 2, 'A': 1, 1: 'A', 2: 'B', 3: 'C', 4: 'D'}","title":"Halls theorem"},{"location":"Python/Bipartite%20matching/#weighted-bipartite-mapping","text":"In the previous problem, we tried to find a perfect maching in an unweighted graph. What if every edge in the graph has certain weight attached to it. The weights could be quality index (for student-dorm matching) or valuations in a market etc. Consider the suppliers-buyers example for a housing market as shown below. We have three suppliers, A, B and C and three buyers (x, y and z). The valuation for each of the sellers is also given. For example, buyer x values house A with 12, house B with 4 and house C with 2. sellers = [ 'A' , 'B' , 'C' ] buyers = [ 'x' , 'y' , 'z' ] valuations_for_buyers = [[ 12 , 4 , 2 ], [ 8 , 7 , 6 ], [ 7 , 5 , 2 ]] sellers_price = [ 0 , 0 , 0 ] g = nx . Graph () pos = {} # Update position for node from each group for i , node in enumerate ( sellers ): g . add_node ( node , pos = ( 0 , len ( sellers ) - i )) for i , node in enumerate ( buyers ): g . add_node ( node , pos = ( 1 , len ( buyers ) - i )) # Plot text for the buyers for i , buyer in enumerate ( buyers ): plt . text ( 1.1 , len ( buyers ) - i , s = valuations_for_buyers [ i ], horizontalalignment = 'left' ) # Plot text for the sellers for i , buyer in enumerate ( buyers ): plt . text ( - 0.1 , len ( buyers ) - i , s = sellers_price [ i ], horizontalalignment = 'right' ) nx . draw ( g , pos = nx . get_node_attributes ( g , 'pos' ), with_labels = True ) What we want to achieve is clearing of the market. Clearing happens when all houses are sold to one buyer, and every buyer bought one house. This can be done using an auction algorithm. 1. Sellers quote a price 2. Buyers calculate utility: Net valuation (payoff) = Gross Valuation - Price charged by seller 3. Buyers select the object that has highest payoff 4. If the market is not cleared, the sellers who have more than one offer (overdetermined) will increase the price by one unit, and the process is repeated. # Function to pick the supplier with the maximumm utility def match_to_maximum_utility ( sellers , buyers , valuation , price ): max_utility_sellers = {} for buyer_index in range ( len ( buyers )): max_utility = 0 for seller_index in range ( len ( sellers )): if ( max_utility < valuation [ buyer_index ][ seller_index ] - price [ seller_index ]): max_utility = valuation [ buyer_index ][ seller_index ] - price [ seller_index ] max_utility_sellers [ buyers [ buyer_index ]] = [ sellers [ seller_index ]] elif ( max_utility == valuation [ buyer_index ][ seller_index ] - price [ seller_index ]): max_utility_sellers [ buyers [ buyer_index ]] . append ( sellers [ seller_index ]) return max_utility_sellers Assuming that the initial price set by the seller is zero (scaled to zero - displayed beside the node), buyer x calculates the following utility: - For A: 12-0 = 12 - For B: 4-0 = 4 - For C: 2-0 = 2 As the utility of A is the highest, x will chose A. Similarly, B and C will also choose A. max_util = match_to_maximum_utility ( sellers , buyers , valuations_for_buyers , sellers_price ) max_util {'x': ['A'], 'y': ['A'], 'z': ['A']} Plotting the selection, we see that A is overdetermined. def plot_max_utility_graph ( sellers , buyers , valuations_for_buyers , sellers_price , max_util ): g = nx . Graph () pos = {} # Update position for node from each group for i , node in enumerate ( sellers ): g . add_node ( node , pos = ( 0 , len ( sellers ) - i )) for i , node in enumerate ( buyers ): g . add_node ( node , pos = ( 1 , len ( buyers ) - i )) # Make edges for key , values in max_util . items (): for value in values : g . add_edge ( key , value ) # Plot text for the buyers for i , buyer in enumerate ( buyers ): plt . text ( 1.1 , len ( buyers ) - i , s = valuations_for_buyers [ i ], horizontalalignment = 'left' ) # Plot text for the sellers for i , buyer in enumerate ( buyers ): plt . text ( - 0.1 , len ( buyers ) - i , s = sellers_price [ i ], horizontalalignment = 'right' ) nx . draw ( g , pos = nx . get_node_attributes ( g , 'pos' ), with_labels = True ) plt . show () plot_max_utility_graph ( sellers , buyers , valuations_for_buyers , sellers_price , max_util ) For the overdetermined edge A, we increase the price by one unit. def get_overdetermined_edge_and_increase_price ( sellers , sellers_price , max_util ): from collections import Counter counts = dict ( Counter ( sum ( max_util . values (), []))) over_determined_list = [] for key , value in counts . items (): if ( value > 1 ): over_determined_list . append ( key ) sellers_price [ sellers . index ( key )] += 1 print ( 'Nodes' , over_determined_list , 'are over determined. Added 1 to the price for the suppliers' ) return counts get_overdetermined_edge_and_increase_price ( sellers , sellers_price , max_util ) Nodes ['A'] are over determined. Added 1 to the price for the suppliers {'A': 3} We then contiue this process until the market is cleared. no_of_sellers_selected = 0 while ( no_of_sellers_selected != len ( sellers )): max_util = match_to_maximum_utility ( sellers , buyers , valuations_for_buyers , sellers_price ) plot_max_utility_graph ( sellers , buyers , valuations_for_buyers , sellers_price , max_util ) counts = get_overdetermined_edge_and_increase_price ( sellers , sellers_price , max_util ) no_of_sellers_selected = len ( counts ) # need not always be the case, check Nodes ['A'] are over determined. Added 1 to the price for the suppliers Nodes ['A', 'B'] are over determined. Added 1 to the price for the suppliers Nodes ['A', 'B'] are over determined. Added 1 to the price for the suppliers We can see that for costs (A:3, B:1, C:0), the market can be cleared with buyer x choosing A, y choosing C and z choosing B. This is the maximum weight perfect matching.","title":"Weighted bipartite mapping"},{"location":"Python/Bipartite%20matching/#matching-with-preferences","text":"In the previous scenario, we had weights on the edges which indicated the utility. In this case we will look at matching where we have preferences in a ranked order. This is more natural way in many scenarios, like students preference to universities/universities selecting students, dating scenarios etc. This was originally implemented by Al Roth for matching hospitals and residency. Our goal is to clear the market, but also have a stable matching. So, what is a stable matching? Stable matching : Stability is an eqvilibrium when no pair on ether side has an incentive to devaite from the mapping. Let us understand this using an example. Take the dating scenario for example. On the left hand side we have men and on the right hand side we have women. All men rank women in strict order and all women rank men in strict order. In a stable matching, no pair of nodes (male-female) prefers each other to their curently assigned partners.","title":"Matching with preferences"},{"location":"Python/Bipartite%20matching/#gale-shapley-algorithm","text":"Let us say that are n players on both sides with males(m) on one side and women(w) on another side. The algorithm is as follows: 1. Every unmatched male (m) proposses to their first preference available. 2. If the proposed women (w) is unmatched, w accepts. If the women is already matched and the m has higher preference for w, w switches. Else, previous mapping remains. 3. This process continues until there is stability Lets take an example with three males and three females. The preferences are mentioned at the side of the node in a list. For example, m2 has a preference w2, followed by w3 and then w1. Similarly w2 has a preference of m3, followed by m1 and then m2. males = [ 'm1' , 'm2' , 'm3' ] females = [ 'w1' , 'w2' , 'w3' ] male_preferences = [[ 'w1' , 'w2' , 'w3' ], [ 'w2' , 'w3' , 'w1' ], [ 'w2' , 'w3' , 'w1' ]] female_preferences = [[ 'm2' , 'm3' , 'm1' ], [ 'm3' , 'm1' , 'm2' ], [ 'm1' , 'm2' , 'm3' ]] def match_next_male ( male_index , males , females , male_preferences , female_preferences ): for female in male_preferences [ male_index ]: if ( female not in current_mapping . values ()): current_mapping [ males [ male_index ]] = female return current_mapping elif ( female in current_mapping . values ()): current_mapping_inverse = dict ( zip ( current_mapping . values (), current_mapping . keys ())) current_male_for_the_female = current_mapping_inverse [ female ] if ( female_preferences [ males . index ( current_male_for_the_female )] > female_preferences [ male_index ]): current_mapping [ males [ male_index ]] = female current_mapping . pop ( current_male_for_the_female ) return current_mapping def plot_max_utility_graph ( males , females , male_preferences , female_preferences , current_mapping ): g = nx . Graph () pos = {} # Update position for node from each group for i , node in enumerate ( males ): g . add_node ( node , pos = ( 0 , len ( males ) - i )) for i , node in enumerate ( females ): g . add_node ( node , pos = ( 1 , len ( females ) - i )) # Make edges for key , value in current_mapping . items (): g . add_edge ( key , value ) # Plot text for the males for i , male in enumerate ( males ): plt . text ( - 0.1 , len ( males ) - i , s = male_preferences [ i ], horizontalalignment = 'right' ) # Plot text for the females for i , female in enumerate ( females ): plt . text ( 1.1 , len ( females ) - i , s = female_preferences [ i ], horizontalalignment = 'left' ) nx . draw ( g , pos = nx . get_node_attributes ( g , 'pos' ), with_labels = True ) plt . show () print ( '_____________________________________________________________________________' ) current_mapping = {} while ( len ( current_mapping ) != len ( males )): for male_index in range ( len ( males )): if ( current_mapping . get ( males [ male_index ]) is None ): current_mapping = match_next_male ( male_index , males , females , male_preferences , female_preferences ) plot_max_utility_graph ( males , females , male_preferences , female_preferences , current_mapping ) _____________________________________________________________________________ _____________________________________________________________________________ _____________________________________________________________________________ _____________________________________________________________________________ The process goes on as follows: 1. m1 proposes to w1 as w1 has maximum rank and as w1 is unselected, w1 accepts. We create an edge between them. 2. m2 proposes to w2 as w2 has maximum rank, and as w2 is unselected, w2 accepts. We crreate a edge between them. 3. m3 also proposes to w2. As w2 is already selected, it checks the preference of the current selection (m2) to m3. As m3 has better preference, w2 switches from m2 to m3. The edge between m2 and w2 is reoved and a new edge between m3 and w2 is created. 4. m2 is currently unmapped, and selects the next best preference, which is w3. As w3 is unselected, w3 accepts. 5. This clears the market and the process stops. This is stable mapping (from the men's side). These are the common matching techniques that exist.","title":"Gale Shapley Algorithm"},{"location":"Python/Community%20detection/","text":"MathJax.Hub.Config({ extensions: [\"tex2jax.js\",\"TeX/AMSmath.js\",\"TeX/AMSsymbols.js\"], jax: [\"input/TeX\", \"output/HTML-CSS\"], tex2jax: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ], }, \"HTML-CSS\": { availableFonts: [\"TeX\"] } }); Community detection \u00b6 Author: Achyuthuni Sri Harsha A community is a group of people who are homogeneous in some features. Marketers want to find communities so that they can target them. It is not trivial to identify communities from a given network. Community detection deals with identifying networks based only on the network topology. nodes in the community are closer than the nodes outside the community. What is closeness? To define closeness, we need to 1. Define distance metric based on network topology 2. Quantify nodes in a community are closer wth each other than nodes outside the community There are two approaches, bottom-up and top-down. Girwan Newman Algorithm \u00b6 The most popular algorithm for network community detection is the Girvan-Newman algorithm. It is a top down approach where we take the whole network and try to break it into two communities. This can be continued till the bottom. Steps: 1. Define betweenneess mesure for each edge 2. Find the edge with maximum betweenness and remove it (The edge most in-between in the network connects up most number of pairs of nodes) 3. Recalculate and repeat The example that we are using in this blog is the Zachary Karate club . It contains 34 members, and after an internal conflict, the club split into two. Our goal is to find if we can predict out how they split based on communities. import matplotlib.pyplot as plt % matplotlib inline import networkx as nx G = nx . karate_club_graph () nx . draw_kamada_kawai ( G , with_labels = True ) # Calculating the betweenness centrality btw_centrality = nx . algorithms . centrality . edge_betweenness_centrality ( G ) # Sorting based on the betweenness centraliy and displaying the first 10 edges. sorted ( btw_centrality . items (), key = lambda item : item [ 1 ], reverse = True )[ 0 : 10 ] [((0, 31), 0.1272599949070537), ((0, 6), 0.07813428401663695), ((0, 5), 0.07813428401663694), ((0, 2), 0.0777876807288572), ((0, 8), 0.07423959482783014), ((2, 32), 0.06898678663384543), ((13, 33), 0.06782389723566191), ((19, 33), 0.05938233879410351), ((0, 11), 0.058823529411764705), ((26, 33), 0.0542908072319837)] def girwan_newman ( G , no_of_components_to_split ): while ( no_of_components_to_split > nx . algorithms . components . number_connected_components ( G )): # Calculate the betweenness centrality btw_centrality = nx . algorithms . centrality . edge_betweenness_centrality ( G ) # sort based on betweenness centrality sorted_edges = sorted ( btw_centrality . items (), key = lambda item : item [ 1 ], reverse = True )[ 0 ] print ( 'Removing the edge' , sorted_edges ) # remove edge which has highest centrality G . remove_edge ( * sorted_edges [ 0 ]) # Check if graph is split if ( no_of_components_to_split <= nx . algorithms . components . number_connected_components ( G )): # Plot the graph with both the nodes having different colors nx . draw_spring ( G , with_labels = True ) # return list of nodes in each community list_of_nodes = [ c for c in sorted ( nx . connected_components ( G ), key = len , reverse = True )] return list_of_nodes G = nx . karate_club_graph () communities = girwan_newman ( G , 2 ) communities Removing the edge ((0, 31), 0.1272599949070537) Removing the edge ((0, 2), 0.11924273983097515) Removing the edge ((0, 8), 0.13782067605597018) Removing the edge ((13, 33), 0.14617273782105492) Removing the edge ((19, 33), 0.21966651886437982) Removing the edge ((2, 32), 0.1786195286195287) Removing the edge ((1, 30), 0.25601957954899124) Removing the edge ((1, 2), 0.1947415329768271) Removing the edge ((2, 3), 0.19191919191919182) Removing the edge ((2, 7), 0.25445632798573975) Removing the edge ((2, 13), 0.5080213903743315) [{2, 8, 9, 14, 15, 18, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33}, {0, 1, 3, 4, 5, 6, 7, 10, 11, 12, 13, 16, 17, 19, 21}] We can see that after removing 11 edges based on the maximum betweenness values, the club splits into two parts. This can be seen below. These groups matches with what actually happened in the karate club. # Reinstanciste G if necessary G = nx . karate_club_graph () # Colors for edges colors_for_edges = 'rbgoy' pos = nx . spring_layout ( G ) for i , nodes in enumerate ( communities ): for node in nodes : G . add_node ( node , node_color = colors_for_edges [ i ]) nx . draw_kamada_kawai ( G , with_labels = True , node_color = list ( nx . get_node_attributes ( G , 'node_color' ) . values ())) NetworkX has a inbuilt function to calculate the same split. karate_club_split = nx . algorithms . community . centrality . girvan_newman ( G ) karate_club_split_tuple = tuple ( sorted ( c ) for c in next ( karate_club_split )) karate_club_split_tuple ([0, 1, 3, 4, 5, 6, 7, 10, 11, 12, 13, 16, 17, 19, 21], [2, 8, 9, 14, 15, 18, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]) colors = 'rgb' for i , nodes in enumerate ( karate_club_split_tuple ): nx . draw_networkx_nodes ( G , pos = pos , nodelist = nodes , node_color = colors [ i ]) nx . draw_networkx_edges ( G , pos = pos ) Ratio cut method \u00b6 A more complex and advanced method is the ratio cut method. A cut in a graph is a set of edges, such that if we remove them, the network breaks up into two components. The procedure is as follows: 1. Find the minimum cut among all the pair of nodes 2. Repeat over each of the two parts cut The minimum cut can be found using Ratio Cut parameter. Minimum cut wil have the least ratio cut among all the other cuts. $$ Ratio\\,Cut\\, (P_{Red}) = \\frac{1}{number\\, of\\, partitions}\\times (\\frac{no\\, of\\, edges\\, in\\, partition1}{no\\, of\\, nodes\\, in\\, partition1} + \\frac{no\\, of\\, edges\\, in\\, partition2}{no\\, of\\, nodes\\, in\\, partition2} + ..) $$ This can be done quickly using an inbuilt function in NetworkX from networkx.algorithms import community communities = community . kernighan_lin_bisection ( G , max_iter = 100 ) pos = nx . spring_layout ( G ) nx . draw ( G , pos , with_labels = True , node_size = 100 , node_color = 'w' , node_shape = '.' ) for i in range ( len ( communities )): nx . draw_networkx_nodes ( G , pos , nodelist = communities [ i ], node_color = colors [ i ]) Other methods \u00b6 There are many other methods that can be used to identify communities, one of which is based on modularity. Modularity is defined as the \"distance\" from a random graph. essentially we will be comparing the number of edges with a community and the expected number of edges with a random graph. In NetworkX, we have a inbuilt function to implement this. # How many ideal nodes: Louvain package from community import community_louvain # https://en.wikipedia.org/wiki/Louvain_method community_louvain . best_partition ( G ) {0: 0, 1: 0, 2: 0, 3: 0, 4: 3, 5: 3, 6: 3, 7: 0, 8: 1, 9: 0, 10: 3, 11: 0, 12: 0, 13: 0, 14: 1, 15: 1, 16: 3, 17: 0, 18: 1, 19: 0, 20: 1, 21: 0, 22: 1, 23: 1, 24: 2, 25: 2, 26: 1, 27: 1, 28: 2, 29: 1, 30: 1, 31: 2, 32: 1, 33: 1}","title":"Community detection (Python)"},{"location":"Python/Community%20detection/#community-detection","text":"Author: Achyuthuni Sri Harsha A community is a group of people who are homogeneous in some features. Marketers want to find communities so that they can target them. It is not trivial to identify communities from a given network. Community detection deals with identifying networks based only on the network topology. nodes in the community are closer than the nodes outside the community. What is closeness? To define closeness, we need to 1. Define distance metric based on network topology 2. Quantify nodes in a community are closer wth each other than nodes outside the community There are two approaches, bottom-up and top-down.","title":"Community detection"},{"location":"Python/Community%20detection/#girwan-newman-algorithm","text":"The most popular algorithm for network community detection is the Girvan-Newman algorithm. It is a top down approach where we take the whole network and try to break it into two communities. This can be continued till the bottom. Steps: 1. Define betweenneess mesure for each edge 2. Find the edge with maximum betweenness and remove it (The edge most in-between in the network connects up most number of pairs of nodes) 3. Recalculate and repeat The example that we are using in this blog is the Zachary Karate club . It contains 34 members, and after an internal conflict, the club split into two. Our goal is to find if we can predict out how they split based on communities. import matplotlib.pyplot as plt % matplotlib inline import networkx as nx G = nx . karate_club_graph () nx . draw_kamada_kawai ( G , with_labels = True ) # Calculating the betweenness centrality btw_centrality = nx . algorithms . centrality . edge_betweenness_centrality ( G ) # Sorting based on the betweenness centraliy and displaying the first 10 edges. sorted ( btw_centrality . items (), key = lambda item : item [ 1 ], reverse = True )[ 0 : 10 ] [((0, 31), 0.1272599949070537), ((0, 6), 0.07813428401663695), ((0, 5), 0.07813428401663694), ((0, 2), 0.0777876807288572), ((0, 8), 0.07423959482783014), ((2, 32), 0.06898678663384543), ((13, 33), 0.06782389723566191), ((19, 33), 0.05938233879410351), ((0, 11), 0.058823529411764705), ((26, 33), 0.0542908072319837)] def girwan_newman ( G , no_of_components_to_split ): while ( no_of_components_to_split > nx . algorithms . components . number_connected_components ( G )): # Calculate the betweenness centrality btw_centrality = nx . algorithms . centrality . edge_betweenness_centrality ( G ) # sort based on betweenness centrality sorted_edges = sorted ( btw_centrality . items (), key = lambda item : item [ 1 ], reverse = True )[ 0 ] print ( 'Removing the edge' , sorted_edges ) # remove edge which has highest centrality G . remove_edge ( * sorted_edges [ 0 ]) # Check if graph is split if ( no_of_components_to_split <= nx . algorithms . components . number_connected_components ( G )): # Plot the graph with both the nodes having different colors nx . draw_spring ( G , with_labels = True ) # return list of nodes in each community list_of_nodes = [ c for c in sorted ( nx . connected_components ( G ), key = len , reverse = True )] return list_of_nodes G = nx . karate_club_graph () communities = girwan_newman ( G , 2 ) communities Removing the edge ((0, 31), 0.1272599949070537) Removing the edge ((0, 2), 0.11924273983097515) Removing the edge ((0, 8), 0.13782067605597018) Removing the edge ((13, 33), 0.14617273782105492) Removing the edge ((19, 33), 0.21966651886437982) Removing the edge ((2, 32), 0.1786195286195287) Removing the edge ((1, 30), 0.25601957954899124) Removing the edge ((1, 2), 0.1947415329768271) Removing the edge ((2, 3), 0.19191919191919182) Removing the edge ((2, 7), 0.25445632798573975) Removing the edge ((2, 13), 0.5080213903743315) [{2, 8, 9, 14, 15, 18, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33}, {0, 1, 3, 4, 5, 6, 7, 10, 11, 12, 13, 16, 17, 19, 21}] We can see that after removing 11 edges based on the maximum betweenness values, the club splits into two parts. This can be seen below. These groups matches with what actually happened in the karate club. # Reinstanciste G if necessary G = nx . karate_club_graph () # Colors for edges colors_for_edges = 'rbgoy' pos = nx . spring_layout ( G ) for i , nodes in enumerate ( communities ): for node in nodes : G . add_node ( node , node_color = colors_for_edges [ i ]) nx . draw_kamada_kawai ( G , with_labels = True , node_color = list ( nx . get_node_attributes ( G , 'node_color' ) . values ())) NetworkX has a inbuilt function to calculate the same split. karate_club_split = nx . algorithms . community . centrality . girvan_newman ( G ) karate_club_split_tuple = tuple ( sorted ( c ) for c in next ( karate_club_split )) karate_club_split_tuple ([0, 1, 3, 4, 5, 6, 7, 10, 11, 12, 13, 16, 17, 19, 21], [2, 8, 9, 14, 15, 18, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]) colors = 'rgb' for i , nodes in enumerate ( karate_club_split_tuple ): nx . draw_networkx_nodes ( G , pos = pos , nodelist = nodes , node_color = colors [ i ]) nx . draw_networkx_edges ( G , pos = pos )","title":"Girwan Newman Algorithm"},{"location":"Python/Community%20detection/#ratio-cut-method","text":"A more complex and advanced method is the ratio cut method. A cut in a graph is a set of edges, such that if we remove them, the network breaks up into two components. The procedure is as follows: 1. Find the minimum cut among all the pair of nodes 2. Repeat over each of the two parts cut The minimum cut can be found using Ratio Cut parameter. Minimum cut wil have the least ratio cut among all the other cuts. $$ Ratio\\,Cut\\, (P_{Red}) = \\frac{1}{number\\, of\\, partitions}\\times (\\frac{no\\, of\\, edges\\, in\\, partition1}{no\\, of\\, nodes\\, in\\, partition1} + \\frac{no\\, of\\, edges\\, in\\, partition2}{no\\, of\\, nodes\\, in\\, partition2} + ..) $$ This can be done quickly using an inbuilt function in NetworkX from networkx.algorithms import community communities = community . kernighan_lin_bisection ( G , max_iter = 100 ) pos = nx . spring_layout ( G ) nx . draw ( G , pos , with_labels = True , node_size = 100 , node_color = 'w' , node_shape = '.' ) for i in range ( len ( communities )): nx . draw_networkx_nodes ( G , pos , nodelist = communities [ i ], node_color = colors [ i ])","title":"Ratio cut method"},{"location":"Python/Community%20detection/#other-methods","text":"There are many other methods that can be used to identify communities, one of which is based on modularity. Modularity is defined as the \"distance\" from a random graph. essentially we will be comparing the number of edges with a community and the expected number of edges with a random graph. In NetworkX, we have a inbuilt function to implement this. # How many ideal nodes: Louvain package from community import community_louvain # https://en.wikipedia.org/wiki/Louvain_method community_louvain . best_partition ( G ) {0: 0, 1: 0, 2: 0, 3: 0, 4: 3, 5: 3, 6: 3, 7: 0, 8: 1, 9: 0, 10: 3, 11: 0, 12: 0, 13: 0, 14: 1, 15: 1, 16: 3, 17: 0, 18: 1, 19: 0, 20: 1, 21: 0, 22: 1, 23: 1, 24: 2, 25: 2, 26: 1, 27: 1, 28: 2, 29: 1, 30: 1, 31: 2, 32: 1, 33: 1}","title":"Other methods"},{"location":"Python/Demonstrating%20online%20learning/","text":"MathJax.Hub.Config({ extensions: [\"tex2jax.js\",\"TeX/AMSmath.js\",\"TeX/AMSsymbols.js\"], jax: [\"input/TeX\", \"output/HTML-CSS\"], tex2jax: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ], }, \"HTML-CSS\": { availableFonts: [\"TeX\"] } }); Streaming machine learning \u00b6 Author: Achyuthuni Sri Harsha Batch learning : In batch machine learning, we use one dataset to train a model, and we deploy the model to predict on new data. This assumes that the dataset in which the model is trained is a proper representative sample of the population. This model is assumed as a static object. In order to learn from new data, the model has to be re-trained from scratch. This is the most common form of deploying models. Online learning : Some of the machine learning models that we know can be modified to learn on a single datapoint (row). When we can learn from a single datapoint, we can learn incrementally from new datapoints. Data is considered as a stream. Once the model is trained, we need not store the historic training set. The model is also more up to date. If the data's distribution happens over time, the model will be able to handle it (drift)[1]. Where can we use them? They are most useful in scenarios where new data and patterns are constantly arriving. eg: 1. Spam filtering 2. Recommendation engines (news feed predictions) 3. Financial transactions 4. Low compute power (only one data point exists in memory as we train using one data point only) Issues 1. New and not a lot of experience 2. Very few tools and packages 3. All algorithms do not have an online version. Kernel SVMs are impossible to fit on a streaming dataset. Likewise, CART and ID3 decision trees can\u2019t be trained online. However, lesser-known online approximations exist, such as random Fourier features for SVMs and Hoeffding trees for decision trees. 4. Slower than batch learning to reach steady state in real life (It is computationally faster by more than an order of magnitude) 5. Do not guarantee that models learnt are similar to the ones obtained in batch mode(some models). Some models do not guarentee of achieving steady state. 6. Overfitting Similarities We have the same limitations of machine learning, such as: 1. We need to do proper preprocessing 2. We need to do feature engineering as usual 3. The concepts of ensembles, feature extraction, feature selection, imbalanced classes, multiclass etc are same Preprocessing steps \u00b6 How do we preprocess data when we are streaming? How do we impute the null values by mean if we do not have complete data? How do we identify outliers when working on one row at a time? How can we do one-hot encoding when we don't know what classes are present overall? The package river is a handy package for online learning. It has a lot of pre-defined preprocessing functions. Let us look at some of them: from river import preprocessing dir ( preprocessing )[ 0 : 12 ] ['AdaptiveStandardScaler', 'Binarizer', 'FeatureHasher', 'LDA', 'MaxAbsScaler', 'MinMaxScaler', 'Normalizer', 'OneHotEncoder', 'PreviousImputer', 'RobustScaler', 'StandardScaler', 'StatImputer'] There are six functions for scaling and normalizing data. They are: 1. AdaptiveStandardScalar 2. MaxAbsScalar 3. MinMaxScalar 4. Normalizer 5. RobustScalar 6. StandardScaler For example, let us look at the documentation for Standard Scaler It scales the data to have zero mean and unit variance. Under the hood, a running mean and a running variance are maintained. The scaling is slightly different than when scaling the data in batch because the exact means and variances are not known in advance. However, this doesn't have a detrimental impact on performance in the long run. Let us look at an example: import numpy as np import matplotlib.pyplot as plt import pandas as pd import random random . seed ( 1995 ) from sklearn.datasets import load_iris iris = load_iris () data1 = pd . DataFrame ( data = np . c_ [ iris [ 'data' ], iris [ 'target' ]], columns = iris [ 'feature_names' ] + [ 'target' ]) data1 = data1 . query ( 'target < 2' ) . sample ( frac = 1 ) def data_feed ( df_datafeed ): # Generator function to give the next candidate for _ctr in range ( len ( df_datafeed )): yield df_datafeed . iloc [ _ctr ] from river import compose from river import linear_model from river import preprocessing preprocessing_model = compose . Pipeline ( preprocessing . StandardScaler (), linear_model . LogisticRegression ()) data_stream = data_feed ( data1 . loc [:, data1 . columns != 'target' ]) for n in range ( 10 ): data_point = next ( data_stream ) . to_frame () . transpose () transformed_data = preprocessing_model . transform_one ( data_point . iloc [ 0 ,:]) print ( '------------------------' ) print ( data_point ) print ( transformed_data ) ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 67 5.8 2.7 4.1 1.0 {'sepal length (cm)': 0.0, 'sepal width (cm)': 0.0, 'petal length (cm)': 0.0, 'petal width (cm)': 0.0} ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 96 5.7 2.9 4.2 1.3 {'sepal length (cm)': -1.0, 'sepal width (cm)': 1.000000000000001, 'petal length (cm)': 0.9999999999999956, 'petal width (cm)': 1.0000000000000004} ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 19 5.1 3.8 1.5 0.3 {'sepal length (cm)': -1.4018260516446994, 'sepal width (cm)': 1.3934660285832352, 'petal length (cm)': -1.4134589797160622, 'petal width (cm)': -1.352447383098741} ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 5 5.4 3.9 1.7 0.4 {'sepal length (cm)': -0.36514837167010933, 'sepal width (cm)': 1.0830277015004253, 'petal length (cm)': -0.9198021534721369, 'petal width (cm)': -0.8427009716003844} ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 64 5.6 2.9 3.6 1.3 {'sepal length (cm)': 0.32232918561015234, 'sepal width (cm)': -0.6740938478604231, 'petal length (cm)': 0.49202037860731096, 'petal width (cm)': 1.019130320146575} ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 87 6.3 2.3 4.4 1.3 {'sepal length (cm)': 1.7636409634199253, 'sepal width (cm)': -1.3539553245018423, 'petal length (cm)': 0.9642101587457326, 'petal width (cm)': 0.8589556903873334} ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 80 5.5 2.4 3.8 1.1 {'sepal length (cm)': -0.37242264987106416, 'sepal width (cm)': -0.9985160994941403, 'petal length (cm)': 0.4205955120960296, 'petal width (cm)': 0.3575992699260759} ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 98 5.1 2.5 3.0 1.1 {'sepal length (cm)': -1.259494647504126, 'sepal width (cm)': -0.743358098059264, 'petal length (cm)': -0.27274857904612027, 'petal width (cm)': 0.331861655799986} ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 15 5.7 4.4 1.5 0.4 {'sepal length (cm)': 0.3503113654141663, 'sepal width (cm)': 1.8442002991885438, 'petal length (cm)': -1.3918304919158082, 'petal width (cm)': -1.282736189026269} ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 23 5.1 3.3 1.7 0.5 {'sepal length (cm)': -1.1921469919530685, 'sepal width (cm)': 0.28047526828011227, 'petal length (cm)': -1.077226017628646, 'petal width (cm)': -0.9305415914315355} Apart from preprocessing, using river, we can perform 1. Feature extraction/selection 2. Ensembles 3. Storing running statistics 4. Building regression and classification models 5. Time series 6. Anamoly detection 7. Clustering There are 6 types of machine learning models that we can build using river. They are: 1. Linear based (linear and logistic regression) 2. Tree based (Decision trees, Hoeffding trees) 3. Nearest neighbours based 4. Bayesian models 5. Neural Networks 6. Ensemble based models 7. Others from river import linear_model , naive_bayes , tree , neural_net , neighbors , expert , ensemble print ( 'Linear models' , dir ( linear_model )[ 0 : 7 ]) print ( 'Tree based modles' , dir ( tree )[ 0 : 6 ]) print ( 'Bayesian models' , dir ( naive_bayes )[ 0 : 4 ]) print ( 'Specialised models' , dir ( expert )[ 0 : 6 ]) print ( 'Ensemble mmodels' , dir ( ensemble )[ 0 : 8 ]) Linear models ['ALMAClassifier', 'LinearRegression', 'LogisticRegression', 'PAClassifier', 'PARegressor', 'Perceptron', 'SoftmaxRegression'] Tree based modles ['ExtremelyFastDecisionTreeClassifier', 'HoeffdingAdaptiveTreeClassifier', 'HoeffdingAdaptiveTreeRegressor', 'HoeffdingTreeClassifier', 'HoeffdingTreeRegressor', 'LabelCombinationHoeffdingTreeClassifier'] Bayesian models ['BernoulliNB', 'ComplementNB', 'GaussianNB', 'MultinomialNB'] Specialised models ['EWARegressor', 'EpsilonGreedyRegressor', 'StackingClassifier', 'SuccessiveHalvingClassifier', 'SuccessiveHalvingRegressor', 'UCBRegressor'] Ensemble mmodels ['ADWINBaggingClassifier', 'AdaBoostClassifier', 'AdaptiveRandomForestClassifier', 'AdaptiveRandomForestRegressor', 'BaggingClassifier', 'BaggingRegressor', 'LeveragingBaggingClassifier', 'SRPClassifier'] Modelling (under the hood) \u00b6 There are two types of streaming models, those which are entirely streaming and pseudo-online models. Pseudo-online models use a small batch of data to build the models, while completely online models build the models using only one data point.[3] Pseudo online models \u00b6 There are many theorems in statistics which can help us to bound the error of a metric between two variables. Hoeffding bound is one such theorem. \"Consider a real-valued random variable r whose range is R (e.g., for a probability the range is one, and for an information gain the range is log c, where c is the number of classes). Suppose we have made n independent observations of this variable, and computed their mean \\(\\bar r\\) . The Hoeffding bound states that, with probability 1 \u2212 \u03b4, the true mean of the variable is at least \\(\\bar r \u2212 \\epsilon\\) , where \\(\\epsilon = \\sqrt{\\frac{R^2ln(\\frac{1}{\\lambda})}{2n}}\\) .\" This is useful while creating decision trees. Using hoeffding bound, we can identify which feature should we use to split the tree. We can find if a variable has sufficient gini index (or other metrics) greater (lesser) than other variables with a probability below a cutoff probability and split the tree based on that metric. This can be acheived with very few data points which can be deleted after splitting and creating child nodes. This algorithm is called Hoeffding trees algorithm. Completely online models \u00b6 How can we update a model using only one data point? Let us look at Gradient Descent to understand this. In gradient descent, we want to minimise a convex loss function(MSE, regret, etc). As an example, consider the function \\(h(x) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ..\\) (or any convex function). The mean squared error is \\(J(\\theta) = \\frac{1}{2} \\sum_{i=1}^{m} (y_{(i)}-h_{\\theta}(x_{(i)}))^{2}\\) . In gradient descent, we find the \\(\\beta_i\\) that minimises \\(J(\\theta)\\) . In batch model, we consider all the data points that exist to identfy the optimal solution. In streaming learning, we initialise the \\(\\beta_i\\) as 0 and keep incrementally changing the \\(\\beta_i\\) 's using \\( \\(\\beta_{i, t} = \\beta_{i, t-1} + \\alpha \\times \\frac{\\partial}{\\partial \\theta_{j}} J(\\theta)\\) \\) Steps in updating the model \u00b6 In online learning, there are 4 steps[3]. For every new datapoint, we will recursively run the following steps. For \\({\\displaystyle t=1,2,...,T}\\) Learner receives input \\({\\displaystyle x_{t}}\\) Learner outputs \\({\\displaystyle w_{t}}\\) from a fixed convex set \\({\\displaystyle S}\\) Nature sends back a convex loss function \\({\\displaystyle v_{t}:S\\rightarrow \\mathbb {R} }\\) . Learner suffers loss \\({\\displaystyle v_{t}(w_{t})}\\) and updates its model. Implimentation using river \u00b6 Every online machine learning model has the following basic 5 functions: dir ( linear_model . LogisticRegression )[ 50 : 55 ] ['learn_many', 'learn_one', 'predict_many', 'predict_one', 'predict_proba_many'] As the name mentions, learn_one and predict_one learn and predict from one data point, learn_many, predict_many and predict_prob_many learn and predict using multiple data points. River is the result of a merger between creme and scikit-multiflow, and the remaining functions in the library follow a similar pattern to the same. Building a model \u00b6 Using the same data as above, let us build a sample model using river for streaming. from river import compose river_model = compose . Pipeline ( preprocessing . StandardScaler (), tree . HoeffdingTreeClassifier () ) from sklearn.metrics import accuracy_score acc_scores = [] cols_x = [ 'sepal length (cm)' , 'sepal width (cm)' , 'petal length (cm)' , 'petal width (cm)' ] def compute_accuracy ( data , model , truth_col ): predict_all = data . apply ( lambda row : model . predict_one ( row ), axis = 1 ) acc_scores . append ( accuracy_score ( np . array ( predict_all ), data [ truth_col ])) print ( 'Accuracy is ' , acc_scores [ - 1 ]) data_stream = data_feed ( data1 ) for n in range ( 20 ): data_point = next ( data_stream ) print ( data_point ) if ( n > 1 ): predict_one = river_model . predict_one ( data_point [ cols_x ]) print ( 'Current_prediction' , predict_one , data_point [ 'target' ]) river_model . learn_one ( data_point [ cols_x ], data_point [ 'target' ]) compute_accuracy ( data1 , river_model , 'target' ) print ( '------------------------' ) sepal length (cm) 5.8 sepal width (cm) 2.7 petal length (cm) 4.1 petal width (cm) 1.0 target 1.0 Name: 67, dtype: float64 Accuracy is 0.5 ------------------------ sepal length (cm) 5.7 sepal width (cm) 2.9 petal length (cm) 4.2 petal width (cm) 1.3 target 1.0 Name: 96, dtype: float64 Accuracy is 0.5 ------------------------ sepal length (cm) 5.1 sepal width (cm) 3.8 petal length (cm) 1.5 petal width (cm) 0.3 target 0.0 Name: 19, dtype: float64 Current_prediction 1.0 0.0 Accuracy is 0.5 ------------------------ sepal length (cm) 5.4 sepal width (cm) 3.9 petal length (cm) 1.7 petal width (cm) 0.4 target 0.0 Name: 5, dtype: float64 Current_prediction 1.0 0.0 Accuracy is 0.71 ------------------------ sepal length (cm) 5.6 sepal width (cm) 2.9 petal length (cm) 3.6 petal width (cm) 1.3 target 1.0 Name: 64, dtype: float64 Current_prediction 1.0 1.0 Accuracy is 0.79 ------------------------ sepal length (cm) 6.3 sepal width (cm) 2.3 petal length (cm) 4.4 petal width (cm) 1.3 target 1.0 Name: 87, dtype: float64 Current_prediction 1.0 1.0 Accuracy is 0.69 ------------------------ sepal length (cm) 5.5 sepal width (cm) 2.4 petal length (cm) 3.8 petal width (cm) 1.1 target 1.0 Name: 80, dtype: float64 Current_prediction 1.0 1.0 Accuracy is 0.69 ------------------------ sepal length (cm) 5.1 sepal width (cm) 2.5 petal length (cm) 3.0 petal width (cm) 1.1 target 1.0 Name: 98, dtype: float64 Current_prediction 1.0 1.0 Accuracy is 0.69 ------------------------ sepal length (cm) 5.7 sepal width (cm) 4.4 petal length (cm) 1.5 petal width (cm) 0.4 target 0.0 Name: 15, dtype: float64 Current_prediction 1.0 0.0 Accuracy is 0.99 ------------------------ sepal length (cm) 5.1 sepal width (cm) 3.3 petal length (cm) 1.7 petal width (cm) 0.5 target 0.0 Name: 23, dtype: float64 Current_prediction 0.0 0.0 Accuracy is 1.0 ------------------------ sepal length (cm) 4.8 sepal width (cm) 3.1 petal length (cm) 1.6 petal width (cm) 0.2 target 0.0 Name: 30, dtype: float64 Current_prediction 0.0 0.0 Accuracy is 1.0 ------------------------ sepal length (cm) 5.5 sepal width (cm) 2.5 petal length (cm) 4.0 petal width (cm) 1.3 target 1.0 Name: 89, dtype: float64 Current_prediction 1.0 1.0 Accuracy is 1.0 ------------------------ sepal length (cm) 4.5 sepal width (cm) 2.3 petal length (cm) 1.3 petal width (cm) 0.3 target 0.0 Name: 41, dtype: float64 Current_prediction 0.0 0.0 Accuracy is 1.0 ------------------------ sepal length (cm) 6.1 sepal width (cm) 3.0 petal length (cm) 4.6 petal width (cm) 1.4 target 1.0 Name: 91, dtype: float64 Current_prediction 1.0 1.0 Accuracy is 1.0 ------------------------ sepal length (cm) 5.2 sepal width (cm) 3.5 petal length (cm) 1.5 petal width (cm) 0.2 target 0.0 Name: 27, dtype: float64 Current_prediction 0.0 0.0 Accuracy is 1.0 ------------------------ sepal length (cm) 7.0 sepal width (cm) 3.2 petal length (cm) 4.7 petal width (cm) 1.4 target 1.0 Name: 50, dtype: float64 Current_prediction 1.0 1.0 Accuracy is 1.0 ------------------------ sepal length (cm) 5.5 sepal width (cm) 4.2 petal length (cm) 1.4 petal width (cm) 0.2 target 0.0 Name: 33, dtype: float64 Current_prediction 0.0 0.0 Accuracy is 1.0 ------------------------ sepal length (cm) 5.9 sepal width (cm) 3.2 petal length (cm) 4.8 petal width (cm) 1.8 target 1.0 Name: 70, dtype: float64 Current_prediction 1.0 1.0 Accuracy is 1.0 ------------------------ sepal length (cm) 6.6 sepal width (cm) 3.0 petal length (cm) 4.4 petal width (cm) 1.4 target 1.0 Name: 75, dtype: float64 Current_prediction 1.0 1.0 Accuracy is 1.0 ------------------------ sepal length (cm) 4.4 sepal width (cm) 3.0 petal length (cm) 1.3 petal width (cm) 0.2 target 0.0 Name: 38, dtype: float64 Current_prediction 0.0 0.0 Accuracy is 1.0 ------------------------ import matplotlib.pyplot as plt % matplotlib inline plt . plot ( acc_scores ) plt . ylabel ( 'Accuracy score' ) plt . xlabel ( 'Sample #' ) river_model2 = compose . Pipeline ( preprocessing . StandardScaler (), linear_model . LogisticRegression () ) acc_scores1 = [] def compute_accuracy ( data , model , truth_col ): predict_all = data . apply ( lambda row : model . predict_one ( row ), axis = 1 ) acc_scores1 . append ( accuracy_score ( np . array ( predict_all ), data [ truth_col ])) print ( 'Accuracy is ' , acc_scores1 [ - 1 ]) data_stream = data_feed ( data1 ) for n in range ( 10 ): data_point = next ( data_stream ) print ( data_point ) if ( n > 1 ): predict_one = river_model2 . predict_one ( data_point [ cols_x ]) print ( 'Current_prediction' , predict_one , data_point [ 'target' ]) river_model2 . learn_one ( data_point [ cols_x ], data_point [ 'target' ]) compute_accuracy ( data1 , river_model2 , 'target' ) print ( '------------------------' ) sepal length (cm) 5.8 sepal width (cm) 2.7 petal length (cm) 4.1 petal width (cm) 1.0 target 1.0 Name: 67, dtype: float64 Accuracy is 0.5 ------------------------ sepal length (cm) 5.7 sepal width (cm) 2.9 petal length (cm) 4.2 petal width (cm) 1.3 target 1.0 Name: 96, dtype: float64 Accuracy is 0.94 ------------------------ sepal length (cm) 5.1 sepal width (cm) 3.8 petal length (cm) 1.5 petal width (cm) 0.3 target 0.0 Name: 19, dtype: float64 Current_prediction False 0.0 Accuracy is 1.0 ------------------------ sepal length (cm) 5.4 sepal width (cm) 3.9 petal length (cm) 1.7 petal width (cm) 0.4 target 0.0 Name: 5, dtype: float64 Current_prediction False 0.0 Accuracy is 1.0 ------------------------ sepal length (cm) 5.6 sepal width (cm) 2.9 petal length (cm) 3.6 petal width (cm) 1.3 target 1.0 Name: 64, dtype: float64 Current_prediction True 1.0 Accuracy is 1.0 ------------------------ sepal length (cm) 6.3 sepal width (cm) 2.3 petal length (cm) 4.4 petal width (cm) 1.3 target 1.0 Name: 87, dtype: float64 Current_prediction True 1.0 Accuracy is 1.0 ------------------------ sepal length (cm) 5.5 sepal width (cm) 2.4 petal length (cm) 3.8 petal width (cm) 1.1 target 1.0 Name: 80, dtype: float64 Current_prediction True 1.0 Accuracy is 0.99 ------------------------ sepal length (cm) 5.1 sepal width (cm) 2.5 petal length (cm) 3.0 petal width (cm) 1.1 target 1.0 Name: 98, dtype: float64 Current_prediction True 1.0 Accuracy is 0.99 ------------------------ sepal length (cm) 5.7 sepal width (cm) 4.4 petal length (cm) 1.5 petal width (cm) 0.4 target 0.0 Name: 15, dtype: float64 Current_prediction False 0.0 Accuracy is 0.99 ------------------------ sepal length (cm) 5.1 sepal width (cm) 3.3 petal length (cm) 1.7 petal width (cm) 0.5 target 0.0 Name: 23, dtype: float64 Current_prediction False 0.0 Accuracy is 0.99 ------------------------ import matplotlib.pyplot as plt % matplotlib inline plt . plot ( acc_scores1 ) plt . ylabel ( 'Accuracy score' ) plt . xlabel ( 'Sample #' ) First published on Rolls Royce data sciecne blogs by Harsha Achyuthuni. References \u00b6 Introductory material: https://towardsdatascience.com/machine-learning-for-streaming-data-with-creme-dacf5fb469df Hoeffding Trees: https://homes.cs.washington.edu/~pedrod/papers/kdd00.pdf Modelling under the hood: https://en.wikipedia.org/wiki/Online_machine_learning River Git: https://github.com/online-ml/river River installation steps: https://riverml.xyz/dev/getting-started/installation/ River documentation: https://riverml.xyz/dev/api/overview/ Batch decision trees: http://aharsha.com/cart-classification/","title":"Streaming Machine Learning (Python)"},{"location":"Python/Demonstrating%20online%20learning/#streaming-machine-learning","text":"Author: Achyuthuni Sri Harsha Batch learning : In batch machine learning, we use one dataset to train a model, and we deploy the model to predict on new data. This assumes that the dataset in which the model is trained is a proper representative sample of the population. This model is assumed as a static object. In order to learn from new data, the model has to be re-trained from scratch. This is the most common form of deploying models. Online learning : Some of the machine learning models that we know can be modified to learn on a single datapoint (row). When we can learn from a single datapoint, we can learn incrementally from new datapoints. Data is considered as a stream. Once the model is trained, we need not store the historic training set. The model is also more up to date. If the data's distribution happens over time, the model will be able to handle it (drift)[1]. Where can we use them? They are most useful in scenarios where new data and patterns are constantly arriving. eg: 1. Spam filtering 2. Recommendation engines (news feed predictions) 3. Financial transactions 4. Low compute power (only one data point exists in memory as we train using one data point only) Issues 1. New and not a lot of experience 2. Very few tools and packages 3. All algorithms do not have an online version. Kernel SVMs are impossible to fit on a streaming dataset. Likewise, CART and ID3 decision trees can\u2019t be trained online. However, lesser-known online approximations exist, such as random Fourier features for SVMs and Hoeffding trees for decision trees. 4. Slower than batch learning to reach steady state in real life (It is computationally faster by more than an order of magnitude) 5. Do not guarantee that models learnt are similar to the ones obtained in batch mode(some models). Some models do not guarentee of achieving steady state. 6. Overfitting Similarities We have the same limitations of machine learning, such as: 1. We need to do proper preprocessing 2. We need to do feature engineering as usual 3. The concepts of ensembles, feature extraction, feature selection, imbalanced classes, multiclass etc are same","title":"Streaming machine learning"},{"location":"Python/Demonstrating%20online%20learning/#preprocessing-steps","text":"How do we preprocess data when we are streaming? How do we impute the null values by mean if we do not have complete data? How do we identify outliers when working on one row at a time? How can we do one-hot encoding when we don't know what classes are present overall? The package river is a handy package for online learning. It has a lot of pre-defined preprocessing functions. Let us look at some of them: from river import preprocessing dir ( preprocessing )[ 0 : 12 ] ['AdaptiveStandardScaler', 'Binarizer', 'FeatureHasher', 'LDA', 'MaxAbsScaler', 'MinMaxScaler', 'Normalizer', 'OneHotEncoder', 'PreviousImputer', 'RobustScaler', 'StandardScaler', 'StatImputer'] There are six functions for scaling and normalizing data. They are: 1. AdaptiveStandardScalar 2. MaxAbsScalar 3. MinMaxScalar 4. Normalizer 5. RobustScalar 6. StandardScaler For example, let us look at the documentation for Standard Scaler It scales the data to have zero mean and unit variance. Under the hood, a running mean and a running variance are maintained. The scaling is slightly different than when scaling the data in batch because the exact means and variances are not known in advance. However, this doesn't have a detrimental impact on performance in the long run. Let us look at an example: import numpy as np import matplotlib.pyplot as plt import pandas as pd import random random . seed ( 1995 ) from sklearn.datasets import load_iris iris = load_iris () data1 = pd . DataFrame ( data = np . c_ [ iris [ 'data' ], iris [ 'target' ]], columns = iris [ 'feature_names' ] + [ 'target' ]) data1 = data1 . query ( 'target < 2' ) . sample ( frac = 1 ) def data_feed ( df_datafeed ): # Generator function to give the next candidate for _ctr in range ( len ( df_datafeed )): yield df_datafeed . iloc [ _ctr ] from river import compose from river import linear_model from river import preprocessing preprocessing_model = compose . Pipeline ( preprocessing . StandardScaler (), linear_model . LogisticRegression ()) data_stream = data_feed ( data1 . loc [:, data1 . columns != 'target' ]) for n in range ( 10 ): data_point = next ( data_stream ) . to_frame () . transpose () transformed_data = preprocessing_model . transform_one ( data_point . iloc [ 0 ,:]) print ( '------------------------' ) print ( data_point ) print ( transformed_data ) ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 67 5.8 2.7 4.1 1.0 {'sepal length (cm)': 0.0, 'sepal width (cm)': 0.0, 'petal length (cm)': 0.0, 'petal width (cm)': 0.0} ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 96 5.7 2.9 4.2 1.3 {'sepal length (cm)': -1.0, 'sepal width (cm)': 1.000000000000001, 'petal length (cm)': 0.9999999999999956, 'petal width (cm)': 1.0000000000000004} ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 19 5.1 3.8 1.5 0.3 {'sepal length (cm)': -1.4018260516446994, 'sepal width (cm)': 1.3934660285832352, 'petal length (cm)': -1.4134589797160622, 'petal width (cm)': -1.352447383098741} ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 5 5.4 3.9 1.7 0.4 {'sepal length (cm)': -0.36514837167010933, 'sepal width (cm)': 1.0830277015004253, 'petal length (cm)': -0.9198021534721369, 'petal width (cm)': -0.8427009716003844} ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 64 5.6 2.9 3.6 1.3 {'sepal length (cm)': 0.32232918561015234, 'sepal width (cm)': -0.6740938478604231, 'petal length (cm)': 0.49202037860731096, 'petal width (cm)': 1.019130320146575} ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 87 6.3 2.3 4.4 1.3 {'sepal length (cm)': 1.7636409634199253, 'sepal width (cm)': -1.3539553245018423, 'petal length (cm)': 0.9642101587457326, 'petal width (cm)': 0.8589556903873334} ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 80 5.5 2.4 3.8 1.1 {'sepal length (cm)': -0.37242264987106416, 'sepal width (cm)': -0.9985160994941403, 'petal length (cm)': 0.4205955120960296, 'petal width (cm)': 0.3575992699260759} ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 98 5.1 2.5 3.0 1.1 {'sepal length (cm)': -1.259494647504126, 'sepal width (cm)': -0.743358098059264, 'petal length (cm)': -0.27274857904612027, 'petal width (cm)': 0.331861655799986} ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 15 5.7 4.4 1.5 0.4 {'sepal length (cm)': 0.3503113654141663, 'sepal width (cm)': 1.8442002991885438, 'petal length (cm)': -1.3918304919158082, 'petal width (cm)': -1.282736189026269} ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 23 5.1 3.3 1.7 0.5 {'sepal length (cm)': -1.1921469919530685, 'sepal width (cm)': 0.28047526828011227, 'petal length (cm)': -1.077226017628646, 'petal width (cm)': -0.9305415914315355} Apart from preprocessing, using river, we can perform 1. Feature extraction/selection 2. Ensembles 3. Storing running statistics 4. Building regression and classification models 5. Time series 6. Anamoly detection 7. Clustering There are 6 types of machine learning models that we can build using river. They are: 1. Linear based (linear and logistic regression) 2. Tree based (Decision trees, Hoeffding trees) 3. Nearest neighbours based 4. Bayesian models 5. Neural Networks 6. Ensemble based models 7. Others from river import linear_model , naive_bayes , tree , neural_net , neighbors , expert , ensemble print ( 'Linear models' , dir ( linear_model )[ 0 : 7 ]) print ( 'Tree based modles' , dir ( tree )[ 0 : 6 ]) print ( 'Bayesian models' , dir ( naive_bayes )[ 0 : 4 ]) print ( 'Specialised models' , dir ( expert )[ 0 : 6 ]) print ( 'Ensemble mmodels' , dir ( ensemble )[ 0 : 8 ]) Linear models ['ALMAClassifier', 'LinearRegression', 'LogisticRegression', 'PAClassifier', 'PARegressor', 'Perceptron', 'SoftmaxRegression'] Tree based modles ['ExtremelyFastDecisionTreeClassifier', 'HoeffdingAdaptiveTreeClassifier', 'HoeffdingAdaptiveTreeRegressor', 'HoeffdingTreeClassifier', 'HoeffdingTreeRegressor', 'LabelCombinationHoeffdingTreeClassifier'] Bayesian models ['BernoulliNB', 'ComplementNB', 'GaussianNB', 'MultinomialNB'] Specialised models ['EWARegressor', 'EpsilonGreedyRegressor', 'StackingClassifier', 'SuccessiveHalvingClassifier', 'SuccessiveHalvingRegressor', 'UCBRegressor'] Ensemble mmodels ['ADWINBaggingClassifier', 'AdaBoostClassifier', 'AdaptiveRandomForestClassifier', 'AdaptiveRandomForestRegressor', 'BaggingClassifier', 'BaggingRegressor', 'LeveragingBaggingClassifier', 'SRPClassifier']","title":"Preprocessing steps"},{"location":"Python/Demonstrating%20online%20learning/#modelling-under-the-hood","text":"There are two types of streaming models, those which are entirely streaming and pseudo-online models. Pseudo-online models use a small batch of data to build the models, while completely online models build the models using only one data point.[3]","title":"Modelling (under the hood)"},{"location":"Python/Demonstrating%20online%20learning/#pseudo-online-models","text":"There are many theorems in statistics which can help us to bound the error of a metric between two variables. Hoeffding bound is one such theorem. \"Consider a real-valued random variable r whose range is R (e.g., for a probability the range is one, and for an information gain the range is log c, where c is the number of classes). Suppose we have made n independent observations of this variable, and computed their mean \\(\\bar r\\) . The Hoeffding bound states that, with probability 1 \u2212 \u03b4, the true mean of the variable is at least \\(\\bar r \u2212 \\epsilon\\) , where \\(\\epsilon = \\sqrt{\\frac{R^2ln(\\frac{1}{\\lambda})}{2n}}\\) .\" This is useful while creating decision trees. Using hoeffding bound, we can identify which feature should we use to split the tree. We can find if a variable has sufficient gini index (or other metrics) greater (lesser) than other variables with a probability below a cutoff probability and split the tree based on that metric. This can be acheived with very few data points which can be deleted after splitting and creating child nodes. This algorithm is called Hoeffding trees algorithm.","title":"Pseudo online models"},{"location":"Python/Demonstrating%20online%20learning/#completely-online-models","text":"How can we update a model using only one data point? Let us look at Gradient Descent to understand this. In gradient descent, we want to minimise a convex loss function(MSE, regret, etc). As an example, consider the function \\(h(x) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ..\\) (or any convex function). The mean squared error is \\(J(\\theta) = \\frac{1}{2} \\sum_{i=1}^{m} (y_{(i)}-h_{\\theta}(x_{(i)}))^{2}\\) . In gradient descent, we find the \\(\\beta_i\\) that minimises \\(J(\\theta)\\) . In batch model, we consider all the data points that exist to identfy the optimal solution. In streaming learning, we initialise the \\(\\beta_i\\) as 0 and keep incrementally changing the \\(\\beta_i\\) 's using \\( \\(\\beta_{i, t} = \\beta_{i, t-1} + \\alpha \\times \\frac{\\partial}{\\partial \\theta_{j}} J(\\theta)\\) \\)","title":"Completely online models"},{"location":"Python/Demonstrating%20online%20learning/#steps-in-updating-the-model","text":"In online learning, there are 4 steps[3]. For every new datapoint, we will recursively run the following steps. For \\({\\displaystyle t=1,2,...,T}\\) Learner receives input \\({\\displaystyle x_{t}}\\) Learner outputs \\({\\displaystyle w_{t}}\\) from a fixed convex set \\({\\displaystyle S}\\) Nature sends back a convex loss function \\({\\displaystyle v_{t}:S\\rightarrow \\mathbb {R} }\\) . Learner suffers loss \\({\\displaystyle v_{t}(w_{t})}\\) and updates its model.","title":"Steps in updating the model"},{"location":"Python/Demonstrating%20online%20learning/#implimentation-using-river","text":"Every online machine learning model has the following basic 5 functions: dir ( linear_model . LogisticRegression )[ 50 : 55 ] ['learn_many', 'learn_one', 'predict_many', 'predict_one', 'predict_proba_many'] As the name mentions, learn_one and predict_one learn and predict from one data point, learn_many, predict_many and predict_prob_many learn and predict using multiple data points. River is the result of a merger between creme and scikit-multiflow, and the remaining functions in the library follow a similar pattern to the same.","title":"Implimentation using river"},{"location":"Python/Demonstrating%20online%20learning/#building-a-model","text":"Using the same data as above, let us build a sample model using river for streaming. from river import compose river_model = compose . Pipeline ( preprocessing . StandardScaler (), tree . HoeffdingTreeClassifier () ) from sklearn.metrics import accuracy_score acc_scores = [] cols_x = [ 'sepal length (cm)' , 'sepal width (cm)' , 'petal length (cm)' , 'petal width (cm)' ] def compute_accuracy ( data , model , truth_col ): predict_all = data . apply ( lambda row : model . predict_one ( row ), axis = 1 ) acc_scores . append ( accuracy_score ( np . array ( predict_all ), data [ truth_col ])) print ( 'Accuracy is ' , acc_scores [ - 1 ]) data_stream = data_feed ( data1 ) for n in range ( 20 ): data_point = next ( data_stream ) print ( data_point ) if ( n > 1 ): predict_one = river_model . predict_one ( data_point [ cols_x ]) print ( 'Current_prediction' , predict_one , data_point [ 'target' ]) river_model . learn_one ( data_point [ cols_x ], data_point [ 'target' ]) compute_accuracy ( data1 , river_model , 'target' ) print ( '------------------------' ) sepal length (cm) 5.8 sepal width (cm) 2.7 petal length (cm) 4.1 petal width (cm) 1.0 target 1.0 Name: 67, dtype: float64 Accuracy is 0.5 ------------------------ sepal length (cm) 5.7 sepal width (cm) 2.9 petal length (cm) 4.2 petal width (cm) 1.3 target 1.0 Name: 96, dtype: float64 Accuracy is 0.5 ------------------------ sepal length (cm) 5.1 sepal width (cm) 3.8 petal length (cm) 1.5 petal width (cm) 0.3 target 0.0 Name: 19, dtype: float64 Current_prediction 1.0 0.0 Accuracy is 0.5 ------------------------ sepal length (cm) 5.4 sepal width (cm) 3.9 petal length (cm) 1.7 petal width (cm) 0.4 target 0.0 Name: 5, dtype: float64 Current_prediction 1.0 0.0 Accuracy is 0.71 ------------------------ sepal length (cm) 5.6 sepal width (cm) 2.9 petal length (cm) 3.6 petal width (cm) 1.3 target 1.0 Name: 64, dtype: float64 Current_prediction 1.0 1.0 Accuracy is 0.79 ------------------------ sepal length (cm) 6.3 sepal width (cm) 2.3 petal length (cm) 4.4 petal width (cm) 1.3 target 1.0 Name: 87, dtype: float64 Current_prediction 1.0 1.0 Accuracy is 0.69 ------------------------ sepal length (cm) 5.5 sepal width (cm) 2.4 petal length (cm) 3.8 petal width (cm) 1.1 target 1.0 Name: 80, dtype: float64 Current_prediction 1.0 1.0 Accuracy is 0.69 ------------------------ sepal length (cm) 5.1 sepal width (cm) 2.5 petal length (cm) 3.0 petal width (cm) 1.1 target 1.0 Name: 98, dtype: float64 Current_prediction 1.0 1.0 Accuracy is 0.69 ------------------------ sepal length (cm) 5.7 sepal width (cm) 4.4 petal length (cm) 1.5 petal width (cm) 0.4 target 0.0 Name: 15, dtype: float64 Current_prediction 1.0 0.0 Accuracy is 0.99 ------------------------ sepal length (cm) 5.1 sepal width (cm) 3.3 petal length (cm) 1.7 petal width (cm) 0.5 target 0.0 Name: 23, dtype: float64 Current_prediction 0.0 0.0 Accuracy is 1.0 ------------------------ sepal length (cm) 4.8 sepal width (cm) 3.1 petal length (cm) 1.6 petal width (cm) 0.2 target 0.0 Name: 30, dtype: float64 Current_prediction 0.0 0.0 Accuracy is 1.0 ------------------------ sepal length (cm) 5.5 sepal width (cm) 2.5 petal length (cm) 4.0 petal width (cm) 1.3 target 1.0 Name: 89, dtype: float64 Current_prediction 1.0 1.0 Accuracy is 1.0 ------------------------ sepal length (cm) 4.5 sepal width (cm) 2.3 petal length (cm) 1.3 petal width (cm) 0.3 target 0.0 Name: 41, dtype: float64 Current_prediction 0.0 0.0 Accuracy is 1.0 ------------------------ sepal length (cm) 6.1 sepal width (cm) 3.0 petal length (cm) 4.6 petal width (cm) 1.4 target 1.0 Name: 91, dtype: float64 Current_prediction 1.0 1.0 Accuracy is 1.0 ------------------------ sepal length (cm) 5.2 sepal width (cm) 3.5 petal length (cm) 1.5 petal width (cm) 0.2 target 0.0 Name: 27, dtype: float64 Current_prediction 0.0 0.0 Accuracy is 1.0 ------------------------ sepal length (cm) 7.0 sepal width (cm) 3.2 petal length (cm) 4.7 petal width (cm) 1.4 target 1.0 Name: 50, dtype: float64 Current_prediction 1.0 1.0 Accuracy is 1.0 ------------------------ sepal length (cm) 5.5 sepal width (cm) 4.2 petal length (cm) 1.4 petal width (cm) 0.2 target 0.0 Name: 33, dtype: float64 Current_prediction 0.0 0.0 Accuracy is 1.0 ------------------------ sepal length (cm) 5.9 sepal width (cm) 3.2 petal length (cm) 4.8 petal width (cm) 1.8 target 1.0 Name: 70, dtype: float64 Current_prediction 1.0 1.0 Accuracy is 1.0 ------------------------ sepal length (cm) 6.6 sepal width (cm) 3.0 petal length (cm) 4.4 petal width (cm) 1.4 target 1.0 Name: 75, dtype: float64 Current_prediction 1.0 1.0 Accuracy is 1.0 ------------------------ sepal length (cm) 4.4 sepal width (cm) 3.0 petal length (cm) 1.3 petal width (cm) 0.2 target 0.0 Name: 38, dtype: float64 Current_prediction 0.0 0.0 Accuracy is 1.0 ------------------------ import matplotlib.pyplot as plt % matplotlib inline plt . plot ( acc_scores ) plt . ylabel ( 'Accuracy score' ) plt . xlabel ( 'Sample #' ) river_model2 = compose . Pipeline ( preprocessing . StandardScaler (), linear_model . LogisticRegression () ) acc_scores1 = [] def compute_accuracy ( data , model , truth_col ): predict_all = data . apply ( lambda row : model . predict_one ( row ), axis = 1 ) acc_scores1 . append ( accuracy_score ( np . array ( predict_all ), data [ truth_col ])) print ( 'Accuracy is ' , acc_scores1 [ - 1 ]) data_stream = data_feed ( data1 ) for n in range ( 10 ): data_point = next ( data_stream ) print ( data_point ) if ( n > 1 ): predict_one = river_model2 . predict_one ( data_point [ cols_x ]) print ( 'Current_prediction' , predict_one , data_point [ 'target' ]) river_model2 . learn_one ( data_point [ cols_x ], data_point [ 'target' ]) compute_accuracy ( data1 , river_model2 , 'target' ) print ( '------------------------' ) sepal length (cm) 5.8 sepal width (cm) 2.7 petal length (cm) 4.1 petal width (cm) 1.0 target 1.0 Name: 67, dtype: float64 Accuracy is 0.5 ------------------------ sepal length (cm) 5.7 sepal width (cm) 2.9 petal length (cm) 4.2 petal width (cm) 1.3 target 1.0 Name: 96, dtype: float64 Accuracy is 0.94 ------------------------ sepal length (cm) 5.1 sepal width (cm) 3.8 petal length (cm) 1.5 petal width (cm) 0.3 target 0.0 Name: 19, dtype: float64 Current_prediction False 0.0 Accuracy is 1.0 ------------------------ sepal length (cm) 5.4 sepal width (cm) 3.9 petal length (cm) 1.7 petal width (cm) 0.4 target 0.0 Name: 5, dtype: float64 Current_prediction False 0.0 Accuracy is 1.0 ------------------------ sepal length (cm) 5.6 sepal width (cm) 2.9 petal length (cm) 3.6 petal width (cm) 1.3 target 1.0 Name: 64, dtype: float64 Current_prediction True 1.0 Accuracy is 1.0 ------------------------ sepal length (cm) 6.3 sepal width (cm) 2.3 petal length (cm) 4.4 petal width (cm) 1.3 target 1.0 Name: 87, dtype: float64 Current_prediction True 1.0 Accuracy is 1.0 ------------------------ sepal length (cm) 5.5 sepal width (cm) 2.4 petal length (cm) 3.8 petal width (cm) 1.1 target 1.0 Name: 80, dtype: float64 Current_prediction True 1.0 Accuracy is 0.99 ------------------------ sepal length (cm) 5.1 sepal width (cm) 2.5 petal length (cm) 3.0 petal width (cm) 1.1 target 1.0 Name: 98, dtype: float64 Current_prediction True 1.0 Accuracy is 0.99 ------------------------ sepal length (cm) 5.7 sepal width (cm) 4.4 petal length (cm) 1.5 petal width (cm) 0.4 target 0.0 Name: 15, dtype: float64 Current_prediction False 0.0 Accuracy is 0.99 ------------------------ sepal length (cm) 5.1 sepal width (cm) 3.3 petal length (cm) 1.7 petal width (cm) 0.5 target 0.0 Name: 23, dtype: float64 Current_prediction False 0.0 Accuracy is 0.99 ------------------------ import matplotlib.pyplot as plt % matplotlib inline plt . plot ( acc_scores1 ) plt . ylabel ( 'Accuracy score' ) plt . xlabel ( 'Sample #' ) First published on Rolls Royce data sciecne blogs by Harsha Achyuthuni.","title":"Building a model"},{"location":"Python/Demonstrating%20online%20learning/#references","text":"Introductory material: https://towardsdatascience.com/machine-learning-for-streaming-data-with-creme-dacf5fb469df Hoeffding Trees: https://homes.cs.washington.edu/~pedrod/papers/kdd00.pdf Modelling under the hood: https://en.wikipedia.org/wiki/Online_machine_learning River Git: https://github.com/online-ml/river River installation steps: https://riverml.xyz/dev/getting-started/installation/ River documentation: https://riverml.xyz/dev/api/overview/ Batch decision trees: http://aharsha.com/cart-classification/","title":"References"},{"location":"Python/Diffusion%20on%20networks/","text":"MathJax.Hub.Config({ extensions: [\"tex2jax.js\",\"TeX/AMSmath.js\",\"TeX/AMSsymbols.js\"], jax: [\"input/TeX\", \"output/HTML-CSS\"], tex2jax: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ], }, \"HTML-CSS\": { availableFonts: [\"TeX\"] } }); Forecasting adoption of a new product \u00b6 Author: Achyuthuni Sri Harsha Introduction \u00b6 Forecasting new adoptions after a product introduction is an important marketing problem. I want to use a forecasting model developed by Frank Bass that has proven to be effective in forecasting the adoption of innovative and new technologies. I am going to use Nonlinear programming to estimate the parameters of the Bass forecasting model. Bass Forecasting model \u00b6 The model has three parameters that must be estimated. parameter explanation m the number of people estimated to eventually adopt the new product q the coefficient of imitation p the coefficient of innovation The coefficient of imitation (q) is a parameter that measures the likelihood of adoption due to a potential adopter being influenced by someone who has already adopted the product. It measures the \u201cword-of-mouth\u201d effect influencing purchases. The coefficient of innovation (p) measures the likelihood of adoption, assuming no influence from someone who has already purchased (adopted) the product. It is the likelihood of someone adopting the product due to her or his own interest in the innovation. If \\(C_{t\u22121}\\) is the number of people that adopted the product by time t-1, then the number of new adopters during time t is given by Bass forecasting model and it is: $$ F_t=(p+q[\\frac{C_{t\u22121}}{m}])(m\u2212C_{t\u22121}) $$ If \\(c_{t} = C_t/m\\) , then $$ c_{t} - c_{t-1} = (p+qc_{t-1})(1-c_{t-1})$$ Doing some maths, instead of one time period, we could consider \\(\\Delta t\\) time period, we can write as: \\[ c_{t+ \\Delta t} - c_{t} = (p+qc_{t})(1-c_{t}) \\Delta t \\] \\[ \\frac{c_{t+ \\Delta t} - c_{t}}{\\Delta t} = (p+qc_{t})(1-c_{t}) \\] \\[ \\frac{d}{dt}c_{t} = (p+qc_{t})(1-c_{t}) \\] Solving we get $$ c(t) = \\frac{1-e {-(p+q)t}}{1+\\frac{q}{p}e } $$ As an example, consider the following revenues for a product. import pandas as pd data = pd . DataFrame ({ 'week' : [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 ], 'revenues' : [ 0.1 , 3 , 5.2 , 7 , 5.25 , 4.9 , 3 , 2.4 , 1.9 , 1.3 , 0.8 , 0.6 ]}) data [ 'cum_sum' ] = data [ 'revenues' ] . cumsum () data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } week revenues cum_sum 0 1 0.10 0.10 1 2 3.00 3.10 2 3 5.20 8.30 3 4 7.00 15.30 4 5 5.25 20.55 5 6 4.90 25.45 6 7 3.00 28.45 7 8 2.40 30.85 8 9 1.90 32.75 9 10 1.30 34.05 10 11 0.80 34.85 11 12 0.60 35.45 Optimising for the ideal p, q and m values, we get from scipy.optimize import curve_fit def c_t ( x , p , q , m ): return ( p + ( q / m ) * ( x )) * ( m - x ) popt , pcov = curve_fit ( c_t , data . cum_sum [ 0 : 11 ], data . revenues [ 1 : 12 ]) popt array([ 0.11467648, 0.37950562, 35.22906717]) The optimal p,d and q are:0.11467648, 0.37950562, 35.22906717. We can use these to predict the future revenues of the product.","title":"Bass Forecasting model (Python)"},{"location":"Python/Diffusion%20on%20networks/#forecasting-adoption-of-a-new-product","text":"Author: Achyuthuni Sri Harsha","title":"Forecasting adoption of a new product"},{"location":"Python/Diffusion%20on%20networks/#introduction","text":"Forecasting new adoptions after a product introduction is an important marketing problem. I want to use a forecasting model developed by Frank Bass that has proven to be effective in forecasting the adoption of innovative and new technologies. I am going to use Nonlinear programming to estimate the parameters of the Bass forecasting model.","title":"Introduction"},{"location":"Python/Diffusion%20on%20networks/#bass-forecasting-model","text":"The model has three parameters that must be estimated. parameter explanation m the number of people estimated to eventually adopt the new product q the coefficient of imitation p the coefficient of innovation The coefficient of imitation (q) is a parameter that measures the likelihood of adoption due to a potential adopter being influenced by someone who has already adopted the product. It measures the \u201cword-of-mouth\u201d effect influencing purchases. The coefficient of innovation (p) measures the likelihood of adoption, assuming no influence from someone who has already purchased (adopted) the product. It is the likelihood of someone adopting the product due to her or his own interest in the innovation. If \\(C_{t\u22121}\\) is the number of people that adopted the product by time t-1, then the number of new adopters during time t is given by Bass forecasting model and it is: $$ F_t=(p+q[\\frac{C_{t\u22121}}{m}])(m\u2212C_{t\u22121}) $$ If \\(c_{t} = C_t/m\\) , then $$ c_{t} - c_{t-1} = (p+qc_{t-1})(1-c_{t-1})$$ Doing some maths, instead of one time period, we could consider \\(\\Delta t\\) time period, we can write as: \\[ c_{t+ \\Delta t} - c_{t} = (p+qc_{t})(1-c_{t}) \\Delta t \\] \\[ \\frac{c_{t+ \\Delta t} - c_{t}}{\\Delta t} = (p+qc_{t})(1-c_{t}) \\] \\[ \\frac{d}{dt}c_{t} = (p+qc_{t})(1-c_{t}) \\] Solving we get $$ c(t) = \\frac{1-e {-(p+q)t}}{1+\\frac{q}{p}e } $$ As an example, consider the following revenues for a product. import pandas as pd data = pd . DataFrame ({ 'week' : [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 ], 'revenues' : [ 0.1 , 3 , 5.2 , 7 , 5.25 , 4.9 , 3 , 2.4 , 1.9 , 1.3 , 0.8 , 0.6 ]}) data [ 'cum_sum' ] = data [ 'revenues' ] . cumsum () data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } week revenues cum_sum 0 1 0.10 0.10 1 2 3.00 3.10 2 3 5.20 8.30 3 4 7.00 15.30 4 5 5.25 20.55 5 6 4.90 25.45 6 7 3.00 28.45 7 8 2.40 30.85 8 9 1.90 32.75 9 10 1.30 34.05 10 11 0.80 34.85 11 12 0.60 35.45 Optimising for the ideal p, q and m values, we get from scipy.optimize import curve_fit def c_t ( x , p , q , m ): return ( p + ( q / m ) * ( x )) * ( m - x ) popt , pcov = curve_fit ( c_t , data . cum_sum [ 0 : 11 ], data . revenues [ 1 : 12 ]) popt array([ 0.11467648, 0.37950562, 35.22906717]) The optimal p,d and q are:0.11467648, 0.37950562, 35.22906717. We can use these to predict the future revenues of the product.","title":"Bass Forecasting model"},{"location":"Python/Introduction%20to%20Networkx/","text":"Networks in python \u00b6 Author: Achyuthuni Sri Harsha Networks play an important role in data science, with Google (page rank), Uber (route optimisation), Amazon (supply chian optimisation) and other companies becoming technology giants using network and data related optimisations. This article is an introduction to using networks in python using networkx package. # Import networkx library and rename it as nx. import networkx as nx # Other packages required import numpy as np import pandas as pd import matplotlib.pyplot as plt Undirected graphs \u00b6 A network can be represented in many ways. Edgelist represents graphs as a list of edges. Graphs can be undirected or directed. Consider the below edgelist where there is a relation between edge 1 and edge 2, and the weight for that edge is also provided. edgelist_df = pd . DataFrame ({ 'node1' :[ 1 , 1 , 1 , 2 , 2 , 3 , 4 , 4 ], 'node2' :[ 2 , 3 , 4 , 4 , 5 , 5 , 3 , 5 ], 'weights' :[ 5 , 2 , 1 , 3 , 1 , 7 , 1 , 4 ]}) edgelist_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } node1 node2 weights 0 1 2 5 1 1 3 2 2 1 4 1 3 2 4 3 4 2 5 1 5 3 5 7 6 4 3 1 7 4 5 4 One way to create a graph is to create an empty graph and add edges (and nodes) to the graph. g = nx . Graph () for i , elrow in edgelist_df . iterrows (): g . add_edge ( elrow [ 0 ], elrow [ 1 ], weight = elrow [ 2 ]) g . add_node ( 1 , pos = ( 0 , 5 )) g . edges ( data = True ) EdgeDataView([(1, 2, {'weight': 5}), (1, 3, {'weight': 2}), (1, 4, {'weight': 1}), (2, 4, {'weight': 3}), (2, 5, {'weight': 1}), (3, 5, {'weight': 7}), (3, 4, {'weight': 1}), (4, 5, {'weight': 4})]) # for each node we are trying to fix the coordinates g . add_node ( 1 , pos = ( 0 , 5 )) g . add_node ( 2 , pos = ( 5 , 10 )) g . add_node ( 3 , pos = ( 5 , 0 )) g . add_node ( 4 , pos = ( 10 , 5 )) g . add_node ( 5 , pos = ( 15 , 5 )) g . nodes ( data = True ) NodeDataView({1: {'pos': (0, 5)}, 2: {'pos': (5, 10)}, 3: {'pos': (5, 0)}, 4: {'pos': (10, 5)}, 5: {'pos': (15, 5)}}) This graph can be visualised as follows: # This function gets the coordinates for nodes pos = nx . get_node_attributes ( g , 'pos' ) # This function gets the weights for the edges weight = nx . get_edge_attributes ( g , 'weight' ) # The simplest way to draw a graph is by using nx.draw. nx . draw ( g , pos , with_labels = True ) # This adds edge lables nx . draw_networkx_edge_labels ( g , pos , edge_labels = weight ) plt . show () Mathematically this grah can be described in many ways. The most important are the adjacency matrix and incidence matrix. The adjacency matrix shows the relationship between the nodes in a matrix format where the nodes represent rows and columns. An edge between two nodes is represented by a positve number in the adjacency matrix, and the magnitude represents the weight. nx . linalg . graphmatrix . adj_matrix ( g ) . toarray () array([[0, 5, 2, 1, 0], [5, 0, 0, 3, 1], [2, 0, 0, 1, 7], [1, 3, 1, 0, 4], [0, 1, 7, 4, 0]], dtype=int64) The concurrency matrix represents the relationship between the nodes and edges. We have 8 edges as shown, and the relationship between the 5 nodes and 8 edges is shown below. g . edges EdgeView([(1, 2), (1, 3), (1, 4), (2, 4), (2, 5), (3, 5), (3, 4), (4, 5)]) nx . linalg . graphmatrix . incidence_matrix ( g ) . toarray () array([[1., 1., 1., 0., 0., 0., 0., 0.], [1., 0., 0., 1., 1., 0., 0., 0.], [0., 1., 0., 0., 0., 1., 1., 0.], [0., 0., 1., 1., 0., 0., 1., 1.], [0., 0., 0., 0., 1., 1., 0., 1.]]) Directed graph. \u00b6 A directed graph has edges which are in a particular direction. In this example, we are creating a DiGraph and reading a file where the edgelist is defined. An edge from a to b with a weight W exists when a line \"a b W\" can be found in the file. G1 = nx . DiGraph () # Read an directed graph from a list of edges, need to specify that using the parameter \u2018create_using\u2019 G1 = nx . read_edgelist ( \"edgelist1.txt\" , nodetype = str , create_using = nx . DiGraph (), data = [( 'weight' , int )]) print ( 'Edgelist' ) import sys nx . write_weighted_edgelist ( G1 , sys . stdout ) print ( 'nodes' ) print ( G1 . nodes ( data = True )) print ( 'edges' ) print ( G1 . edges ( data = True )) Edgelist 0 1 4 0 3 4 0 4 2 1 0 4 1 2 9 1 4 1 2 3 4 3 1 8 3 4 7 nodes [('0', {}), ('1', {}), ('2', {}), ('3', {}), ('4', {})] edges [('0', '1', {'weight': 4}), ('0', '3', {'weight': 4}), ('0', '4', {'weight': 2}), ('1', '0', {'weight': 4}), ('1', '2', {'weight': 9}), ('1', '4', {'weight': 1}), ('2', '3', {'weight': 4}), ('3', '1', {'weight': 8}), ('3', '4', {'weight': 7})] This graph can be visualised in many ways. Some of the visualisation patterns are (And these visualisations are same for directed or undirected graphs) 1. Spring layout (The length of edges is proportional to the weights) 2. Circular layout (The nodes are present in a circle) 3. Random layout All the three layouts are shown below nx . draw_spring ( G1 , with_labels = True , node_color = 'skyblue' , node_size = 200 , edge_color = 'black' ) nx . draw_circular ( G1 , with_labels = True , node_color = 'red' , node_size = 300 , edge_color = 'black' ) nx . draw_random ( G1 , with_labels = True , node_color = 'red' , node_size = 300 , edge_color = 'black' ) The concurrency matrix looks differently for directed graphs. If a edge is leaving a node it is represented by -1 and if it is incoming to a node i is +1. nx . linalg . graphmatrix . incidence_matrix ( G1 , oriented = True ) . toarray () array([[-1., -1., -1., 1., 0., 0., 0., 0., 0.], [ 1., 0., 0., -1., -1., -1., 0., 1., 0.], [ 0., 0., 0., 0., 1., 0., -1., 0., 0.], [ 0., 1., 0., 0., 0., 0., 1., -1., -1.], [ 0., 0., 1., 0., 0., 1., 0., 0., 1.]]) We can read from not just edgelists but also from adjacency matrix. For instance, we can create a new graph using the adjacency matrix from the directed graph. # Adjacencey matrix of the graph G1 M1 = nx . linalg . graphmatrix . adj_matrix ( G1 ) . toarray () print ( 'Adjacency matrix of G1' ) print ( M1 ) # convert the matrix into a graph G2 = nx . from_numpy_matrix ( M1 ) # Print the graph information print ( \"Printing G2\" ) nx . write_weighted_edgelist ( G2 , sys . stdout ) Adjacency matrix of G1 [[0 4 0 4 2] [4 0 9 0 1] [0 0 0 4 0] [0 8 0 0 7] [0 0 0 0 0]] Printing G2 0 1 4 0 3 4 0 4 2 1 2 9 1 4 1 1 3 8 2 3 4 3 4 7 Visualisation of graphs \u00b6 We can display the graphs using a variety of methods. The weights among the edges can be represented as thickness (or color) of the edges, the importance of nodes can be defined, etc. This will provide additional information of the graph. # get the outward degree for each node and store them as a list of (node_number, degree) out_deg = G1 . out_degree () # Making all the required parameters out_deg = [ int ( out_deg [ node ]) * 1000 for node in G1 . nodes ()] color = [ 'g' if val > np . mean ( out_deg ) else 'r' for val in out_deg ] weight = list ( nx . get_edge_attributes ( G1 , 'weight' ) . values ()) degree = dict ( G1 . degree ) # create the plot and title plt . subplots ( figsize = ( 10 , 10 )) plt . title ( 'Graph containing everything' ) nx . draw_circular ( G1 , with_labels = True , edge_color = color , width = weight , node_size = out_deg ) Anther different type of graph is the bipartite graph, which has a visualisation of its own. In the below example, we take an inbilt biparite graph to show how it can be visualised. B = nx . bipartite . gnmk_random_graph ( 3 , 5 , 10 , seed = 123 ) top = nx . bipartite . sets ( B )[ 0 ] pos = nx . bipartite_layout ( B , top ) nx . draw_networkx ( B , pos = nx . drawing . layout . bipartite_layout ( B , top ))","title":"Introduction to NetworkX (Python)"},{"location":"Python/Introduction%20to%20Networkx/#networks-in-python","text":"Author: Achyuthuni Sri Harsha Networks play an important role in data science, with Google (page rank), Uber (route optimisation), Amazon (supply chian optimisation) and other companies becoming technology giants using network and data related optimisations. This article is an introduction to using networks in python using networkx package. # Import networkx library and rename it as nx. import networkx as nx # Other packages required import numpy as np import pandas as pd import matplotlib.pyplot as plt","title":"Networks in python"},{"location":"Python/Introduction%20to%20Networkx/#undirected-graphs","text":"A network can be represented in many ways. Edgelist represents graphs as a list of edges. Graphs can be undirected or directed. Consider the below edgelist where there is a relation between edge 1 and edge 2, and the weight for that edge is also provided. edgelist_df = pd . DataFrame ({ 'node1' :[ 1 , 1 , 1 , 2 , 2 , 3 , 4 , 4 ], 'node2' :[ 2 , 3 , 4 , 4 , 5 , 5 , 3 , 5 ], 'weights' :[ 5 , 2 , 1 , 3 , 1 , 7 , 1 , 4 ]}) edgelist_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } node1 node2 weights 0 1 2 5 1 1 3 2 2 1 4 1 3 2 4 3 4 2 5 1 5 3 5 7 6 4 3 1 7 4 5 4 One way to create a graph is to create an empty graph and add edges (and nodes) to the graph. g = nx . Graph () for i , elrow in edgelist_df . iterrows (): g . add_edge ( elrow [ 0 ], elrow [ 1 ], weight = elrow [ 2 ]) g . add_node ( 1 , pos = ( 0 , 5 )) g . edges ( data = True ) EdgeDataView([(1, 2, {'weight': 5}), (1, 3, {'weight': 2}), (1, 4, {'weight': 1}), (2, 4, {'weight': 3}), (2, 5, {'weight': 1}), (3, 5, {'weight': 7}), (3, 4, {'weight': 1}), (4, 5, {'weight': 4})]) # for each node we are trying to fix the coordinates g . add_node ( 1 , pos = ( 0 , 5 )) g . add_node ( 2 , pos = ( 5 , 10 )) g . add_node ( 3 , pos = ( 5 , 0 )) g . add_node ( 4 , pos = ( 10 , 5 )) g . add_node ( 5 , pos = ( 15 , 5 )) g . nodes ( data = True ) NodeDataView({1: {'pos': (0, 5)}, 2: {'pos': (5, 10)}, 3: {'pos': (5, 0)}, 4: {'pos': (10, 5)}, 5: {'pos': (15, 5)}}) This graph can be visualised as follows: # This function gets the coordinates for nodes pos = nx . get_node_attributes ( g , 'pos' ) # This function gets the weights for the edges weight = nx . get_edge_attributes ( g , 'weight' ) # The simplest way to draw a graph is by using nx.draw. nx . draw ( g , pos , with_labels = True ) # This adds edge lables nx . draw_networkx_edge_labels ( g , pos , edge_labels = weight ) plt . show () Mathematically this grah can be described in many ways. The most important are the adjacency matrix and incidence matrix. The adjacency matrix shows the relationship between the nodes in a matrix format where the nodes represent rows and columns. An edge between two nodes is represented by a positve number in the adjacency matrix, and the magnitude represents the weight. nx . linalg . graphmatrix . adj_matrix ( g ) . toarray () array([[0, 5, 2, 1, 0], [5, 0, 0, 3, 1], [2, 0, 0, 1, 7], [1, 3, 1, 0, 4], [0, 1, 7, 4, 0]], dtype=int64) The concurrency matrix represents the relationship between the nodes and edges. We have 8 edges as shown, and the relationship between the 5 nodes and 8 edges is shown below. g . edges EdgeView([(1, 2), (1, 3), (1, 4), (2, 4), (2, 5), (3, 5), (3, 4), (4, 5)]) nx . linalg . graphmatrix . incidence_matrix ( g ) . toarray () array([[1., 1., 1., 0., 0., 0., 0., 0.], [1., 0., 0., 1., 1., 0., 0., 0.], [0., 1., 0., 0., 0., 1., 1., 0.], [0., 0., 1., 1., 0., 0., 1., 1.], [0., 0., 0., 0., 1., 1., 0., 1.]])","title":"Undirected graphs"},{"location":"Python/Introduction%20to%20Networkx/#directed-graph","text":"A directed graph has edges which are in a particular direction. In this example, we are creating a DiGraph and reading a file where the edgelist is defined. An edge from a to b with a weight W exists when a line \"a b W\" can be found in the file. G1 = nx . DiGraph () # Read an directed graph from a list of edges, need to specify that using the parameter \u2018create_using\u2019 G1 = nx . read_edgelist ( \"edgelist1.txt\" , nodetype = str , create_using = nx . DiGraph (), data = [( 'weight' , int )]) print ( 'Edgelist' ) import sys nx . write_weighted_edgelist ( G1 , sys . stdout ) print ( 'nodes' ) print ( G1 . nodes ( data = True )) print ( 'edges' ) print ( G1 . edges ( data = True )) Edgelist 0 1 4 0 3 4 0 4 2 1 0 4 1 2 9 1 4 1 2 3 4 3 1 8 3 4 7 nodes [('0', {}), ('1', {}), ('2', {}), ('3', {}), ('4', {})] edges [('0', '1', {'weight': 4}), ('0', '3', {'weight': 4}), ('0', '4', {'weight': 2}), ('1', '0', {'weight': 4}), ('1', '2', {'weight': 9}), ('1', '4', {'weight': 1}), ('2', '3', {'weight': 4}), ('3', '1', {'weight': 8}), ('3', '4', {'weight': 7})] This graph can be visualised in many ways. Some of the visualisation patterns are (And these visualisations are same for directed or undirected graphs) 1. Spring layout (The length of edges is proportional to the weights) 2. Circular layout (The nodes are present in a circle) 3. Random layout All the three layouts are shown below nx . draw_spring ( G1 , with_labels = True , node_color = 'skyblue' , node_size = 200 , edge_color = 'black' ) nx . draw_circular ( G1 , with_labels = True , node_color = 'red' , node_size = 300 , edge_color = 'black' ) nx . draw_random ( G1 , with_labels = True , node_color = 'red' , node_size = 300 , edge_color = 'black' ) The concurrency matrix looks differently for directed graphs. If a edge is leaving a node it is represented by -1 and if it is incoming to a node i is +1. nx . linalg . graphmatrix . incidence_matrix ( G1 , oriented = True ) . toarray () array([[-1., -1., -1., 1., 0., 0., 0., 0., 0.], [ 1., 0., 0., -1., -1., -1., 0., 1., 0.], [ 0., 0., 0., 0., 1., 0., -1., 0., 0.], [ 0., 1., 0., 0., 0., 0., 1., -1., -1.], [ 0., 0., 1., 0., 0., 1., 0., 0., 1.]]) We can read from not just edgelists but also from adjacency matrix. For instance, we can create a new graph using the adjacency matrix from the directed graph. # Adjacencey matrix of the graph G1 M1 = nx . linalg . graphmatrix . adj_matrix ( G1 ) . toarray () print ( 'Adjacency matrix of G1' ) print ( M1 ) # convert the matrix into a graph G2 = nx . from_numpy_matrix ( M1 ) # Print the graph information print ( \"Printing G2\" ) nx . write_weighted_edgelist ( G2 , sys . stdout ) Adjacency matrix of G1 [[0 4 0 4 2] [4 0 9 0 1] [0 0 0 4 0] [0 8 0 0 7] [0 0 0 0 0]] Printing G2 0 1 4 0 3 4 0 4 2 1 2 9 1 4 1 1 3 8 2 3 4 3 4 7","title":"Directed graph."},{"location":"Python/Introduction%20to%20Networkx/#visualisation-of-graphs","text":"We can display the graphs using a variety of methods. The weights among the edges can be represented as thickness (or color) of the edges, the importance of nodes can be defined, etc. This will provide additional information of the graph. # get the outward degree for each node and store them as a list of (node_number, degree) out_deg = G1 . out_degree () # Making all the required parameters out_deg = [ int ( out_deg [ node ]) * 1000 for node in G1 . nodes ()] color = [ 'g' if val > np . mean ( out_deg ) else 'r' for val in out_deg ] weight = list ( nx . get_edge_attributes ( G1 , 'weight' ) . values ()) degree = dict ( G1 . degree ) # create the plot and title plt . subplots ( figsize = ( 10 , 10 )) plt . title ( 'Graph containing everything' ) nx . draw_circular ( G1 , with_labels = True , edge_color = color , width = weight , node_size = out_deg ) Anther different type of graph is the bipartite graph, which has a visualisation of its own. In the below example, we take an inbilt biparite graph to show how it can be visualised. B = nx . bipartite . gnmk_random_graph ( 3 , 5 , 10 , seed = 123 ) top = nx . bipartite . sets ( B )[ 0 ] pos = nx . bipartite_layout ( B , top ) nx . draw_networkx ( B , pos = nx . drawing . layout . bipartite_layout ( B , top ))","title":"Visualisation of graphs"},{"location":"Python/Machine%20Learning%20Part%201/","text":"Feature engineering for Machine Learning \u00b6 Predicting absenteeism \u00b6 A large problem within organisations is how to motivate their employees. In this blog, we will use HappyForce in order to predict employment absenteeism. The goal is to identify who are likely to be abscent in the near future, and find th reasons for absenteeism. This blog is the first part and contains Feature engineering and EDA for machine learning. Part 2 contains the machine learning part where we build models and compare the results. This blog shows various ways in which feature engineering can be carried out on time series datasets. Employee absenteeism dataset \u00b6 The datasets contains 7 files, first let us look at the employeeAbsenteeism dataset import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt from datetime import date df = pd . read_csv ( 'employeeAbsenteeism.csv' ) df . to = pd . to_datetime ( df . to ) # converts string to type datetime df [ 'from' ] = pd . to_datetime ( df [ 'from' ]) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee companyAlias from to reason 0 19Q C1 2018-10-01 00:10:00 2018-10-26 00:10:00 Workplace accident 1 NY3 C1 2018-10-01 00:10:00 2018-10-31 00:10:00 Common sickness or accident not related to th... 2 qKO C1 2018-10-01 00:10:00 2018-10-05 00:10:00 Common sickness or accident not related to th... 3 qKO C1 2018-10-10 00:10:00 2018-10-31 00:10:00 Common sickness or accident not related to th... 4 2wx C1 2018-10-01 00:10:00 2018-10-31 00:10:00 Common sickness or accident not related to th... ... ... ... ... ... ... 101 JD1 C1 2018-06-19 00:06:00 2018-06-22 00:06:00 Common sickness or accident not related to th... 102 3WW C1 2018-06-20 00:06:00 2018-06-21 00:06:00 Common sickness or accident not related to th... 103 2ER C1 2018-06-01 00:06:00 2018-06-30 00:06:00 Common sickness or accident not related to th... 104 ONv C1 2018-06-08 00:06:00 2018-06-08 00:06:00 Common sickness or accident not related to th... 105 xyG C1 2018-06-01 00:06:00 2018-06-30 00:06:00 Non job related sickness 106 rows \u00d7 5 columns We can see that the dataset contains details of leaves taken by employees, the from-date, to-date, the leave time, and the reason for leave given by the employee. df . describe ( include = 'all' , datetime_is_numeric = True ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee companyAlias from to reason count 106 106 106 106 106 unique 62 1 NaN NaN 4 top yKX C1 NaN NaN Common sickness or accident not related to th... freq 4 106 NaN NaN 96 mean NaN NaN 2018-08-11 04:12:33.962264320 2018-08-26 23:13:41.886792448 NaN min NaN NaN 2018-06-01 00:06:00 2018-06-06 00:06:00 NaN 25% NaN NaN 2018-06-18 06:06:00 2018-06-30 00:06:00 NaN 50% NaN NaN 2018-08-01 00:08:00 2018-08-31 00:08:00 NaN 75% NaN NaN 2018-10-01 00:10:00 2018-10-23 12:10:00 NaN max NaN NaN 2018-10-31 00:10:00 2018-10-31 00:10:00 NaN Further we can see that the data is across only one company, across 62 employees taking a total of around 106 leaves (not days of leave) with 4 unique reasons. The maximum period of leaves is 30 days, and the freuqency of the leave perod is df [ 'leave_time' ] = ( df [ 'to' ] - df [ 'from' ]) / np . timedelta64 ( 1 , 'D' ) # takes the difference in days df [ 'date' ] = df [ 'from' ] . dt . date . astype ( 'datetime64[ns]' ) # gets only the date part from the timeframe f , ax = plt . subplots ( 1 , figsize = ( 20 , 10 )) df [ 'leave_time' ] . plot . hist () plt . title ( 'Histogram of the leave period' ) plt . show () There are four reasons given for leaves, with \"Common sickness not related to the job\" as the most common reason. df . groupby ( 'reason' ) . \\ aggregate ({ 'leave_time' : 'sum' }) . \\ plot . pie ( y = 'leave_time' , figsize = ( 10 , 10 ), autopct = ' %1.1f%% ' , # to add the percentages text wedgeprops = dict ( width = .5 ) ) plt . ylabel ( \"\" ) plt . title ( 'Proportion of the number of days leaves were taken due to different reasons' ) plt . show () Aditionally, we can observe that the most of the leaves were taken from the first of the month, and ended at the end of the month. f , ax = plt . subplots ( 1 , 2 , figsize = ( 20 , 10 ), sharey = True ) df . groupby ( df [ 'from' ] . dt . day )[ 'employee' ] . count () . \\ plot ( kind = 'bar' , ax = ax [ 0 ], title = 'Number of leaves with \"from\" date' ) df . groupby ( df [ 'to' ] . dt . day )[ 'employee' ] . count () . \\ plot ( kind = 'bar' , ax = ax [ 1 ], title = 'Number of leaves with \"to\" date' ) plt . show () # Feature engineering on absentism dataset # creating a dataframe with leaves as per employee and dates leave_df = pd . DataFrame ({ 'date' :[], 'employee' :[], 'reason' :[]}) for index , row in df . iterrows (): dates = pd . date_range ( row [ 'from' ] . date (), row [ 'to' ] . date ()) for date in dates : leave_df = leave_df . append ({ 'date' : date , 'employee' : row [ 'employee' ], 'reason' : row [ 'reason' ]} , ignore_index = True ) leave_df [ 'on_leave' ] = 1 # Creating a cumulative sum to get tehe number of leaves taken by the employee till date leave_df [ 'no_leaves_till_date' ] = leave_df . groupby ( 'employee' )[ 'on_leave' ] . transform ( lambda x : x . cumsum () . shift ()) . fillna ( 0 ) f , ax = plt . subplots ( 1 , figsize = ( 20 , 10 )) leave_df . groupby ([ 'date' ])[ 'on_leave' ] . sum () . reset_index () . plot ( x = 'date' , y = 'on_leave' , kind = 'scatter' , ax = ax ) plt . title ( 'Number of people on leave' ) plt . show () We can see that most of the leaves are in three months in 2018, indicating that these could be a subset of the data. Employee details \u00b6 The next dataset to look at lastparticipationExists which has the last participation date along with the details of the employee. employee_details = pd . read_csv ( \"lastParticipationExists.csv\" ) employee_details . lastParticipationDate = pd . to_datetime ( employee_details . lastParticipationDate ) employee_details . deletedOn = pd . to_datetime ( employee_details . deletedOn ) employee_details . stillExists = employee_details . stillExists . astype ( int ) employee_details .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee companyAlias numVotes lastParticipationDate timezone stillExists deletedOn 0 l8 C1 285 2019-03-08 01:03:00 Europe/Madrid 1 NaT 1 Xv C1 143 2018-04-21 02:04:00 Europe/Berlin 1 NaT 2 w7 C1 381 2019-03-11 01:03:00 Europe/Madrid 1 NaT 3 jE C1 173 2019-03-01 01:03:00 Europe/Madrid 1 NaT 4 QP C1 312 2019-03-08 01:03:00 Europe/Berlin 1 NaT ... ... ... ... ... ... ... ... 475 D7J C1 29 2018-11-19 01:11:00 Europe/Madrid 0 2018-11-20 13:11:00 476 9KA C1 50 2018-11-09 01:11:00 Europe/Madrid 0 2018-12-13 16:12:00 477 zR7 C1 42 2018-10-26 02:10:00 Europe/Madrid 0 2018-11-20 13:11:00 478 B7E C1 16 2019-01-21 01:01:00 Europe/Madrid 0 2019-02-11 18:02:00 479 QJg C1 1 2018-11-28 01:11:00 Europe/Madrid 0 2019-01-28 10:01:00 480 rows \u00d7 7 columns employee_details . describe ( include = 'all' , datetime_is_numeric = True ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee companyAlias numVotes lastParticipationDate timezone stillExists deletedOn count 480 480 480.000000 470 480 480.00000 105 unique 480 1 NaN NaN 6 NaN NaN top 92v C1 NaN NaN Europe/Madrid NaN NaN freq 1 480 NaN NaN 411 NaN NaN mean NaN NaN 222.570833 2018-12-05 02:49:16.212765696 NaN 0.78125 2018-04-20 20:56:21.142856960 min NaN NaN 0.000000 2017-05-06 02:05:00 NaN 0.00000 2017-05-12 10:05:00 25% NaN NaN 44.000000 2018-12-29 07:09:15 NaN 1.00000 2017-11-09 17:11:00 50% NaN NaN 175.000000 2019-03-08 01:03:00 NaN 1.00000 2018-05-15 17:05:00 75% NaN NaN 374.500000 2019-03-11 01:03:00 NaN 1.00000 2018-08-29 09:08:00 max NaN NaN 671.000000 2019-03-11 01:03:00 NaN 1.00000 2019-03-07 14:03:00 std NaN NaN 193.047398 NaN NaN 0.41383 NaN We can see that this dataset contains more employees than the previous dataset. The previous dataset might be a subset of all the leaves that different people have taken. We can see that most of the people are from Madrid in Europe, while considerable number of employees are from Berlin. employee_details . groupby ( 'timezone' ) . \\ aggregate ({ 'lastParticipationDate' : 'count' }) . \\ plot . pie ( y = 'lastParticipationDate' , figsize = ( 10 , 10 ), autopct = ' %1.1f%% ' , # to add the percentages text wedgeprops = dict ( width = .5 ) ) plt . ylabel ( \"\" ) plt . title ( 'Employees Locations' ) plt . show () f , ax = plt . subplots ( 1 , figsize = ( 20 , 10 )) employee_details . loc [ employee_details . lastParticipationDate . __ne__ ( None )] . \\ loc [ employee_details . stillExists == 0 ] . \\ groupby ( 'lastParticipationDate' ) . aggregate ({ 'stillExists' : 'count' }) . reset_index () . \\ plot ( x = 'lastParticipationDate' , y = 'stillExists' , kind = 'scatter' , ax = ax ) plt . title ( 'Number of people who have left across time' ) plt . xlabel ( 'Number of employees' ) plt . ylabel ( 'Date' ) plt . show () Votes \u00b6 The next dataset of interest is the votes dataset . A listing of all votes registered on Happyforce to the question \"How are you today?\" from the employees on the dataset. All employees do not participate in this survey, but quite a lot of them do regularly. votes = pd . read_csv ( \"votes.csv\" ) votes [ 'voteDate' ] = pd . to_datetime ( votes [ 'voteDate' ]) votes = votes . sort_values ([ 'employee' , 'voteDate' ]) votes .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee companyAlias voteDate vote 17862 13L C1 2017-09-26 02:09:00 2 18093 13L C1 2017-09-27 02:09:00 3 18406 13L C1 2017-09-29 02:09:00 3 18562 13L C1 2017-09-30 02:09:00 4 18635 13L C1 2017-10-01 02:10:00 3 ... ... ... ... ... 90055 zyx C1 2018-12-15 01:12:00 3 90797 zyx C1 2018-12-19 01:12:00 2 91235 zyx C1 2018-12-21 01:12:00 2 91983 zyx C1 2018-12-26 01:12:00 3 92186 zyx C1 2018-12-28 01:12:00 2 106834 rows \u00d7 4 columns votes . describe ( include = 'all' , datetime_is_numeric = True ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee companyAlias voteDate vote count 106834 106834 106834 106834.000000 unique 472 1 NaN NaN top xE4 C1 NaN NaN freq 671 106834 NaN NaN mean NaN NaN 2018-05-06 22:41:28.348840192 2.849580 min NaN NaN 2017-05-03 02:05:00 1.000000 25% NaN NaN 2017-11-27 01:11:00 2.000000 50% NaN NaN 2018-05-21 02:05:00 3.000000 75% NaN NaN 2018-10-19 02:10:00 4.000000 max NaN NaN 2019-03-11 01:03:00 4.000000 std NaN NaN NaN 0.980259 EDA \u00b6 We can see that the votes vary between 1 to 4, with averae being 2.89. This survey has data from 3-5-2017 to 11-3-2019. This survey was taken by 472 employees. While the data has details from 3-5-2017, there would be employees who joined after the survey has already started. Identifying them will help us to look for patterns over time for new employees. We are considering any employee that has the first vote 100 days after the inception (3-5-2017) as a new employee. f , ax = plt . subplots ( 1 , figsize = ( 20 , 10 )) votes . vote . hist ( grid = False , ax = ax ) plt . title ( 'Histogram of the votes' ) plt . xlabel ( 'Vote' ) plt . ylabel ( 'Frequency' ) plt . show () Feature engineering \u00b6 We can observe that the default vote is 3, with more happy employees using 4 and less happy employees using 2 or 3. For the modelling, we want to create features which take only the data before the observation into account, and it is useful to have running means to capture the default charectersticks of the employee, and also to capture the latest 2 votes to capture if there has been any change from the normal recently. # Feature engineering on the votes dataset votes [ 'min_date' ] = votes . groupby ( 'employee' )[ 'voteDate' ] . transform ( min ) # new employee is 100 day since inception of the software votes [ 'new_employee' ] = ( votes . min_date >= '13-06-2017 00:00:00' ) . astype ( int ) # Getting the difference in days since first vote in days votes [ 'no_of_days_since_first_vote' ] = (( votes . voteDate - votes . min_date ) / np . timedelta64 ( 1 , 'D' )) . astype ( int ) + 1 # Calculating the cumulative number of votes till date (only considering the past: this step should be done after order by) votes [ 'no_of_votes_till_date' ] = votes . groupby ( 'employee' )[ 'voteDate' ] . transform ( 'cumcount' ) + 1 # calculating the percentage of days that the employee has voted till date votes [ 'perc_days_voted' ] = votes [ 'no_of_votes_till_date' ] / votes [ 'no_of_days_since_first_vote' ] # average of the vote till date by the employee votes [ 'avg_vote_till_date' ] = votes . groupby ( 'employee' )[ 'vote' ] . transform ( 'cumsum' ) / ( votes [ 'no_of_votes_till_date' ]) # Average vote in general of the employee votes [ 'avg_vote' ] = votes . groupby ( 'employee' )[ 'vote' ] . transform ( np . mean ) # The average of the last two votes (The last two votes might redict the probability of taking a leave now) votes [ 'last_2_votes_avg' ] = list ( votes . groupby ( 'employee' ) . rolling ( 2 , min_periods = 1 )[ 'vote' ] . apply ( np . mean )) votes [ 'date' ] = votes . voteDate . dt . date . astype ( 'datetime64[ns]' ) One observation is that the new employee is consistantly more enthusiastic than the older employees, and we can see a trend of declining average votes as time progresses for both new as well as older employee. fs , axs = plt . subplots ( 1 , figsize = ( 20 , 10 )) votes . groupby ([ 'new_employee' , 'no_of_days_since_first_vote' ]) . agg ({ 'vote' : 'mean' }) . reset_index () . \\ plot . scatter ( x = 'no_of_days_since_first_vote' , y = 'vote' , c = 'new_employee' , alpha = 0.4 , cmap = sns . diverging_palette ( 10 , 133 , as_cmap = True ), ax = axs ) #, vmin = 2.0, vmax=3.5) axs . set_ylim ([ 2 , 3.5 ]) plt . title ( 'Averge vote across new and old employee' ) plt . xlabel ( 'Number of days since first vote' ) plt . show () We can also se a seasonal pattern in the average vote fs , axs = plt . subplots ( 1 , figsize = ( 20 , 10 )) votes . groupby ([ 'voteDate' ]) . agg ({ 'vote' : 'mean' }) . reset_index () . \\ plot . scatter ( x = 'voteDate' , y = 'vote' , alpha = 0.4 , ax = axs ) plt . title ( 'Seasonality in the average vote' ) plt . xlabel ( 'Date of vote' ) plt . show () # joining the votes table with employee details to add the details of the employee votes = votes . merge ( employee_details , how = 'left' , on = 'employee' ) Does the average vote decrease just before a person quits the organisation? We can identify this by filtering for the employees who have already quit, and look at the average votes since the last day. We identify that the votes are actually pretty consistant with a mean around 3 and decreases in the notice period(of presumably 60 days). people_who_left = votes [ votes . stillExists == 0 ] . copy () people_who_left . loc [:, 'no_of_days_since_exit' ] = (( people_who_left . voteDate - people_who_left . deletedOn ) / np . timedelta64 ( 1 , 'D' )) . astype ( int ) - 1 fs , axs = plt . subplots ( 1 , figsize = ( 20 , 10 )) people_who_left . groupby ([ 'new_employee' , 'no_of_days_since_exit' ]) . agg ({ 'vote' : 'mean' }) . reset_index () . \\ plot . scatter ( x = 'no_of_days_since_exit' , y = 'vote' , alpha = 0.4 , ax = axs ) axs . set_xlim ([ - 200 , 0 ]) axs . set_ylim ([ 2 , 3.5 ]) plt . title ( 'Averge vote for employees who have left the organisation' ) plt . xlabel ( 'Number of days since exit date' ) plt . show () As we have employees across 5 timezones, the average votes across timezones are also shown. We can see distinct seasonality patterns across different geographies. Some gerographies have low variation as they have very few employees. fs , axs = plt . subplots ( 1 , figsize = ( 20 , 10 )) sns . scatterplot ( data = votes . groupby ([ 'timezone' , 'voteDate' ]) . agg ({ 'vote' : 'mean' }) . reset_index (), x = 'voteDate' , y = 'vote' , hue = 'timezone' , alpha = 0.4 , ax = axs ) plt . title ( 'Averge vote for employees in different geographies' ) plt . xlabel ( 'Vote date' ) plt . show () Feedback dataset \u00b6 The next dataset of interest is the comments feedback dataset. This contains all the different comments given by employees, and the number of likes and dislikes on each of the comment. feedback = pd . read_csv ( \"comments_by_employees_in_anonymous_forum.csv\" ) feedback .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee companyAlias commentId commentLength likes dislikes commentDate feedbackType 0 aKP C1 5909b33da2ede4000473da6f 17 9 0 2017-05-03 12:05 OTHER 1 dNx C1 5909b6aca2ede4000473da72 25 12 0 2017-05-03 12:05 OTHER 2 ONv C1 5909c2dea2ede4000473db8c 58 33 5 2017-05-03 13:05 OTHER 3 e9M C1 5909d32ea2ede4000473db97 56 11 4 2017-05-03 14:05 OTHER 4 RWM C1 5909f227a2ede4000473dcbe 105 18 0 2017-05-03 17:05 OTHER ... ... ... ... ... ... ... ... ... 5067 7o1 C1 5c7108e8434c4500041722b0 28 0 0 2019-02-23 09:02 OTHER 5068 N3 C1 5c71519ca9f66e00042896f6 14 0 0 2019-02-23 14:02 OTHER 5069 DNY C1 5c73b11e50b72e0004cab283 63 0 0 2019-02-25 10:02 OTHER 5070 72j C1 5c744971e29c7b0004391da3 44 0 0 2019-02-25 21:02 OTHER 5071 qKO C1 5c781339efad100004ebb886 39 0 0 2019-02-28 17:02 OTHER 5072 rows \u00d7 8 columns feedback . describe ( include = 'all' , datetime_is_numeric = True ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee companyAlias commentId commentLength likes dislikes commentDate feedbackType count 5072 5072 5072 5072.000000 5072.000000 5072.00000 5072 5072 unique 305 1 5072 NaN NaN NaN 3580 5 top 4ov C1 59ab0ee58021aa0004bb7258 NaN NaN NaN 2018-09-10 09:09 OTHER freq 373 5072 1 NaN NaN NaN 26 3188 mean NaN NaN NaN 168.518336 13.605875 4.89097 NaN NaN std NaN NaN NaN 193.802568 15.280530 6.62993 NaN NaN min NaN NaN NaN 1.000000 0.000000 0.00000 NaN NaN 25% NaN NaN NaN 48.000000 2.000000 0.00000 NaN NaN 50% NaN NaN NaN 111.000000 9.000000 3.00000 NaN NaN 75% NaN NaN NaN 223.000000 20.000000 7.00000 NaN NaN max NaN NaN NaN 2509.000000 135.000000 65.00000 NaN NaN EDA \u00b6 We can observe that as the number of interactions becomes larger, the overall trend goes either towards likes or dislikes. We can also observe that comments above 80 interactions are usually highly liked. feedback [ 'log_comment_length' ] = np . log10 ( feedback . commentLength ) feedback [ 'total_interactions' ] = feedback . likes + feedback . dislikes feedback [ 'mean_feedback' ] = ( feedback . likes - feedback . dislikes ) / feedback . total_interactions fs , axs = plt . subplots ( 1 , figsize = ( 20 , 10 )) sns . scatterplot ( data = feedback , x = 'total_interactions' , y = 'mean_feedback' , hue = 'log_comment_length' , alpha = 0.8 , ax = axs ) plt . title ( 'Mean interaction vs response level for employee' ) plt . xlabel ( 'Number of interactions' ) plt . ylabel ( 'Mean of feedback' ) plt . show () We can see that we have five types of comments feedback . feedbackType . unique () feedback . groupby ( 'feedbackType' ) . \\ aggregate ({ 'commentId' : 'count' }) . \\ plot . pie ( y = 'commentId' , figsize = ( 10 , 10 ), autopct = ' %1.1f%% ' , # to add the percentages text wedgeprops = dict ( width = .5 ) ) plt . ylabel ( \"\" ) plt . title ( 'Frequency of somments' ) plt . show () Similarly we can see a trend between different comment types, with congratulations receiving more likes than dislikes, while suggestions and information having similar distibutions. We can also see how the number of likes are skewed. fs , axs = plt . subplots ( 1 , figsize = ( 20 , 10 )) sns . scatterplot ( data = feedback , x = 'likes' , y = 'dislikes' , hue = 'feedbackType' , alpha = 0.4 , ax = axs ) plt . title ( 'Total likes and dislikes for comments' ) plt . show () feedback . groupby ( 'feedbackType' ) . aggregate ({ 'likes' : 'mean' , 'dislikes' : 'mean' }) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } likes dislikes feedbackType CONGRATULATION 30.941538 3.824615 CRITICISM 11.125000 1.562500 INFORMATION 18.751152 7.657450 OTHER 9.325282 3.705144 SUGGESTION 19.219917 7.290456 f , ax = plt . subplots ( 2 , figsize = ( 20 , 20 ), sharex = True ) sns . boxplot ( data = feedback . groupby ([ 'feedbackType' , 'employee' ])[ 'commentId' ] . count () . reset_index () . assign ( log_count_comments = lambda df : np . log10 ( df . commentId ) ), x = 'feedbackType' , y = 'log_count_comments' , ax = ax [ 0 ]) sns . boxplot ( data = feedback , x = 'feedbackType' , y = 'log_comment_length' , ax = ax [ 1 ]) ax [ 0 ] . set_ylabel ( 'Log of number of comments' ) ax [ 1 ] . set_ylabel ( 'Log of comment length' ) ax [ 0 ] . set_title ( 'Number of comments and comment legth across different feedback types' ) plt . show () Feature engineering \u00b6 # feature engineering on the feedback dataset feedback [ 'commentDate' ] = pd . to_datetime ( feedback [ 'commentDate' ]) # sort by date in ascending to do cumulative sum and rolling calculations feedback = feedback . sort_values ([ 'employee' , 'commentDate' ]) # Count the likes and dislikes till the date given feedback [ 'likes_till_date' ] = feedback . groupby ( 'employee' )[ 'likes' ] . transform ( 'cumsum' ) feedback [ 'dislikes_till_date' ] = feedback . groupby ( 'employee' )[ 'dislikes' ] . transform ( 'cumsum' ) feedback [ 'comments_till_date' ] = feedback . groupby ( 'employee' )[ 'commentId' ] . transform ( 'cumcount' ) # Rolling two likes and dislikes, indicating the last two likes and dislikes feedback [ 'last_2_likes' ] = list ( feedback . groupby ( 'employee' ) . rolling ( 2 , min_periods = 1 )[ 'likes' ] . apply ( sum )) feedback [ 'last_2_dislikes' ] = list ( feedback . groupby ( 'employee' ) . rolling ( 2 , min_periods = 1 )[ 'dislikes' ] . apply ( sum )) feedback [ 'date' ] = feedback . commentDate . dt . date . astype ( 'datetime64[ns]' ) feedback .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee companyAlias commentId commentLength likes dislikes commentDate feedbackType log_comment_length total_interactions mean_feedback likes_till_date dislikes_till_date comments_till_date last_2_likes last_2_dislikes date 4373 19Q C1 5959ec954040610004272a21 11 0 0 2017-07-03 09:07:00 OTHER 1.041393 0 NaN 0 0 0 0.0 0.0 2017-07-03 970 19Q C1 5970e1483da0e10004b17a27 64 3 4 2017-07-20 18:07:00 OTHER 1.806180 7 -0.142857 3 4 1 3.0 4.0 2017-07-20 1903 19Q C1 5a36f82b26c0110004c55d90 57 17 6 2017-12-18 00:12:00 OTHER 1.755875 23 0.478261 20 10 2 20.0 10.0 2017-12-18 1949 19Q C1 5a40e7d3de51cb00042dfda4 31 5 0 2017-12-25 12:12:00 OTHER 1.491362 5 1.000000 25 10 3 22.0 6.0 2017-12-25 1972 19Q C1 5a4a8275eb84e0000492659f 11 8 0 2018-01-01 19:01:00 OTHER 1.041393 8 1.000000 33 10 4 13.0 0.0 2018-01-01 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 4740 zRx C1 5be95746ed7ae70004b83417 11 0 0 2018-11-12 11:11:00 OTHER 1.041393 0 NaN 0 0 0 0.0 0.0 2018-11-12 4746 zRx C1 5be95746ed7ae70004b83415 26 0 0 2018-11-12 11:11:00 OTHER 1.414973 0 NaN 0 0 1 0.0 0.0 2018-11-12 4758 zRx C1 5be95747ed7ae70004b83419 33 0 0 2018-11-12 11:11:00 OTHER 1.518514 0 NaN 0 0 2 0.0 0.0 2018-11-12 4764 zRx C1 5be95746ed7ae70004b83418 11 0 0 2018-11-12 11:11:00 OTHER 1.041393 0 NaN 0 0 3 0.0 0.0 2018-11-12 4781 zRx C1 5be95746ed7ae70004b83416 8 0 0 2018-11-12 11:11:00 OTHER 0.903090 0 NaN 0 0 4 0.0 0.0 2018-11-12 5072 rows \u00d7 17 columns Merging all the datasets \u00b6 Now we combine employee abseteeism data with votes and comments datasets. We have the following issues: 1. The absenteeism dataset is a subset of the data and does not cover all employees and months 2. Not all employees have voted and even those who have voted have voted infrequently 3. Not all employees have commented and even those who have posted a comment posted infrequently So we have the following assumptions: 1. We are working with the data only for the employees that exist in the absenteeism dataset 2. We are assuming that the last vote of the employee talks about the how the employee today 3. We are also assuming that the last comment and the likes and dislikes have an effect on the employee With these assumptions, we combine the datasets. # Creating a dataframe containing the time from the start to the end of voting period time_dataframe = pd . DataFrame ({ 'date' : pd . date_range ( min ( votes [ 'voteDate' ]) . date (), max ( votes [ 'voteDate' ]) . date ())}) time_dataframe [ 'tmp' ] = 1 # Creating a dataframe with all employees in the absenteeism dataset complete_df = pd . DataFrame ({ 'employee' : df . employee . unique ()}) complete_df [ 'tmp' ] = 1 # Creating a dataset that contains the combinations of all days for all employees complete_df = pd . merge ( complete_df , time_dataframe , on = [ 'tmp' ]) . drop ( 'tmp' , axis = 1 ) # Joining the feedback (comments) given by the employees complete_df = pd . merge ( complete_df , feedback , how = 'left' , on = [ 'date' , 'employee' ]) complete_df = complete_df [[ 'employee' , 'date' , 'likes' , 'dislikes' , 'feedbackType' , 'likes_till_date' , 'dislikes_till_date' , 'last_2_likes' , 'last_2_dislikes' , 'commentDate' ]] # Filling the last available feedback data the days when there was no feedback data for an employee. complete_df [[ 'date' , 'likes' , 'dislikes' , 'feedbackType' , 'likes_till_date' , 'dislikes_till_date' , 'last_2_likes' , 'last_2_dislikes' , 'commentDate' ]] = \\ complete_df . groupby ( 'employee' ) . fillna ( method = 'ffill' ) # Creating new features complete_df [ 'days_since_last_comment' ] = ( complete_df . date - complete_df . commentDate ) complete_df [ 'days_since_last_comment' ] = ( complete_df [ 'days_since_last_comment' ] . fillna ( pd . Timedelta ( seconds = 0 )) / np . timedelta64 ( 1 , 'D' )) . astype ( int ) # Assuming that for the employees that hae not commented (yet) the feedback is 0. complete_df = \\ complete_df [[ 'employee' , 'date' , 'likes' , 'dislikes' , 'feedbackType' , 'likes_till_date' , 'dislikes_till_date' , 'last_2_likes' , 'last_2_dislikes' , 'days_since_last_comment' ]] . \\ fillna ( 0 ) # Joining the votes dataset fr every eployee-date complete_df = pd . merge ( complete_df , votes , how = 'left' , on = [ 'date' , 'employee' ]) # Filling the last available votes data the days when there was no vote for an employee. complete_df [[ 'no_of_votes_till_date' , 'perc_days_voted' , 'deletedOn' , 'new_employee' , 'min_date' , 'stillExists' , 'vote' , 'avg_vote_till_date' , 'avg_vote' , 'last_2_votes_avg' , 'voteDate' , 'timezone' ]] = \\ complete_df . groupby ( 'employee' )[[ 'no_of_votes_till_date' , 'perc_days_voted' , 'deletedOn' , 'new_employee' , 'min_date' , 'stillExists' , 'vote' , 'avg_vote_till_date' , 'avg_vote' , 'last_2_votes_avg' , 'voteDate' , 'timezone' ]] . \\ fillna ( method = 'ffill' ) # Remove the data before the employee joined complete_df = complete_df [ complete_df . avg_vote >= 0 ] # Remove data after employee left complete_df = complete_df [( complete_df . stillExists == 1 ) | (( complete_df . stillExists == 0 ) & ( complete_df . date <= complete_df . deletedOn ))] # Recomputing no_of_days_since_first_vote complete_df . no_of_days_since_first_vote = (( complete_df . date - complete_df . min_date ) / np . timedelta64 ( 1 , 'D' )) . astype ( int ) + 1 # Adding new features # assuming a 60 day notice period # 60 days before the employee leaves are recorded complete_df . deletedOn = complete_df . deletedOn . fillna ( pd . to_datetime ( date . today ())) complete_df [ 'countdown_to_last_day' ] = (( complete_df . date - complete_df . deletedOn ) / np . timedelta64 ( 1 , 'D' )) . astype ( int ) + 1 complete_df . loc [ complete_df . countdown_to_last_day < - 15 , 'countdown_to_last_day' ] = 999 # computing days since last vote complete_df [ 'days_since_last_vote' ] = ( complete_df . date - complete_df . voteDate ) complete_df [ 'days_since_last_vote' ] = ( complete_df [ 'days_since_last_vote' ] . fillna ( pd . Timedelta ( seconds = 0 )) / np . timedelta64 ( 1 , 'D' )) . astype ( int ) # Imputing still exists column complete_df [[ 'timezone' , 'stillExists' ]] = complete_df . groupby ( 'employee' )[[ 'timezone' , 'stillExists' ]] . fillna ( method = 'bfill' ) # Selecting the features used in the model complete_df = complete_df [ [ 'employee' , 'date' , 'likes' , 'dislikes' , 'feedbackType' , 'likes_till_date' , 'dislikes_till_date' , 'last_2_likes' , 'last_2_dislikes' , 'days_since_last_comment' , 'vote' , 'timezone' , 'stillExists' , 'no_of_days_since_first_vote' , 'no_of_votes_till_date' , 'perc_days_voted' , 'avg_vote_till_date' , 'avg_vote' , 'last_2_votes_avg' , 'days_since_last_vote' , 'new_employee' , 'countdown_to_last_day' ]] . fillna ( 0 ) pd . options . display . max_columns = None # Combining the leaves dataset data = pd . merge ( complete_df , leave_df , how = 'left' , on = [ 'date' , 'employee' ]) data . on_leave = data . on_leave . fillna ( 0 ) # Creating new columns data . no_leaves_till_date = data . no_leaves_till_date . fillna ( method = 'bfill' ) . fillna ( method = 'ffill' ) # Get the number of days the person was on leave in the last two days def get_rolling_sum ( grp , freq , col ): return ( grp . rolling ( freq , on = 'date' )[ col ] . sum ()) data = data . sort_values ([ 'employee' , 'date' ]) data [ 'last_2_days_leaves' ] = data . groupby ( 'employee' , as_index = False , group_keys = False ) . \\ apply ( get_rolling_sum , '2D' , 'on_leave' ) # 2d for 2 days # Get the leave status of the previous day data [ 'previous_day_leave' ] = data . groupby ( 'employee' )[ 'on_leave' ] . shift () . fillna ( 0 ) # Renaming columns to more suitable ones data . rename ( columns = { \"likes\" : \"last_likes\" , \"dislikes\" : \"last_dislikes\" , \"feedback_type\" : \"last_feedback_type\" , \"vote\" : \"last_vote\" , \"new_employee\" : \"employee_joined_after_jun17\" }, inplace = True ) The correlation matrix for all the columns is red_green_cmap = sns . diverging_palette ( 10 , 133 , as_cmap = True ) f = plt . figure ( figsize = ( 20 , 20 )) plt . matshow ( data . corr (), fignum = f , cmap = red_green_cmap , vmin =- 1 , vmax = 1 ) plt . xticks ( range ( data . select_dtypes ([ 'number' ]) . shape [ 1 ]), data . select_dtypes ([ 'number' ]) . columns , rotation = 90 , fontsize = 15 ) plt . yticks ( range ( data . select_dtypes ([ 'number' ]) . shape [ 1 ]), data . select_dtypes ([ 'number' ]) . columns , rotation = 0 , fontsize = 15 ) cb = plt . colorbar () cb . ax . tick_params ( labelsize = 14 ) plt . title ( \"Correlation between different parameters\" , fontsize = 20 , loc = 'left' ) plt . show () The distributions across all the columns are _ = data . hist ( figsize = ( 20 , 20 )) The second part of the blog is here . References \u00b6 Notes, Wrokforce Analytics Live lecture, Imperial college London, Class 2020-22 Kyburz J, Morelli D, Schaaf A, Villani F, Wheatley D: Predicting absenteeism Harsha A, Shaked A, Artem G, Tebogo M, Gokhan M: Predicting absenteeism","title":"Feature engineering (Python)"},{"location":"Python/Machine%20Learning%20Part%201/#feature-engineering-for-machine-learning","text":"","title":"Feature engineering for Machine Learning"},{"location":"Python/Machine%20Learning%20Part%201/#predicting-absenteeism","text":"A large problem within organisations is how to motivate their employees. In this blog, we will use HappyForce in order to predict employment absenteeism. The goal is to identify who are likely to be abscent in the near future, and find th reasons for absenteeism. This blog is the first part and contains Feature engineering and EDA for machine learning. Part 2 contains the machine learning part where we build models and compare the results. This blog shows various ways in which feature engineering can be carried out on time series datasets.","title":"Predicting absenteeism"},{"location":"Python/Machine%20Learning%20Part%201/#employee-absenteeism-dataset","text":"The datasets contains 7 files, first let us look at the employeeAbsenteeism dataset import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt from datetime import date df = pd . read_csv ( 'employeeAbsenteeism.csv' ) df . to = pd . to_datetime ( df . to ) # converts string to type datetime df [ 'from' ] = pd . to_datetime ( df [ 'from' ]) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee companyAlias from to reason 0 19Q C1 2018-10-01 00:10:00 2018-10-26 00:10:00 Workplace accident 1 NY3 C1 2018-10-01 00:10:00 2018-10-31 00:10:00 Common sickness or accident not related to th... 2 qKO C1 2018-10-01 00:10:00 2018-10-05 00:10:00 Common sickness or accident not related to th... 3 qKO C1 2018-10-10 00:10:00 2018-10-31 00:10:00 Common sickness or accident not related to th... 4 2wx C1 2018-10-01 00:10:00 2018-10-31 00:10:00 Common sickness or accident not related to th... ... ... ... ... ... ... 101 JD1 C1 2018-06-19 00:06:00 2018-06-22 00:06:00 Common sickness or accident not related to th... 102 3WW C1 2018-06-20 00:06:00 2018-06-21 00:06:00 Common sickness or accident not related to th... 103 2ER C1 2018-06-01 00:06:00 2018-06-30 00:06:00 Common sickness or accident not related to th... 104 ONv C1 2018-06-08 00:06:00 2018-06-08 00:06:00 Common sickness or accident not related to th... 105 xyG C1 2018-06-01 00:06:00 2018-06-30 00:06:00 Non job related sickness 106 rows \u00d7 5 columns We can see that the dataset contains details of leaves taken by employees, the from-date, to-date, the leave time, and the reason for leave given by the employee. df . describe ( include = 'all' , datetime_is_numeric = True ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee companyAlias from to reason count 106 106 106 106 106 unique 62 1 NaN NaN 4 top yKX C1 NaN NaN Common sickness or accident not related to th... freq 4 106 NaN NaN 96 mean NaN NaN 2018-08-11 04:12:33.962264320 2018-08-26 23:13:41.886792448 NaN min NaN NaN 2018-06-01 00:06:00 2018-06-06 00:06:00 NaN 25% NaN NaN 2018-06-18 06:06:00 2018-06-30 00:06:00 NaN 50% NaN NaN 2018-08-01 00:08:00 2018-08-31 00:08:00 NaN 75% NaN NaN 2018-10-01 00:10:00 2018-10-23 12:10:00 NaN max NaN NaN 2018-10-31 00:10:00 2018-10-31 00:10:00 NaN Further we can see that the data is across only one company, across 62 employees taking a total of around 106 leaves (not days of leave) with 4 unique reasons. The maximum period of leaves is 30 days, and the freuqency of the leave perod is df [ 'leave_time' ] = ( df [ 'to' ] - df [ 'from' ]) / np . timedelta64 ( 1 , 'D' ) # takes the difference in days df [ 'date' ] = df [ 'from' ] . dt . date . astype ( 'datetime64[ns]' ) # gets only the date part from the timeframe f , ax = plt . subplots ( 1 , figsize = ( 20 , 10 )) df [ 'leave_time' ] . plot . hist () plt . title ( 'Histogram of the leave period' ) plt . show () There are four reasons given for leaves, with \"Common sickness not related to the job\" as the most common reason. df . groupby ( 'reason' ) . \\ aggregate ({ 'leave_time' : 'sum' }) . \\ plot . pie ( y = 'leave_time' , figsize = ( 10 , 10 ), autopct = ' %1.1f%% ' , # to add the percentages text wedgeprops = dict ( width = .5 ) ) plt . ylabel ( \"\" ) plt . title ( 'Proportion of the number of days leaves were taken due to different reasons' ) plt . show () Aditionally, we can observe that the most of the leaves were taken from the first of the month, and ended at the end of the month. f , ax = plt . subplots ( 1 , 2 , figsize = ( 20 , 10 ), sharey = True ) df . groupby ( df [ 'from' ] . dt . day )[ 'employee' ] . count () . \\ plot ( kind = 'bar' , ax = ax [ 0 ], title = 'Number of leaves with \"from\" date' ) df . groupby ( df [ 'to' ] . dt . day )[ 'employee' ] . count () . \\ plot ( kind = 'bar' , ax = ax [ 1 ], title = 'Number of leaves with \"to\" date' ) plt . show () # Feature engineering on absentism dataset # creating a dataframe with leaves as per employee and dates leave_df = pd . DataFrame ({ 'date' :[], 'employee' :[], 'reason' :[]}) for index , row in df . iterrows (): dates = pd . date_range ( row [ 'from' ] . date (), row [ 'to' ] . date ()) for date in dates : leave_df = leave_df . append ({ 'date' : date , 'employee' : row [ 'employee' ], 'reason' : row [ 'reason' ]} , ignore_index = True ) leave_df [ 'on_leave' ] = 1 # Creating a cumulative sum to get tehe number of leaves taken by the employee till date leave_df [ 'no_leaves_till_date' ] = leave_df . groupby ( 'employee' )[ 'on_leave' ] . transform ( lambda x : x . cumsum () . shift ()) . fillna ( 0 ) f , ax = plt . subplots ( 1 , figsize = ( 20 , 10 )) leave_df . groupby ([ 'date' ])[ 'on_leave' ] . sum () . reset_index () . plot ( x = 'date' , y = 'on_leave' , kind = 'scatter' , ax = ax ) plt . title ( 'Number of people on leave' ) plt . show () We can see that most of the leaves are in three months in 2018, indicating that these could be a subset of the data.","title":"Employee absenteeism dataset"},{"location":"Python/Machine%20Learning%20Part%201/#employee-details","text":"The next dataset to look at lastparticipationExists which has the last participation date along with the details of the employee. employee_details = pd . read_csv ( \"lastParticipationExists.csv\" ) employee_details . lastParticipationDate = pd . to_datetime ( employee_details . lastParticipationDate ) employee_details . deletedOn = pd . to_datetime ( employee_details . deletedOn ) employee_details . stillExists = employee_details . stillExists . astype ( int ) employee_details .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee companyAlias numVotes lastParticipationDate timezone stillExists deletedOn 0 l8 C1 285 2019-03-08 01:03:00 Europe/Madrid 1 NaT 1 Xv C1 143 2018-04-21 02:04:00 Europe/Berlin 1 NaT 2 w7 C1 381 2019-03-11 01:03:00 Europe/Madrid 1 NaT 3 jE C1 173 2019-03-01 01:03:00 Europe/Madrid 1 NaT 4 QP C1 312 2019-03-08 01:03:00 Europe/Berlin 1 NaT ... ... ... ... ... ... ... ... 475 D7J C1 29 2018-11-19 01:11:00 Europe/Madrid 0 2018-11-20 13:11:00 476 9KA C1 50 2018-11-09 01:11:00 Europe/Madrid 0 2018-12-13 16:12:00 477 zR7 C1 42 2018-10-26 02:10:00 Europe/Madrid 0 2018-11-20 13:11:00 478 B7E C1 16 2019-01-21 01:01:00 Europe/Madrid 0 2019-02-11 18:02:00 479 QJg C1 1 2018-11-28 01:11:00 Europe/Madrid 0 2019-01-28 10:01:00 480 rows \u00d7 7 columns employee_details . describe ( include = 'all' , datetime_is_numeric = True ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee companyAlias numVotes lastParticipationDate timezone stillExists deletedOn count 480 480 480.000000 470 480 480.00000 105 unique 480 1 NaN NaN 6 NaN NaN top 92v C1 NaN NaN Europe/Madrid NaN NaN freq 1 480 NaN NaN 411 NaN NaN mean NaN NaN 222.570833 2018-12-05 02:49:16.212765696 NaN 0.78125 2018-04-20 20:56:21.142856960 min NaN NaN 0.000000 2017-05-06 02:05:00 NaN 0.00000 2017-05-12 10:05:00 25% NaN NaN 44.000000 2018-12-29 07:09:15 NaN 1.00000 2017-11-09 17:11:00 50% NaN NaN 175.000000 2019-03-08 01:03:00 NaN 1.00000 2018-05-15 17:05:00 75% NaN NaN 374.500000 2019-03-11 01:03:00 NaN 1.00000 2018-08-29 09:08:00 max NaN NaN 671.000000 2019-03-11 01:03:00 NaN 1.00000 2019-03-07 14:03:00 std NaN NaN 193.047398 NaN NaN 0.41383 NaN We can see that this dataset contains more employees than the previous dataset. The previous dataset might be a subset of all the leaves that different people have taken. We can see that most of the people are from Madrid in Europe, while considerable number of employees are from Berlin. employee_details . groupby ( 'timezone' ) . \\ aggregate ({ 'lastParticipationDate' : 'count' }) . \\ plot . pie ( y = 'lastParticipationDate' , figsize = ( 10 , 10 ), autopct = ' %1.1f%% ' , # to add the percentages text wedgeprops = dict ( width = .5 ) ) plt . ylabel ( \"\" ) plt . title ( 'Employees Locations' ) plt . show () f , ax = plt . subplots ( 1 , figsize = ( 20 , 10 )) employee_details . loc [ employee_details . lastParticipationDate . __ne__ ( None )] . \\ loc [ employee_details . stillExists == 0 ] . \\ groupby ( 'lastParticipationDate' ) . aggregate ({ 'stillExists' : 'count' }) . reset_index () . \\ plot ( x = 'lastParticipationDate' , y = 'stillExists' , kind = 'scatter' , ax = ax ) plt . title ( 'Number of people who have left across time' ) plt . xlabel ( 'Number of employees' ) plt . ylabel ( 'Date' ) plt . show ()","title":"Employee details"},{"location":"Python/Machine%20Learning%20Part%201/#votes","text":"The next dataset of interest is the votes dataset . A listing of all votes registered on Happyforce to the question \"How are you today?\" from the employees on the dataset. All employees do not participate in this survey, but quite a lot of them do regularly. votes = pd . read_csv ( \"votes.csv\" ) votes [ 'voteDate' ] = pd . to_datetime ( votes [ 'voteDate' ]) votes = votes . sort_values ([ 'employee' , 'voteDate' ]) votes .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee companyAlias voteDate vote 17862 13L C1 2017-09-26 02:09:00 2 18093 13L C1 2017-09-27 02:09:00 3 18406 13L C1 2017-09-29 02:09:00 3 18562 13L C1 2017-09-30 02:09:00 4 18635 13L C1 2017-10-01 02:10:00 3 ... ... ... ... ... 90055 zyx C1 2018-12-15 01:12:00 3 90797 zyx C1 2018-12-19 01:12:00 2 91235 zyx C1 2018-12-21 01:12:00 2 91983 zyx C1 2018-12-26 01:12:00 3 92186 zyx C1 2018-12-28 01:12:00 2 106834 rows \u00d7 4 columns votes . describe ( include = 'all' , datetime_is_numeric = True ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee companyAlias voteDate vote count 106834 106834 106834 106834.000000 unique 472 1 NaN NaN top xE4 C1 NaN NaN freq 671 106834 NaN NaN mean NaN NaN 2018-05-06 22:41:28.348840192 2.849580 min NaN NaN 2017-05-03 02:05:00 1.000000 25% NaN NaN 2017-11-27 01:11:00 2.000000 50% NaN NaN 2018-05-21 02:05:00 3.000000 75% NaN NaN 2018-10-19 02:10:00 4.000000 max NaN NaN 2019-03-11 01:03:00 4.000000 std NaN NaN NaN 0.980259","title":"Votes"},{"location":"Python/Machine%20Learning%20Part%201/#eda","text":"We can see that the votes vary between 1 to 4, with averae being 2.89. This survey has data from 3-5-2017 to 11-3-2019. This survey was taken by 472 employees. While the data has details from 3-5-2017, there would be employees who joined after the survey has already started. Identifying them will help us to look for patterns over time for new employees. We are considering any employee that has the first vote 100 days after the inception (3-5-2017) as a new employee. f , ax = plt . subplots ( 1 , figsize = ( 20 , 10 )) votes . vote . hist ( grid = False , ax = ax ) plt . title ( 'Histogram of the votes' ) plt . xlabel ( 'Vote' ) plt . ylabel ( 'Frequency' ) plt . show ()","title":"EDA"},{"location":"Python/Machine%20Learning%20Part%201/#feature-engineering","text":"We can observe that the default vote is 3, with more happy employees using 4 and less happy employees using 2 or 3. For the modelling, we want to create features which take only the data before the observation into account, and it is useful to have running means to capture the default charectersticks of the employee, and also to capture the latest 2 votes to capture if there has been any change from the normal recently. # Feature engineering on the votes dataset votes [ 'min_date' ] = votes . groupby ( 'employee' )[ 'voteDate' ] . transform ( min ) # new employee is 100 day since inception of the software votes [ 'new_employee' ] = ( votes . min_date >= '13-06-2017 00:00:00' ) . astype ( int ) # Getting the difference in days since first vote in days votes [ 'no_of_days_since_first_vote' ] = (( votes . voteDate - votes . min_date ) / np . timedelta64 ( 1 , 'D' )) . astype ( int ) + 1 # Calculating the cumulative number of votes till date (only considering the past: this step should be done after order by) votes [ 'no_of_votes_till_date' ] = votes . groupby ( 'employee' )[ 'voteDate' ] . transform ( 'cumcount' ) + 1 # calculating the percentage of days that the employee has voted till date votes [ 'perc_days_voted' ] = votes [ 'no_of_votes_till_date' ] / votes [ 'no_of_days_since_first_vote' ] # average of the vote till date by the employee votes [ 'avg_vote_till_date' ] = votes . groupby ( 'employee' )[ 'vote' ] . transform ( 'cumsum' ) / ( votes [ 'no_of_votes_till_date' ]) # Average vote in general of the employee votes [ 'avg_vote' ] = votes . groupby ( 'employee' )[ 'vote' ] . transform ( np . mean ) # The average of the last two votes (The last two votes might redict the probability of taking a leave now) votes [ 'last_2_votes_avg' ] = list ( votes . groupby ( 'employee' ) . rolling ( 2 , min_periods = 1 )[ 'vote' ] . apply ( np . mean )) votes [ 'date' ] = votes . voteDate . dt . date . astype ( 'datetime64[ns]' ) One observation is that the new employee is consistantly more enthusiastic than the older employees, and we can see a trend of declining average votes as time progresses for both new as well as older employee. fs , axs = plt . subplots ( 1 , figsize = ( 20 , 10 )) votes . groupby ([ 'new_employee' , 'no_of_days_since_first_vote' ]) . agg ({ 'vote' : 'mean' }) . reset_index () . \\ plot . scatter ( x = 'no_of_days_since_first_vote' , y = 'vote' , c = 'new_employee' , alpha = 0.4 , cmap = sns . diverging_palette ( 10 , 133 , as_cmap = True ), ax = axs ) #, vmin = 2.0, vmax=3.5) axs . set_ylim ([ 2 , 3.5 ]) plt . title ( 'Averge vote across new and old employee' ) plt . xlabel ( 'Number of days since first vote' ) plt . show () We can also se a seasonal pattern in the average vote fs , axs = plt . subplots ( 1 , figsize = ( 20 , 10 )) votes . groupby ([ 'voteDate' ]) . agg ({ 'vote' : 'mean' }) . reset_index () . \\ plot . scatter ( x = 'voteDate' , y = 'vote' , alpha = 0.4 , ax = axs ) plt . title ( 'Seasonality in the average vote' ) plt . xlabel ( 'Date of vote' ) plt . show () # joining the votes table with employee details to add the details of the employee votes = votes . merge ( employee_details , how = 'left' , on = 'employee' ) Does the average vote decrease just before a person quits the organisation? We can identify this by filtering for the employees who have already quit, and look at the average votes since the last day. We identify that the votes are actually pretty consistant with a mean around 3 and decreases in the notice period(of presumably 60 days). people_who_left = votes [ votes . stillExists == 0 ] . copy () people_who_left . loc [:, 'no_of_days_since_exit' ] = (( people_who_left . voteDate - people_who_left . deletedOn ) / np . timedelta64 ( 1 , 'D' )) . astype ( int ) - 1 fs , axs = plt . subplots ( 1 , figsize = ( 20 , 10 )) people_who_left . groupby ([ 'new_employee' , 'no_of_days_since_exit' ]) . agg ({ 'vote' : 'mean' }) . reset_index () . \\ plot . scatter ( x = 'no_of_days_since_exit' , y = 'vote' , alpha = 0.4 , ax = axs ) axs . set_xlim ([ - 200 , 0 ]) axs . set_ylim ([ 2 , 3.5 ]) plt . title ( 'Averge vote for employees who have left the organisation' ) plt . xlabel ( 'Number of days since exit date' ) plt . show () As we have employees across 5 timezones, the average votes across timezones are also shown. We can see distinct seasonality patterns across different geographies. Some gerographies have low variation as they have very few employees. fs , axs = plt . subplots ( 1 , figsize = ( 20 , 10 )) sns . scatterplot ( data = votes . groupby ([ 'timezone' , 'voteDate' ]) . agg ({ 'vote' : 'mean' }) . reset_index (), x = 'voteDate' , y = 'vote' , hue = 'timezone' , alpha = 0.4 , ax = axs ) plt . title ( 'Averge vote for employees in different geographies' ) plt . xlabel ( 'Vote date' ) plt . show ()","title":"Feature engineering"},{"location":"Python/Machine%20Learning%20Part%201/#feedback-dataset","text":"The next dataset of interest is the comments feedback dataset. This contains all the different comments given by employees, and the number of likes and dislikes on each of the comment. feedback = pd . read_csv ( \"comments_by_employees_in_anonymous_forum.csv\" ) feedback .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee companyAlias commentId commentLength likes dislikes commentDate feedbackType 0 aKP C1 5909b33da2ede4000473da6f 17 9 0 2017-05-03 12:05 OTHER 1 dNx C1 5909b6aca2ede4000473da72 25 12 0 2017-05-03 12:05 OTHER 2 ONv C1 5909c2dea2ede4000473db8c 58 33 5 2017-05-03 13:05 OTHER 3 e9M C1 5909d32ea2ede4000473db97 56 11 4 2017-05-03 14:05 OTHER 4 RWM C1 5909f227a2ede4000473dcbe 105 18 0 2017-05-03 17:05 OTHER ... ... ... ... ... ... ... ... ... 5067 7o1 C1 5c7108e8434c4500041722b0 28 0 0 2019-02-23 09:02 OTHER 5068 N3 C1 5c71519ca9f66e00042896f6 14 0 0 2019-02-23 14:02 OTHER 5069 DNY C1 5c73b11e50b72e0004cab283 63 0 0 2019-02-25 10:02 OTHER 5070 72j C1 5c744971e29c7b0004391da3 44 0 0 2019-02-25 21:02 OTHER 5071 qKO C1 5c781339efad100004ebb886 39 0 0 2019-02-28 17:02 OTHER 5072 rows \u00d7 8 columns feedback . describe ( include = 'all' , datetime_is_numeric = True ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee companyAlias commentId commentLength likes dislikes commentDate feedbackType count 5072 5072 5072 5072.000000 5072.000000 5072.00000 5072 5072 unique 305 1 5072 NaN NaN NaN 3580 5 top 4ov C1 59ab0ee58021aa0004bb7258 NaN NaN NaN 2018-09-10 09:09 OTHER freq 373 5072 1 NaN NaN NaN 26 3188 mean NaN NaN NaN 168.518336 13.605875 4.89097 NaN NaN std NaN NaN NaN 193.802568 15.280530 6.62993 NaN NaN min NaN NaN NaN 1.000000 0.000000 0.00000 NaN NaN 25% NaN NaN NaN 48.000000 2.000000 0.00000 NaN NaN 50% NaN NaN NaN 111.000000 9.000000 3.00000 NaN NaN 75% NaN NaN NaN 223.000000 20.000000 7.00000 NaN NaN max NaN NaN NaN 2509.000000 135.000000 65.00000 NaN NaN","title":"Feedback dataset"},{"location":"Python/Machine%20Learning%20Part%201/#eda_1","text":"We can observe that as the number of interactions becomes larger, the overall trend goes either towards likes or dislikes. We can also observe that comments above 80 interactions are usually highly liked. feedback [ 'log_comment_length' ] = np . log10 ( feedback . commentLength ) feedback [ 'total_interactions' ] = feedback . likes + feedback . dislikes feedback [ 'mean_feedback' ] = ( feedback . likes - feedback . dislikes ) / feedback . total_interactions fs , axs = plt . subplots ( 1 , figsize = ( 20 , 10 )) sns . scatterplot ( data = feedback , x = 'total_interactions' , y = 'mean_feedback' , hue = 'log_comment_length' , alpha = 0.8 , ax = axs ) plt . title ( 'Mean interaction vs response level for employee' ) plt . xlabel ( 'Number of interactions' ) plt . ylabel ( 'Mean of feedback' ) plt . show () We can see that we have five types of comments feedback . feedbackType . unique () feedback . groupby ( 'feedbackType' ) . \\ aggregate ({ 'commentId' : 'count' }) . \\ plot . pie ( y = 'commentId' , figsize = ( 10 , 10 ), autopct = ' %1.1f%% ' , # to add the percentages text wedgeprops = dict ( width = .5 ) ) plt . ylabel ( \"\" ) plt . title ( 'Frequency of somments' ) plt . show () Similarly we can see a trend between different comment types, with congratulations receiving more likes than dislikes, while suggestions and information having similar distibutions. We can also see how the number of likes are skewed. fs , axs = plt . subplots ( 1 , figsize = ( 20 , 10 )) sns . scatterplot ( data = feedback , x = 'likes' , y = 'dislikes' , hue = 'feedbackType' , alpha = 0.4 , ax = axs ) plt . title ( 'Total likes and dislikes for comments' ) plt . show () feedback . groupby ( 'feedbackType' ) . aggregate ({ 'likes' : 'mean' , 'dislikes' : 'mean' }) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } likes dislikes feedbackType CONGRATULATION 30.941538 3.824615 CRITICISM 11.125000 1.562500 INFORMATION 18.751152 7.657450 OTHER 9.325282 3.705144 SUGGESTION 19.219917 7.290456 f , ax = plt . subplots ( 2 , figsize = ( 20 , 20 ), sharex = True ) sns . boxplot ( data = feedback . groupby ([ 'feedbackType' , 'employee' ])[ 'commentId' ] . count () . reset_index () . assign ( log_count_comments = lambda df : np . log10 ( df . commentId ) ), x = 'feedbackType' , y = 'log_count_comments' , ax = ax [ 0 ]) sns . boxplot ( data = feedback , x = 'feedbackType' , y = 'log_comment_length' , ax = ax [ 1 ]) ax [ 0 ] . set_ylabel ( 'Log of number of comments' ) ax [ 1 ] . set_ylabel ( 'Log of comment length' ) ax [ 0 ] . set_title ( 'Number of comments and comment legth across different feedback types' ) plt . show ()","title":"EDA"},{"location":"Python/Machine%20Learning%20Part%201/#feature-engineering_1","text":"# feature engineering on the feedback dataset feedback [ 'commentDate' ] = pd . to_datetime ( feedback [ 'commentDate' ]) # sort by date in ascending to do cumulative sum and rolling calculations feedback = feedback . sort_values ([ 'employee' , 'commentDate' ]) # Count the likes and dislikes till the date given feedback [ 'likes_till_date' ] = feedback . groupby ( 'employee' )[ 'likes' ] . transform ( 'cumsum' ) feedback [ 'dislikes_till_date' ] = feedback . groupby ( 'employee' )[ 'dislikes' ] . transform ( 'cumsum' ) feedback [ 'comments_till_date' ] = feedback . groupby ( 'employee' )[ 'commentId' ] . transform ( 'cumcount' ) # Rolling two likes and dislikes, indicating the last two likes and dislikes feedback [ 'last_2_likes' ] = list ( feedback . groupby ( 'employee' ) . rolling ( 2 , min_periods = 1 )[ 'likes' ] . apply ( sum )) feedback [ 'last_2_dislikes' ] = list ( feedback . groupby ( 'employee' ) . rolling ( 2 , min_periods = 1 )[ 'dislikes' ] . apply ( sum )) feedback [ 'date' ] = feedback . commentDate . dt . date . astype ( 'datetime64[ns]' ) feedback .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee companyAlias commentId commentLength likes dislikes commentDate feedbackType log_comment_length total_interactions mean_feedback likes_till_date dislikes_till_date comments_till_date last_2_likes last_2_dislikes date 4373 19Q C1 5959ec954040610004272a21 11 0 0 2017-07-03 09:07:00 OTHER 1.041393 0 NaN 0 0 0 0.0 0.0 2017-07-03 970 19Q C1 5970e1483da0e10004b17a27 64 3 4 2017-07-20 18:07:00 OTHER 1.806180 7 -0.142857 3 4 1 3.0 4.0 2017-07-20 1903 19Q C1 5a36f82b26c0110004c55d90 57 17 6 2017-12-18 00:12:00 OTHER 1.755875 23 0.478261 20 10 2 20.0 10.0 2017-12-18 1949 19Q C1 5a40e7d3de51cb00042dfda4 31 5 0 2017-12-25 12:12:00 OTHER 1.491362 5 1.000000 25 10 3 22.0 6.0 2017-12-25 1972 19Q C1 5a4a8275eb84e0000492659f 11 8 0 2018-01-01 19:01:00 OTHER 1.041393 8 1.000000 33 10 4 13.0 0.0 2018-01-01 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 4740 zRx C1 5be95746ed7ae70004b83417 11 0 0 2018-11-12 11:11:00 OTHER 1.041393 0 NaN 0 0 0 0.0 0.0 2018-11-12 4746 zRx C1 5be95746ed7ae70004b83415 26 0 0 2018-11-12 11:11:00 OTHER 1.414973 0 NaN 0 0 1 0.0 0.0 2018-11-12 4758 zRx C1 5be95747ed7ae70004b83419 33 0 0 2018-11-12 11:11:00 OTHER 1.518514 0 NaN 0 0 2 0.0 0.0 2018-11-12 4764 zRx C1 5be95746ed7ae70004b83418 11 0 0 2018-11-12 11:11:00 OTHER 1.041393 0 NaN 0 0 3 0.0 0.0 2018-11-12 4781 zRx C1 5be95746ed7ae70004b83416 8 0 0 2018-11-12 11:11:00 OTHER 0.903090 0 NaN 0 0 4 0.0 0.0 2018-11-12 5072 rows \u00d7 17 columns","title":"Feature engineering"},{"location":"Python/Machine%20Learning%20Part%201/#merging-all-the-datasets","text":"Now we combine employee abseteeism data with votes and comments datasets. We have the following issues: 1. The absenteeism dataset is a subset of the data and does not cover all employees and months 2. Not all employees have voted and even those who have voted have voted infrequently 3. Not all employees have commented and even those who have posted a comment posted infrequently So we have the following assumptions: 1. We are working with the data only for the employees that exist in the absenteeism dataset 2. We are assuming that the last vote of the employee talks about the how the employee today 3. We are also assuming that the last comment and the likes and dislikes have an effect on the employee With these assumptions, we combine the datasets. # Creating a dataframe containing the time from the start to the end of voting period time_dataframe = pd . DataFrame ({ 'date' : pd . date_range ( min ( votes [ 'voteDate' ]) . date (), max ( votes [ 'voteDate' ]) . date ())}) time_dataframe [ 'tmp' ] = 1 # Creating a dataframe with all employees in the absenteeism dataset complete_df = pd . DataFrame ({ 'employee' : df . employee . unique ()}) complete_df [ 'tmp' ] = 1 # Creating a dataset that contains the combinations of all days for all employees complete_df = pd . merge ( complete_df , time_dataframe , on = [ 'tmp' ]) . drop ( 'tmp' , axis = 1 ) # Joining the feedback (comments) given by the employees complete_df = pd . merge ( complete_df , feedback , how = 'left' , on = [ 'date' , 'employee' ]) complete_df = complete_df [[ 'employee' , 'date' , 'likes' , 'dislikes' , 'feedbackType' , 'likes_till_date' , 'dislikes_till_date' , 'last_2_likes' , 'last_2_dislikes' , 'commentDate' ]] # Filling the last available feedback data the days when there was no feedback data for an employee. complete_df [[ 'date' , 'likes' , 'dislikes' , 'feedbackType' , 'likes_till_date' , 'dislikes_till_date' , 'last_2_likes' , 'last_2_dislikes' , 'commentDate' ]] = \\ complete_df . groupby ( 'employee' ) . fillna ( method = 'ffill' ) # Creating new features complete_df [ 'days_since_last_comment' ] = ( complete_df . date - complete_df . commentDate ) complete_df [ 'days_since_last_comment' ] = ( complete_df [ 'days_since_last_comment' ] . fillna ( pd . Timedelta ( seconds = 0 )) / np . timedelta64 ( 1 , 'D' )) . astype ( int ) # Assuming that for the employees that hae not commented (yet) the feedback is 0. complete_df = \\ complete_df [[ 'employee' , 'date' , 'likes' , 'dislikes' , 'feedbackType' , 'likes_till_date' , 'dislikes_till_date' , 'last_2_likes' , 'last_2_dislikes' , 'days_since_last_comment' ]] . \\ fillna ( 0 ) # Joining the votes dataset fr every eployee-date complete_df = pd . merge ( complete_df , votes , how = 'left' , on = [ 'date' , 'employee' ]) # Filling the last available votes data the days when there was no vote for an employee. complete_df [[ 'no_of_votes_till_date' , 'perc_days_voted' , 'deletedOn' , 'new_employee' , 'min_date' , 'stillExists' , 'vote' , 'avg_vote_till_date' , 'avg_vote' , 'last_2_votes_avg' , 'voteDate' , 'timezone' ]] = \\ complete_df . groupby ( 'employee' )[[ 'no_of_votes_till_date' , 'perc_days_voted' , 'deletedOn' , 'new_employee' , 'min_date' , 'stillExists' , 'vote' , 'avg_vote_till_date' , 'avg_vote' , 'last_2_votes_avg' , 'voteDate' , 'timezone' ]] . \\ fillna ( method = 'ffill' ) # Remove the data before the employee joined complete_df = complete_df [ complete_df . avg_vote >= 0 ] # Remove data after employee left complete_df = complete_df [( complete_df . stillExists == 1 ) | (( complete_df . stillExists == 0 ) & ( complete_df . date <= complete_df . deletedOn ))] # Recomputing no_of_days_since_first_vote complete_df . no_of_days_since_first_vote = (( complete_df . date - complete_df . min_date ) / np . timedelta64 ( 1 , 'D' )) . astype ( int ) + 1 # Adding new features # assuming a 60 day notice period # 60 days before the employee leaves are recorded complete_df . deletedOn = complete_df . deletedOn . fillna ( pd . to_datetime ( date . today ())) complete_df [ 'countdown_to_last_day' ] = (( complete_df . date - complete_df . deletedOn ) / np . timedelta64 ( 1 , 'D' )) . astype ( int ) + 1 complete_df . loc [ complete_df . countdown_to_last_day < - 15 , 'countdown_to_last_day' ] = 999 # computing days since last vote complete_df [ 'days_since_last_vote' ] = ( complete_df . date - complete_df . voteDate ) complete_df [ 'days_since_last_vote' ] = ( complete_df [ 'days_since_last_vote' ] . fillna ( pd . Timedelta ( seconds = 0 )) / np . timedelta64 ( 1 , 'D' )) . astype ( int ) # Imputing still exists column complete_df [[ 'timezone' , 'stillExists' ]] = complete_df . groupby ( 'employee' )[[ 'timezone' , 'stillExists' ]] . fillna ( method = 'bfill' ) # Selecting the features used in the model complete_df = complete_df [ [ 'employee' , 'date' , 'likes' , 'dislikes' , 'feedbackType' , 'likes_till_date' , 'dislikes_till_date' , 'last_2_likes' , 'last_2_dislikes' , 'days_since_last_comment' , 'vote' , 'timezone' , 'stillExists' , 'no_of_days_since_first_vote' , 'no_of_votes_till_date' , 'perc_days_voted' , 'avg_vote_till_date' , 'avg_vote' , 'last_2_votes_avg' , 'days_since_last_vote' , 'new_employee' , 'countdown_to_last_day' ]] . fillna ( 0 ) pd . options . display . max_columns = None # Combining the leaves dataset data = pd . merge ( complete_df , leave_df , how = 'left' , on = [ 'date' , 'employee' ]) data . on_leave = data . on_leave . fillna ( 0 ) # Creating new columns data . no_leaves_till_date = data . no_leaves_till_date . fillna ( method = 'bfill' ) . fillna ( method = 'ffill' ) # Get the number of days the person was on leave in the last two days def get_rolling_sum ( grp , freq , col ): return ( grp . rolling ( freq , on = 'date' )[ col ] . sum ()) data = data . sort_values ([ 'employee' , 'date' ]) data [ 'last_2_days_leaves' ] = data . groupby ( 'employee' , as_index = False , group_keys = False ) . \\ apply ( get_rolling_sum , '2D' , 'on_leave' ) # 2d for 2 days # Get the leave status of the previous day data [ 'previous_day_leave' ] = data . groupby ( 'employee' )[ 'on_leave' ] . shift () . fillna ( 0 ) # Renaming columns to more suitable ones data . rename ( columns = { \"likes\" : \"last_likes\" , \"dislikes\" : \"last_dislikes\" , \"feedback_type\" : \"last_feedback_type\" , \"vote\" : \"last_vote\" , \"new_employee\" : \"employee_joined_after_jun17\" }, inplace = True ) The correlation matrix for all the columns is red_green_cmap = sns . diverging_palette ( 10 , 133 , as_cmap = True ) f = plt . figure ( figsize = ( 20 , 20 )) plt . matshow ( data . corr (), fignum = f , cmap = red_green_cmap , vmin =- 1 , vmax = 1 ) plt . xticks ( range ( data . select_dtypes ([ 'number' ]) . shape [ 1 ]), data . select_dtypes ([ 'number' ]) . columns , rotation = 90 , fontsize = 15 ) plt . yticks ( range ( data . select_dtypes ([ 'number' ]) . shape [ 1 ]), data . select_dtypes ([ 'number' ]) . columns , rotation = 0 , fontsize = 15 ) cb = plt . colorbar () cb . ax . tick_params ( labelsize = 14 ) plt . title ( \"Correlation between different parameters\" , fontsize = 20 , loc = 'left' ) plt . show () The distributions across all the columns are _ = data . hist ( figsize = ( 20 , 20 )) The second part of the blog is here .","title":"Merging all the datasets"},{"location":"Python/Machine%20Learning%20Part%201/#references","text":"Notes, Wrokforce Analytics Live lecture, Imperial college London, Class 2020-22 Kyburz J, Morelli D, Schaaf A, Villani F, Wheatley D: Predicting absenteeism Harsha A, Shaked A, Artem G, Tebogo M, Gokhan M: Predicting absenteeism","title":"References"},{"location":"Python/Machine%20learning%20as%20HTTP%20Request/","text":"Deploying a machine learning application \u00b6 Building a machine learning model is only half the story. Deploying this application so that the business uses it is the other half. Generally, deployment is not done by machine learning engineers or data scientists. Therefore I see my peers lacking these skills, especially the data scientists from non-Computer Science backgrounds. Although python developers do the deployment, data scientists need to know the basics of deploying a machine learning solution. In the below example, I am using data taken on the amount of PM25 pollutant near my house (in Hyderabad, India) from aqicn.org . In the previous blog , I demonstrated a simple ARIMA model that can predict PM25 and discussed different ways. I want to implement this model as an API so that any website can access it for predictions. I have used pythonanywhere to deploy a flask application mentioned above. First let me build a machine learning model. Historical data has been taken from aqicn's api import pandas as pd import matplotlib.pyplot as plt import seaborn as sns data = pd . read_csv ( 'hyderabad-us consulate-air-quality.csv' , parse_dates = [ 'date' ]) data . columns = [ 'date' , 'pm25' ] data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date pm25 0 2021-11-01 155 1 2021-11-02 115 2 2021-11-03 67 3 2021-11-04 112 4 2021-11-05 115 ... ... ... 2309 2014-12-24 165 2310 2014-12-25 165 2311 2014-12-26 163 2312 2014-12-27 165 2313 2014-12-28 160 2314 rows \u00d7 2 columns data . plot . scatter ( x = 'date' , y = 'pm25' ) from statsmodels.tsa.seasonal import seasonal_decompose result = seasonal_decompose ( data . pm25 , model = 'additive' , period = 365 ) print ( result . plot ()) We can see the seasonality in the data where the pollution increases during winter and is lower during the summer months. The complete ARIMA model is discussed in a different blog post. The final results of the model are shown below: Deployment \u00b6 The best way to deploy the machine learning model (according to me) is to encapsulate the training and prediction logic behind the data science model along with the final model in an object. This can be done using a class as shown below. This object can be serialised/deserialised, and we need not re-write the prediction logic on the server-side every time we change the machine learning model or code. We can only change the final model file, and the application should work seamlessly. We are effectively removing the machine learning from the server-side code and instead encapsulating it on an object. Consider the below code, which encapsulates the machine learning model: import dill # dill is an alternative to pickle which is better for serialising objects along with their class definitions class predict_pm25 : def __init__ ( self ): self . model = None self . version = 1 def predict ( self , date ): # This predict function can have anything import requests import pandas as pd import numpy as np from math import sqrt import datetime from dateutil.relativedelta import relativedelta # Getting the actual and predictions of the last two days for ARIMA(2,0,2) date = ( datetime . datetime . strptime ( date , \"%Y-%m- %d \" ) - relativedelta ( days = 2 )) . strftime ( \"%Y-%m- %d \" ) response = requests . get ( \"https://hydpm25.herokuapp.com/get_last_n_days_data\" , params = { 'date' : date , 'n' : 2 }) df = pd . DataFrame ( response . json ()[ 'result' ]) # Calculating the MA values df [ 'ma' ] = df . actual - df . predicted # Making the next prediction with ARIMA(2,0,2) model parameters shown above df [ 'ma_slope' ] = [ - 0.7915 , - 0.0775 ] df [ 'ar_slope' ] = [ 1.5876 , - 0.5914 ] pred = 0.4454 + sum ( df . ma * df . ma_slope + df . actual * df . ar_slope ) + abs ( np . random . normal ( 0 , sqrt ( 250.55 ), 1 )) return pred def save_model ( self ): with open ( 'predict_hyderabad_pm25.pkl' , \"wb\" ) as pkl_file : dill . dump ( self , pkl_file ) Running the code to save the model as a serialised file. predict_pm = predict_pm25 () predict_pm . save_model () Flask \u00b6 Flask server can be used to deploy this model. First, we set up flask server over local host. First, write the following code in a file named flask_app.py (any name except flask.py) # File flask_app.py from flask import Flask , request , jsonify import pandas as pd from mc_predict import predict as machine_learning_predict # has code for the predict function app = Flask ( __name__ ) # initialising the flask app @app . route ( \"/\" ) # specifying the app route over the web def base_website (): # what should happen at this route return \"Welcome to machine learning model APIs!\" @app . route ( '/predict' , methods = [ 'GET' ]) # Get request defined def predict_request (): # what should happen at this get request json_ = request . json query_df = pd . DataFrame ( json_ ) prediction = machine_learning_predict ( query_df ) # we call the predict function for the machine learning model return jsonify ({ 'prediction' : list ( prediction )}) if __name__ == '__main__' : app . run ( debug = True ) The predict function is defined in a different file called mc_predict.py . In this function, we load (unserialise) the saved model and call the predict function in the model. Here we can observe that this is a function on the server, and it does not contain any machine learning logic. All the machine learning logic is present in the object, and changing the object can change the machine learning logic without changing this code. # File mc_predict.py import dill def predict ( date = '2021-11-12' ): with open ( 'predict_hyderabad_pm25.pkl' , \"rb\" ) as pkl_file : model = dill . load ( pkl_file ) # unserialise the model return model . predict ( date ) For example, the prediction for '2021-11-12' is predict () array([142.32741472]) That's it. We have our local deployment ready. We will have to go to the folder where these files are present and type 'python flask_app.py'. We will get the app running on http://127.0.0.1:5000/ . Pythonanywhere \u00b6 The next step is to deploy it on pythonanywhere. The first step is to sign up for a new account. We can then \"Add a new web app\" with Flask 3.7. This will create a default flask based web app with your username.pythonanywhere.com. We can install any packages necessary using the \"Console\" (example pip install dill). In the files tab, under 'mysite', are the flask files. These should be replaced with the files that we have above. The model file should also be uploaded. (We should take care of the relative location of the model file while loading it). Under 'Web' tab, we can 'Reload the model', which will rebuild the application. We now have our machine learning model deployed. I can access the API GET request at https://harshaash.pythonanywhere.com/predict with the parameter date=YYYY-MM-DD. References: \u00b6 Deployment: https://medium.com/analytics-vidhya/how-to-deploy-simple-machine-learning-models-for-free-56cdccc62b8d https://www.datacamp.com/community/tutorials/machine-learning-models-api-python Data: https://aqicn.org/json-api/doc/ https://help.pythonanywhere.com/pages/Flask/","title":"ML deployment in Flask (Python)"},{"location":"Python/Machine%20learning%20as%20HTTP%20Request/#deploying-a-machine-learning-application","text":"Building a machine learning model is only half the story. Deploying this application so that the business uses it is the other half. Generally, deployment is not done by machine learning engineers or data scientists. Therefore I see my peers lacking these skills, especially the data scientists from non-Computer Science backgrounds. Although python developers do the deployment, data scientists need to know the basics of deploying a machine learning solution. In the below example, I am using data taken on the amount of PM25 pollutant near my house (in Hyderabad, India) from aqicn.org . In the previous blog , I demonstrated a simple ARIMA model that can predict PM25 and discussed different ways. I want to implement this model as an API so that any website can access it for predictions. I have used pythonanywhere to deploy a flask application mentioned above. First let me build a machine learning model. Historical data has been taken from aqicn's api import pandas as pd import matplotlib.pyplot as plt import seaborn as sns data = pd . read_csv ( 'hyderabad-us consulate-air-quality.csv' , parse_dates = [ 'date' ]) data . columns = [ 'date' , 'pm25' ] data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date pm25 0 2021-11-01 155 1 2021-11-02 115 2 2021-11-03 67 3 2021-11-04 112 4 2021-11-05 115 ... ... ... 2309 2014-12-24 165 2310 2014-12-25 165 2311 2014-12-26 163 2312 2014-12-27 165 2313 2014-12-28 160 2314 rows \u00d7 2 columns data . plot . scatter ( x = 'date' , y = 'pm25' ) from statsmodels.tsa.seasonal import seasonal_decompose result = seasonal_decompose ( data . pm25 , model = 'additive' , period = 365 ) print ( result . plot ()) We can see the seasonality in the data where the pollution increases during winter and is lower during the summer months. The complete ARIMA model is discussed in a different blog post. The final results of the model are shown below:","title":"Deploying a machine learning application"},{"location":"Python/Machine%20learning%20as%20HTTP%20Request/#deployment","text":"The best way to deploy the machine learning model (according to me) is to encapsulate the training and prediction logic behind the data science model along with the final model in an object. This can be done using a class as shown below. This object can be serialised/deserialised, and we need not re-write the prediction logic on the server-side every time we change the machine learning model or code. We can only change the final model file, and the application should work seamlessly. We are effectively removing the machine learning from the server-side code and instead encapsulating it on an object. Consider the below code, which encapsulates the machine learning model: import dill # dill is an alternative to pickle which is better for serialising objects along with their class definitions class predict_pm25 : def __init__ ( self ): self . model = None self . version = 1 def predict ( self , date ): # This predict function can have anything import requests import pandas as pd import numpy as np from math import sqrt import datetime from dateutil.relativedelta import relativedelta # Getting the actual and predictions of the last two days for ARIMA(2,0,2) date = ( datetime . datetime . strptime ( date , \"%Y-%m- %d \" ) - relativedelta ( days = 2 )) . strftime ( \"%Y-%m- %d \" ) response = requests . get ( \"https://hydpm25.herokuapp.com/get_last_n_days_data\" , params = { 'date' : date , 'n' : 2 }) df = pd . DataFrame ( response . json ()[ 'result' ]) # Calculating the MA values df [ 'ma' ] = df . actual - df . predicted # Making the next prediction with ARIMA(2,0,2) model parameters shown above df [ 'ma_slope' ] = [ - 0.7915 , - 0.0775 ] df [ 'ar_slope' ] = [ 1.5876 , - 0.5914 ] pred = 0.4454 + sum ( df . ma * df . ma_slope + df . actual * df . ar_slope ) + abs ( np . random . normal ( 0 , sqrt ( 250.55 ), 1 )) return pred def save_model ( self ): with open ( 'predict_hyderabad_pm25.pkl' , \"wb\" ) as pkl_file : dill . dump ( self , pkl_file ) Running the code to save the model as a serialised file. predict_pm = predict_pm25 () predict_pm . save_model ()","title":"Deployment"},{"location":"Python/Machine%20learning%20as%20HTTP%20Request/#flask","text":"Flask server can be used to deploy this model. First, we set up flask server over local host. First, write the following code in a file named flask_app.py (any name except flask.py) # File flask_app.py from flask import Flask , request , jsonify import pandas as pd from mc_predict import predict as machine_learning_predict # has code for the predict function app = Flask ( __name__ ) # initialising the flask app @app . route ( \"/\" ) # specifying the app route over the web def base_website (): # what should happen at this route return \"Welcome to machine learning model APIs!\" @app . route ( '/predict' , methods = [ 'GET' ]) # Get request defined def predict_request (): # what should happen at this get request json_ = request . json query_df = pd . DataFrame ( json_ ) prediction = machine_learning_predict ( query_df ) # we call the predict function for the machine learning model return jsonify ({ 'prediction' : list ( prediction )}) if __name__ == '__main__' : app . run ( debug = True ) The predict function is defined in a different file called mc_predict.py . In this function, we load (unserialise) the saved model and call the predict function in the model. Here we can observe that this is a function on the server, and it does not contain any machine learning logic. All the machine learning logic is present in the object, and changing the object can change the machine learning logic without changing this code. # File mc_predict.py import dill def predict ( date = '2021-11-12' ): with open ( 'predict_hyderabad_pm25.pkl' , \"rb\" ) as pkl_file : model = dill . load ( pkl_file ) # unserialise the model return model . predict ( date ) For example, the prediction for '2021-11-12' is predict () array([142.32741472]) That's it. We have our local deployment ready. We will have to go to the folder where these files are present and type 'python flask_app.py'. We will get the app running on http://127.0.0.1:5000/ .","title":"Flask"},{"location":"Python/Machine%20learning%20as%20HTTP%20Request/#pythonanywhere","text":"The next step is to deploy it on pythonanywhere. The first step is to sign up for a new account. We can then \"Add a new web app\" with Flask 3.7. This will create a default flask based web app with your username.pythonanywhere.com. We can install any packages necessary using the \"Console\" (example pip install dill). In the files tab, under 'mysite', are the flask files. These should be replaced with the files that we have above. The model file should also be uploaded. (We should take care of the relative location of the model file while loading it). Under 'Web' tab, we can 'Reload the model', which will rebuild the application. We now have our machine learning model deployed. I can access the API GET request at https://harshaash.pythonanywhere.com/predict with the parameter date=YYYY-MM-DD.","title":"Pythonanywhere"},{"location":"Python/Machine%20learning%20as%20HTTP%20Request/#references","text":"Deployment: https://medium.com/analytics-vidhya/how-to-deploy-simple-machine-learning-models-for-free-56cdccc62b8d https://www.datacamp.com/community/tutorials/machine-learning-models-api-python Data: https://aqicn.org/json-api/doc/ https://help.pythonanywhere.com/pages/Flask/","title":"References:"},{"location":"Python/Network%20Flow%20problems/","text":"MathJax.Hub.Config({ extensions: [\"tex2jax.js\",\"TeX/AMSmath.js\",\"TeX/AMSsymbols.js\"], jax: [\"input/TeX\", \"output/HTML-CSS\"], tex2jax: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ], }, \"HTML-CSS\": { availableFonts: [\"TeX\"] } }); Flow maximisation problems \u00b6 Author: Achyuthuni Sri Harsha A directed graph has the capacities on all the edges and our job is to find the maximum amount of flow that can happen from one node (starting node) to another node(outflow node). There are two types of flow maximisation problems: 1. Max flow: What is the maximum flow that can be sent from source to outfow without exceeding capacity. 2. Min cost flow: We have the cost along wih capacities on each edge. We want to find the minimum cost path of sending f uits of flow from source to outflow. We can use integer programming to solve both these problems. import networkx as nx # Other packages for manupulating data import numpy as np import pandas as pd # packages for plotting import matplotlib.pyplot as plt Maximum flow problem \u00b6 Let us first consider the max flow problem. Consider the below graph. edgelist_df = pd . DataFrame ({ 'node1' :[ 's' , 's' , 'u' , 'u' , 'v' ], 'node2' :[ 'u' , 'v' , 'v' , 't' , 't' ], 'weights' :[ 20 , 10 , 30 , 10 , 20 ] }) edgelist_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } node1 node2 weights 0 s u 20 1 s v 10 2 u v 30 3 u t 10 4 v t 20 g = nx . DiGraph () for i , elrow in edgelist_df . iterrows (): g . add_edge ( elrow [ 0 ], elrow [ 1 ], weight = elrow [ 2 ]) g . edges ( data = True ) OutEdgeDataView([('s', 'u', {'weight': 20}), ('s', 'v', {'weight': 10}), ('u', 'v', {'weight': 30}), ('u', 't', {'weight': 10}), ('v', 't', {'weight': 20})]) # for each node we are trying to fix the coordinates g . add_node ( 's' , pos = ( 0 , 1 )) g . add_node ( 'u' , pos = ( 1 , 2 )) g . add_node ( 'v' , pos = ( 1 , 0 )) g . add_node ( 't' , pos = ( 2 , 1 )) g . nodes ( data = True ) NodeDataView({'s': {'pos': (0, 1)}, 'u': {'pos': (1, 2)}, 'v': {'pos': (1, 0)}, 't': {'pos': (2, 1)}}) pos = nx . get_node_attributes ( g , 'pos' ) weight = nx . get_edge_attributes ( g , 'weight' ) nx . draw ( g , pos , with_labels = True ) nx . draw_networkx_edge_labels ( g , pos , edge_labels = weight ) plt . show () We want to find the maximum flow between s and t. This can be formulated as an integer programming problem, with Decision variable 1. Integer variable \\(edge_{i,j}\\) representing the amount of flow in edge ij. 2. Integer variable f representing the maximum flow from ortools.sat.python import cp_model max_flow_model = cp_model . CpModel () # Creating one integer decision variable for each edge edge_int_vars = {} for edge in g . edges : edge_int_vars [ edge [ 0 ], edge [ 1 ]] = max_flow_model . NewIntVar ( 0 , 100 , 'edge_ %s _ %s ' % edge ) print ( 'Creating the integer variable ' , edge_int_vars [ edge [ 0 ], edge [ 1 ]], 'representing the amount to flow in edge' , ( edge [ 0 ], edge [ 1 ])) Creating the integer variable edge_s_u representing the amount to flow in edge ('s', 'u') Creating the integer variable edge_s_v representing the amount to flow in edge ('s', 'v') Creating the integer variable edge_u_v representing the amount to flow in edge ('u', 'v') Creating the integer variable edge_u_t representing the amount to flow in edge ('u', 't') Creating the integer variable edge_v_t representing the amount to flow in edge ('v', 't') Constraints The first set of constraints are flow constraints, 1. At the input node, the net flow should be -f 2. At the output node, the net flow should be +f 3. The net flow in all other nodes should be 0 input_node = 's' output_node = 't' # Adding constraints on the nodes flow = max_flow_model . NewIntVar ( 0 , 100 , 'flow' ) # Initialising flow this will be maximised later for node in g . nodes : in_edges = g . in_edges ( node ) out_edges = g . out_edges ( node ) print ( 'Adding the constraint on node ' , node ) equation_at_this_edge = sum ( edge_int_vars [ edge [ 0 ], edge [ 1 ]] for edge in in_edges ) - \\ sum ( edge_int_vars [ edge [ 0 ], edge [ 1 ]] for edge in out_edges ) if ( node == input_node ): max_flow_model . Add ( equation_at_this_edge == - flow ) print ( equation_at_this_edge == - flow ) elif ( node == output_node ): max_flow_model . Add ( equation_at_this_edge == flow ) print ( equation_at_this_edge == flow ) else : max_flow_model . Add ( equation_at_this_edge == 0 ) print ( equation_at_this_edge , '== 0' ) print ( '' ) Adding the constraint on node s ((-((edge_s_u) + edge_s_v)) + (1 * flow)) == 0 Adding the constraint on node u ((edge_s_u) + -((edge_u_v) + edge_u_t)) == 0 Adding the constraint on node v (((edge_s_v) + edge_u_v) + -(edge_v_t)) == 0 Adding the constraint on node t ((((edge_u_t) + edge_v_t)) + -flow) == 0 Another set of constraints are the capacity restrictions on every edge. # Adding constraints on the edges for edge in g . edges : print ( 'Adding constraint on edge ' , edge ) max_flow_in_edge = g . get_edge_data ( * edge )[ 'weight' ] max_flow_model . Add ( edge_int_vars [ edge [ 0 ], edge [ 1 ]] <= max_flow_in_edge ) print ( edge_int_vars [ edge [ 0 ], edge [ 1 ]] <= max_flow_in_edge ) Adding constraint on edge ('s', 'u') edge_s_u <= 20 Adding constraint on edge ('s', 'v') edge_s_v <= 10 Adding constraint on edge ('u', 'v') edge_u_v <= 30 Adding constraint on edge ('u', 't') edge_u_t <= 10 Adding constraint on edge ('v', 't') edge_v_t <= 20 Objective The objective is to maximise flow. # The objective is to maximise flow max_flow_model . Maximize ( flow ) Solving the problem, we get # Solving the problem solver = cp_model . CpSolver () solution_printer = cp_model . ObjectiveSolutionPrinter () status = solver . SolveWithSolutionCallback ( max_flow_model , solution_printer ) Solution 0, time = 0.44 s, objective = 30 cp_model . OPTIMAL == status True result_edges = {} for edge in g . edges : result_edges [ edge [ 0 ], edge [ 1 ]] = solver . Value ( edge_int_vars [ edge [ 0 ], edge [ 1 ]]) result_edges {('s', 'u'): 20, ('s', 'v'): 10, ('u', 'v'): 10, ('u', 't'): 10, ('v', 't'): 20} The flow in each of the nodes is shown in the figure pos = nx . get_node_attributes ( g , 'pos' ) nx . draw ( g , pos , with_labels = True ) nx . draw_networkx_edge_labels ( g , pos , edge_labels = result_edges ) plt . show () Minimum cost flow problems \u00b6 We want to find the minimum cost to transfer a fixed amount of flow from one edge to another. The costs are given int he table bwlow. The costs are also displayed in the network below. edgelist_df = pd . DataFrame ({ 'node1' :[ 's' , 's' , 'u' , 'u' , 'v' ], 'node2' :[ 'u' , 'v' , 'v' , 't' , 't' ], 'weights' :[ 20 , 10 , 30 , 10 , 20 ], 'costs' :[ 3 , 2 , 0.7 , 1 , 7 ] }) edgelist_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } node1 node2 weights costs 0 s u 20 3.0 1 s v 10 2.0 2 u v 30 0.7 3 u t 10 1.0 4 v t 20 7.0 g = nx . DiGraph () for i , elrow in edgelist_df . iterrows (): g . add_edge ( elrow [ 0 ], elrow [ 1 ], weight = elrow [ 2 ], cost = elrow [ 3 ]) g . edges ( data = True ) OutEdgeDataView([('s', 'u', {'weight': 20, 'cost': 3.0}), ('s', 'v', {'weight': 10, 'cost': 2.0}), ('u', 'v', {'weight': 30, 'cost': 0.7}), ('u', 't', {'weight': 10, 'cost': 1.0}), ('v', 't', {'weight': 20, 'cost': 7.0})]) # for each node we are trying to fix the coordinates g . add_node ( 's' , pos = ( 0 , 1 )) g . add_node ( 'u' , pos = ( 1 , 2 )) g . add_node ( 'v' , pos = ( 1 , 0 )) g . add_node ( 't' , pos = ( 2 , 1 )) g . nodes ( data = True ) NodeDataView({'s': {'pos': (0, 1)}, 'u': {'pos': (1, 2)}, 'v': {'pos': (1, 0)}, 't': {'pos': (2, 1)}}) pos = nx . get_node_attributes ( g , 'pos' ) weight = nx . get_edge_attributes ( g , 'weight' ) cost = nx . get_edge_attributes ( g , 'cost' ) res = { key : str ( weight [ key ]) + '/$' + str ( cost . get ( key , '' )) for key in cost . keys ()} nx . draw ( g , pos , with_labels = True ) nx . draw_networkx_edge_labels ( g , pos , edge_labels = res ) plt . show () Just like maximum flow, we can use integer programming to find out the solution in this scenario also. Decision variables Integer variables \\(edge_{i,j}\\) representing the amount of flow in edge (i,j) Constraints Capacity restrictions on every edge Objective The objective is to minimise the overall cost from ortools.sat.python import cp_model min_cost_flow_model = cp_model . CpModel () # Creating one integer decision variable for each edge edge_int_vars = {} for edge in g . edges : edge_int_vars [ edge [ 0 ], edge [ 1 ]] = min_cost_flow_model . NewIntVar ( 0 , 100 , 'edge_ %s _ %s ' % edge ) print ( 'Creating the integer variable ' , edge_int_vars [ edge [ 0 ], edge [ 1 ]], 'representing the amount to flow in edge' , ( edge [ 0 ], edge [ 1 ])) Creating the integer variable edge_s_u representing the amount to flow in edge ('s', 'u') Creating the integer variable edge_s_v representing the amount to flow in edge ('s', 'v') Creating the integer variable edge_u_v representing the amount to flow in edge ('u', 'v') Creating the integer variable edge_u_t representing the amount to flow in edge ('u', 't') Creating the integer variable edge_v_t representing the amount to flow in edge ('v', 't') input_node = 's' output_node = 't' flow = 20 # Assuming a constant flow of 20 units # Adding constraints on the nodes for node in g . nodes : in_edges = g . in_edges ( node ) out_edges = g . out_edges ( node ) print ( 'Adding the constraint on node ' , node ) print ( 'This node has %i in-edges and %i out-edges' % ( len ( in_edges ), len ( out_edges ))) equation_at_this_edge = sum ( edge_int_vars [ edge [ 0 ], edge [ 1 ]] for edge in in_edges ) - \\ sum ( edge_int_vars [ edge [ 0 ], edge [ 1 ]] for edge in out_edges ) if ( node == input_node ): min_cost_flow_model . Add ( equation_at_this_edge == - flow ) print ( equation_at_this_edge == - flow ) elif ( node == output_node ): min_cost_flow_model . Add ( equation_at_this_edge == flow ) print ( equation_at_this_edge == flow ) else : min_cost_flow_model . Add ( equation_at_this_edge == 0 ) print ( equation_at_this_edge , '== 0' ) print ( '' ) Adding the constraint on node s This node has 0 in-edges and 2 out-edges (-((edge_s_u) + edge_s_v)) == -20 Adding the constraint on node u This node has 1 in-edges and 2 out-edges ((edge_s_u) + -((edge_u_v) + edge_u_t)) == 0 Adding the constraint on node v This node has 2 in-edges and 1 out-edges (((edge_s_v) + edge_u_v) + -(edge_v_t)) == 0 Adding the constraint on node t This node has 2 in-edges and 0 out-edges (((edge_u_t) + edge_v_t)) == 20 # Adding constraints on the edges for edge in g . edges : print ( 'Adding constraint on edge ' , edge ) max_flow_in_edge = g . get_edge_data ( * edge )[ 'weight' ] min_cost_flow_model . Add ( edge_int_vars [ edge [ 0 ], edge [ 1 ]] <= max_flow_in_edge ) print ( edge_int_vars [ edge [ 0 ], edge [ 1 ]] <= max_flow_in_edge ) Adding constraint on edge ('s', 'u') edge_s_u <= 20 Adding constraint on edge ('s', 'v') edge_s_v <= 10 Adding constraint on edge ('u', 'v') edge_u_v <= 30 Adding constraint on edge ('u', 't') edge_u_t <= 10 Adding constraint on edge ('v', 't') edge_v_t <= 20 # factor to make everything including costs integer factor_to_int = 10 # The objective is to maximise flow total_cost = sum ( int ( g . get_edge_data ( * edge )[ 'cost' ] * factor_to_int ) * edge_int_vars [ edge [ 0 ], edge [ 1 ]] for edge in g . edges ) print ( 'Objective is to optimise cost' ) print ( total_cost ) min_cost_flow_model . Minimize ( total_cost ) Objective is to optimise cost ((((((30 * edge_s_u)) + (20 * edge_s_v)) + (7 * edge_u_v)) + (10 * edge_u_t)) + (70 * edge_v_t)) # Solving the problem solver = cp_model . CpSolver () solution_printer = cp_model . ObjectiveSolutionPrinter () status = solver . SolveWithSolutionCallback ( min_cost_flow_model , solution_printer ) Solution 0, time = 0.24 s, objective = 1300 cp_model . OPTIMAL == status True The solution to the minimum flow problem is result_edges = {} for edge in g . edges : result_edges [ edge [ 0 ], edge [ 1 ]] = solver . Value ( edge_int_vars [ edge [ 0 ], edge [ 1 ]]) result_edges {('s', 'u'): 10, ('s', 'v'): 10, ('u', 'v'): 0, ('u', 't'): 10, ('v', 't'): 10} # This function gets the coordinates for nodes pos = nx . get_node_attributes ( g , 'pos' ) # The simplest way to draw a graph is by using nx.draw. nx . draw ( g , pos , with_labels = True ) # This adds edge lables nx . draw_networkx_edge_labels ( g , pos , edge_labels = result_edges ) plt . show ()","title":"Network flow problems (Python)"},{"location":"Python/Network%20Flow%20problems/#flow-maximisation-problems","text":"Author: Achyuthuni Sri Harsha A directed graph has the capacities on all the edges and our job is to find the maximum amount of flow that can happen from one node (starting node) to another node(outflow node). There are two types of flow maximisation problems: 1. Max flow: What is the maximum flow that can be sent from source to outfow without exceeding capacity. 2. Min cost flow: We have the cost along wih capacities on each edge. We want to find the minimum cost path of sending f uits of flow from source to outflow. We can use integer programming to solve both these problems. import networkx as nx # Other packages for manupulating data import numpy as np import pandas as pd # packages for plotting import matplotlib.pyplot as plt","title":"Flow maximisation problems"},{"location":"Python/Network%20Flow%20problems/#maximum-flow-problem","text":"Let us first consider the max flow problem. Consider the below graph. edgelist_df = pd . DataFrame ({ 'node1' :[ 's' , 's' , 'u' , 'u' , 'v' ], 'node2' :[ 'u' , 'v' , 'v' , 't' , 't' ], 'weights' :[ 20 , 10 , 30 , 10 , 20 ] }) edgelist_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } node1 node2 weights 0 s u 20 1 s v 10 2 u v 30 3 u t 10 4 v t 20 g = nx . DiGraph () for i , elrow in edgelist_df . iterrows (): g . add_edge ( elrow [ 0 ], elrow [ 1 ], weight = elrow [ 2 ]) g . edges ( data = True ) OutEdgeDataView([('s', 'u', {'weight': 20}), ('s', 'v', {'weight': 10}), ('u', 'v', {'weight': 30}), ('u', 't', {'weight': 10}), ('v', 't', {'weight': 20})]) # for each node we are trying to fix the coordinates g . add_node ( 's' , pos = ( 0 , 1 )) g . add_node ( 'u' , pos = ( 1 , 2 )) g . add_node ( 'v' , pos = ( 1 , 0 )) g . add_node ( 't' , pos = ( 2 , 1 )) g . nodes ( data = True ) NodeDataView({'s': {'pos': (0, 1)}, 'u': {'pos': (1, 2)}, 'v': {'pos': (1, 0)}, 't': {'pos': (2, 1)}}) pos = nx . get_node_attributes ( g , 'pos' ) weight = nx . get_edge_attributes ( g , 'weight' ) nx . draw ( g , pos , with_labels = True ) nx . draw_networkx_edge_labels ( g , pos , edge_labels = weight ) plt . show () We want to find the maximum flow between s and t. This can be formulated as an integer programming problem, with Decision variable 1. Integer variable \\(edge_{i,j}\\) representing the amount of flow in edge ij. 2. Integer variable f representing the maximum flow from ortools.sat.python import cp_model max_flow_model = cp_model . CpModel () # Creating one integer decision variable for each edge edge_int_vars = {} for edge in g . edges : edge_int_vars [ edge [ 0 ], edge [ 1 ]] = max_flow_model . NewIntVar ( 0 , 100 , 'edge_ %s _ %s ' % edge ) print ( 'Creating the integer variable ' , edge_int_vars [ edge [ 0 ], edge [ 1 ]], 'representing the amount to flow in edge' , ( edge [ 0 ], edge [ 1 ])) Creating the integer variable edge_s_u representing the amount to flow in edge ('s', 'u') Creating the integer variable edge_s_v representing the amount to flow in edge ('s', 'v') Creating the integer variable edge_u_v representing the amount to flow in edge ('u', 'v') Creating the integer variable edge_u_t representing the amount to flow in edge ('u', 't') Creating the integer variable edge_v_t representing the amount to flow in edge ('v', 't') Constraints The first set of constraints are flow constraints, 1. At the input node, the net flow should be -f 2. At the output node, the net flow should be +f 3. The net flow in all other nodes should be 0 input_node = 's' output_node = 't' # Adding constraints on the nodes flow = max_flow_model . NewIntVar ( 0 , 100 , 'flow' ) # Initialising flow this will be maximised later for node in g . nodes : in_edges = g . in_edges ( node ) out_edges = g . out_edges ( node ) print ( 'Adding the constraint on node ' , node ) equation_at_this_edge = sum ( edge_int_vars [ edge [ 0 ], edge [ 1 ]] for edge in in_edges ) - \\ sum ( edge_int_vars [ edge [ 0 ], edge [ 1 ]] for edge in out_edges ) if ( node == input_node ): max_flow_model . Add ( equation_at_this_edge == - flow ) print ( equation_at_this_edge == - flow ) elif ( node == output_node ): max_flow_model . Add ( equation_at_this_edge == flow ) print ( equation_at_this_edge == flow ) else : max_flow_model . Add ( equation_at_this_edge == 0 ) print ( equation_at_this_edge , '== 0' ) print ( '' ) Adding the constraint on node s ((-((edge_s_u) + edge_s_v)) + (1 * flow)) == 0 Adding the constraint on node u ((edge_s_u) + -((edge_u_v) + edge_u_t)) == 0 Adding the constraint on node v (((edge_s_v) + edge_u_v) + -(edge_v_t)) == 0 Adding the constraint on node t ((((edge_u_t) + edge_v_t)) + -flow) == 0 Another set of constraints are the capacity restrictions on every edge. # Adding constraints on the edges for edge in g . edges : print ( 'Adding constraint on edge ' , edge ) max_flow_in_edge = g . get_edge_data ( * edge )[ 'weight' ] max_flow_model . Add ( edge_int_vars [ edge [ 0 ], edge [ 1 ]] <= max_flow_in_edge ) print ( edge_int_vars [ edge [ 0 ], edge [ 1 ]] <= max_flow_in_edge ) Adding constraint on edge ('s', 'u') edge_s_u <= 20 Adding constraint on edge ('s', 'v') edge_s_v <= 10 Adding constraint on edge ('u', 'v') edge_u_v <= 30 Adding constraint on edge ('u', 't') edge_u_t <= 10 Adding constraint on edge ('v', 't') edge_v_t <= 20 Objective The objective is to maximise flow. # The objective is to maximise flow max_flow_model . Maximize ( flow ) Solving the problem, we get # Solving the problem solver = cp_model . CpSolver () solution_printer = cp_model . ObjectiveSolutionPrinter () status = solver . SolveWithSolutionCallback ( max_flow_model , solution_printer ) Solution 0, time = 0.44 s, objective = 30 cp_model . OPTIMAL == status True result_edges = {} for edge in g . edges : result_edges [ edge [ 0 ], edge [ 1 ]] = solver . Value ( edge_int_vars [ edge [ 0 ], edge [ 1 ]]) result_edges {('s', 'u'): 20, ('s', 'v'): 10, ('u', 'v'): 10, ('u', 't'): 10, ('v', 't'): 20} The flow in each of the nodes is shown in the figure pos = nx . get_node_attributes ( g , 'pos' ) nx . draw ( g , pos , with_labels = True ) nx . draw_networkx_edge_labels ( g , pos , edge_labels = result_edges ) plt . show ()","title":"Maximum flow problem"},{"location":"Python/Network%20Flow%20problems/#minimum-cost-flow-problems","text":"We want to find the minimum cost to transfer a fixed amount of flow from one edge to another. The costs are given int he table bwlow. The costs are also displayed in the network below. edgelist_df = pd . DataFrame ({ 'node1' :[ 's' , 's' , 'u' , 'u' , 'v' ], 'node2' :[ 'u' , 'v' , 'v' , 't' , 't' ], 'weights' :[ 20 , 10 , 30 , 10 , 20 ], 'costs' :[ 3 , 2 , 0.7 , 1 , 7 ] }) edgelist_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } node1 node2 weights costs 0 s u 20 3.0 1 s v 10 2.0 2 u v 30 0.7 3 u t 10 1.0 4 v t 20 7.0 g = nx . DiGraph () for i , elrow in edgelist_df . iterrows (): g . add_edge ( elrow [ 0 ], elrow [ 1 ], weight = elrow [ 2 ], cost = elrow [ 3 ]) g . edges ( data = True ) OutEdgeDataView([('s', 'u', {'weight': 20, 'cost': 3.0}), ('s', 'v', {'weight': 10, 'cost': 2.0}), ('u', 'v', {'weight': 30, 'cost': 0.7}), ('u', 't', {'weight': 10, 'cost': 1.0}), ('v', 't', {'weight': 20, 'cost': 7.0})]) # for each node we are trying to fix the coordinates g . add_node ( 's' , pos = ( 0 , 1 )) g . add_node ( 'u' , pos = ( 1 , 2 )) g . add_node ( 'v' , pos = ( 1 , 0 )) g . add_node ( 't' , pos = ( 2 , 1 )) g . nodes ( data = True ) NodeDataView({'s': {'pos': (0, 1)}, 'u': {'pos': (1, 2)}, 'v': {'pos': (1, 0)}, 't': {'pos': (2, 1)}}) pos = nx . get_node_attributes ( g , 'pos' ) weight = nx . get_edge_attributes ( g , 'weight' ) cost = nx . get_edge_attributes ( g , 'cost' ) res = { key : str ( weight [ key ]) + '/$' + str ( cost . get ( key , '' )) for key in cost . keys ()} nx . draw ( g , pos , with_labels = True ) nx . draw_networkx_edge_labels ( g , pos , edge_labels = res ) plt . show () Just like maximum flow, we can use integer programming to find out the solution in this scenario also. Decision variables Integer variables \\(edge_{i,j}\\) representing the amount of flow in edge (i,j) Constraints Capacity restrictions on every edge Objective The objective is to minimise the overall cost from ortools.sat.python import cp_model min_cost_flow_model = cp_model . CpModel () # Creating one integer decision variable for each edge edge_int_vars = {} for edge in g . edges : edge_int_vars [ edge [ 0 ], edge [ 1 ]] = min_cost_flow_model . NewIntVar ( 0 , 100 , 'edge_ %s _ %s ' % edge ) print ( 'Creating the integer variable ' , edge_int_vars [ edge [ 0 ], edge [ 1 ]], 'representing the amount to flow in edge' , ( edge [ 0 ], edge [ 1 ])) Creating the integer variable edge_s_u representing the amount to flow in edge ('s', 'u') Creating the integer variable edge_s_v representing the amount to flow in edge ('s', 'v') Creating the integer variable edge_u_v representing the amount to flow in edge ('u', 'v') Creating the integer variable edge_u_t representing the amount to flow in edge ('u', 't') Creating the integer variable edge_v_t representing the amount to flow in edge ('v', 't') input_node = 's' output_node = 't' flow = 20 # Assuming a constant flow of 20 units # Adding constraints on the nodes for node in g . nodes : in_edges = g . in_edges ( node ) out_edges = g . out_edges ( node ) print ( 'Adding the constraint on node ' , node ) print ( 'This node has %i in-edges and %i out-edges' % ( len ( in_edges ), len ( out_edges ))) equation_at_this_edge = sum ( edge_int_vars [ edge [ 0 ], edge [ 1 ]] for edge in in_edges ) - \\ sum ( edge_int_vars [ edge [ 0 ], edge [ 1 ]] for edge in out_edges ) if ( node == input_node ): min_cost_flow_model . Add ( equation_at_this_edge == - flow ) print ( equation_at_this_edge == - flow ) elif ( node == output_node ): min_cost_flow_model . Add ( equation_at_this_edge == flow ) print ( equation_at_this_edge == flow ) else : min_cost_flow_model . Add ( equation_at_this_edge == 0 ) print ( equation_at_this_edge , '== 0' ) print ( '' ) Adding the constraint on node s This node has 0 in-edges and 2 out-edges (-((edge_s_u) + edge_s_v)) == -20 Adding the constraint on node u This node has 1 in-edges and 2 out-edges ((edge_s_u) + -((edge_u_v) + edge_u_t)) == 0 Adding the constraint on node v This node has 2 in-edges and 1 out-edges (((edge_s_v) + edge_u_v) + -(edge_v_t)) == 0 Adding the constraint on node t This node has 2 in-edges and 0 out-edges (((edge_u_t) + edge_v_t)) == 20 # Adding constraints on the edges for edge in g . edges : print ( 'Adding constraint on edge ' , edge ) max_flow_in_edge = g . get_edge_data ( * edge )[ 'weight' ] min_cost_flow_model . Add ( edge_int_vars [ edge [ 0 ], edge [ 1 ]] <= max_flow_in_edge ) print ( edge_int_vars [ edge [ 0 ], edge [ 1 ]] <= max_flow_in_edge ) Adding constraint on edge ('s', 'u') edge_s_u <= 20 Adding constraint on edge ('s', 'v') edge_s_v <= 10 Adding constraint on edge ('u', 'v') edge_u_v <= 30 Adding constraint on edge ('u', 't') edge_u_t <= 10 Adding constraint on edge ('v', 't') edge_v_t <= 20 # factor to make everything including costs integer factor_to_int = 10 # The objective is to maximise flow total_cost = sum ( int ( g . get_edge_data ( * edge )[ 'cost' ] * factor_to_int ) * edge_int_vars [ edge [ 0 ], edge [ 1 ]] for edge in g . edges ) print ( 'Objective is to optimise cost' ) print ( total_cost ) min_cost_flow_model . Minimize ( total_cost ) Objective is to optimise cost ((((((30 * edge_s_u)) + (20 * edge_s_v)) + (7 * edge_u_v)) + (10 * edge_u_t)) + (70 * edge_v_t)) # Solving the problem solver = cp_model . CpSolver () solution_printer = cp_model . ObjectiveSolutionPrinter () status = solver . SolveWithSolutionCallback ( min_cost_flow_model , solution_printer ) Solution 0, time = 0.24 s, objective = 1300 cp_model . OPTIMAL == status True The solution to the minimum flow problem is result_edges = {} for edge in g . edges : result_edges [ edge [ 0 ], edge [ 1 ]] = solver . Value ( edge_int_vars [ edge [ 0 ], edge [ 1 ]]) result_edges {('s', 'u'): 10, ('s', 'v'): 10, ('u', 'v'): 0, ('u', 't'): 10, ('v', 't'): 10} # This function gets the coordinates for nodes pos = nx . get_node_attributes ( g , 'pos' ) # The simplest way to draw a graph is by using nx.draw. nx . draw ( g , pos , with_labels = True ) # This adds edge lables nx . draw_networkx_edge_labels ( g , pos , edge_labels = result_edges ) plt . show ()","title":"Minimum cost flow problems"},{"location":"Python/Network%20Science/","text":"Network Science \u00b6 Author: Achyuthuni Sri Harsha We live in a highly connected world where social networks significantly affect our lives, from getting jobs, connecting with friends, and dating and news. Network science is the study of such complex networks. There is some common phenomenon that applies to social networks: How do networks evolve \u00b6 A social network is constantly evolving; people make new friends and lose touch with other friends. Many external factors in life, like marriage, new college, school, workplace or club, can spark the creation of new friends. This phenomenon is observed in vast networks such as the internet. Is there any model that can explain the growth of networks? If so, we can simulate new networks, identify what parameters influence the growth of new networks etc. Power law explains some real-world examples of the growth and scale of networks. Small World Phenomena \u00b6 Small world phenomenon states that everyone in the world can be reached thru a short chain of acquaintances. This was first demonstrated in the Milgram experiment where random people were asked to send a letter to a stockbroker in Boston by passing it through people they know. Of the letters received, they went thru an average of 6 people. Facebook repeated this on a large scale later, which validated this with a lower number of connections. This indicates that most people are connected to every other random person on earth within six common friends. Homophily \u00b6 This can be described as \"Birds of a feather flock together\". We tend to be friends with friends who are similar to us, especially people from the same socio-economic background, social background, etc. Strength of weak ties \u00b6 If two people in a social network have a friend in common, then there is an increased likelihood that they will become friends at some point in the future. This also depends on the strength of the friendship(ties). Generally, people in the same network will have the same kind of information. Take an example of jobs; everyone in the same friend circle will have the same information about open positions for jobs. It is the weak ties, friends whom we have not interacted with within some time, who have new information which will help most people to find jobs. This is called as the strength of weak ties. Let us look at a small network of friends in Zachary karate club . Zachary's karate club is a university-based karate club consisting of a social network of 34 members. W.W. Zachary famously observed the karate club for three years (from 1970 to 1972), documenting links between pairs of members who interacted outside the everyday activities of the club. The nodes in the graph represent people, and the edges represent friendships (interactions) between them. import matplotlib.pyplot as plt % matplotlib inline import networkx as nx G = nx . karate_club_graph () nx . draw_kamada_kawai ( G , with_labels = True ) We can understand the network using some statistics about the network. Network Statistics \u00b6 Degree Distribution \u00b6 The degree of a node is the number of nodes a particular node is connected to. For example, node 11 has one degree in the karate club network as it is connected to one node only(node 0). The degree distribution is the relative frequencies of the nodes that have particular degrees. There are two types of degree distributions generally, Poisson and scale-free. For the karate club network, the degree distribution is: degree_dist = nx . degree_histogram ( G ) plt . bar ( range ( len ( degree_dist )), degree_dist ) plt . ylabel ( 'No of people' ) plt . xlabel ( 'No of friends' ); This indicates that most people have 2-5 friends while a few people are connected to >15 people in the network. Clustering coefficient \u00b6 Another statistic that we can understand is the clustering coefficient. This is the degree to which nodes cluster together. If A is friends with B and C, then B and c have a high probability of being friends if they belong to the same cluster. the clustering coefficient captures the fraction of triads (connected A, B and C) in the network. The clustering coefficient of all the nodes is calculated. The clustering coefficient of a node is the percentage of triangles that are there among the nodes that it is connected to. For example, node 11 is connected to node 6 and 5, both of which are connected. So the percentage of triads is 100%. Node 4 is connected to 0, 6 and 10. 0-6-4 and 0-10-0 make a triad, but 0-10-4 does not exist. Therefore the percentage is 66.6%. The clustering coefficient of the network is the average of the clustering coefficient across all the nodes, and it is 57% for the karate club. nx . clustering ( G ) {0: 0.15, 1: 0.3333333333333333, 2: 0.24444444444444444, 3: 0.6666666666666666, 4: 0.6666666666666666, 5: 0.5, 6: 0.5, 7: 1.0, 8: 0.5, 9: 0, 10: 0.6666666666666666, 11: 0, 12: 1.0, 13: 0.6, 14: 1.0, 15: 1.0, 16: 1.0, 17: 1.0, 18: 1.0, 19: 0.3333333333333333, 20: 1.0, 21: 1.0, 22: 1.0, 23: 0.4, 24: 0.3333333333333333, 25: 0.3333333333333333, 26: 1.0, 27: 0.16666666666666666, 28: 0.3333333333333333, 29: 0.6666666666666666, 30: 0.5, 31: 0.2, 32: 0.19696969696969696, 33: 0.11029411764705882} nx . algorithms . cluster . average_clustering ( G ) 0.5706384782076823 Community Detection \u00b6 During the study in Karate club, a conflict arose between the club president 'A' and instructor 'B' (pseudonyms), which led to a split in the club. Half of the members formed a new club around A (group one) and the remaining members under B. Can we identify the 2 communities in this network? Girvan Newman \u00b6 Community detection is close to clustering in machine learning. In clustering, we do the clustering based on the distance between the points when placed in an n-dimensional space. Similar to clustering, in community detection, we want to cluster the nodes closer to each other than others. The only difference is the concept of distance is based on the topology of the network. Like clustering, we can divide the network into two communities using top-down and bottom-up methods. This is represented by a dendrogram where we can draw a horizontal line to split the communities. This is discussed in more detail in a different blog. The two groups based on Girvan Newman are: karate_club_split = nx . algorithms . community . centrality . girvan_newman ( G ) karate_club_split_tuple = tuple ( sorted ( c ) for c in next ( karate_club_split )) karate_club_split_tuple ([0, 1, 3, 4, 5, 6, 7, 10, 11, 12, 13, 16, 17, 19, 21], [2, 8, 9, 14, 15, 18, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]) # To get colors in the graph color_list = [ 'r' if node in karate_club_split_tuple [ 0 ] else 'g' for node in G . nodes ] nx . draw_kamada_kawai ( G , node_color = color_list , with_labels = True ) Ratio cut \u00b6 Another method is the ratio cut method. A cut is a set of edges such that if we remove them, the network breaks into two components. A network will have many cuts, and to find an optimal split, we need to: 1. Find the min-balanced-cut (cut with the minimum number of edges with an almost equal number of nodes on each side) among all pairs of nodes 2. Repeat over each of the two parts This is discussed in more detail in a different blog. References \u00b6 Zachary, W. W. (1977). An Information Flow Model for Conflict and Fission in Small Groups. Journal of Anthropological Research. 33 (4), pp. 452-473. Networkx Karate club","title":"Network Science (Python)"},{"location":"Python/Network%20Science/#network-science","text":"Author: Achyuthuni Sri Harsha We live in a highly connected world where social networks significantly affect our lives, from getting jobs, connecting with friends, and dating and news. Network science is the study of such complex networks. There is some common phenomenon that applies to social networks:","title":"Network Science"},{"location":"Python/Network%20Science/#how-do-networks-evolve","text":"A social network is constantly evolving; people make new friends and lose touch with other friends. Many external factors in life, like marriage, new college, school, workplace or club, can spark the creation of new friends. This phenomenon is observed in vast networks such as the internet. Is there any model that can explain the growth of networks? If so, we can simulate new networks, identify what parameters influence the growth of new networks etc. Power law explains some real-world examples of the growth and scale of networks.","title":"How do networks evolve"},{"location":"Python/Network%20Science/#small-world-phenomena","text":"Small world phenomenon states that everyone in the world can be reached thru a short chain of acquaintances. This was first demonstrated in the Milgram experiment where random people were asked to send a letter to a stockbroker in Boston by passing it through people they know. Of the letters received, they went thru an average of 6 people. Facebook repeated this on a large scale later, which validated this with a lower number of connections. This indicates that most people are connected to every other random person on earth within six common friends.","title":"Small World Phenomena"},{"location":"Python/Network%20Science/#homophily","text":"This can be described as \"Birds of a feather flock together\". We tend to be friends with friends who are similar to us, especially people from the same socio-economic background, social background, etc.","title":"Homophily"},{"location":"Python/Network%20Science/#strength-of-weak-ties","text":"If two people in a social network have a friend in common, then there is an increased likelihood that they will become friends at some point in the future. This also depends on the strength of the friendship(ties). Generally, people in the same network will have the same kind of information. Take an example of jobs; everyone in the same friend circle will have the same information about open positions for jobs. It is the weak ties, friends whom we have not interacted with within some time, who have new information which will help most people to find jobs. This is called as the strength of weak ties. Let us look at a small network of friends in Zachary karate club . Zachary's karate club is a university-based karate club consisting of a social network of 34 members. W.W. Zachary famously observed the karate club for three years (from 1970 to 1972), documenting links between pairs of members who interacted outside the everyday activities of the club. The nodes in the graph represent people, and the edges represent friendships (interactions) between them. import matplotlib.pyplot as plt % matplotlib inline import networkx as nx G = nx . karate_club_graph () nx . draw_kamada_kawai ( G , with_labels = True ) We can understand the network using some statistics about the network.","title":"Strength of weak ties"},{"location":"Python/Network%20Science/#network-statistics","text":"","title":"Network Statistics"},{"location":"Python/Network%20Science/#degree-distribution","text":"The degree of a node is the number of nodes a particular node is connected to. For example, node 11 has one degree in the karate club network as it is connected to one node only(node 0). The degree distribution is the relative frequencies of the nodes that have particular degrees. There are two types of degree distributions generally, Poisson and scale-free. For the karate club network, the degree distribution is: degree_dist = nx . degree_histogram ( G ) plt . bar ( range ( len ( degree_dist )), degree_dist ) plt . ylabel ( 'No of people' ) plt . xlabel ( 'No of friends' ); This indicates that most people have 2-5 friends while a few people are connected to >15 people in the network.","title":"Degree Distribution"},{"location":"Python/Network%20Science/#clustering-coefficient","text":"Another statistic that we can understand is the clustering coefficient. This is the degree to which nodes cluster together. If A is friends with B and C, then B and c have a high probability of being friends if they belong to the same cluster. the clustering coefficient captures the fraction of triads (connected A, B and C) in the network. The clustering coefficient of all the nodes is calculated. The clustering coefficient of a node is the percentage of triangles that are there among the nodes that it is connected to. For example, node 11 is connected to node 6 and 5, both of which are connected. So the percentage of triads is 100%. Node 4 is connected to 0, 6 and 10. 0-6-4 and 0-10-0 make a triad, but 0-10-4 does not exist. Therefore the percentage is 66.6%. The clustering coefficient of the network is the average of the clustering coefficient across all the nodes, and it is 57% for the karate club. nx . clustering ( G ) {0: 0.15, 1: 0.3333333333333333, 2: 0.24444444444444444, 3: 0.6666666666666666, 4: 0.6666666666666666, 5: 0.5, 6: 0.5, 7: 1.0, 8: 0.5, 9: 0, 10: 0.6666666666666666, 11: 0, 12: 1.0, 13: 0.6, 14: 1.0, 15: 1.0, 16: 1.0, 17: 1.0, 18: 1.0, 19: 0.3333333333333333, 20: 1.0, 21: 1.0, 22: 1.0, 23: 0.4, 24: 0.3333333333333333, 25: 0.3333333333333333, 26: 1.0, 27: 0.16666666666666666, 28: 0.3333333333333333, 29: 0.6666666666666666, 30: 0.5, 31: 0.2, 32: 0.19696969696969696, 33: 0.11029411764705882} nx . algorithms . cluster . average_clustering ( G ) 0.5706384782076823","title":"Clustering coefficient"},{"location":"Python/Network%20Science/#community-detection","text":"During the study in Karate club, a conflict arose between the club president 'A' and instructor 'B' (pseudonyms), which led to a split in the club. Half of the members formed a new club around A (group one) and the remaining members under B. Can we identify the 2 communities in this network?","title":"Community Detection"},{"location":"Python/Network%20Science/#girvan-newman","text":"Community detection is close to clustering in machine learning. In clustering, we do the clustering based on the distance between the points when placed in an n-dimensional space. Similar to clustering, in community detection, we want to cluster the nodes closer to each other than others. The only difference is the concept of distance is based on the topology of the network. Like clustering, we can divide the network into two communities using top-down and bottom-up methods. This is represented by a dendrogram where we can draw a horizontal line to split the communities. This is discussed in more detail in a different blog. The two groups based on Girvan Newman are: karate_club_split = nx . algorithms . community . centrality . girvan_newman ( G ) karate_club_split_tuple = tuple ( sorted ( c ) for c in next ( karate_club_split )) karate_club_split_tuple ([0, 1, 3, 4, 5, 6, 7, 10, 11, 12, 13, 16, 17, 19, 21], [2, 8, 9, 14, 15, 18, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]) # To get colors in the graph color_list = [ 'r' if node in karate_club_split_tuple [ 0 ] else 'g' for node in G . nodes ] nx . draw_kamada_kawai ( G , node_color = color_list , with_labels = True )","title":"Girvan Newman"},{"location":"Python/Network%20Science/#ratio-cut","text":"Another method is the ratio cut method. A cut is a set of edges such that if we remove them, the network breaks into two components. A network will have many cuts, and to find an optimal split, we need to: 1. Find the min-balanced-cut (cut with the minimum number of edges with an almost equal number of nodes on each side) among all pairs of nodes 2. Repeat over each of the two parts This is discussed in more detail in a different blog.","title":"Ratio cut"},{"location":"Python/Network%20Science/#references","text":"Zachary, W. W. (1977). An Information Flow Model for Conflict and Fission in Small Groups. Journal of Anthropological Research. 33 (4), pp. 452-473. Networkx Karate club","title":"References"},{"location":"Python/Network%20centrality/","text":"Centrality measures \u00b6 Author: Achyuthuni Sri Harsha Centrality measures help us understand who are the most important people in the network. For this example, \u201cemail-Eu-core network,\u201d from Stanford\u2019s SNAP is used. You may find the original dataset here: https://snap.stanford.edu/data/email-Eu-core.html .. This is a dataset of various email communications between people, and we are interested about the important people, leaders and opinion makers in this network. import pandas as pd import numpy as np import matplotlib.pyplot as plt import networkx as nx % matplotlib inline For the purposes of this blog, I am filtering the data only for the first 100 people for brevity. Same analysis can be done across the complete dataset. df = pd . read_csv ( \"https://raw.githubusercontent.com/jinhangjiang/Datasets/main/Network%20Data/emailEUcore/email-Eu-core.txt\" , delimiter = \" \" , names = [ \"Source\" , \"Target\" ]) df = df . loc [( df . Source < 100 ) & ( df . Target < 100 )] df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Source Target 0 0 1 1 2 3 2 2 4 3 5 6 4 5 7 We are using a directed graph as we are interested emails being sent and received. G = nx . from_pandas_edgelist ( df , source = 'Source' , target = 'Target' , create_using = nx . DiGraph ()) plt . subplots ( figsize = ( 20 , 20 )) plt . title ( 'Email, EU-Core network' ) nx . draw_kamada_kawai ( G , with_labels = True ) We can see that there are two densily connected networks and many connections between them. Who is the most important person in the network? This is based on what is the question you are asking. Is it the person who can send the emails the fastest? Is it the person why can connect with maximum number of people? Some basic features would be: 1. Just by the number of mails sent/received, who has sent the maximum number of mails 2. By the number of people that a person has a immediate relation with (a relation is defined as sending an email to that person): degree centrality 3. We can also look at the persons who are essential for communication between different closely knit groups. (betweenness centrality) The person with the maximum number of interactions is: student_sent_mails = list ( df . groupby ( 'Source' )[ 'Target' ] . count ()) student_sent_mails . index ( max ( student_sent_mails )) + 1 86 We can say that the person with index 86 has the most number of interactions and is one of the most important person. He would be the go to person who has connections with maximum number of people. We can see this in the graph also. Degree centrality \u00b6 This way of looking at the importance of a person based on number of connections (Degrees) is called degree centrality. In ascending order, the top 10 people with maximum number of connections are: degree_centrality = nx . algorithms . centrality . degree_centrality ( G ) for i , w in enumerate ( sorted ( degree_centrality , key = degree_centrality . get , reverse = True )): if ( i < 10 ): print ( w , degree_centrality [ w ]) else : break 86 0.8282828282828284 62 0.7676767676767677 82 0.6464646464646465 96 0.6161616161616162 28 0.6060606060606061 21 0.5555555555555556 13 0.5353535353535354 23 0.5151515151515152 30 0.494949494949495 64 0.494949494949495 This network can be visualised with most size proportional to the importance according to degree centrality. plt . subplots ( figsize = ( 20 , 20 )) plt . title ( 'Email, EU-Core network: degree centrality' ) nx . draw_kamada_kawai ( G , with_labels = True , node_size = list ( np . array ( list ( degree_centrality . values ())) * 500 )) Betweenness centrality \u00b6 Although person 86 has sent mails to the maximum number of people, he has still sent mails to only 45 out of the 100 people. This means if we want to send a mail to the whole class, that there might be other important people thru whom the remaining people receive mails. We should identify people who are part of different closely knit groups and who are essential for passing information between groups. This is given by betweenness centrality. The most important people according to this metric in descending order are: btw_centrality = nx . algorithms . centrality . betweenness_centrality ( G ) for i , w in enumerate ( sorted ( btw_centrality . items (), key = lambda item : item [ 1 ], reverse = True )): if ( i < 10 ): print ( w ) else : break (86, 0.14149427475629858) (62, 0.12154539177634052) (96, 0.08160276331043798) (82, 0.0670387255089669) (64, 0.06502342115189583) (21, 0.05061277358840613) (13, 0.04673865990243992) (44, 0.04327552467861193) (5, 0.0412363399245883) (28, 0.03679924185307917) plt . subplots ( figsize = ( 20 , 20 )) plt . title ( 'Email, EU-Core network: betweenness centrality' ) nx . draw_kamada_kawai ( G , with_labels = True , node_size = list ( np . array ( list ( btw_centrality . values ())) * 5000 )) We get the same people at the top, nut this time inclusding a few additional people. Page rank (Eigenvector centrality) \u00b6 Another way to identify the important people in a network is to not only look at the number of people who are connected to the person but also the importance of the people whom they are connected to. The importance of the person can be defined as proportional to the sum of importance of the immediate people connected to him/her. This is defined by eigenvector centrality. Below is the top ten people who are important based on who they know and whom they are connected to: ev_centrality = nx . algorithms . centrality . eigenvector_centrality ( G ) for i , w in enumerate ( sorted ( ev_centrality . items (), key = lambda item : item [ 1 ], reverse = True )): if ( i < 10 ): print ( w ) else : break (28, 0.25779806388704485) (23, 0.2412215124270137) (30, 0.21579251773540695) (62, 0.2061936996975851) (29, 0.1962488726486241) (35, 0.19522994610327346) (86, 0.18934292971595598) (27, 0.1823911579409791) (96, 0.18188010730196982) (40, 0.1803640484660976) plt . subplots ( figsize = ( 20 , 20 )) plt . title ( 'Email, EU-Core network: eigenvector centrality' ) nx . draw_kamada_kawai ( G , with_labels = True , node_size = list ( np . array ( list ( ev_centrality . values ())) * 2500 )) Clustering coefficient \u00b6 Any graph in general can be densely connected or sparcely connected. The behaviour of the graph, and therefore the actions that have to be taken, are different for sparce graphs vs densely connected graphs. The betweenness measures above should be looked at along with clustering coefficient. The clustering coefficient is a way of measuring the degree to which the nodes in a graph cluster together. Networks with high number of clustering coefficient are more social. The average clustering coefficient for the graph is (assuming a undirected graph): G_undir = nx . from_pandas_edgelist ( df , source = 'Source' , target = 'Target' ) nx . algorithms . cluster . average_clustering ( G_undir ) 0.5130839320504162 From the above analysis, we can find the most important people in the network. The people who has the most connections, the people who are part of multiple groups and are important to send message across and the people who are important based on whom they know immediately. References \u00b6 https://towardsdatascience.com/visualize-high-dimensional-network-data-with-3d-360-degree-animated-scatter-plot-d583932d3693 Assignment and student notes, Business Analytics MSc, Imperial College London, Network Analytics module Hao Yin, Austin R. Benson, Jure Leskovec, and David F. Gleich. \"Local Higher-order Graph Clustering.\" In Proceedings of the 23 rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 2017 https://snap.stanford.edu/data/email-Eu-core.html Networks, Crowds, and Markets: Reasoning about a Highly Connected World by David Easley and Jon Kleinberg","title":"Network Centrality (Python)"},{"location":"Python/Network%20centrality/#centrality-measures","text":"Author: Achyuthuni Sri Harsha Centrality measures help us understand who are the most important people in the network. For this example, \u201cemail-Eu-core network,\u201d from Stanford\u2019s SNAP is used. You may find the original dataset here: https://snap.stanford.edu/data/email-Eu-core.html .. This is a dataset of various email communications between people, and we are interested about the important people, leaders and opinion makers in this network. import pandas as pd import numpy as np import matplotlib.pyplot as plt import networkx as nx % matplotlib inline For the purposes of this blog, I am filtering the data only for the first 100 people for brevity. Same analysis can be done across the complete dataset. df = pd . read_csv ( \"https://raw.githubusercontent.com/jinhangjiang/Datasets/main/Network%20Data/emailEUcore/email-Eu-core.txt\" , delimiter = \" \" , names = [ \"Source\" , \"Target\" ]) df = df . loc [( df . Source < 100 ) & ( df . Target < 100 )] df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Source Target 0 0 1 1 2 3 2 2 4 3 5 6 4 5 7 We are using a directed graph as we are interested emails being sent and received. G = nx . from_pandas_edgelist ( df , source = 'Source' , target = 'Target' , create_using = nx . DiGraph ()) plt . subplots ( figsize = ( 20 , 20 )) plt . title ( 'Email, EU-Core network' ) nx . draw_kamada_kawai ( G , with_labels = True ) We can see that there are two densily connected networks and many connections between them. Who is the most important person in the network? This is based on what is the question you are asking. Is it the person who can send the emails the fastest? Is it the person why can connect with maximum number of people? Some basic features would be: 1. Just by the number of mails sent/received, who has sent the maximum number of mails 2. By the number of people that a person has a immediate relation with (a relation is defined as sending an email to that person): degree centrality 3. We can also look at the persons who are essential for communication between different closely knit groups. (betweenness centrality) The person with the maximum number of interactions is: student_sent_mails = list ( df . groupby ( 'Source' )[ 'Target' ] . count ()) student_sent_mails . index ( max ( student_sent_mails )) + 1 86 We can say that the person with index 86 has the most number of interactions and is one of the most important person. He would be the go to person who has connections with maximum number of people. We can see this in the graph also.","title":"Centrality measures"},{"location":"Python/Network%20centrality/#degree-centrality","text":"This way of looking at the importance of a person based on number of connections (Degrees) is called degree centrality. In ascending order, the top 10 people with maximum number of connections are: degree_centrality = nx . algorithms . centrality . degree_centrality ( G ) for i , w in enumerate ( sorted ( degree_centrality , key = degree_centrality . get , reverse = True )): if ( i < 10 ): print ( w , degree_centrality [ w ]) else : break 86 0.8282828282828284 62 0.7676767676767677 82 0.6464646464646465 96 0.6161616161616162 28 0.6060606060606061 21 0.5555555555555556 13 0.5353535353535354 23 0.5151515151515152 30 0.494949494949495 64 0.494949494949495 This network can be visualised with most size proportional to the importance according to degree centrality. plt . subplots ( figsize = ( 20 , 20 )) plt . title ( 'Email, EU-Core network: degree centrality' ) nx . draw_kamada_kawai ( G , with_labels = True , node_size = list ( np . array ( list ( degree_centrality . values ())) * 500 ))","title":"Degree centrality"},{"location":"Python/Network%20centrality/#betweenness-centrality","text":"Although person 86 has sent mails to the maximum number of people, he has still sent mails to only 45 out of the 100 people. This means if we want to send a mail to the whole class, that there might be other important people thru whom the remaining people receive mails. We should identify people who are part of different closely knit groups and who are essential for passing information between groups. This is given by betweenness centrality. The most important people according to this metric in descending order are: btw_centrality = nx . algorithms . centrality . betweenness_centrality ( G ) for i , w in enumerate ( sorted ( btw_centrality . items (), key = lambda item : item [ 1 ], reverse = True )): if ( i < 10 ): print ( w ) else : break (86, 0.14149427475629858) (62, 0.12154539177634052) (96, 0.08160276331043798) (82, 0.0670387255089669) (64, 0.06502342115189583) (21, 0.05061277358840613) (13, 0.04673865990243992) (44, 0.04327552467861193) (5, 0.0412363399245883) (28, 0.03679924185307917) plt . subplots ( figsize = ( 20 , 20 )) plt . title ( 'Email, EU-Core network: betweenness centrality' ) nx . draw_kamada_kawai ( G , with_labels = True , node_size = list ( np . array ( list ( btw_centrality . values ())) * 5000 )) We get the same people at the top, nut this time inclusding a few additional people.","title":"Betweenness centrality"},{"location":"Python/Network%20centrality/#page-rank-eigenvector-centrality","text":"Another way to identify the important people in a network is to not only look at the number of people who are connected to the person but also the importance of the people whom they are connected to. The importance of the person can be defined as proportional to the sum of importance of the immediate people connected to him/her. This is defined by eigenvector centrality. Below is the top ten people who are important based on who they know and whom they are connected to: ev_centrality = nx . algorithms . centrality . eigenvector_centrality ( G ) for i , w in enumerate ( sorted ( ev_centrality . items (), key = lambda item : item [ 1 ], reverse = True )): if ( i < 10 ): print ( w ) else : break (28, 0.25779806388704485) (23, 0.2412215124270137) (30, 0.21579251773540695) (62, 0.2061936996975851) (29, 0.1962488726486241) (35, 0.19522994610327346) (86, 0.18934292971595598) (27, 0.1823911579409791) (96, 0.18188010730196982) (40, 0.1803640484660976) plt . subplots ( figsize = ( 20 , 20 )) plt . title ( 'Email, EU-Core network: eigenvector centrality' ) nx . draw_kamada_kawai ( G , with_labels = True , node_size = list ( np . array ( list ( ev_centrality . values ())) * 2500 ))","title":"Page rank (Eigenvector centrality)"},{"location":"Python/Network%20centrality/#clustering-coefficient","text":"Any graph in general can be densely connected or sparcely connected. The behaviour of the graph, and therefore the actions that have to be taken, are different for sparce graphs vs densely connected graphs. The betweenness measures above should be looked at along with clustering coefficient. The clustering coefficient is a way of measuring the degree to which the nodes in a graph cluster together. Networks with high number of clustering coefficient are more social. The average clustering coefficient for the graph is (assuming a undirected graph): G_undir = nx . from_pandas_edgelist ( df , source = 'Source' , target = 'Target' ) nx . algorithms . cluster . average_clustering ( G_undir ) 0.5130839320504162 From the above analysis, we can find the most important people in the network. The people who has the most connections, the people who are part of multiple groups and are important to send message across and the people who are important based on whom they know immediately.","title":"Clustering coefficient"},{"location":"Python/Network%20centrality/#references","text":"https://towardsdatascience.com/visualize-high-dimensional-network-data-with-3d-360-degree-animated-scatter-plot-d583932d3693 Assignment and student notes, Business Analytics MSc, Imperial College London, Network Analytics module Hao Yin, Austin R. Benson, Jure Leskovec, and David F. Gleich. \"Local Higher-order Graph Clustering.\" In Proceedings of the 23 rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 2017 https://snap.stanford.edu/data/email-Eu-core.html Networks, Crowds, and Markets: Reasoning about a Highly Connected World by David Easley and Jon Kleinberg","title":"References"},{"location":"Python/ORM/","text":"ORM's in python \u00b6 Object Relational Mappers convert a python object to a database row, and vice versa. The most popular ORM in python is SqlAlchemy. import sqlalchemy from sqlalchemy import create_engine , inspect # helper functions db_url = 'postgressql://username:password@hostname:hostnumber/database' SqlAlchemy \u00b6 SqlAlchemy engine is a helper (database client) that does our connection to our database and runs our statements for us. In this blog, we are connecting to the database that I created as described in Handling databases using python . engine = create_engine ( db_url ) An inspector is another helper that explores the database, for example to get table names, SqlAlchemy talks to the database, consults its schema and gets the table names. inspector = inspect ( engine ) print ( inspector . get_table_names ()) ['daily_historic_pollution', 'daily_prediction_pollution'] We have two tables in the database, daily_historic_prediction and daily_prediction_pollution. We can now use SqlAlchemy to run a query result = engine . execute ( 'SELECT * FROM daily_historic_pollution ORDER BY date DESC LIMIT 10' ) for r in result : print ( r . date , r . value ) 2021-12-10 163 2021-10-15 158 2021-10-14 148 2021-10-03 50 2021-10-02 47 2021-10-01 87 2021-09-30 63 2021-09-29 43 2021-09-28 41 2021-09-27 61 In this way, we can get the latest 10 datapoints in our dataset. We could integrate this to a webserver so that our webserver can talk to our database. Flask webapp \u00b6 In this blog, we will be using a python webserver called Flask . Flask is simple, light and customisable that can generate simple webapps with a few pages, read/set cookies and connect to databases using ORM's. To use the ORM, we have to first develop a flask app. One of the important things to set up in an ORM is the list of models. Models are python objects that map onto entities in the database. For example, we want a historic_pollution table with a row for each date in the past. The ORM object will generate python objects for each row allowing us to easily work with rows in python rather than in SQL. One of the main jobs of the ORM is to notice changes of the python object and construct SQL statements to automatically modify the database accordingly. The flask app is present in the github repository flask-web-app with the coplete code discussed here. Inside the app folder, we have a file called models.py . This holds our database models, and tells us how Python is going to translate between data in the database tables and python objects. We are defining two python classes here. Each of them maps direcly onto a table in the database. Each of the lines inside the class tells how to treat one of the columns in the database. For instance, we have date, type and value in the daily_historic_pollution table, and for each of them we have to tell SqlAlchemy the type. We can also mention the primary key, index and uninque columns etc. This file allows SqlAlchemy to do two things: 1. Read from the database 2. Build SQL statements and execute them # File models.py from app import db from sqlalchemy.orm import relationship from sqlalchemy import Table , Column , Integer , ForeignKey class Historic ( db . Model ): __tablename__ = 'daily_historic_pollution' date = db . Column ( db . DateTime , ForeignKey ( 'daily_prediction_pollution.date' ), primary_key = True ) type = db . Column ( db . String ( 255 )) value = db . Column ( db . Integer ) def __repr__ ( self ): return 'Date: ' + self . date + ' PM25: ' + self . value class Prediction ( db . Model ): __tablename__ = 'daily_prediction_pollution' date = db . Column ( db . DateTime , primary_key = True ) type = db . Column ( db . String ( 255 )) prediction = db . Column ( db . Float ) past_pred = relationship ( 'Historic' ) def __repr__ ( self ): return 'Date: ' + self . date + ' PM25: ' + self . value In the routes.py file we are declaring the routes, or the web addresses that we can visit using our python server. The first one is for the home page, and it renders index.html page. The @ sign is a decorator saying that if we request \"/\" or \"/index\" the first function (index function) will be run. For the \"/raw_historic_data\" a simple query to select the latest 15 values are selected and the raw_historic_data.html is rendered. # File routes.py from flask import render_template from app import app , db from app.models import Historic , Prediction import pdb from sqlalchemy import desc @app . route ( '/' ) @app . route ( '/index' ) def index (): return render_template ( 'index.html' ) @app . route ( '/raw_historic_data' ) def historic (): historic = Historic . query . order_by ( desc ( Historic . date )) . limit ( 15 ) . all () return render_template ( 'raw_historic_data.html' , historic = historic ) @app . route ( '/raw_predicted_data' ) def predicted (): prediction = Prediction . query . order_by ( desc ( Prediction . date )) . limit ( 15 ) . all () return render_template ( 'raw_prediction_data.html' , predicted = prediction ) In 'raw_historic_data.html' we have a template with a python loop taking the input from the query and displaying the data. The snippet of the code is shown: < table > < tr >< td >< b > Date </ b ></ td >& nbsp ; < td >< b > PM25 </ b ></ td ></ tr > { % for day in historic % } < tr >< td > {{ day . date }} </ td >& nbsp ; < td > {{ day . value }} </ td ></ tr > { % endfor % } </ table > To run flask, we simply need to run \"flask run\" and the Flask app runs on local server http://127.0.0.1:5000/ . Deployment on Heroku \u00b6 Heroku is a cloud platform which supports Python, Ruby and various other programming languages. It has many cloud based products including Heroku platform (runs customer apps in virtual containers), Heroku Postgres (cloud database) and many others making it a platform as a service product. First we have to create a ap on heroku. I have created the app hydpm25 (hyderabad-PM25). Then deployment is easy thru connecting it with git repository. A proc file containing \"web: gunicorn app:app\" will serve the web app on heroku. The final app can be found at hydpm25.herokuapp.com","title":"ORM (Python)"},{"location":"Python/ORM/#orms-in-python","text":"Object Relational Mappers convert a python object to a database row, and vice versa. The most popular ORM in python is SqlAlchemy. import sqlalchemy from sqlalchemy import create_engine , inspect # helper functions db_url = 'postgressql://username:password@hostname:hostnumber/database'","title":"ORM's in python"},{"location":"Python/ORM/#sqlalchemy","text":"SqlAlchemy engine is a helper (database client) that does our connection to our database and runs our statements for us. In this blog, we are connecting to the database that I created as described in Handling databases using python . engine = create_engine ( db_url ) An inspector is another helper that explores the database, for example to get table names, SqlAlchemy talks to the database, consults its schema and gets the table names. inspector = inspect ( engine ) print ( inspector . get_table_names ()) ['daily_historic_pollution', 'daily_prediction_pollution'] We have two tables in the database, daily_historic_prediction and daily_prediction_pollution. We can now use SqlAlchemy to run a query result = engine . execute ( 'SELECT * FROM daily_historic_pollution ORDER BY date DESC LIMIT 10' ) for r in result : print ( r . date , r . value ) 2021-12-10 163 2021-10-15 158 2021-10-14 148 2021-10-03 50 2021-10-02 47 2021-10-01 87 2021-09-30 63 2021-09-29 43 2021-09-28 41 2021-09-27 61 In this way, we can get the latest 10 datapoints in our dataset. We could integrate this to a webserver so that our webserver can talk to our database.","title":"SqlAlchemy"},{"location":"Python/ORM/#flask-webapp","text":"In this blog, we will be using a python webserver called Flask . Flask is simple, light and customisable that can generate simple webapps with a few pages, read/set cookies and connect to databases using ORM's. To use the ORM, we have to first develop a flask app. One of the important things to set up in an ORM is the list of models. Models are python objects that map onto entities in the database. For example, we want a historic_pollution table with a row for each date in the past. The ORM object will generate python objects for each row allowing us to easily work with rows in python rather than in SQL. One of the main jobs of the ORM is to notice changes of the python object and construct SQL statements to automatically modify the database accordingly. The flask app is present in the github repository flask-web-app with the coplete code discussed here. Inside the app folder, we have a file called models.py . This holds our database models, and tells us how Python is going to translate between data in the database tables and python objects. We are defining two python classes here. Each of them maps direcly onto a table in the database. Each of the lines inside the class tells how to treat one of the columns in the database. For instance, we have date, type and value in the daily_historic_pollution table, and for each of them we have to tell SqlAlchemy the type. We can also mention the primary key, index and uninque columns etc. This file allows SqlAlchemy to do two things: 1. Read from the database 2. Build SQL statements and execute them # File models.py from app import db from sqlalchemy.orm import relationship from sqlalchemy import Table , Column , Integer , ForeignKey class Historic ( db . Model ): __tablename__ = 'daily_historic_pollution' date = db . Column ( db . DateTime , ForeignKey ( 'daily_prediction_pollution.date' ), primary_key = True ) type = db . Column ( db . String ( 255 )) value = db . Column ( db . Integer ) def __repr__ ( self ): return 'Date: ' + self . date + ' PM25: ' + self . value class Prediction ( db . Model ): __tablename__ = 'daily_prediction_pollution' date = db . Column ( db . DateTime , primary_key = True ) type = db . Column ( db . String ( 255 )) prediction = db . Column ( db . Float ) past_pred = relationship ( 'Historic' ) def __repr__ ( self ): return 'Date: ' + self . date + ' PM25: ' + self . value In the routes.py file we are declaring the routes, or the web addresses that we can visit using our python server. The first one is for the home page, and it renders index.html page. The @ sign is a decorator saying that if we request \"/\" or \"/index\" the first function (index function) will be run. For the \"/raw_historic_data\" a simple query to select the latest 15 values are selected and the raw_historic_data.html is rendered. # File routes.py from flask import render_template from app import app , db from app.models import Historic , Prediction import pdb from sqlalchemy import desc @app . route ( '/' ) @app . route ( '/index' ) def index (): return render_template ( 'index.html' ) @app . route ( '/raw_historic_data' ) def historic (): historic = Historic . query . order_by ( desc ( Historic . date )) . limit ( 15 ) . all () return render_template ( 'raw_historic_data.html' , historic = historic ) @app . route ( '/raw_predicted_data' ) def predicted (): prediction = Prediction . query . order_by ( desc ( Prediction . date )) . limit ( 15 ) . all () return render_template ( 'raw_prediction_data.html' , predicted = prediction ) In 'raw_historic_data.html' we have a template with a python loop taking the input from the query and displaying the data. The snippet of the code is shown: < table > < tr >< td >< b > Date </ b ></ td >& nbsp ; < td >< b > PM25 </ b ></ td ></ tr > { % for day in historic % } < tr >< td > {{ day . date }} </ td >& nbsp ; < td > {{ day . value }} </ td ></ tr > { % endfor % } </ table > To run flask, we simply need to run \"flask run\" and the Flask app runs on local server http://127.0.0.1:5000/ .","title":"Flask webapp"},{"location":"Python/ORM/#deployment-on-heroku","text":"Heroku is a cloud platform which supports Python, Ruby and various other programming languages. It has many cloud based products including Heroku platform (runs customer apps in virtual containers), Heroku Postgres (cloud database) and many others making it a platform as a service product. First we have to create a ap on heroku. I have created the app hydpm25 (hyderabad-PM25). Then deployment is easy thru connecting it with git repository. A proc file containing \"web: gunicorn app:app\" will serve the web app on heroku. The final app can be found at hydpm25.herokuapp.com","title":"Deployment on Heroku"},{"location":"Python/Saving%20predictions%20in%20database/","text":"Handling databases using python \u00b6 I want to create a database for storing the predictions and actual values of the air pollution near my home in Hyderabad. In a previous blog , I have demonstrated how we can deploy a flask application using pythonanywhere which predicts the pollution using a machine learning model that I built. In this blog, we will see how we can store this information in a database to be used either for visualisation or other analytics. Free database on ElephantSQL \u00b6 Elephant SQL automates setup and running of PostgreSQL clusters. They have a free plan called \"Tiny turtle\" which gives us 5 concurrent connections and 20MB of data. This is sufficient for storing our data. Using the free plan, we can create an instance called \"hydpm25\" (For Hyderabad PM25 data) Creating tables \u00b6 Once we have a cluster created, we can create tables in three ways. 1. Using SQL commands on \"SQL Browser\" 2. Using psycopg2 in Python 3. Using SQLAlchemy SQL Queries \u00b6 First, let us create a table using SQL Browser. For creating a new table \"daily_historic_pollution\" which will contain the historic pollution data of Hyderabad we can use the following query. CREATE DATABASE hyderabad_aqi ; CREATE TABLE daily_historic_pollution ( date DATE PRIMARY KEY , type VARCHAR ( 20 ), value FLOAT ); Load data using python \u00b6 We can use psycopg to connect and create the same table. The URL to connect is provided by ElephantSQL under \"Details\". After connecting to the database, we can create tables using SQL commands. import psycopg2 connection_string = \"postgressql://username:password@hostname:hostnumber/database\" conn = psycopg2 . connect ( connection_string ) conn . set_session ( autocommit = True ) cur = conn . cursor () cur . execute ( \"\"\" DROP TABLE IF EXISTS daily_historic_pollution \"\"\" ) cur . execute ( \"\"\" CREATE TABLE daily_historic_pollution (date DATE PRIMARY KEY, type VARCHAR(20), value INTEGER ) \"\"\" ) print ( \"Created table\" ) cur . close () conn . close () Created table SqlAlchemy \u00b6 According to me, the best way to interface with a database is using SqlAlchemy. For example, consider the data from aqicn's api on the air pollution data for Hyderabad. import pandas as pd df = pd . read_csv ( 'hyderabad-us consulate, india-air-quality.csv' ) df [ 'type' ] = 'pm25' df . columns = [ 'date' , 'value' , 'type' ] df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date value type 0 2021/10/1 87 pm25 1 2021/10/2 47 pm25 2 2021/10/3 50 pm25 3 2021/9/1 66 pm25 4 2021/9/2 74 pm25 Using SqlAlchemy, we can upload this data to the previously created table. from sqlalchemy import create_engine import numpy from psycopg2.extensions import register_adapter , AsIs def addapt_numpy_float64 ( numpy_float64 ): return AsIs ( numpy_float64 ) def addapt_numpy_int64 ( numpy_int64 ): return AsIs ( numpy_int64 ) register_adapter ( numpy . float64 , addapt_numpy_float64 ) register_adapter ( numpy . int64 , addapt_numpy_int64 ) engine = create_engine ( connection_string ) for i in range ( len ( df )): query = \"\"\"INSERT INTO daily_historic_pollution (date, type, value) VALUES( %s , %s , %s ) ON CONFLICT (date) DO UPDATE SET type = excluded.type, value = excluded.value; \"\"\" engine . execute ( query , ( df . date [ i ], df . type [ i ], df . value [ i ])) HTTP requests \u00b6 In this way, we can load data into a database. Now I want to create a new table that can store the predictions from the flask application into a table. conn = psycopg2 . connect ( connection_string ) conn . set_session ( autocommit = True ) cur = conn . cursor () cur . execute ( \"\"\" DROP TABLE IF EXISTS daily_prediction_pollution \"\"\" ) cur . execute ( \"\"\" CREATE TABLE daily_prediction_pollution (date DATE PRIMARY KEY, type VARCHAR(20), prediction FLOAT ) \"\"\" ) print ( \"Created table\" ) cur . close () conn . close () Created table We can use the GET request from the flask application in the previous blog to predict PM25 for the date. import requests from datetime import date , datetime today = str ( date . today () . year ) + '-' + str ( date . today () . month ) + '-' + str ( date . today () . day ) response = requests . get ( \"https://harshaash.pythonanywhere.com/predict\" , params = { 'date' : today }) response . json ()[ 'prediction' ][ 0 ] 115.6 The prediction is 115.6 PM25. We can use our knowledge on SqlAlchemy to store this value in a database. query2 = \"\"\"INSERT INTO daily_prediction_pollution (date, type, prediction) VALUES( %s , %s , %s ) ON CONFLICT (date) DO UPDATE SET type = excluded.type, prediction = excluded.prediction; \"\"\" engine . execute ( query2 , ( today , 'pm25' , response . json ()[ 'prediction' ][ 0 ])) We can see this result in the database We can get actual pollution data for the past from JSON API of AQICN.org . We can use this data to update the database with the previous day's value. aqi_token = { 'token' : 'default_token' } response = requests . get ( \"https://api.waqi.info/feed/hyderabad/\" , params = aqi_token ) data = pd . DataFrame ( response . json ()[ 'data' ][ 'forecast' ][ 'daily' ][ 'pm25' ]) data . day = pd . to_datetime ( data . day ) data [ 'type' ] = 'pm25' data . columns = [ 'value' , 'date' , 'max' , 'min' , 'type' ] data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } value date max min type 0 158 2021-10-15 159 145 pm25 1 150 2021-10-16 159 138 pm25 2 152 2021-10-17 169 138 pm25 3 140 2021-10-18 159 138 pm25 4 145 2021-10-19 158 138 pm25 5 138 2021-10-20 138 138 pm25 6 134 2021-10-21 138 89 pm25 7 133 2021-10-22 138 89 pm25 8 138 2021-10-23 138 138 pm25 From the API result, we can see the previous two day's data and predictions (by AQICN) for the next few days. We can use this API to update the data for previous days data in our table. for i in range ( 2 ): query = \"\"\"INSERT INTO daily_historic_pollution (date, type, value) VALUES( %s , %s , %s ) ON CONFLICT (date) DO UPDATE SET type = excluded.type, value = excluded.value; \"\"\" engine . execute ( query , ( data . date [ i ], data . type [ i ], data . value [ i ])) Scheduling using Github Actions \u00b6 I want to run these predictions every day, and the data/predictions are stored at a set point of time every day. To do this, we can combine all the codes above into one python file that gets executed every day. This file contains code that will predict the pollution for today and store this value along with the pollution for yesterday in the database. # File name: aqi_script.py import pandas as pd import requests from datetime import date , datetime from sqlalchemy import create_engine import numpy from psycopg2.extensions import register_adapter , AsIs def addapt_numpy_float64 ( numpy_float64 ): return AsIs ( numpy_float64 ) def addapt_numpy_int64 ( numpy_int64 ): return AsIs ( numpy_int64 ) register_adapter ( numpy . float64 , addapt_numpy_float64 ) register_adapter ( numpy . int64 , addapt_numpy_int64 ) connection_string = \"postgressql://username:password@hostname:hostnumber/database\" engine = create_engine ( connection_string ) today = str ( date . today () . year ) + '-' + str ( date . today () . month ) + '-' + str ( date . today () . day ) # Upload the prediction data response = requests . get ( \"https://harshaash.pythonanywhere.com/predict\" , params = { 'date' : today }) query2 = \"\"\"INSERT INTO daily_prediction_pollution (date, type, prediction) VALUES( %s , %s , %s ) ON CONFLICT (date) DO UPDATE SET type = excluded.type, prediction = excluded.prediction; \"\"\" engine . execute ( query2 , ( today , 'pm25' , response . json ()[ 'prediction' ][ 0 ])) # Upload the actual data aqi_token = { 'token' : 'dummy_token' } response = requests . get ( \"https://api.waqi.info/feed/hyderabad/\" , params = aqi_token ) data = pd . DataFrame ( response . json ()[ 'data' ][ 'forecast' ][ 'daily' ][ 'pm25' ]) data . day = pd . to_datetime ( data . day ) data [ 'type' ] = 'pm25' data . columns = [ 'value' , 'date' , 'max' , 'min' , 'type' ] for i in range ( 2 ): query = \"\"\"INSERT INTO daily_historic_pollution (date, type, value) VALUES( %s , %s , %s ) ON CONFLICT (date) DO UPDATE SET type = excluded.type, value = excluded.value; \"\"\" engine . execute ( query , ( data . date [ i ], data . type [ i ], data . value [ i ])) We can schedule this code to run every day at a particular time so that our database is updated every day with the predictions. This can be done using cron jobs. One way to implement cron jobs is using GitHub actions. To create a cron job, a yml file should be created within the .github/workfows folder in the Github repository. This yml file is converted to actions, and this is shown on the actions page. For more details, refer here . The yml file for our use case would be: # FIle name: any_name.yml # location: .github/workflows name : update_database_AQI_data on : schedule : - cron : '5 1 * * *' # to run at 1:05 AM GMT everyday jobs : build : name : Build project runs - on : ubuntu - latest steps : - name : Checkout repository uses : actions / checkout @v2 # Downloads the current git repo - name : Setup Python uses : actions / setup - python @v2 # Installs the python setup - name : Install dependancies # Installs the required packages run : | python - m pip install -- upgrade pip pip install - r requirements . txt - name : Execute python file # Run the script file run : python aqi_script . py Created by Achyuthuni Sri Harsha","title":"Handling databases using python"},{"location":"Python/Saving%20predictions%20in%20database/#handling-databases-using-python","text":"I want to create a database for storing the predictions and actual values of the air pollution near my home in Hyderabad. In a previous blog , I have demonstrated how we can deploy a flask application using pythonanywhere which predicts the pollution using a machine learning model that I built. In this blog, we will see how we can store this information in a database to be used either for visualisation or other analytics.","title":"Handling databases using python"},{"location":"Python/Saving%20predictions%20in%20database/#free-database-on-elephantsql","text":"Elephant SQL automates setup and running of PostgreSQL clusters. They have a free plan called \"Tiny turtle\" which gives us 5 concurrent connections and 20MB of data. This is sufficient for storing our data. Using the free plan, we can create an instance called \"hydpm25\" (For Hyderabad PM25 data)","title":"Free database on ElephantSQL"},{"location":"Python/Saving%20predictions%20in%20database/#creating-tables","text":"Once we have a cluster created, we can create tables in three ways. 1. Using SQL commands on \"SQL Browser\" 2. Using psycopg2 in Python 3. Using SQLAlchemy","title":"Creating tables"},{"location":"Python/Saving%20predictions%20in%20database/#sql-queries","text":"First, let us create a table using SQL Browser. For creating a new table \"daily_historic_pollution\" which will contain the historic pollution data of Hyderabad we can use the following query. CREATE DATABASE hyderabad_aqi ; CREATE TABLE daily_historic_pollution ( date DATE PRIMARY KEY , type VARCHAR ( 20 ), value FLOAT );","title":"SQL Queries"},{"location":"Python/Saving%20predictions%20in%20database/#load-data-using-python","text":"We can use psycopg to connect and create the same table. The URL to connect is provided by ElephantSQL under \"Details\". After connecting to the database, we can create tables using SQL commands. import psycopg2 connection_string = \"postgressql://username:password@hostname:hostnumber/database\" conn = psycopg2 . connect ( connection_string ) conn . set_session ( autocommit = True ) cur = conn . cursor () cur . execute ( \"\"\" DROP TABLE IF EXISTS daily_historic_pollution \"\"\" ) cur . execute ( \"\"\" CREATE TABLE daily_historic_pollution (date DATE PRIMARY KEY, type VARCHAR(20), value INTEGER ) \"\"\" ) print ( \"Created table\" ) cur . close () conn . close () Created table","title":"Load data using python"},{"location":"Python/Saving%20predictions%20in%20database/#sqlalchemy","text":"According to me, the best way to interface with a database is using SqlAlchemy. For example, consider the data from aqicn's api on the air pollution data for Hyderabad. import pandas as pd df = pd . read_csv ( 'hyderabad-us consulate, india-air-quality.csv' ) df [ 'type' ] = 'pm25' df . columns = [ 'date' , 'value' , 'type' ] df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date value type 0 2021/10/1 87 pm25 1 2021/10/2 47 pm25 2 2021/10/3 50 pm25 3 2021/9/1 66 pm25 4 2021/9/2 74 pm25 Using SqlAlchemy, we can upload this data to the previously created table. from sqlalchemy import create_engine import numpy from psycopg2.extensions import register_adapter , AsIs def addapt_numpy_float64 ( numpy_float64 ): return AsIs ( numpy_float64 ) def addapt_numpy_int64 ( numpy_int64 ): return AsIs ( numpy_int64 ) register_adapter ( numpy . float64 , addapt_numpy_float64 ) register_adapter ( numpy . int64 , addapt_numpy_int64 ) engine = create_engine ( connection_string ) for i in range ( len ( df )): query = \"\"\"INSERT INTO daily_historic_pollution (date, type, value) VALUES( %s , %s , %s ) ON CONFLICT (date) DO UPDATE SET type = excluded.type, value = excluded.value; \"\"\" engine . execute ( query , ( df . date [ i ], df . type [ i ], df . value [ i ]))","title":"SqlAlchemy"},{"location":"Python/Saving%20predictions%20in%20database/#http-requests","text":"In this way, we can load data into a database. Now I want to create a new table that can store the predictions from the flask application into a table. conn = psycopg2 . connect ( connection_string ) conn . set_session ( autocommit = True ) cur = conn . cursor () cur . execute ( \"\"\" DROP TABLE IF EXISTS daily_prediction_pollution \"\"\" ) cur . execute ( \"\"\" CREATE TABLE daily_prediction_pollution (date DATE PRIMARY KEY, type VARCHAR(20), prediction FLOAT ) \"\"\" ) print ( \"Created table\" ) cur . close () conn . close () Created table We can use the GET request from the flask application in the previous blog to predict PM25 for the date. import requests from datetime import date , datetime today = str ( date . today () . year ) + '-' + str ( date . today () . month ) + '-' + str ( date . today () . day ) response = requests . get ( \"https://harshaash.pythonanywhere.com/predict\" , params = { 'date' : today }) response . json ()[ 'prediction' ][ 0 ] 115.6 The prediction is 115.6 PM25. We can use our knowledge on SqlAlchemy to store this value in a database. query2 = \"\"\"INSERT INTO daily_prediction_pollution (date, type, prediction) VALUES( %s , %s , %s ) ON CONFLICT (date) DO UPDATE SET type = excluded.type, prediction = excluded.prediction; \"\"\" engine . execute ( query2 , ( today , 'pm25' , response . json ()[ 'prediction' ][ 0 ])) We can see this result in the database We can get actual pollution data for the past from JSON API of AQICN.org . We can use this data to update the database with the previous day's value. aqi_token = { 'token' : 'default_token' } response = requests . get ( \"https://api.waqi.info/feed/hyderabad/\" , params = aqi_token ) data = pd . DataFrame ( response . json ()[ 'data' ][ 'forecast' ][ 'daily' ][ 'pm25' ]) data . day = pd . to_datetime ( data . day ) data [ 'type' ] = 'pm25' data . columns = [ 'value' , 'date' , 'max' , 'min' , 'type' ] data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } value date max min type 0 158 2021-10-15 159 145 pm25 1 150 2021-10-16 159 138 pm25 2 152 2021-10-17 169 138 pm25 3 140 2021-10-18 159 138 pm25 4 145 2021-10-19 158 138 pm25 5 138 2021-10-20 138 138 pm25 6 134 2021-10-21 138 89 pm25 7 133 2021-10-22 138 89 pm25 8 138 2021-10-23 138 138 pm25 From the API result, we can see the previous two day's data and predictions (by AQICN) for the next few days. We can use this API to update the data for previous days data in our table. for i in range ( 2 ): query = \"\"\"INSERT INTO daily_historic_pollution (date, type, value) VALUES( %s , %s , %s ) ON CONFLICT (date) DO UPDATE SET type = excluded.type, value = excluded.value; \"\"\" engine . execute ( query , ( data . date [ i ], data . type [ i ], data . value [ i ]))","title":"HTTP requests"},{"location":"Python/Saving%20predictions%20in%20database/#scheduling-using-github-actions","text":"I want to run these predictions every day, and the data/predictions are stored at a set point of time every day. To do this, we can combine all the codes above into one python file that gets executed every day. This file contains code that will predict the pollution for today and store this value along with the pollution for yesterday in the database. # File name: aqi_script.py import pandas as pd import requests from datetime import date , datetime from sqlalchemy import create_engine import numpy from psycopg2.extensions import register_adapter , AsIs def addapt_numpy_float64 ( numpy_float64 ): return AsIs ( numpy_float64 ) def addapt_numpy_int64 ( numpy_int64 ): return AsIs ( numpy_int64 ) register_adapter ( numpy . float64 , addapt_numpy_float64 ) register_adapter ( numpy . int64 , addapt_numpy_int64 ) connection_string = \"postgressql://username:password@hostname:hostnumber/database\" engine = create_engine ( connection_string ) today = str ( date . today () . year ) + '-' + str ( date . today () . month ) + '-' + str ( date . today () . day ) # Upload the prediction data response = requests . get ( \"https://harshaash.pythonanywhere.com/predict\" , params = { 'date' : today }) query2 = \"\"\"INSERT INTO daily_prediction_pollution (date, type, prediction) VALUES( %s , %s , %s ) ON CONFLICT (date) DO UPDATE SET type = excluded.type, prediction = excluded.prediction; \"\"\" engine . execute ( query2 , ( today , 'pm25' , response . json ()[ 'prediction' ][ 0 ])) # Upload the actual data aqi_token = { 'token' : 'dummy_token' } response = requests . get ( \"https://api.waqi.info/feed/hyderabad/\" , params = aqi_token ) data = pd . DataFrame ( response . json ()[ 'data' ][ 'forecast' ][ 'daily' ][ 'pm25' ]) data . day = pd . to_datetime ( data . day ) data [ 'type' ] = 'pm25' data . columns = [ 'value' , 'date' , 'max' , 'min' , 'type' ] for i in range ( 2 ): query = \"\"\"INSERT INTO daily_historic_pollution (date, type, value) VALUES( %s , %s , %s ) ON CONFLICT (date) DO UPDATE SET type = excluded.type, value = excluded.value; \"\"\" engine . execute ( query , ( data . date [ i ], data . type [ i ], data . value [ i ])) We can schedule this code to run every day at a particular time so that our database is updated every day with the predictions. This can be done using cron jobs. One way to implement cron jobs is using GitHub actions. To create a cron job, a yml file should be created within the .github/workfows folder in the Github repository. This yml file is converted to actions, and this is shown on the actions page. For more details, refer here . The yml file for our use case would be: # FIle name: any_name.yml # location: .github/workflows name : update_database_AQI_data on : schedule : - cron : '5 1 * * *' # to run at 1:05 AM GMT everyday jobs : build : name : Build project runs - on : ubuntu - latest steps : - name : Checkout repository uses : actions / checkout @v2 # Downloads the current git repo - name : Setup Python uses : actions / setup - python @v2 # Installs the python setup - name : Install dependancies # Installs the required packages run : | python - m pip install -- upgrade pip pip install - r requirements . txt - name : Execute python file # Run the script file run : python aqi_script . py Created by Achyuthuni Sri Harsha","title":"Scheduling using Github Actions"},{"location":"Python/Shortest%20path%20problems/","text":"MathJax.Hub.Config({ extensions: [\"tex2jax.js\",\"TeX/AMSmath.js\",\"TeX/AMSsymbols.js\"], jax: [\"input/TeX\", \"output/HTML-CSS\"], tex2jax: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ], }, \"HTML-CSS\": { availableFonts: [\"TeX\"] } }); Shortest path problems \u00b6 Author: Achyuthuni Sri Harsha Shortest path is one problem in networks which appears in many forms across many industries. It tells the user how to find the shortest path between two pair of nodes. In this particular example, we will look at finding the shortest path between a pair of nodes in a directed network using an integer programming solver. import networkx as nx import numpy as np import pandas as pd import matplotlib.pyplot as plt Consider the following simple weighted directed network, with four nodes and five edges. edgelist_df = pd . DataFrame ({ 'node1' :[ 's' , 's' , 'u' , 'u' , 'v' ], 'node2' :[ 'u' , 'v' , 'v' , 't' , 't' ], 'weights' :[ 3 , 2 , 0.7 , 1 , 7 ] }) edgelist_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } node1 node2 weights 0 s u 3.0 1 s v 2.0 2 u v 0.7 3 u t 1.0 4 v t 7.0 g = nx . DiGraph () for i , elrow in edgelist_df . iterrows (): g . add_edge ( elrow [ 0 ], elrow [ 1 ], weight = elrow [ 2 ]) g . edges ( data = True ) OutEdgeDataView([('s', 'u', {'weight': 3.0}), ('s', 'v', {'weight': 2.0}), ('u', 'v', {'weight': 0.7}), ('u', 't', {'weight': 1.0}), ('v', 't', {'weight': 7.0})]) # for each node we are trying to fix the coordinates g . add_node ( 's' , pos = ( 0 , 1 )) g . add_node ( 'u' , pos = ( 1 , 2 )) g . add_node ( 'v' , pos = ( 1 , 0 )) g . add_node ( 't' , pos = ( 2 , 1 )) g . nodes ( data = True ) NodeDataView({'s': {'pos': (0, 1)}, 'u': {'pos': (1, 2)}, 'v': {'pos': (1, 0)}, 't': {'pos': (2, 1)}}) # This function gets the coordinates for nodes pos = nx . get_node_attributes ( g , 'pos' ) # This function gets the weights for the edges weight = nx . get_edge_attributes ( g , 'weight' ) nx . draw ( g , pos , with_labels = True ) nx . draw_networkx_edge_labels ( g , pos , edge_labels = weight ) plt . show () Now, we want to find the shortest path between the node 's' and 't'. In this network, edge-weights represent the costs for each edge. The weights could be distance, or time. NetworkX has an inbuilt function shortest_path which returs the shortest path. Using networkX, we get the following shortest path: path_2_0 = nx . shortest_path ( g , source = 's' , target = 't' ) path_2_0 ['s', 'u', 't'] Formulating the problem using integer programming \u00b6 We have n nodes V and m edges E (n=4, m=5 for this example). OR tools is an open source software built by Google for solving integer programming problems. Cp-Sat solver is one such model by OR Tools, which we are going to use today. We can use binary decision variables \\(edge_{i,j}\\) representing the edge that goes from node i to node j. If \\(edge_{i,j}=1\\) the shortest path belongs to the path between i and j, 0 otherwise. from ortools.sat.python import cp_model shortest_path_model = cp_model . CpModel () # Creating one integer decision variable for each edge edge_bool_vars = {} for edge in g . edges : edge_bool_vars [ edge [ 0 ], edge [ 1 ]] = shortest_path_model . NewBoolVar ( 'edge_ %s _ %s ' % edge ) print ( 'Creating the boolean variable ' , edge_bool_vars [ edge [ 0 ], edge [ 1 ]], 'representing the if we should travel through ' , ( edge [ 0 ], edge [ 1 ])) Creating the boolean variable edge_s_u representing the if we should travel through ('s', 'u') Creating the boolean variable edge_s_v representing the if we should travel through ('s', 'v') Creating the boolean variable edge_u_v representing the if we should travel through ('u', 'v') Creating the boolean variable edge_u_t representing the if we should travel through ('u', 't') Creating the boolean variable edge_v_t representing the if we should travel through ('v', 't') The shortest path (in isolation) will have the following properties: 1. Starting node has a degree -1 2. Ending node has a degree +1 3. All intermediatory nodes have degree 0 This can be written in the form of flow balance constraints as follows: input_node = 's' output_node = 't' # Adding constraints on the nodes for node in g . nodes : in_edges = g . in_edges ( node ) out_edges = g . out_edges ( node ) print ( 'Adding the constraint on node ' , node ) print ( 'This node has %i in-edges and %i out-edges' % ( len ( in_edges ), len ( out_edges ))) equation_at_this_edge = sum ( edge_bool_vars [ edge [ 0 ], edge [ 1 ]] for edge in in_edges ) - \\ sum ( edge_bool_vars [ edge [ 0 ], edge [ 1 ]] for edge in out_edges ) if ( node == input_node ): shortest_path_model . Add ( equation_at_this_edge == - 1 ) print ( equation_at_this_edge == - 1 ) elif ( node == output_node ): shortest_path_model . Add ( equation_at_this_edge == 1 ) print ( equation_at_this_edge == 1 ) else : shortest_path_model . Add ( equation_at_this_edge == 0 ) print ( equation_at_this_edge , '== 0' ) print ( '' ) Adding the constraint on node s This node has 0 in-edges and 2 out-edges (-((edge_s_u) + edge_s_v)) == -1 Adding the constraint on node u This node has 1 in-edges and 2 out-edges ((edge_s_u) + -((edge_u_v) + edge_u_t)) == 0 Adding the constraint on node v This node has 2 in-edges and 1 out-edges (((edge_s_v) + edge_u_v) + -(edge_v_t)) == 0 Adding the constraint on node t This node has 2 in-edges and 0 out-edges (((edge_u_t) + edge_v_t)) == 1 The objective of the shortest path problem is to find the path with the minimum cost. This can be written as minimising the costs as follows: # factor to make everything including costs integer factor_to_int = 10 # The objective is to maximise flow total_cost = sum ( int ( g . get_edge_data ( * edge )[ 'weight' ] * factor_to_int ) * edge_bool_vars [ edge [ 0 ], edge [ 1 ]] for edge in g . edges ) print ( 'Objective is to optimise cost' ) print ( total_cost ) shortest_path_model . Minimize ( total_cost ) Objective is to optimise cost ((((((30 * edge_s_u)) + (20 * edge_s_v)) + (7 * edge_u_v)) + (10 * edge_u_t)) + (70 * edge_v_t)) Solving the problem, we have an optimal solution with the overall cost as 40 units. # Solving the problem solver = cp_model . CpSolver () solution_printer = cp_model . ObjectiveSolutionPrinter () status = solver . SolveWithSolutionCallback ( shortest_path_model , solution_printer ) Solution 0, time = 0.02 s, objective = 40 cp_model . OPTIMAL == status True The solution is given as result_edges = {} for edge in g . edges : result_edges [ edge [ 0 ], edge [ 1 ]] = solver . Value ( edge_bool_vars [ edge [ 0 ], edge [ 1 ]]) result_edges {('s', 'u'): 1, ('s', 'v'): 0, ('u', 'v'): 0, ('u', 't'): 1, ('v', 't'): 0} Plotting the network in such a way that the green lines represent the shortest path, we get pos = nx . get_node_attributes ( g , 'pos' ) color = [ 'g' if val == 1 else 'r' for val in result_edges . values ()] nx . draw ( g , pos , with_labels = True , edge_color = color ) nx . draw_networkx_edge_labels ( g , pos , edge_labels = result_edges ) plt . show ()","title":"Shortest path using integer programming (Python)"},{"location":"Python/Shortest%20path%20problems/#shortest-path-problems","text":"Author: Achyuthuni Sri Harsha Shortest path is one problem in networks which appears in many forms across many industries. It tells the user how to find the shortest path between two pair of nodes. In this particular example, we will look at finding the shortest path between a pair of nodes in a directed network using an integer programming solver. import networkx as nx import numpy as np import pandas as pd import matplotlib.pyplot as plt Consider the following simple weighted directed network, with four nodes and five edges. edgelist_df = pd . DataFrame ({ 'node1' :[ 's' , 's' , 'u' , 'u' , 'v' ], 'node2' :[ 'u' , 'v' , 'v' , 't' , 't' ], 'weights' :[ 3 , 2 , 0.7 , 1 , 7 ] }) edgelist_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } node1 node2 weights 0 s u 3.0 1 s v 2.0 2 u v 0.7 3 u t 1.0 4 v t 7.0 g = nx . DiGraph () for i , elrow in edgelist_df . iterrows (): g . add_edge ( elrow [ 0 ], elrow [ 1 ], weight = elrow [ 2 ]) g . edges ( data = True ) OutEdgeDataView([('s', 'u', {'weight': 3.0}), ('s', 'v', {'weight': 2.0}), ('u', 'v', {'weight': 0.7}), ('u', 't', {'weight': 1.0}), ('v', 't', {'weight': 7.0})]) # for each node we are trying to fix the coordinates g . add_node ( 's' , pos = ( 0 , 1 )) g . add_node ( 'u' , pos = ( 1 , 2 )) g . add_node ( 'v' , pos = ( 1 , 0 )) g . add_node ( 't' , pos = ( 2 , 1 )) g . nodes ( data = True ) NodeDataView({'s': {'pos': (0, 1)}, 'u': {'pos': (1, 2)}, 'v': {'pos': (1, 0)}, 't': {'pos': (2, 1)}}) # This function gets the coordinates for nodes pos = nx . get_node_attributes ( g , 'pos' ) # This function gets the weights for the edges weight = nx . get_edge_attributes ( g , 'weight' ) nx . draw ( g , pos , with_labels = True ) nx . draw_networkx_edge_labels ( g , pos , edge_labels = weight ) plt . show () Now, we want to find the shortest path between the node 's' and 't'. In this network, edge-weights represent the costs for each edge. The weights could be distance, or time. NetworkX has an inbuilt function shortest_path which returs the shortest path. Using networkX, we get the following shortest path: path_2_0 = nx . shortest_path ( g , source = 's' , target = 't' ) path_2_0 ['s', 'u', 't']","title":"Shortest path problems"},{"location":"Python/Shortest%20path%20problems/#formulating-the-problem-using-integer-programming","text":"We have n nodes V and m edges E (n=4, m=5 for this example). OR tools is an open source software built by Google for solving integer programming problems. Cp-Sat solver is one such model by OR Tools, which we are going to use today. We can use binary decision variables \\(edge_{i,j}\\) representing the edge that goes from node i to node j. If \\(edge_{i,j}=1\\) the shortest path belongs to the path between i and j, 0 otherwise. from ortools.sat.python import cp_model shortest_path_model = cp_model . CpModel () # Creating one integer decision variable for each edge edge_bool_vars = {} for edge in g . edges : edge_bool_vars [ edge [ 0 ], edge [ 1 ]] = shortest_path_model . NewBoolVar ( 'edge_ %s _ %s ' % edge ) print ( 'Creating the boolean variable ' , edge_bool_vars [ edge [ 0 ], edge [ 1 ]], 'representing the if we should travel through ' , ( edge [ 0 ], edge [ 1 ])) Creating the boolean variable edge_s_u representing the if we should travel through ('s', 'u') Creating the boolean variable edge_s_v representing the if we should travel through ('s', 'v') Creating the boolean variable edge_u_v representing the if we should travel through ('u', 'v') Creating the boolean variable edge_u_t representing the if we should travel through ('u', 't') Creating the boolean variable edge_v_t representing the if we should travel through ('v', 't') The shortest path (in isolation) will have the following properties: 1. Starting node has a degree -1 2. Ending node has a degree +1 3. All intermediatory nodes have degree 0 This can be written in the form of flow balance constraints as follows: input_node = 's' output_node = 't' # Adding constraints on the nodes for node in g . nodes : in_edges = g . in_edges ( node ) out_edges = g . out_edges ( node ) print ( 'Adding the constraint on node ' , node ) print ( 'This node has %i in-edges and %i out-edges' % ( len ( in_edges ), len ( out_edges ))) equation_at_this_edge = sum ( edge_bool_vars [ edge [ 0 ], edge [ 1 ]] for edge in in_edges ) - \\ sum ( edge_bool_vars [ edge [ 0 ], edge [ 1 ]] for edge in out_edges ) if ( node == input_node ): shortest_path_model . Add ( equation_at_this_edge == - 1 ) print ( equation_at_this_edge == - 1 ) elif ( node == output_node ): shortest_path_model . Add ( equation_at_this_edge == 1 ) print ( equation_at_this_edge == 1 ) else : shortest_path_model . Add ( equation_at_this_edge == 0 ) print ( equation_at_this_edge , '== 0' ) print ( '' ) Adding the constraint on node s This node has 0 in-edges and 2 out-edges (-((edge_s_u) + edge_s_v)) == -1 Adding the constraint on node u This node has 1 in-edges and 2 out-edges ((edge_s_u) + -((edge_u_v) + edge_u_t)) == 0 Adding the constraint on node v This node has 2 in-edges and 1 out-edges (((edge_s_v) + edge_u_v) + -(edge_v_t)) == 0 Adding the constraint on node t This node has 2 in-edges and 0 out-edges (((edge_u_t) + edge_v_t)) == 1 The objective of the shortest path problem is to find the path with the minimum cost. This can be written as minimising the costs as follows: # factor to make everything including costs integer factor_to_int = 10 # The objective is to maximise flow total_cost = sum ( int ( g . get_edge_data ( * edge )[ 'weight' ] * factor_to_int ) * edge_bool_vars [ edge [ 0 ], edge [ 1 ]] for edge in g . edges ) print ( 'Objective is to optimise cost' ) print ( total_cost ) shortest_path_model . Minimize ( total_cost ) Objective is to optimise cost ((((((30 * edge_s_u)) + (20 * edge_s_v)) + (7 * edge_u_v)) + (10 * edge_u_t)) + (70 * edge_v_t)) Solving the problem, we have an optimal solution with the overall cost as 40 units. # Solving the problem solver = cp_model . CpSolver () solution_printer = cp_model . ObjectiveSolutionPrinter () status = solver . SolveWithSolutionCallback ( shortest_path_model , solution_printer ) Solution 0, time = 0.02 s, objective = 40 cp_model . OPTIMAL == status True The solution is given as result_edges = {} for edge in g . edges : result_edges [ edge [ 0 ], edge [ 1 ]] = solver . Value ( edge_bool_vars [ edge [ 0 ], edge [ 1 ]]) result_edges {('s', 'u'): 1, ('s', 'v'): 0, ('u', 'v'): 0, ('u', 't'): 1, ('v', 't'): 0} Plotting the network in such a way that the green lines represent the shortest path, we get pos = nx . get_node_attributes ( g , 'pos' ) color = [ 'g' if val == 1 else 'r' for val in result_edges . values ()] nx . draw ( g , pos , with_labels = True , edge_color = color ) nx . draw_networkx_edge_labels ( g , pos , edge_labels = result_edges ) plt . show ()","title":"Formulating the problem using integer programming"},{"location":"Python/Time%20series%20deep%20learning/","text":"Time Series forecasting using Deep learning \u00b6 Placeholder: To write in the future. Deadline before end of deep learning module import pandas as pd import matplotlib.pyplot as plt import seaborn as sns import numpy as np data = pd . read_csv ( 'hyderabad-us consulate-air-quality.csv' , parse_dates = [ 'date' ]) data = data . sort_values ( 'date' ) data . columns = [ 'date' , 'pm25' ] data = data . reset_index () data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } index date pm25 0 2295 2014-12-10 172 1 2296 2014-12-11 166 2 2297 2014-12-12 159 3 2298 2014-12-13 164 4 2299 2014-12-14 166 ... ... ... ... 2309 0 2021-11-01 155 2310 1 2021-11-02 115 2311 2 2021-11-03 67 2312 3 2021-11-04 112 2313 4 2021-11-05 115 2314 rows \u00d7 3 columns plt . figure ( figsize = ( 20 , 10 )) plt . plot ( data . date , data . pm25 , color = 'tab:red' ) plt . gca () . set ( title = 'Pollution at Hyderabad' , xlabel = 'Date' , ylabel = 'PM25' ) plt . show () From this plot, we can see that pollution is higher during winter months while its lower during summer months. This effect is observed every year indicating a seasonal pattern in the data. There seems to be no increasing or decreasing trend in the data. This can be better visualised by decomposing the data into three components: 1. Seasonal component: The component that varies with season 2. Trend: Increasing or decreasing pattern 3. Random component: Remaining component that has no pattern from statsmodels.tsa.seasonal import seasonal_decompose result = seasonal_decompose ( data . pm25 , model = 'additive' , period = 365 ) fs , axs = plt . subplots ( 3 , figsize = ( 20 , 10 )) plt . suptitle ( 'Pollution (PM25) at Hyderabad' , fontsize = 20 , y = 0.95 ) axs [ 0 ] . plot ( data . date , result . trend ) axs [ 1 ] . plot ( data . date , result . seasonal ) axs [ 2 ] . plot ( data . date , result . resid ) axs [ 0 ] . set_ylabel ( 'Trend' , fontsize = 15 ) axs [ 1 ] . set_ylabel ( 'Seasonality' , fontsize = 15 ) axs [ 2 ] . set_ylabel ( 'Random component' , fontsize = 15 ) plt . show () Looking at the trend, we can see how the pollution decreased during 2020 (probably due to covid) and is slowly rising as the country is getting back to its feet. data [ 'year' ] = data . date . dt . year data [ 'day' ] = data . date . dt . dayofyear plt . figure ( figsize = ( 16 , 12 ), dpi = 80 ) for i , y in enumerate ( data . year . unique ()): plt . plot ( 'day' , 'pm25' , data = data . loc [ data . year == y , :], label = y ) plt . title ( \"Seasonal Plot of Hyd AQ15\" , fontsize = 20 ) plt . legend ( loc = \"upper left\" ) Simple Neural Net (Perceptron) \u00b6 from keras.models import Sequential from keras.layers import Dense , SimpleRNN , Lambda , LSTM import tensorflow as tf dataset = tf . data . Dataset . range ( 20 ) dataset = dataset . window ( 15 , shift = 1 , drop_remainder = True ) dataset = dataset . flat_map ( lambda window : window . batch ( 15 )) dataset = dataset . map ( lambda window : ( window [: - 1 ], window [ - 1 :])) dataset = dataset . shuffle ( buffer_size = 3 ) dataset = dataset . batch ( 1 ) . prefetch ( 1 ) for x , y in dataset : print ( \"x = \" , x . numpy ()) print ( \"y = \" , y . numpy ()) x = [[ 1 2 3 4 5 6 7 8 9 10 11 12 13 14]] y = [[15]] x = [[ 3 4 5 6 7 8 9 10 11 12 13 14 15 16]] y = [[17]] x = [[ 2 3 4 5 6 7 8 9 10 11 12 13 14 15]] y = [[16]] x = [[ 5 6 7 8 9 10 11 12 13 14 15 16 17 18]] y = [[19]] x = [[ 0 1 2 3 4 5 6 7 8 9 10 11 12 13]] y = [[14]] x = [[ 4 5 6 7 8 9 10 11 12 13 14 15 16 17]] y = [[18]] def windowed_dataset ( series , window_size , batch_size , shuffle_buffer ): dataset = tf . data . Dataset . from_tensor_slices ( series ) dataset = dataset . window ( window_size + 1 , shift = 1 , drop_remainder = True ) dataset = dataset . flat_map ( lambda window : window . batch ( window_size + 1 )) dataset = dataset . shuffle ( shuffle_buffer ) . map ( lambda window : ( window [: - 1 ], window [ - 1 ])) dataset = dataset . batch ( batch_size ) . prefetch ( 1 ) return dataset dataset = windowed_dataset ( data . pm25 , 14 , 1 , 3 ) for x , y in dataset : print ( \"x = \" , x . numpy ()) print ( \"y = \" , y . numpy ()) x = [[159 164 166 152 155 157 138 154 158 162 160 165 165 165]] y = [163] x = [[172 166 159 164 166 152 155 157 138 154 158 162 160 165]] y = [165] and so on split_time = 2314 - 365 * 2 time_train = data . date [: split_time ] x_train = data . pm25 [: split_time ] time_valid = data . date [ split_time :] x_valid = data . pm25 [ split_time :] window_size = 14 batch_size = 1 shuffle_buffer_size = 2314 - 365 * 2 dataset = windowed_dataset ( x_train , window_size , batch_size , shuffle_buffer_size ) l0 = tf . keras . layers . Dense ( 1 , input_shape = [ window_size ]) model = tf . keras . models . Sequential ([ l0 ]) model . compile ( loss = \"mse\" , optimizer = tf . keras . optimizers . SGD ( learning_rate = 1e-6 , momentum = 0.9 ), metrics = [ \"mae\" ]) model . fit ( dataset , epochs = 100 ) print ( \"Layer weights {} \" . format ( l0 . get_weights ())) Epoch 1/100 1570/1570 [==============================] - 2s 764us/step - loss: 2513.6338 - mae: 36.4270 Epoch 2/100 1570/1570 [==============================] - 2s 1ms/step - loss: 2627.8843 - mae: 38.2651 Epoch 3/100 1570/1570 [==============================] - 1s 625us/step - loss: 4424.3721 - mae: 48.4370 Epoch 4/100 1570/1570 [==============================] - 1s 623us/step - loss: 5549.5840 - mae: 53.6082 Epoch 5/100 1570/1570 [==============================] - 1s 637us/step - loss: 2992.0337 - mae: 40.5288 Epoch 6/100 1570/1570 [==============================] - 1s 846us/step - loss: 3471.9915 - mae: 44.3860 Epoch 7/100 1570/1570 [==============================] - 3s 2ms/step - loss: 2531.1494 - mae: 34.7863 Epoch 8/100 1570/1570 [==============================] - 2s 1ms/step - loss: 1852.7971 - mae: 32.0643 Epoch 9/100 1570/1570 [==============================] - 1s 661us/step - loss: 1253.9712 - mae: 26.1902 Epoch 10/100 1570/1570 [==============================] - 1s 618us/step - loss: 2161.4375 - mae: 35.7045 Epoch 11/100 1570/1570 [==============================] - 1s 767us/step - loss: 1726.3955 - mae: 30.9938 Epoch 12/100 1570/1570 [==============================] - 2s 967us/step - loss: 1668.6731 - mae: 31.8759 Epoch 13/100 1570/1570 [==============================] - 1s 662us/step - loss: 2064.4319 - mae: 34.0762 Epoch 14/100 1570/1570 [==============================] - 1s 630us/step - loss: 2944.2009 - mae: 39.0366 Epoch 15/100 1570/1570 [==============================] - 2s 1ms/step - loss: 2634.4287 - mae: 37.2135 Epoch 16/100 1570/1570 [==============================] - 1s 616us/step - loss: 2405.7512 - mae: 35.9506 Epoch 17/100 1570/1570 [==============================] - 1s 617us/step - loss: 3331.7898 - mae: 42.2314 Epoch 18/100 1570/1570 [==============================] - 2s 949us/step - loss: 1733.8339 - mae: 30.9589 Epoch 19/100 1570/1570 [==============================] - 1s 734us/step - loss: 2795.8682 - mae: 38.8242 Epoch 20/100 1570/1570 [==============================] - 1s 872us/step - loss: 3353.5576 - mae: 41.7762 Epoch 21/100 1570/1570 [==============================] - 1s 808us/step - loss: 1974.5398 - mae: 33.4402 Epoch 22/100 1570/1570 [==============================] - 1s 873us/step - loss: 1905.5634 - mae: 32.1671 Epoch 23/100 1570/1570 [==============================] - 2s 837us/step - loss: 2250.3162 - mae: 34.8362 Epoch 24/100 1570/1570 [==============================] - 2s 1ms/step - loss: 2360.7571 - mae: 36.5247 Epoch 25/100 1570/1570 [==============================] - 1s 705us/step - loss: 2153.8865 - mae: 35.0538 Epoch 26/100 1570/1570 [==============================] - 2s 1ms/step - loss: 1548.2344 - mae: 29.6058 Epoch 27/100 1570/1570 [==============================] - 2s 942us/step - loss: 2877.3025 - mae: 39.9668 Epoch 28/100 1570/1570 [==============================] - 2s 1ms/step - loss: 1990.7844 - mae: 32.4618 Epoch 29/100 1570/1570 [==============================] - 2s 831us/step - loss: 1433.8342 - mae: 28.8914 Epoch 30/100 1570/1570 [==============================] - 1s 680us/step - loss: 2065.6316 - mae: 34.1275 Epoch 31/100 1570/1570 [==============================] - 2s 977us/step - loss: 2812.8083 - mae: 38.1380 Epoch 32/100 1570/1570 [==============================] - 2s 814us/step - loss: 1840.2141 - mae: 32.0926 Epoch 33/100 1570/1570 [==============================] - 2s 970us/step - loss: 1858.1345 - mae: 32.9561 Epoch 34/100 1570/1570 [==============================] - 1s 735us/step - loss: 2243.5950 - mae: 33.8061 Epoch 35/100 1570/1570 [==============================] - 1s 624us/step - loss: 1999.2378 - mae: 33.1796 Epoch 36/100 1570/1570 [==============================] - 1s 609us/step - loss: 2803.6067 - mae: 37.3053 Epoch 37/100 1570/1570 [==============================] - 2s 1ms/step - loss: 3408.6633 - mae: 43.0755 Epoch 38/100 1570/1570 [==============================] - 1s 610us/step - loss: 7617.3584 - mae: 53.5411 Epoch 39/100 1570/1570 [==============================] - 2s 1ms/step - loss: 3608.6226 - mae: 41.9082 Epoch 40/100 1570/1570 [==============================] - 1s 622us/step - loss: 1665.9635 - mae: 30.1138 Epoch 41/100 1570/1570 [==============================] - 1s 664us/step - loss: 2472.9622 - mae: 34.7668 Epoch 42/100 1570/1570 [==============================] - 2s 1ms/step - loss: 4391.3267 - mae: 44.2831 Epoch 43/100 1570/1570 [==============================] - 1s 658us/step - loss: 3736.1123 - mae: 43.4484 Epoch 44/100 1570/1570 [==============================] - 2s 1ms/step - loss: 2405.3022 - mae: 37.2903 Epoch 45/100 1570/1570 [==============================] - 1s 637us/step - loss: 2640.0657 - mae: 37.7513 Epoch 46/100 1570/1570 [==============================] - 1s 624us/step - loss: 1698.6646 - mae: 30.3015 Epoch 47/100 1570/1570 [==============================] - 2s 904us/step - loss: 2157.7537 - mae: 34.5056 Epoch 48/100 1570/1570 [==============================] - 2s 850us/step - loss: 2351.3657 - mae: 36.3811 Epoch 49/100 1570/1570 [==============================] - 2s 923us/step - loss: 3389.1948 - mae: 42.5566 Epoch 50/100 1570/1570 [==============================] - 1s 753us/step - loss: 2557.7690 - mae: 36.7457 Epoch 51/100 1570/1570 [==============================] - 1s 866us/step - loss: 2274.4500 - mae: 35.3559 Epoch 52/100 1570/1570 [==============================] - 2s 814us/step - loss: 1476.2098 - mae: 29.2134 Epoch 53/100 1570/1570 [==============================] - 2s 895us/step - loss: 3051.8118 - mae: 41.4144 Epoch 54/100 1570/1570 [==============================] - 2s 837us/step - loss: 3997.4836 - mae: 42.0719 Epoch 55/100 1570/1570 [==============================] - 1s 632us/step - loss: 2640.8892 - mae: 37.8363 Epoch 56/100 1570/1570 [==============================] - 1s 676us/step - loss: 2127.0208 - mae: 33.5877 Epoch 57/100 1570/1570 [==============================] - 2s 1ms/step - loss: 2155.6133 - mae: 35.2079 Epoch 58/100 1570/1570 [==============================] - 1s 628us/step - loss: 2110.5273 - mae: 35.1694 Epoch 59/100 1570/1570 [==============================] - 2s 1ms/step - loss: 3205.5698 - mae: 41.6258 Epoch 60/100 1570/1570 [==============================] - 1s 681us/step - loss: 2516.9368 - mae: 37.6260 Epoch 61/100 1570/1570 [==============================] - 2s 1ms/step - loss: 3302.0437 - mae: 40.9913 Epoch 62/100 1570/1570 [==============================] - 1s 621us/step - loss: 2135.1140 - mae: 34.7838 Epoch 63/100 1570/1570 [==============================] - 1s 631us/step - loss: 1920.6626 - mae: 32.1499 Epoch 64/100 1570/1570 [==============================] - 2s 1ms/step - loss: 2632.5876 - mae: 37.2169 Epoch 65/100 1570/1570 [==============================] - 1s 642us/step - loss: 1719.2740 - mae: 30.8670 Epoch 66/100 1570/1570 [==============================] - 1s 618us/step - loss: 2797.1809 - mae: 38.1031 Epoch 67/100 1570/1570 [==============================] - 1s 821us/step - loss: 2052.6494 - mae: 33.3538 Epoch 68/100 1570/1570 [==============================] - 2s 826us/step - loss: 2140.2795 - mae: 33.2761 Epoch 69/100 1570/1570 [==============================] - 1s 810us/step - loss: 2568.9514 - mae: 35.5570 Epoch 70/100 1570/1570 [==============================] - 2s 916us/step - loss: 1741.9554 - mae: 31.4823 Epoch 71/100 1570/1570 [==============================] - 1s 640us/step - loss: 3985.8354 - mae: 41.0881 Epoch 72/100 1570/1570 [==============================] - 1s 625us/step - loss: 1813.1409 - mae: 31.6561 Epoch 73/100 1570/1570 [==============================] - 2s 1ms/step - loss: 4502.4375 - mae: 45.8338 Epoch 74/100 1570/1570 [==============================] - 1s 621us/step - loss: 2171.1902 - mae: 34.1269 Epoch 75/100 1570/1570 [==============================] - 2s 1ms/step - loss: 1887.9999 - mae: 33.1114 Epoch 76/100 1570/1570 [==============================] - 1s 614us/step - loss: 1521.0691 - mae: 29.7172 Epoch 77/100 1570/1570 [==============================] - 2s 1ms/step - loss: 1950.2441 - mae: 32.6277 Epoch 78/100 1570/1570 [==============================] - 1s 617us/step - loss: 1778.5680 - mae: 30.9782 Epoch 79/100 1570/1570 [==============================] - 2s 1ms/step - loss: 1485.2815 - mae: 29.3998 Epoch 80/100 1570/1570 [==============================] - 1s 602us/step - loss: 1437.3258 - mae: 29.3189 Epoch 81/100 1570/1570 [==============================] - 2s 1ms/step - loss: 1997.8091 - mae: 34.4411 Epoch 82/100 1570/1570 [==============================] - 1s 587us/step - loss: 2212.0806 - mae: 34.5182 Epoch 83/100 1570/1570 [==============================] - 1s 587us/step - loss: 2208.1921 - mae: 33.5488 Epoch 84/100 1570/1570 [==============================] - 1s 590us/step - loss: 2176.0520 - mae: 35.4611 Epoch 85/100 1570/1570 [==============================] - 2s 1ms/step - loss: 1925.0211 - mae: 32.5826 Epoch 86/100 1570/1570 [==============================] - 1s 599us/step - loss: 3626.3096 - mae: 42.7268 Epoch 87/100 1570/1570 [==============================] - 2s 1ms/step - loss: 1902.9331 - mae: 33.2850 Epoch 88/100 1570/1570 [==============================] - 1s 588us/step - loss: 1713.0077 - mae: 31.0925 Epoch 89/100 1570/1570 [==============================] - 1s 584us/step - loss: 1231.0505 - mae: 26.7580 Epoch 90/100 1570/1570 [==============================] - 2s 902us/step - loss: 2903.7974 - mae: 39.2899 Epoch 91/100 1570/1570 [==============================] - 1s 704us/step - loss: 4899.5703 - mae: 49.9197 Epoch 92/100 1570/1570 [==============================] - 1s 720us/step - loss: 4719.0454 - mae: 45.7755 Epoch 93/100 1570/1570 [==============================] - 2s 915us/step - loss: 2016.2034 - mae: 33.3431 Epoch 94/100 1570/1570 [==============================] - 1s 746us/step - loss: 1934.7463 - mae: 32.4981 Epoch 95/100 1570/1570 [==============================] - 2s 930us/step - loss: 2149.3330 - mae: 34.3910 Epoch 96/100 1570/1570 [==============================] - 1s 583us/step - loss: 2434.7302 - mae: 36.1103 Epoch 97/100 1570/1570 [==============================] - 1s 585us/step - loss: 2350.9609 - mae: 35.0144 Epoch 98/100 1570/1570 [==============================] - 1s 612us/step - loss: 2069.1951 - mae: 33.9281 Epoch 99/100 1570/1570 [==============================] - 1s 596us/step - loss: 2446.0693 - mae: 36.7350 Epoch 100/100 1570/1570 [==============================] - 1s 626us/step - loss: 2440.7476 - mae: 33.8709 Layer weights [array([[-0.04500467], [-0.05519092], [ 0.09815453], [-0.00597653], [ 0.30430666], [-0.00321147], [ 0.07927534], [ 0.00668947], [ 0.04348003], [ 0.11238142], [-0.05560566], [-0.29955685], [ 0.01853935], [ 0.76262796]], dtype=float32), array([0.7891302], dtype=float32)] def plot_series ( time , series , format = \"-\" , start = 0 , end = None ): plt . plot ( time [ start : end ], series [ start : end ], format ) plt . xlabel ( \"Time\" ) plt . ylabel ( \"Value\" ) plt . grid ( True ) forecast = [] for time in range ( len ( data . pm25 ) - window_size ): forecast . append ( model . predict ( np . array ( data . pm25 [ time : time + window_size ])[ np . newaxis ])) forecast = forecast [ split_time - window_size :] results = np . array ( forecast )[:, 0 , 0 ] plt . figure ( figsize = ( 10 , 6 )) plot_series ( time_valid , x_valid ) plot_series ( time_valid , results ) tf . keras . metrics . mean_absolute_error ( x_valid , results ) . numpy () 15.45758 This model is currently deployed in Azure Deep neural network \u00b6 dataset = windowed_dataset ( x_train , window_size , batch_size , shuffle_buffer_size ) model = tf . keras . models . Sequential ([ tf . keras . layers . Dense ( 14 , input_shape = [ window_size ], activation = \"relu\" ), tf . keras . layers . Dense ( 14 , activation = \"relu\" ), tf . keras . layers . Dense ( 1 ) ]) model . compile ( loss = \"mse\" , optimizer = tf . keras . optimizers . SGD ( learning_rate = 1e-6 , momentum = 0.9 ), metrics = [ \"mae\" ]) model . fit ( dataset , epochs = 100 ) Epoch 1/100 1570/1570 [==============================] - 1s 730us/step - loss: 845.5267 - mae: 21.9859 Epoch 2/100 1570/1570 [==============================] - 1s 724us/step - loss: 1019.4009 - mae: 24.1638 Epoch 3/100 1570/1570 [==============================] - 1s 717us/step - loss: 722.2228 - mae: 20.5732 Epoch 4/100 1570/1570 [==============================] - 1s 718us/step - loss: 701.0610 - mae: 20.4133 Epoch 5/100 1570/1570 [==============================] - 1s 716us/step - loss: 677.8721 - mae: 19.7713 Epoch 6/100 1570/1570 [==============================] - 1s 689us/step - loss: 897.4562 - mae: 22.3025 Epoch 7/100 1570/1570 [==============================] - 1s 698us/step - loss: 739.0591 - mae: 20.7214 Epoch 8/100 1570/1570 [==============================] - 1s 688us/step - loss: 659.6113 - mae: 19.5167 Epoch 9/100 1570/1570 [==============================] - 1s 749us/step - loss: 774.5299 - mae: 21.2334 Epoch 10/100 1570/1570 [==============================] - 1s 725us/step - loss: 732.1213 - mae: 20.3679 Epoch 11/100 1570/1570 [==============================] - 1s 727us/step - loss: 600.1824 - mae: 18.8780 Epoch 12/100 1570/1570 [==============================] - 1s 661us/step - loss: 717.2169 - mae: 20.2712 Epoch 13/100 1570/1570 [==============================] - 1s 685us/step - loss: 627.1095 - mae: 19.3082 Epoch 14/100 1570/1570 [==============================] - 1s 686us/step - loss: 713.9377 - mae: 19.4658 Epoch 15/100 1570/1570 [==============================] - 1s 696us/step - loss: 671.5330 - mae: 19.8928 Epoch 16/100 1570/1570 [==============================] - 1s 689us/step - loss: 809.9895 - mae: 21.1933 Epoch 17/100 1570/1570 [==============================] - 1s 697us/step - loss: 626.8463 - mae: 18.9511 Epoch 18/100 1570/1570 [==============================] - 1s 690us/step - loss: 664.7856 - mae: 19.5640 Epoch 19/100 1570/1570 [==============================] - 1s 702us/step - loss: 695.1313 - mae: 20.3170 Epoch 20/100 1570/1570 [==============================] - 1s 685us/step - loss: 844.7645 - mae: 21.5548 Epoch 21/100 1570/1570 [==============================] - 1s 749us/step - loss: 750.3210 - mae: 20.9530 Epoch 22/100 1570/1570 [==============================] - 1s 682us/step - loss: 631.6428 - mae: 19.1385 Epoch 23/100 1570/1570 [==============================] - 1s 694us/step - loss: 721.4250 - mae: 20.0112 Epoch 24/100 1570/1570 [==============================] - 1s 687us/step - loss: 879.2552 - mae: 22.0182 Epoch 25/100 1570/1570 [==============================] - 1s 684us/step - loss: 827.2421 - mae: 21.6464 Epoch 26/100 1570/1570 [==============================] - 1s 700us/step - loss: 701.4094 - mae: 20.2091 Epoch 27/100 1570/1570 [==============================] - 1s 691us/step - loss: 695.8590 - mae: 20.0878 Epoch 28/100 1570/1570 [==============================] - 1s 692us/step - loss: 703.5710 - mae: 20.0760 Epoch 29/100 1570/1570 [==============================] - 1s 699us/step - loss: 632.8063 - mae: 19.4138 Epoch 30/100 1570/1570 [==============================] - 1s 701us/step - loss: 653.7804 - mae: 19.7783 Epoch 31/100 1570/1570 [==============================] - 1s 691us/step - loss: 1016.0288 - mae: 23.3376 Epoch 32/100 1570/1570 [==============================] - 1s 701us/step - loss: 695.7961 - mae: 20.5422 Epoch 33/100 1570/1570 [==============================] - 1s 719us/step - loss: 642.6339 - mae: 19.6469 Epoch 34/100 1570/1570 [==============================] - 1s 753us/step - loss: 604.4878 - mae: 18.8055 Epoch 35/100 1570/1570 [==============================] - 1s 696us/step - loss: 669.7369 - mae: 20.0942 Epoch 36/100 1570/1570 [==============================] - 1s 697us/step - loss: 778.4235 - mae: 20.4621 Epoch 37/100 1570/1570 [==============================] - 1s 700us/step - loss: 697.3196 - mae: 19.9764 Epoch 38/100 1570/1570 [==============================] - 1s 697us/step - loss: 600.9893 - mae: 18.5254 Epoch 39/100 1570/1570 [==============================] - 1s 704us/step - loss: 710.3656 - mae: 20.0062 Epoch 40/100 1570/1570 [==============================] - 1s 697us/step - loss: 704.0516 - mae: 20.1501 Epoch 41/100 1570/1570 [==============================] - 1s 695us/step - loss: 733.9904 - mae: 20.4308 Epoch 42/100 1570/1570 [==============================] - 1s 697us/step - loss: 555.9091 - mae: 18.1039 Epoch 43/100 1570/1570 [==============================] - 1s 683us/step - loss: 647.9240 - mae: 18.9008 Epoch 44/100 1570/1570 [==============================] - 1s 708us/step - loss: 696.8072 - mae: 20.2532 Epoch 45/100 1570/1570 [==============================] - 1s 721us/step - loss: 639.6487 - mae: 19.3308 Epoch 46/100 1570/1570 [==============================] - 1s 726us/step - loss: 554.9702 - mae: 17.9829 Epoch 47/100 1570/1570 [==============================] - 1s 725us/step - loss: 605.5833 - mae: 18.8098 Epoch 48/100 1570/1570 [==============================] - 1s 745us/step - loss: 823.3652 - mae: 22.1876 Epoch 49/100 1570/1570 [==============================] - 1s 745us/step - loss: 700.0248 - mae: 20.3396 Epoch 50/100 1570/1570 [==============================] - 1s 763us/step - loss: 707.9413 - mae: 20.2279 Epoch 51/100 1570/1570 [==============================] - 1s 757us/step - loss: 700.5992 - mae: 19.8542 Epoch 52/100 1570/1570 [==============================] - 1s 702us/step - loss: 645.3628 - mae: 19.1472 Epoch 53/100 1570/1570 [==============================] - 1s 701us/step - loss: 636.1317 - mae: 19.1446 Epoch 54/100 1570/1570 [==============================] - 1s 778us/step - loss: 667.7475 - mae: 19.5081 Epoch 55/100 1570/1570 [==============================] - 1s 750us/step - loss: 676.5157 - mae: 19.5577 Epoch 56/100 1570/1570 [==============================] - 1s 709us/step - loss: 929.1287 - mae: 22.7297 Epoch 57/100 1570/1570 [==============================] - 1s 741us/step - loss: 804.6200 - mae: 21.4121 Epoch 58/100 1570/1570 [==============================] - 1s 750us/step - loss: 725.5538 - mae: 20.5640 Epoch 59/100 1570/1570 [==============================] - 1s 726us/step - loss: 732.9226 - mae: 20.6287 Epoch 60/100 1570/1570 [==============================] - 1s 774us/step - loss: 763.7737 - mae: 20.9384 Epoch 61/100 1570/1570 [==============================] - 1s 771us/step - loss: 598.0344 - mae: 18.4678 Epoch 62/100 1570/1570 [==============================] - 1s 731us/step - loss: 783.2723 - mae: 21.0092 Epoch 63/100 1570/1570 [==============================] - 1s 689us/step - loss: 613.8795 - mae: 18.4673 Epoch 64/100 1570/1570 [==============================] - 1s 689us/step - loss: 891.0996 - mae: 21.7655 Epoch 65/100 1570/1570 [==============================] - 1s 683us/step - loss: 680.8638 - mae: 19.8157 Epoch 66/100 1570/1570 [==============================] - 1s 705us/step - loss: 660.6561 - mae: 19.6660 Epoch 67/100 1570/1570 [==============================] - 1s 698us/step - loss: 768.7937 - mae: 21.4989 Epoch 68/100 1570/1570 [==============================] - 1s 688us/step - loss: 667.2055 - mae: 19.1970 Epoch 69/100 1570/1570 [==============================] - 1s 706us/step - loss: 735.8928 - mae: 20.3921 Epoch 70/100 1570/1570 [==============================] - 1s 696us/step - loss: 843.5947 - mae: 21.6246 Epoch 71/100 1570/1570 [==============================] - 1s 676us/step - loss: 612.9269 - mae: 18.9968 Epoch 72/100 1570/1570 [==============================] - 1s 745us/step - loss: 631.1614 - mae: 18.9914 Epoch 73/100 1570/1570 [==============================] - 1s 693us/step - loss: 713.9399 - mae: 20.2510 Epoch 74/100 1570/1570 [==============================] - 1s 635us/step - loss: 628.4302 - mae: 19.2150 Epoch 75/100 1570/1570 [==============================] - 1s 651us/step - loss: 740.0641 - mae: 20.8704 Epoch 76/100 1570/1570 [==============================] - 1s 622us/step - loss: 634.4455 - mae: 19.2745 Epoch 77/100 1570/1570 [==============================] - 1s 609us/step - loss: 716.5535 - mae: 20.6797 Epoch 78/100 1570/1570 [==============================] - 1s 639us/step - loss: 735.1273 - mae: 20.3919 Epoch 79/100 1570/1570 [==============================] - 1s 657us/step - loss: 630.1802 - mae: 19.2159 Epoch 80/100 1570/1570 [==============================] - 1s 616us/step - loss: 597.2530 - mae: 18.6527 Epoch 81/100 1570/1570 [==============================] - 1s 684us/step - loss: 662.5781 - mae: 19.5194 Epoch 82/100 1570/1570 [==============================] - 1s 699us/step - loss: 631.2231 - mae: 19.0265 Epoch 83/100 1570/1570 [==============================] - 1s 638us/step - loss: 719.9138 - mae: 20.2006 Epoch 84/100 1570/1570 [==============================] - 1s 613us/step - loss: 578.8290 - mae: 18.5974 Epoch 85/100 1570/1570 [==============================] - 1s 630us/step - loss: 833.9090 - mae: 21.5101 Epoch 86/100 1570/1570 [==============================] - 1s 689us/step - loss: 724.4709 - mae: 20.2260 Epoch 87/100 1570/1570 [==============================] - 1s 650us/step - loss: 659.7275 - mae: 19.9460 Epoch 88/100 1570/1570 [==============================] - 1s 655us/step - loss: 710.6397 - mae: 20.5307 Epoch 89/100 1570/1570 [==============================] - 1s 641us/step - loss: 703.9041 - mae: 20.2339 Epoch 90/100 1570/1570 [==============================] - 1s 641us/step - loss: 747.5262 - mae: 20.9598 Epoch 91/100 1570/1570 [==============================] - 1s 671us/step - loss: 803.8232 - mae: 21.4400 Epoch 92/100 1570/1570 [==============================] - 1s 637us/step - loss: 643.2718 - mae: 19.1185 Epoch 93/100 1570/1570 [==============================] - 1s 656us/step - loss: 735.5874 - mae: 20.7587 Epoch 94/100 1570/1570 [==============================] - 1s 646us/step - loss: 659.4421 - mae: 19.6404 Epoch 95/100 1570/1570 [==============================] - 1s 699us/step - loss: 632.8841 - mae: 19.2961 Epoch 96/100 1570/1570 [==============================] - 1s 702us/step - loss: 713.7954 - mae: 20.0682 Epoch 97/100 1570/1570 [==============================] - 1s 623us/step - loss: 650.3159 - mae: 19.1298 Epoch 98/100 1570/1570 [==============================] - 1s 642us/step - loss: 786.0706 - mae: 21.4033 Epoch 99/100 1570/1570 [==============================] - 1s 625us/step - loss: 687.3713 - mae: 19.8312 Epoch 100/100 1570/1570 [==============================] - 1s 683us/step - loss: 692.3904 - mae: 20.3091 forecast = [] for time in range ( len ( data . pm25 ) - window_size ): forecast . append ( model . predict ( np . array ( data . pm25 [ time : time + window_size ])[ np . newaxis ])) forecast = forecast [ split_time - window_size :] results = np . array ( forecast )[:, 0 , 0 ] plt . figure ( figsize = ( 10 , 6 )) plot_series ( time_valid , x_valid ) plot_series ( time_valid , results ) tf . keras . metrics . mean_absolute_error ( x_valid , results ) . numpy () 13.240775 model . save ( 'pm25_DL_model' ) INFO:tensorflow:Assets written to: pm25_DL_model\\assets model = tf . keras . models . load_model ( 'pm25_DL_model' ) model . predict ( np . array ( data . pm25 [ 1 : 15 ])[ np . newaxis ]) array([[163.71252]], dtype=float32) This model is currently deployed as \"Deep learning model\" in Git actions RNN \u00b6 tf . keras . backend . clear_session () tf . random . set_seed ( 51 ) np . random . seed ( 51 ) train_set = windowed_dataset ( x_train , window_size , batch_size , shuffle_buffer_size ) model = tf . keras . models . Sequential ([ tf . keras . layers . Lambda ( lambda x : tf . expand_dims ( x , axis =- 1 ), input_shape = [ None ]), tf . keras . layers . SimpleRNN ( 14 , return_sequences = True ), tf . keras . layers . SimpleRNN ( 14 , return_sequences = True ), tf . keras . layers . Dense ( 1 ), tf . keras . layers . Lambda ( lambda x : x * 200.0 ) ]) lr_schedule = tf . keras . callbacks . LearningRateScheduler ( lambda epoch : 1e-8 * 10 ** ( epoch / 20 )) optimizer = tf . keras . optimizers . SGD ( learning_rate = 1e-8 , momentum = 0.9 ) model . compile ( loss = tf . keras . losses . Huber (), optimizer = optimizer , metrics = [ \"mae\" ]) history = model . fit ( train_set , epochs = 25 , callbacks = [ lr_schedule ]) Epoch 1/25 1570/1570 [==============================] - 7s 3ms/step - loss: 255.5130 - mae: 256.0130 Epoch 2/25 1570/1570 [==============================] - 6s 3ms/step - loss: 136.1440 - mae: 136.6439 Epoch 3/25 1570/1570 [==============================] - 5s 3ms/step - loss: 44.4807 - mae: 44.9783 Epoch 4/25 1570/1570 [==============================] - 5s 3ms/step - loss: 36.5102 - mae: 37.0077 Epoch 5/25 1570/1570 [==============================] - 5s 3ms/step - loss: 36.1142 - mae: 36.6120 Epoch 6/25 1570/1570 [==============================] - 5s 3ms/step - loss: 35.6905 - mae: 36.1887 Epoch 7/25 1570/1570 [==============================] - 5s 3ms/step - loss: 35.2254 - mae: 35.7231 Epoch 8/25 1570/1570 [==============================] - 5s 3ms/step - loss: 34.7579 - mae: 35.2558 Epoch 9/25 1570/1570 [==============================] - 5s 3ms/step - loss: 34.1971 - mae: 34.6951 Epoch 10/25 1570/1570 [==============================] - 5s 3ms/step - loss: 33.6498 - mae: 34.1480 Epoch 11/25 1570/1570 [==============================] - 5s 3ms/step - loss: 32.9927 - mae: 33.4904 Epoch 12/25 1570/1570 [==============================] - 5s 3ms/step - loss: 32.3265 - mae: 32.8238 Epoch 13/25 1570/1570 [==============================] - 5s 3ms/step - loss: 31.7288 - mae: 32.2260 Epoch 14/25 1570/1570 [==============================] - 5s 3ms/step - loss: 31.1841 - mae: 31.6818 Epoch 15/25 1570/1570 [==============================] - 5s 3ms/step - loss: 30.7113 - mae: 31.2086 Epoch 16/25 1570/1570 [==============================] - 5s 3ms/step - loss: 30.3737 - mae: 30.8711 Epoch 17/25 1570/1570 [==============================] - 5s 3ms/step - loss: 30.1141 - mae: 30.6113 Epoch 18/25 1570/1570 [==============================] - 5s 3ms/step - loss: 29.7798 - mae: 30.2774 Epoch 19/25 1570/1570 [==============================] - 5s 3ms/step - loss: 29.6268 - mae: 30.1236 Epoch 20/25 1570/1570 [==============================] - 5s 3ms/step - loss: 29.4335 - mae: 29.9309 Epoch 21/25 1570/1570 [==============================] - 5s 3ms/step - loss: 29.3501 - mae: 29.8474 Epoch 22/25 1570/1570 [==============================] - 5s 3ms/step - loss: 30.9055 - mae: 31.4033 Epoch 23/25 1570/1570 [==============================] - 5s 3ms/step - loss: 29.6605 - mae: 30.1580 Epoch 24/25 1570/1570 [==============================] - 5s 3ms/step - loss: 29.5224 - mae: 30.0199 Epoch 25/25 1570/1570 [==============================] - 5s 3ms/step - loss: 28.9797 - mae: 29.4764 plt . semilogx ( history . history [ \"lr\" ], history . history [ \"loss\" ]) plt . axis ([ 1e-8 , 1e-6 , 20 , 300 ]) forecast = [] for time in range ( len ( data . pm25 ) - window_size ): forecast . append ( model . predict ( np . array ( data . pm25 [ time : time + window_size ])[ np . newaxis ])) forecast = forecast [ split_time - window_size :] results = np . array ( forecast )[:, 0 , 0 ] plt . figure ( figsize = ( 10 , 6 )) plot_series ( time_valid , x_valid ) plot_series ( time_valid , results ) tf . keras . metrics . mean_absolute_error ( x_valid , results ) . numpy () array([39.826717, 39.908363, 40.442093, ..., 39.340984, 39.340984], dtype=float32) import matplotlib.image as mpimg import matplotlib.pyplot as plt #----------------------------------------------------------- # Retrieve a list of list results on training and test data # sets for each training epoch #----------------------------------------------------------- mae = history . history [ 'mae' ] loss = history . history [ 'loss' ] epochs = range ( len ( loss )) # Get number of epochs #------------------------------------------------ # Plot MAE and Loss #------------------------------------------------ plt . plot ( epochs , mae , 'r' ) plt . plot ( epochs , loss , 'b' ) plt . title ( 'MAE and Loss' ) plt . xlabel ( \"Epochs\" ) plt . ylabel ( \"Accuracy\" ) plt . legend ([ \"MAE\" , \"Loss\" ]) plt . figure () epochs_zoom = epochs [ 200 :] mae_zoom = mae [ 200 :] loss_zoom = loss [ 200 :] #------------------------------------------------ # Plot Zoomed MAE and Loss #------------------------------------------------ plt . plot ( epochs_zoom , mae_zoom , 'r' ) plt . plot ( epochs_zoom , loss_zoom , 'b' ) plt . title ( 'MAE and Loss' ) plt . xlabel ( \"Epochs\" ) plt . ylabel ( \"Accuracy\" ) plt . legend ([ \"MAE\" , \"Loss\" ]) plt . figure () LSTM \u00b6 def model_forecast ( model , series , window_size ): ds = tf . data . Dataset . from_tensor_slices ( series ) ds = ds . window ( window_size , shift = 1 , drop_remainder = True ) ds = ds . flat_map ( lambda w : w . batch ( window_size )) ds = ds . batch ( 32 ) . prefetch ( 1 ) forecast = model . predict ( ds ) return forecast tf . keras . backend . clear_session () tf . random . set_seed ( 51 ) np . random . seed ( 51 ) x_train_LSTM = tf . expand_dims ( x_train , axis =- 1 ) train_set = windowed_dataset ( x_train_LSTM , window_size , batch_size , shuffle_buffer_size ) train_set model = tf . keras . models . Sequential ([ tf . keras . layers . Conv1D ( filters = 32 , kernel_size = 5 , strides = 1 , padding = \"causal\" , activation = \"relu\" , input_shape = [ None , 1 ]), tf . keras . layers . Bidirectional ( tf . keras . layers . LSTM ( 32 , return_sequences = True )), tf . keras . layers . Bidirectional ( tf . keras . layers . LSTM ( 32 , return_sequences = True )), tf . keras . layers . Dense ( 1 ), tf . keras . layers . Lambda ( lambda x : x * 200 ) ]) lr_schedule = tf . keras . callbacks . LearningRateScheduler ( lambda epoch : 1e-8 * 10 ** ( epoch / 20 )) optimizer = tf . keras . optimizers . SGD ( learning_rate = 1e-8 , momentum = 0.9 ) model . compile ( loss = tf . keras . losses . Huber (), optimizer = optimizer , metrics = [ \"mae\" ]) history = model . fit ( train_set , epochs = 25 , callbacks = [ lr_schedule ]) Epoch 1/25 1570/1570 [==============================] - 27s 12ms/step - loss: 51.0839 - mae: 51.5817 Epoch 2/25 1570/1570 [==============================] - 21s 13ms/step - loss: 30.5634 - mae: 31.0600 0s - loss: 30.5700 - mae: 31.06 Epoch 3/25 1570/1570 [==============================] - 20s 13ms/step - loss: 28.5261 - mae: 29.0220 Epoch 4/25 1570/1570 [==============================] - 23s 14ms/step - loss: 27.1795 - mae: 27.6754 Epoch 5/25 1570/1570 [==============================] - 21s 13ms/step - loss: 25.9100 - mae: 26.4058 Epoch 6/25 1570/1570 [==============================] - 21s 13ms/step - loss: 24.7144 - mae: 25.2104 Epoch 7/25 1570/1570 [==============================] - 22s 14ms/step - loss: 23.5953 - mae: 24.0904 Epoch 8/25 1570/1570 [==============================] - 21s 13ms/step - loss: 22.6447 - mae: 23.1403 Epoch 9/25 1570/1570 [==============================] - 22s 14ms/step - loss: 21.7317 - mae: 22.2269 Epoch 10/25 1570/1570 [==============================] - 22s 14ms/step - loss: 20.8462 - mae: 21.3408 Epoch 11/25 1570/1570 [==============================] - 22s 14ms/step - loss: 20.1405 - mae: 20.6347 Epoch 12/25 1570/1570 [==============================] - 22s 14ms/step - loss: 19.4537 - mae: 19.9481 Epoch 13/25 1570/1570 [==============================] - 22s 14ms/step - loss: 18.8216 - mae: 19.3158 Epoch 14/25 1570/1570 [==============================] - 22s 14ms/step - loss: 18.2066 - mae: 18.7004 Epoch 15/25 1570/1570 [==============================] - 22s 14ms/step - loss: 17.5309 - mae: 18.0234 Epoch 16/25 1570/1570 [==============================] - 22s 14ms/step - loss: 17.3460 - mae: 17.8395 Epoch 17/25 1570/1570 [==============================] - 22s 14ms/step - loss: 16.8951 - mae: 17.3882 Epoch 18/25 1570/1570 [==============================] - 23s 15ms/step - loss: 16.5255 - mae: 17.0192 Epoch 19/25 1570/1570 [==============================] - 22s 14ms/step - loss: 16.4288 - mae: 16.9225 Epoch 20/25 1570/1570 [==============================] - 22s 14ms/step - loss: 16.1572 - mae: 16.6509 Epoch 21/25 1570/1570 [==============================] - 22s 14ms/step - loss: 15.7839 - mae: 16.2775 Epoch 22/25 1570/1570 [==============================] - 22s 14ms/step - loss: 15.7438 - mae: 16.2366 Epoch 23/25 1570/1570 [==============================] - 23s 15ms/step - loss: 15.6320 - mae: 16.1244 Epoch 24/25 1570/1570 [==============================] - 23s 14ms/step - loss: 15.7910 - mae: 16.2841 Epoch 25/25 1570/1570 [==============================] - 22s 14ms/step - loss: 15.5656 - mae: 16.0587 rnn_forecast = model_forecast ( model , np . array ( data . pm25 )[ ... , np . newaxis ], window_size ) rnn_forecast = rnn_forecast [ split_time - window_size : - 1 , - 1 , 0 ] plt . figure ( figsize = ( 10 , 6 )) plot_series ( time_valid , x_valid ) plot_series ( time_valid , rnn_forecast ) tf . keras . metrics . mean_absolute_error ( x_valid , rnn_forecast ) . numpy () 17.472342 import matplotlib.image as mpimg import matplotlib.pyplot as plt #----------------------------------------------------------- # Retrieve a list of list results on training and test data # sets for each training epoch #----------------------------------------------------------- mae = history . history [ 'mae' ] loss = history . history [ 'loss' ] epochs = range ( len ( loss )) # Get number of epochs #------------------------------------------------ # Plot MAE and Loss #------------------------------------------------ plt . plot ( epochs , mae , 'r' ) plt . plot ( epochs , loss , 'b' ) plt . title ( 'MAE and Loss' ) plt . xlabel ( \"Epochs\" ) plt . ylabel ( \"Accuracy\" ) plt . legend ([ \"MAE\" , \"Loss\" ]) plt . figure () epochs_zoom = epochs [ 200 :] mae_zoom = mae [ 200 :] loss_zoom = loss [ 200 :] #------------------------------------------------ # Plot Zoomed MAE and Loss #------------------------------------------------ plt . plot ( epochs_zoom , mae_zoom , 'r' ) plt . plot ( epochs_zoom , loss_zoom , 'b' ) plt . title ( 'MAE and Loss' ) plt . xlabel ( \"Epochs\" ) plt . ylabel ( \"Accuracy\" ) plt . legend ([ \"MAE\" , \"Loss\" ]) plt . figure ()","title":"Time Series forecasting using Deep learning"},{"location":"Python/Time%20series%20deep%20learning/#time-series-forecasting-using-deep-learning","text":"Placeholder: To write in the future. Deadline before end of deep learning module import pandas as pd import matplotlib.pyplot as plt import seaborn as sns import numpy as np data = pd . read_csv ( 'hyderabad-us consulate-air-quality.csv' , parse_dates = [ 'date' ]) data = data . sort_values ( 'date' ) data . columns = [ 'date' , 'pm25' ] data = data . reset_index () data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } index date pm25 0 2295 2014-12-10 172 1 2296 2014-12-11 166 2 2297 2014-12-12 159 3 2298 2014-12-13 164 4 2299 2014-12-14 166 ... ... ... ... 2309 0 2021-11-01 155 2310 1 2021-11-02 115 2311 2 2021-11-03 67 2312 3 2021-11-04 112 2313 4 2021-11-05 115 2314 rows \u00d7 3 columns plt . figure ( figsize = ( 20 , 10 )) plt . plot ( data . date , data . pm25 , color = 'tab:red' ) plt . gca () . set ( title = 'Pollution at Hyderabad' , xlabel = 'Date' , ylabel = 'PM25' ) plt . show () From this plot, we can see that pollution is higher during winter months while its lower during summer months. This effect is observed every year indicating a seasonal pattern in the data. There seems to be no increasing or decreasing trend in the data. This can be better visualised by decomposing the data into three components: 1. Seasonal component: The component that varies with season 2. Trend: Increasing or decreasing pattern 3. Random component: Remaining component that has no pattern from statsmodels.tsa.seasonal import seasonal_decompose result = seasonal_decompose ( data . pm25 , model = 'additive' , period = 365 ) fs , axs = plt . subplots ( 3 , figsize = ( 20 , 10 )) plt . suptitle ( 'Pollution (PM25) at Hyderabad' , fontsize = 20 , y = 0.95 ) axs [ 0 ] . plot ( data . date , result . trend ) axs [ 1 ] . plot ( data . date , result . seasonal ) axs [ 2 ] . plot ( data . date , result . resid ) axs [ 0 ] . set_ylabel ( 'Trend' , fontsize = 15 ) axs [ 1 ] . set_ylabel ( 'Seasonality' , fontsize = 15 ) axs [ 2 ] . set_ylabel ( 'Random component' , fontsize = 15 ) plt . show () Looking at the trend, we can see how the pollution decreased during 2020 (probably due to covid) and is slowly rising as the country is getting back to its feet. data [ 'year' ] = data . date . dt . year data [ 'day' ] = data . date . dt . dayofyear plt . figure ( figsize = ( 16 , 12 ), dpi = 80 ) for i , y in enumerate ( data . year . unique ()): plt . plot ( 'day' , 'pm25' , data = data . loc [ data . year == y , :], label = y ) plt . title ( \"Seasonal Plot of Hyd AQ15\" , fontsize = 20 ) plt . legend ( loc = \"upper left\" )","title":"Time Series forecasting using Deep learning"},{"location":"Python/Time%20series%20deep%20learning/#simple-neural-net-perceptron","text":"from keras.models import Sequential from keras.layers import Dense , SimpleRNN , Lambda , LSTM import tensorflow as tf dataset = tf . data . Dataset . range ( 20 ) dataset = dataset . window ( 15 , shift = 1 , drop_remainder = True ) dataset = dataset . flat_map ( lambda window : window . batch ( 15 )) dataset = dataset . map ( lambda window : ( window [: - 1 ], window [ - 1 :])) dataset = dataset . shuffle ( buffer_size = 3 ) dataset = dataset . batch ( 1 ) . prefetch ( 1 ) for x , y in dataset : print ( \"x = \" , x . numpy ()) print ( \"y = \" , y . numpy ()) x = [[ 1 2 3 4 5 6 7 8 9 10 11 12 13 14]] y = [[15]] x = [[ 3 4 5 6 7 8 9 10 11 12 13 14 15 16]] y = [[17]] x = [[ 2 3 4 5 6 7 8 9 10 11 12 13 14 15]] y = [[16]] x = [[ 5 6 7 8 9 10 11 12 13 14 15 16 17 18]] y = [[19]] x = [[ 0 1 2 3 4 5 6 7 8 9 10 11 12 13]] y = [[14]] x = [[ 4 5 6 7 8 9 10 11 12 13 14 15 16 17]] y = [[18]] def windowed_dataset ( series , window_size , batch_size , shuffle_buffer ): dataset = tf . data . Dataset . from_tensor_slices ( series ) dataset = dataset . window ( window_size + 1 , shift = 1 , drop_remainder = True ) dataset = dataset . flat_map ( lambda window : window . batch ( window_size + 1 )) dataset = dataset . shuffle ( shuffle_buffer ) . map ( lambda window : ( window [: - 1 ], window [ - 1 ])) dataset = dataset . batch ( batch_size ) . prefetch ( 1 ) return dataset dataset = windowed_dataset ( data . pm25 , 14 , 1 , 3 ) for x , y in dataset : print ( \"x = \" , x . numpy ()) print ( \"y = \" , y . numpy ()) x = [[159 164 166 152 155 157 138 154 158 162 160 165 165 165]] y = [163] x = [[172 166 159 164 166 152 155 157 138 154 158 162 160 165]] y = [165] and so on split_time = 2314 - 365 * 2 time_train = data . date [: split_time ] x_train = data . pm25 [: split_time ] time_valid = data . date [ split_time :] x_valid = data . pm25 [ split_time :] window_size = 14 batch_size = 1 shuffle_buffer_size = 2314 - 365 * 2 dataset = windowed_dataset ( x_train , window_size , batch_size , shuffle_buffer_size ) l0 = tf . keras . layers . Dense ( 1 , input_shape = [ window_size ]) model = tf . keras . models . Sequential ([ l0 ]) model . compile ( loss = \"mse\" , optimizer = tf . keras . optimizers . SGD ( learning_rate = 1e-6 , momentum = 0.9 ), metrics = [ \"mae\" ]) model . fit ( dataset , epochs = 100 ) print ( \"Layer weights {} \" . format ( l0 . get_weights ())) Epoch 1/100 1570/1570 [==============================] - 2s 764us/step - loss: 2513.6338 - mae: 36.4270 Epoch 2/100 1570/1570 [==============================] - 2s 1ms/step - loss: 2627.8843 - mae: 38.2651 Epoch 3/100 1570/1570 [==============================] - 1s 625us/step - loss: 4424.3721 - mae: 48.4370 Epoch 4/100 1570/1570 [==============================] - 1s 623us/step - loss: 5549.5840 - mae: 53.6082 Epoch 5/100 1570/1570 [==============================] - 1s 637us/step - loss: 2992.0337 - mae: 40.5288 Epoch 6/100 1570/1570 [==============================] - 1s 846us/step - loss: 3471.9915 - mae: 44.3860 Epoch 7/100 1570/1570 [==============================] - 3s 2ms/step - loss: 2531.1494 - mae: 34.7863 Epoch 8/100 1570/1570 [==============================] - 2s 1ms/step - loss: 1852.7971 - mae: 32.0643 Epoch 9/100 1570/1570 [==============================] - 1s 661us/step - loss: 1253.9712 - mae: 26.1902 Epoch 10/100 1570/1570 [==============================] - 1s 618us/step - loss: 2161.4375 - mae: 35.7045 Epoch 11/100 1570/1570 [==============================] - 1s 767us/step - loss: 1726.3955 - mae: 30.9938 Epoch 12/100 1570/1570 [==============================] - 2s 967us/step - loss: 1668.6731 - mae: 31.8759 Epoch 13/100 1570/1570 [==============================] - 1s 662us/step - loss: 2064.4319 - mae: 34.0762 Epoch 14/100 1570/1570 [==============================] - 1s 630us/step - loss: 2944.2009 - mae: 39.0366 Epoch 15/100 1570/1570 [==============================] - 2s 1ms/step - loss: 2634.4287 - mae: 37.2135 Epoch 16/100 1570/1570 [==============================] - 1s 616us/step - loss: 2405.7512 - mae: 35.9506 Epoch 17/100 1570/1570 [==============================] - 1s 617us/step - loss: 3331.7898 - mae: 42.2314 Epoch 18/100 1570/1570 [==============================] - 2s 949us/step - loss: 1733.8339 - mae: 30.9589 Epoch 19/100 1570/1570 [==============================] - 1s 734us/step - loss: 2795.8682 - mae: 38.8242 Epoch 20/100 1570/1570 [==============================] - 1s 872us/step - loss: 3353.5576 - mae: 41.7762 Epoch 21/100 1570/1570 [==============================] - 1s 808us/step - loss: 1974.5398 - mae: 33.4402 Epoch 22/100 1570/1570 [==============================] - 1s 873us/step - loss: 1905.5634 - mae: 32.1671 Epoch 23/100 1570/1570 [==============================] - 2s 837us/step - loss: 2250.3162 - mae: 34.8362 Epoch 24/100 1570/1570 [==============================] - 2s 1ms/step - loss: 2360.7571 - mae: 36.5247 Epoch 25/100 1570/1570 [==============================] - 1s 705us/step - loss: 2153.8865 - mae: 35.0538 Epoch 26/100 1570/1570 [==============================] - 2s 1ms/step - loss: 1548.2344 - mae: 29.6058 Epoch 27/100 1570/1570 [==============================] - 2s 942us/step - loss: 2877.3025 - mae: 39.9668 Epoch 28/100 1570/1570 [==============================] - 2s 1ms/step - loss: 1990.7844 - mae: 32.4618 Epoch 29/100 1570/1570 [==============================] - 2s 831us/step - loss: 1433.8342 - mae: 28.8914 Epoch 30/100 1570/1570 [==============================] - 1s 680us/step - loss: 2065.6316 - mae: 34.1275 Epoch 31/100 1570/1570 [==============================] - 2s 977us/step - loss: 2812.8083 - mae: 38.1380 Epoch 32/100 1570/1570 [==============================] - 2s 814us/step - loss: 1840.2141 - mae: 32.0926 Epoch 33/100 1570/1570 [==============================] - 2s 970us/step - loss: 1858.1345 - mae: 32.9561 Epoch 34/100 1570/1570 [==============================] - 1s 735us/step - loss: 2243.5950 - mae: 33.8061 Epoch 35/100 1570/1570 [==============================] - 1s 624us/step - loss: 1999.2378 - mae: 33.1796 Epoch 36/100 1570/1570 [==============================] - 1s 609us/step - loss: 2803.6067 - mae: 37.3053 Epoch 37/100 1570/1570 [==============================] - 2s 1ms/step - loss: 3408.6633 - mae: 43.0755 Epoch 38/100 1570/1570 [==============================] - 1s 610us/step - loss: 7617.3584 - mae: 53.5411 Epoch 39/100 1570/1570 [==============================] - 2s 1ms/step - loss: 3608.6226 - mae: 41.9082 Epoch 40/100 1570/1570 [==============================] - 1s 622us/step - loss: 1665.9635 - mae: 30.1138 Epoch 41/100 1570/1570 [==============================] - 1s 664us/step - loss: 2472.9622 - mae: 34.7668 Epoch 42/100 1570/1570 [==============================] - 2s 1ms/step - loss: 4391.3267 - mae: 44.2831 Epoch 43/100 1570/1570 [==============================] - 1s 658us/step - loss: 3736.1123 - mae: 43.4484 Epoch 44/100 1570/1570 [==============================] - 2s 1ms/step - loss: 2405.3022 - mae: 37.2903 Epoch 45/100 1570/1570 [==============================] - 1s 637us/step - loss: 2640.0657 - mae: 37.7513 Epoch 46/100 1570/1570 [==============================] - 1s 624us/step - loss: 1698.6646 - mae: 30.3015 Epoch 47/100 1570/1570 [==============================] - 2s 904us/step - loss: 2157.7537 - mae: 34.5056 Epoch 48/100 1570/1570 [==============================] - 2s 850us/step - loss: 2351.3657 - mae: 36.3811 Epoch 49/100 1570/1570 [==============================] - 2s 923us/step - loss: 3389.1948 - mae: 42.5566 Epoch 50/100 1570/1570 [==============================] - 1s 753us/step - loss: 2557.7690 - mae: 36.7457 Epoch 51/100 1570/1570 [==============================] - 1s 866us/step - loss: 2274.4500 - mae: 35.3559 Epoch 52/100 1570/1570 [==============================] - 2s 814us/step - loss: 1476.2098 - mae: 29.2134 Epoch 53/100 1570/1570 [==============================] - 2s 895us/step - loss: 3051.8118 - mae: 41.4144 Epoch 54/100 1570/1570 [==============================] - 2s 837us/step - loss: 3997.4836 - mae: 42.0719 Epoch 55/100 1570/1570 [==============================] - 1s 632us/step - loss: 2640.8892 - mae: 37.8363 Epoch 56/100 1570/1570 [==============================] - 1s 676us/step - loss: 2127.0208 - mae: 33.5877 Epoch 57/100 1570/1570 [==============================] - 2s 1ms/step - loss: 2155.6133 - mae: 35.2079 Epoch 58/100 1570/1570 [==============================] - 1s 628us/step - loss: 2110.5273 - mae: 35.1694 Epoch 59/100 1570/1570 [==============================] - 2s 1ms/step - loss: 3205.5698 - mae: 41.6258 Epoch 60/100 1570/1570 [==============================] - 1s 681us/step - loss: 2516.9368 - mae: 37.6260 Epoch 61/100 1570/1570 [==============================] - 2s 1ms/step - loss: 3302.0437 - mae: 40.9913 Epoch 62/100 1570/1570 [==============================] - 1s 621us/step - loss: 2135.1140 - mae: 34.7838 Epoch 63/100 1570/1570 [==============================] - 1s 631us/step - loss: 1920.6626 - mae: 32.1499 Epoch 64/100 1570/1570 [==============================] - 2s 1ms/step - loss: 2632.5876 - mae: 37.2169 Epoch 65/100 1570/1570 [==============================] - 1s 642us/step - loss: 1719.2740 - mae: 30.8670 Epoch 66/100 1570/1570 [==============================] - 1s 618us/step - loss: 2797.1809 - mae: 38.1031 Epoch 67/100 1570/1570 [==============================] - 1s 821us/step - loss: 2052.6494 - mae: 33.3538 Epoch 68/100 1570/1570 [==============================] - 2s 826us/step - loss: 2140.2795 - mae: 33.2761 Epoch 69/100 1570/1570 [==============================] - 1s 810us/step - loss: 2568.9514 - mae: 35.5570 Epoch 70/100 1570/1570 [==============================] - 2s 916us/step - loss: 1741.9554 - mae: 31.4823 Epoch 71/100 1570/1570 [==============================] - 1s 640us/step - loss: 3985.8354 - mae: 41.0881 Epoch 72/100 1570/1570 [==============================] - 1s 625us/step - loss: 1813.1409 - mae: 31.6561 Epoch 73/100 1570/1570 [==============================] - 2s 1ms/step - loss: 4502.4375 - mae: 45.8338 Epoch 74/100 1570/1570 [==============================] - 1s 621us/step - loss: 2171.1902 - mae: 34.1269 Epoch 75/100 1570/1570 [==============================] - 2s 1ms/step - loss: 1887.9999 - mae: 33.1114 Epoch 76/100 1570/1570 [==============================] - 1s 614us/step - loss: 1521.0691 - mae: 29.7172 Epoch 77/100 1570/1570 [==============================] - 2s 1ms/step - loss: 1950.2441 - mae: 32.6277 Epoch 78/100 1570/1570 [==============================] - 1s 617us/step - loss: 1778.5680 - mae: 30.9782 Epoch 79/100 1570/1570 [==============================] - 2s 1ms/step - loss: 1485.2815 - mae: 29.3998 Epoch 80/100 1570/1570 [==============================] - 1s 602us/step - loss: 1437.3258 - mae: 29.3189 Epoch 81/100 1570/1570 [==============================] - 2s 1ms/step - loss: 1997.8091 - mae: 34.4411 Epoch 82/100 1570/1570 [==============================] - 1s 587us/step - loss: 2212.0806 - mae: 34.5182 Epoch 83/100 1570/1570 [==============================] - 1s 587us/step - loss: 2208.1921 - mae: 33.5488 Epoch 84/100 1570/1570 [==============================] - 1s 590us/step - loss: 2176.0520 - mae: 35.4611 Epoch 85/100 1570/1570 [==============================] - 2s 1ms/step - loss: 1925.0211 - mae: 32.5826 Epoch 86/100 1570/1570 [==============================] - 1s 599us/step - loss: 3626.3096 - mae: 42.7268 Epoch 87/100 1570/1570 [==============================] - 2s 1ms/step - loss: 1902.9331 - mae: 33.2850 Epoch 88/100 1570/1570 [==============================] - 1s 588us/step - loss: 1713.0077 - mae: 31.0925 Epoch 89/100 1570/1570 [==============================] - 1s 584us/step - loss: 1231.0505 - mae: 26.7580 Epoch 90/100 1570/1570 [==============================] - 2s 902us/step - loss: 2903.7974 - mae: 39.2899 Epoch 91/100 1570/1570 [==============================] - 1s 704us/step - loss: 4899.5703 - mae: 49.9197 Epoch 92/100 1570/1570 [==============================] - 1s 720us/step - loss: 4719.0454 - mae: 45.7755 Epoch 93/100 1570/1570 [==============================] - 2s 915us/step - loss: 2016.2034 - mae: 33.3431 Epoch 94/100 1570/1570 [==============================] - 1s 746us/step - loss: 1934.7463 - mae: 32.4981 Epoch 95/100 1570/1570 [==============================] - 2s 930us/step - loss: 2149.3330 - mae: 34.3910 Epoch 96/100 1570/1570 [==============================] - 1s 583us/step - loss: 2434.7302 - mae: 36.1103 Epoch 97/100 1570/1570 [==============================] - 1s 585us/step - loss: 2350.9609 - mae: 35.0144 Epoch 98/100 1570/1570 [==============================] - 1s 612us/step - loss: 2069.1951 - mae: 33.9281 Epoch 99/100 1570/1570 [==============================] - 1s 596us/step - loss: 2446.0693 - mae: 36.7350 Epoch 100/100 1570/1570 [==============================] - 1s 626us/step - loss: 2440.7476 - mae: 33.8709 Layer weights [array([[-0.04500467], [-0.05519092], [ 0.09815453], [-0.00597653], [ 0.30430666], [-0.00321147], [ 0.07927534], [ 0.00668947], [ 0.04348003], [ 0.11238142], [-0.05560566], [-0.29955685], [ 0.01853935], [ 0.76262796]], dtype=float32), array([0.7891302], dtype=float32)] def plot_series ( time , series , format = \"-\" , start = 0 , end = None ): plt . plot ( time [ start : end ], series [ start : end ], format ) plt . xlabel ( \"Time\" ) plt . ylabel ( \"Value\" ) plt . grid ( True ) forecast = [] for time in range ( len ( data . pm25 ) - window_size ): forecast . append ( model . predict ( np . array ( data . pm25 [ time : time + window_size ])[ np . newaxis ])) forecast = forecast [ split_time - window_size :] results = np . array ( forecast )[:, 0 , 0 ] plt . figure ( figsize = ( 10 , 6 )) plot_series ( time_valid , x_valid ) plot_series ( time_valid , results ) tf . keras . metrics . mean_absolute_error ( x_valid , results ) . numpy () 15.45758 This model is currently deployed in Azure","title":"Simple Neural Net (Perceptron)"},{"location":"Python/Time%20series%20deep%20learning/#deep-neural-network","text":"dataset = windowed_dataset ( x_train , window_size , batch_size , shuffle_buffer_size ) model = tf . keras . models . Sequential ([ tf . keras . layers . Dense ( 14 , input_shape = [ window_size ], activation = \"relu\" ), tf . keras . layers . Dense ( 14 , activation = \"relu\" ), tf . keras . layers . Dense ( 1 ) ]) model . compile ( loss = \"mse\" , optimizer = tf . keras . optimizers . SGD ( learning_rate = 1e-6 , momentum = 0.9 ), metrics = [ \"mae\" ]) model . fit ( dataset , epochs = 100 ) Epoch 1/100 1570/1570 [==============================] - 1s 730us/step - loss: 845.5267 - mae: 21.9859 Epoch 2/100 1570/1570 [==============================] - 1s 724us/step - loss: 1019.4009 - mae: 24.1638 Epoch 3/100 1570/1570 [==============================] - 1s 717us/step - loss: 722.2228 - mae: 20.5732 Epoch 4/100 1570/1570 [==============================] - 1s 718us/step - loss: 701.0610 - mae: 20.4133 Epoch 5/100 1570/1570 [==============================] - 1s 716us/step - loss: 677.8721 - mae: 19.7713 Epoch 6/100 1570/1570 [==============================] - 1s 689us/step - loss: 897.4562 - mae: 22.3025 Epoch 7/100 1570/1570 [==============================] - 1s 698us/step - loss: 739.0591 - mae: 20.7214 Epoch 8/100 1570/1570 [==============================] - 1s 688us/step - loss: 659.6113 - mae: 19.5167 Epoch 9/100 1570/1570 [==============================] - 1s 749us/step - loss: 774.5299 - mae: 21.2334 Epoch 10/100 1570/1570 [==============================] - 1s 725us/step - loss: 732.1213 - mae: 20.3679 Epoch 11/100 1570/1570 [==============================] - 1s 727us/step - loss: 600.1824 - mae: 18.8780 Epoch 12/100 1570/1570 [==============================] - 1s 661us/step - loss: 717.2169 - mae: 20.2712 Epoch 13/100 1570/1570 [==============================] - 1s 685us/step - loss: 627.1095 - mae: 19.3082 Epoch 14/100 1570/1570 [==============================] - 1s 686us/step - loss: 713.9377 - mae: 19.4658 Epoch 15/100 1570/1570 [==============================] - 1s 696us/step - loss: 671.5330 - mae: 19.8928 Epoch 16/100 1570/1570 [==============================] - 1s 689us/step - loss: 809.9895 - mae: 21.1933 Epoch 17/100 1570/1570 [==============================] - 1s 697us/step - loss: 626.8463 - mae: 18.9511 Epoch 18/100 1570/1570 [==============================] - 1s 690us/step - loss: 664.7856 - mae: 19.5640 Epoch 19/100 1570/1570 [==============================] - 1s 702us/step - loss: 695.1313 - mae: 20.3170 Epoch 20/100 1570/1570 [==============================] - 1s 685us/step - loss: 844.7645 - mae: 21.5548 Epoch 21/100 1570/1570 [==============================] - 1s 749us/step - loss: 750.3210 - mae: 20.9530 Epoch 22/100 1570/1570 [==============================] - 1s 682us/step - loss: 631.6428 - mae: 19.1385 Epoch 23/100 1570/1570 [==============================] - 1s 694us/step - loss: 721.4250 - mae: 20.0112 Epoch 24/100 1570/1570 [==============================] - 1s 687us/step - loss: 879.2552 - mae: 22.0182 Epoch 25/100 1570/1570 [==============================] - 1s 684us/step - loss: 827.2421 - mae: 21.6464 Epoch 26/100 1570/1570 [==============================] - 1s 700us/step - loss: 701.4094 - mae: 20.2091 Epoch 27/100 1570/1570 [==============================] - 1s 691us/step - loss: 695.8590 - mae: 20.0878 Epoch 28/100 1570/1570 [==============================] - 1s 692us/step - loss: 703.5710 - mae: 20.0760 Epoch 29/100 1570/1570 [==============================] - 1s 699us/step - loss: 632.8063 - mae: 19.4138 Epoch 30/100 1570/1570 [==============================] - 1s 701us/step - loss: 653.7804 - mae: 19.7783 Epoch 31/100 1570/1570 [==============================] - 1s 691us/step - loss: 1016.0288 - mae: 23.3376 Epoch 32/100 1570/1570 [==============================] - 1s 701us/step - loss: 695.7961 - mae: 20.5422 Epoch 33/100 1570/1570 [==============================] - 1s 719us/step - loss: 642.6339 - mae: 19.6469 Epoch 34/100 1570/1570 [==============================] - 1s 753us/step - loss: 604.4878 - mae: 18.8055 Epoch 35/100 1570/1570 [==============================] - 1s 696us/step - loss: 669.7369 - mae: 20.0942 Epoch 36/100 1570/1570 [==============================] - 1s 697us/step - loss: 778.4235 - mae: 20.4621 Epoch 37/100 1570/1570 [==============================] - 1s 700us/step - loss: 697.3196 - mae: 19.9764 Epoch 38/100 1570/1570 [==============================] - 1s 697us/step - loss: 600.9893 - mae: 18.5254 Epoch 39/100 1570/1570 [==============================] - 1s 704us/step - loss: 710.3656 - mae: 20.0062 Epoch 40/100 1570/1570 [==============================] - 1s 697us/step - loss: 704.0516 - mae: 20.1501 Epoch 41/100 1570/1570 [==============================] - 1s 695us/step - loss: 733.9904 - mae: 20.4308 Epoch 42/100 1570/1570 [==============================] - 1s 697us/step - loss: 555.9091 - mae: 18.1039 Epoch 43/100 1570/1570 [==============================] - 1s 683us/step - loss: 647.9240 - mae: 18.9008 Epoch 44/100 1570/1570 [==============================] - 1s 708us/step - loss: 696.8072 - mae: 20.2532 Epoch 45/100 1570/1570 [==============================] - 1s 721us/step - loss: 639.6487 - mae: 19.3308 Epoch 46/100 1570/1570 [==============================] - 1s 726us/step - loss: 554.9702 - mae: 17.9829 Epoch 47/100 1570/1570 [==============================] - 1s 725us/step - loss: 605.5833 - mae: 18.8098 Epoch 48/100 1570/1570 [==============================] - 1s 745us/step - loss: 823.3652 - mae: 22.1876 Epoch 49/100 1570/1570 [==============================] - 1s 745us/step - loss: 700.0248 - mae: 20.3396 Epoch 50/100 1570/1570 [==============================] - 1s 763us/step - loss: 707.9413 - mae: 20.2279 Epoch 51/100 1570/1570 [==============================] - 1s 757us/step - loss: 700.5992 - mae: 19.8542 Epoch 52/100 1570/1570 [==============================] - 1s 702us/step - loss: 645.3628 - mae: 19.1472 Epoch 53/100 1570/1570 [==============================] - 1s 701us/step - loss: 636.1317 - mae: 19.1446 Epoch 54/100 1570/1570 [==============================] - 1s 778us/step - loss: 667.7475 - mae: 19.5081 Epoch 55/100 1570/1570 [==============================] - 1s 750us/step - loss: 676.5157 - mae: 19.5577 Epoch 56/100 1570/1570 [==============================] - 1s 709us/step - loss: 929.1287 - mae: 22.7297 Epoch 57/100 1570/1570 [==============================] - 1s 741us/step - loss: 804.6200 - mae: 21.4121 Epoch 58/100 1570/1570 [==============================] - 1s 750us/step - loss: 725.5538 - mae: 20.5640 Epoch 59/100 1570/1570 [==============================] - 1s 726us/step - loss: 732.9226 - mae: 20.6287 Epoch 60/100 1570/1570 [==============================] - 1s 774us/step - loss: 763.7737 - mae: 20.9384 Epoch 61/100 1570/1570 [==============================] - 1s 771us/step - loss: 598.0344 - mae: 18.4678 Epoch 62/100 1570/1570 [==============================] - 1s 731us/step - loss: 783.2723 - mae: 21.0092 Epoch 63/100 1570/1570 [==============================] - 1s 689us/step - loss: 613.8795 - mae: 18.4673 Epoch 64/100 1570/1570 [==============================] - 1s 689us/step - loss: 891.0996 - mae: 21.7655 Epoch 65/100 1570/1570 [==============================] - 1s 683us/step - loss: 680.8638 - mae: 19.8157 Epoch 66/100 1570/1570 [==============================] - 1s 705us/step - loss: 660.6561 - mae: 19.6660 Epoch 67/100 1570/1570 [==============================] - 1s 698us/step - loss: 768.7937 - mae: 21.4989 Epoch 68/100 1570/1570 [==============================] - 1s 688us/step - loss: 667.2055 - mae: 19.1970 Epoch 69/100 1570/1570 [==============================] - 1s 706us/step - loss: 735.8928 - mae: 20.3921 Epoch 70/100 1570/1570 [==============================] - 1s 696us/step - loss: 843.5947 - mae: 21.6246 Epoch 71/100 1570/1570 [==============================] - 1s 676us/step - loss: 612.9269 - mae: 18.9968 Epoch 72/100 1570/1570 [==============================] - 1s 745us/step - loss: 631.1614 - mae: 18.9914 Epoch 73/100 1570/1570 [==============================] - 1s 693us/step - loss: 713.9399 - mae: 20.2510 Epoch 74/100 1570/1570 [==============================] - 1s 635us/step - loss: 628.4302 - mae: 19.2150 Epoch 75/100 1570/1570 [==============================] - 1s 651us/step - loss: 740.0641 - mae: 20.8704 Epoch 76/100 1570/1570 [==============================] - 1s 622us/step - loss: 634.4455 - mae: 19.2745 Epoch 77/100 1570/1570 [==============================] - 1s 609us/step - loss: 716.5535 - mae: 20.6797 Epoch 78/100 1570/1570 [==============================] - 1s 639us/step - loss: 735.1273 - mae: 20.3919 Epoch 79/100 1570/1570 [==============================] - 1s 657us/step - loss: 630.1802 - mae: 19.2159 Epoch 80/100 1570/1570 [==============================] - 1s 616us/step - loss: 597.2530 - mae: 18.6527 Epoch 81/100 1570/1570 [==============================] - 1s 684us/step - loss: 662.5781 - mae: 19.5194 Epoch 82/100 1570/1570 [==============================] - 1s 699us/step - loss: 631.2231 - mae: 19.0265 Epoch 83/100 1570/1570 [==============================] - 1s 638us/step - loss: 719.9138 - mae: 20.2006 Epoch 84/100 1570/1570 [==============================] - 1s 613us/step - loss: 578.8290 - mae: 18.5974 Epoch 85/100 1570/1570 [==============================] - 1s 630us/step - loss: 833.9090 - mae: 21.5101 Epoch 86/100 1570/1570 [==============================] - 1s 689us/step - loss: 724.4709 - mae: 20.2260 Epoch 87/100 1570/1570 [==============================] - 1s 650us/step - loss: 659.7275 - mae: 19.9460 Epoch 88/100 1570/1570 [==============================] - 1s 655us/step - loss: 710.6397 - mae: 20.5307 Epoch 89/100 1570/1570 [==============================] - 1s 641us/step - loss: 703.9041 - mae: 20.2339 Epoch 90/100 1570/1570 [==============================] - 1s 641us/step - loss: 747.5262 - mae: 20.9598 Epoch 91/100 1570/1570 [==============================] - 1s 671us/step - loss: 803.8232 - mae: 21.4400 Epoch 92/100 1570/1570 [==============================] - 1s 637us/step - loss: 643.2718 - mae: 19.1185 Epoch 93/100 1570/1570 [==============================] - 1s 656us/step - loss: 735.5874 - mae: 20.7587 Epoch 94/100 1570/1570 [==============================] - 1s 646us/step - loss: 659.4421 - mae: 19.6404 Epoch 95/100 1570/1570 [==============================] - 1s 699us/step - loss: 632.8841 - mae: 19.2961 Epoch 96/100 1570/1570 [==============================] - 1s 702us/step - loss: 713.7954 - mae: 20.0682 Epoch 97/100 1570/1570 [==============================] - 1s 623us/step - loss: 650.3159 - mae: 19.1298 Epoch 98/100 1570/1570 [==============================] - 1s 642us/step - loss: 786.0706 - mae: 21.4033 Epoch 99/100 1570/1570 [==============================] - 1s 625us/step - loss: 687.3713 - mae: 19.8312 Epoch 100/100 1570/1570 [==============================] - 1s 683us/step - loss: 692.3904 - mae: 20.3091 forecast = [] for time in range ( len ( data . pm25 ) - window_size ): forecast . append ( model . predict ( np . array ( data . pm25 [ time : time + window_size ])[ np . newaxis ])) forecast = forecast [ split_time - window_size :] results = np . array ( forecast )[:, 0 , 0 ] plt . figure ( figsize = ( 10 , 6 )) plot_series ( time_valid , x_valid ) plot_series ( time_valid , results ) tf . keras . metrics . mean_absolute_error ( x_valid , results ) . numpy () 13.240775 model . save ( 'pm25_DL_model' ) INFO:tensorflow:Assets written to: pm25_DL_model\\assets model = tf . keras . models . load_model ( 'pm25_DL_model' ) model . predict ( np . array ( data . pm25 [ 1 : 15 ])[ np . newaxis ]) array([[163.71252]], dtype=float32) This model is currently deployed as \"Deep learning model\" in Git actions","title":"Deep neural network"},{"location":"Python/Time%20series%20deep%20learning/#rnn","text":"tf . keras . backend . clear_session () tf . random . set_seed ( 51 ) np . random . seed ( 51 ) train_set = windowed_dataset ( x_train , window_size , batch_size , shuffle_buffer_size ) model = tf . keras . models . Sequential ([ tf . keras . layers . Lambda ( lambda x : tf . expand_dims ( x , axis =- 1 ), input_shape = [ None ]), tf . keras . layers . SimpleRNN ( 14 , return_sequences = True ), tf . keras . layers . SimpleRNN ( 14 , return_sequences = True ), tf . keras . layers . Dense ( 1 ), tf . keras . layers . Lambda ( lambda x : x * 200.0 ) ]) lr_schedule = tf . keras . callbacks . LearningRateScheduler ( lambda epoch : 1e-8 * 10 ** ( epoch / 20 )) optimizer = tf . keras . optimizers . SGD ( learning_rate = 1e-8 , momentum = 0.9 ) model . compile ( loss = tf . keras . losses . Huber (), optimizer = optimizer , metrics = [ \"mae\" ]) history = model . fit ( train_set , epochs = 25 , callbacks = [ lr_schedule ]) Epoch 1/25 1570/1570 [==============================] - 7s 3ms/step - loss: 255.5130 - mae: 256.0130 Epoch 2/25 1570/1570 [==============================] - 6s 3ms/step - loss: 136.1440 - mae: 136.6439 Epoch 3/25 1570/1570 [==============================] - 5s 3ms/step - loss: 44.4807 - mae: 44.9783 Epoch 4/25 1570/1570 [==============================] - 5s 3ms/step - loss: 36.5102 - mae: 37.0077 Epoch 5/25 1570/1570 [==============================] - 5s 3ms/step - loss: 36.1142 - mae: 36.6120 Epoch 6/25 1570/1570 [==============================] - 5s 3ms/step - loss: 35.6905 - mae: 36.1887 Epoch 7/25 1570/1570 [==============================] - 5s 3ms/step - loss: 35.2254 - mae: 35.7231 Epoch 8/25 1570/1570 [==============================] - 5s 3ms/step - loss: 34.7579 - mae: 35.2558 Epoch 9/25 1570/1570 [==============================] - 5s 3ms/step - loss: 34.1971 - mae: 34.6951 Epoch 10/25 1570/1570 [==============================] - 5s 3ms/step - loss: 33.6498 - mae: 34.1480 Epoch 11/25 1570/1570 [==============================] - 5s 3ms/step - loss: 32.9927 - mae: 33.4904 Epoch 12/25 1570/1570 [==============================] - 5s 3ms/step - loss: 32.3265 - mae: 32.8238 Epoch 13/25 1570/1570 [==============================] - 5s 3ms/step - loss: 31.7288 - mae: 32.2260 Epoch 14/25 1570/1570 [==============================] - 5s 3ms/step - loss: 31.1841 - mae: 31.6818 Epoch 15/25 1570/1570 [==============================] - 5s 3ms/step - loss: 30.7113 - mae: 31.2086 Epoch 16/25 1570/1570 [==============================] - 5s 3ms/step - loss: 30.3737 - mae: 30.8711 Epoch 17/25 1570/1570 [==============================] - 5s 3ms/step - loss: 30.1141 - mae: 30.6113 Epoch 18/25 1570/1570 [==============================] - 5s 3ms/step - loss: 29.7798 - mae: 30.2774 Epoch 19/25 1570/1570 [==============================] - 5s 3ms/step - loss: 29.6268 - mae: 30.1236 Epoch 20/25 1570/1570 [==============================] - 5s 3ms/step - loss: 29.4335 - mae: 29.9309 Epoch 21/25 1570/1570 [==============================] - 5s 3ms/step - loss: 29.3501 - mae: 29.8474 Epoch 22/25 1570/1570 [==============================] - 5s 3ms/step - loss: 30.9055 - mae: 31.4033 Epoch 23/25 1570/1570 [==============================] - 5s 3ms/step - loss: 29.6605 - mae: 30.1580 Epoch 24/25 1570/1570 [==============================] - 5s 3ms/step - loss: 29.5224 - mae: 30.0199 Epoch 25/25 1570/1570 [==============================] - 5s 3ms/step - loss: 28.9797 - mae: 29.4764 plt . semilogx ( history . history [ \"lr\" ], history . history [ \"loss\" ]) plt . axis ([ 1e-8 , 1e-6 , 20 , 300 ]) forecast = [] for time in range ( len ( data . pm25 ) - window_size ): forecast . append ( model . predict ( np . array ( data . pm25 [ time : time + window_size ])[ np . newaxis ])) forecast = forecast [ split_time - window_size :] results = np . array ( forecast )[:, 0 , 0 ] plt . figure ( figsize = ( 10 , 6 )) plot_series ( time_valid , x_valid ) plot_series ( time_valid , results ) tf . keras . metrics . mean_absolute_error ( x_valid , results ) . numpy () array([39.826717, 39.908363, 40.442093, ..., 39.340984, 39.340984], dtype=float32) import matplotlib.image as mpimg import matplotlib.pyplot as plt #----------------------------------------------------------- # Retrieve a list of list results on training and test data # sets for each training epoch #----------------------------------------------------------- mae = history . history [ 'mae' ] loss = history . history [ 'loss' ] epochs = range ( len ( loss )) # Get number of epochs #------------------------------------------------ # Plot MAE and Loss #------------------------------------------------ plt . plot ( epochs , mae , 'r' ) plt . plot ( epochs , loss , 'b' ) plt . title ( 'MAE and Loss' ) plt . xlabel ( \"Epochs\" ) plt . ylabel ( \"Accuracy\" ) plt . legend ([ \"MAE\" , \"Loss\" ]) plt . figure () epochs_zoom = epochs [ 200 :] mae_zoom = mae [ 200 :] loss_zoom = loss [ 200 :] #------------------------------------------------ # Plot Zoomed MAE and Loss #------------------------------------------------ plt . plot ( epochs_zoom , mae_zoom , 'r' ) plt . plot ( epochs_zoom , loss_zoom , 'b' ) plt . title ( 'MAE and Loss' ) plt . xlabel ( \"Epochs\" ) plt . ylabel ( \"Accuracy\" ) plt . legend ([ \"MAE\" , \"Loss\" ]) plt . figure ()","title":"RNN"},{"location":"Python/Time%20series%20deep%20learning/#lstm","text":"def model_forecast ( model , series , window_size ): ds = tf . data . Dataset . from_tensor_slices ( series ) ds = ds . window ( window_size , shift = 1 , drop_remainder = True ) ds = ds . flat_map ( lambda w : w . batch ( window_size )) ds = ds . batch ( 32 ) . prefetch ( 1 ) forecast = model . predict ( ds ) return forecast tf . keras . backend . clear_session () tf . random . set_seed ( 51 ) np . random . seed ( 51 ) x_train_LSTM = tf . expand_dims ( x_train , axis =- 1 ) train_set = windowed_dataset ( x_train_LSTM , window_size , batch_size , shuffle_buffer_size ) train_set model = tf . keras . models . Sequential ([ tf . keras . layers . Conv1D ( filters = 32 , kernel_size = 5 , strides = 1 , padding = \"causal\" , activation = \"relu\" , input_shape = [ None , 1 ]), tf . keras . layers . Bidirectional ( tf . keras . layers . LSTM ( 32 , return_sequences = True )), tf . keras . layers . Bidirectional ( tf . keras . layers . LSTM ( 32 , return_sequences = True )), tf . keras . layers . Dense ( 1 ), tf . keras . layers . Lambda ( lambda x : x * 200 ) ]) lr_schedule = tf . keras . callbacks . LearningRateScheduler ( lambda epoch : 1e-8 * 10 ** ( epoch / 20 )) optimizer = tf . keras . optimizers . SGD ( learning_rate = 1e-8 , momentum = 0.9 ) model . compile ( loss = tf . keras . losses . Huber (), optimizer = optimizer , metrics = [ \"mae\" ]) history = model . fit ( train_set , epochs = 25 , callbacks = [ lr_schedule ]) Epoch 1/25 1570/1570 [==============================] - 27s 12ms/step - loss: 51.0839 - mae: 51.5817 Epoch 2/25 1570/1570 [==============================] - 21s 13ms/step - loss: 30.5634 - mae: 31.0600 0s - loss: 30.5700 - mae: 31.06 Epoch 3/25 1570/1570 [==============================] - 20s 13ms/step - loss: 28.5261 - mae: 29.0220 Epoch 4/25 1570/1570 [==============================] - 23s 14ms/step - loss: 27.1795 - mae: 27.6754 Epoch 5/25 1570/1570 [==============================] - 21s 13ms/step - loss: 25.9100 - mae: 26.4058 Epoch 6/25 1570/1570 [==============================] - 21s 13ms/step - loss: 24.7144 - mae: 25.2104 Epoch 7/25 1570/1570 [==============================] - 22s 14ms/step - loss: 23.5953 - mae: 24.0904 Epoch 8/25 1570/1570 [==============================] - 21s 13ms/step - loss: 22.6447 - mae: 23.1403 Epoch 9/25 1570/1570 [==============================] - 22s 14ms/step - loss: 21.7317 - mae: 22.2269 Epoch 10/25 1570/1570 [==============================] - 22s 14ms/step - loss: 20.8462 - mae: 21.3408 Epoch 11/25 1570/1570 [==============================] - 22s 14ms/step - loss: 20.1405 - mae: 20.6347 Epoch 12/25 1570/1570 [==============================] - 22s 14ms/step - loss: 19.4537 - mae: 19.9481 Epoch 13/25 1570/1570 [==============================] - 22s 14ms/step - loss: 18.8216 - mae: 19.3158 Epoch 14/25 1570/1570 [==============================] - 22s 14ms/step - loss: 18.2066 - mae: 18.7004 Epoch 15/25 1570/1570 [==============================] - 22s 14ms/step - loss: 17.5309 - mae: 18.0234 Epoch 16/25 1570/1570 [==============================] - 22s 14ms/step - loss: 17.3460 - mae: 17.8395 Epoch 17/25 1570/1570 [==============================] - 22s 14ms/step - loss: 16.8951 - mae: 17.3882 Epoch 18/25 1570/1570 [==============================] - 23s 15ms/step - loss: 16.5255 - mae: 17.0192 Epoch 19/25 1570/1570 [==============================] - 22s 14ms/step - loss: 16.4288 - mae: 16.9225 Epoch 20/25 1570/1570 [==============================] - 22s 14ms/step - loss: 16.1572 - mae: 16.6509 Epoch 21/25 1570/1570 [==============================] - 22s 14ms/step - loss: 15.7839 - mae: 16.2775 Epoch 22/25 1570/1570 [==============================] - 22s 14ms/step - loss: 15.7438 - mae: 16.2366 Epoch 23/25 1570/1570 [==============================] - 23s 15ms/step - loss: 15.6320 - mae: 16.1244 Epoch 24/25 1570/1570 [==============================] - 23s 14ms/step - loss: 15.7910 - mae: 16.2841 Epoch 25/25 1570/1570 [==============================] - 22s 14ms/step - loss: 15.5656 - mae: 16.0587 rnn_forecast = model_forecast ( model , np . array ( data . pm25 )[ ... , np . newaxis ], window_size ) rnn_forecast = rnn_forecast [ split_time - window_size : - 1 , - 1 , 0 ] plt . figure ( figsize = ( 10 , 6 )) plot_series ( time_valid , x_valid ) plot_series ( time_valid , rnn_forecast ) tf . keras . metrics . mean_absolute_error ( x_valid , rnn_forecast ) . numpy () 17.472342 import matplotlib.image as mpimg import matplotlib.pyplot as plt #----------------------------------------------------------- # Retrieve a list of list results on training and test data # sets for each training epoch #----------------------------------------------------------- mae = history . history [ 'mae' ] loss = history . history [ 'loss' ] epochs = range ( len ( loss )) # Get number of epochs #------------------------------------------------ # Plot MAE and Loss #------------------------------------------------ plt . plot ( epochs , mae , 'r' ) plt . plot ( epochs , loss , 'b' ) plt . title ( 'MAE and Loss' ) plt . xlabel ( \"Epochs\" ) plt . ylabel ( \"Accuracy\" ) plt . legend ([ \"MAE\" , \"Loss\" ]) plt . figure () epochs_zoom = epochs [ 200 :] mae_zoom = mae [ 200 :] loss_zoom = loss [ 200 :] #------------------------------------------------ # Plot Zoomed MAE and Loss #------------------------------------------------ plt . plot ( epochs_zoom , mae_zoom , 'r' ) plt . plot ( epochs_zoom , loss_zoom , 'b' ) plt . title ( 'MAE and Loss' ) plt . xlabel ( \"Epochs\" ) plt . ylabel ( \"Accuracy\" ) plt . legend ([ \"MAE\" , \"Loss\" ]) plt . figure ()","title":"LSTM"},{"location":"Python/Visualization%20for%20predictive%20analytics/","text":"Data visualisation for predictive analytics \u00b6 Author: Achyuthuni Sri Harsha Data visualisation can be performed in many ways. There are infinite ways to visualise the data, and what works is dependant on the patterns in the data. In this post, we are trying to categorise the visualisation of data for regression and classification problems. Every regression, classification and clustering problem has some or all of the following assumptions: 1. Change in independent variables changes the dependant variable. In other words, there is a relationship between the dependant variable and the independent variables. Before building a model, it is advised to visualise this relationship. 2. Assumptions on the distribution of the dependant or independent variable. For example, for Naive Bayes classifier, the independent variables should follow a normal distribution. 3. Assumptions of relationships between independent variables. For example, for linear regression, the independent variables should not be correlated. 4. Unbalanced dataset. The frequency of the smaller class should be significant when compared to the frequency of the larger class. 5. The time series of data/features are stationary. Apart from validating the assumptions and identifying trends in the data, data visualisation can also be used for gathering insights and feature engineering. The below example is from the marketing department of a consulting firm. The problem is to identify the projects that they can win. # Importing the necessary libraries import pandas as pd import numpy as np import os import matplotlib.pyplot as plt from statsmodels.graphics.mosaicplot import mosaic import seaborn as sns % matplotlib inline # loading th data path = \"data/marketing dept.csv\" df = pd . read_csv ( path ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } reporting_status product industry region strength_in_segment profit_for_customer sales_value profit_perc joint_bid_portion 0 Lost F Cap Oth 57 1.225 6.5 64 59 1 Lost L Def UK 51 1.469 9.9 56 58 2 Lost Lo Cli UK 79 0.887 7.0 59 48 3 Lost G Fin UK 55 1.316 8.9 34 41 4 Won G Sec UK 32 1.010 5.7 43 63 Univariate analysis \u00b6 The univariate analysis deals with EDA on one variable alone. In describing or characterising the observations of an individual variable, three basic properties are of interest: 1. The location of observations, or how large or small the values of the individual observations are 2. The dispersion (sometimes called scale or spread) of the observations 3. The distribution of the observations Uni-variate plots provide one way to find out about those properties. There are two basic kinds of univariate plots: 1. Enumeration plots, or plots that show every observation 2. Summary plots that generalise the data into a simplified representation 3. Enumerative plots Index Plot/Univariate Scatter Diagram \u00b6 The most common enumerative plot is the index plot. It displays the values of a single variable for each observation using symbols plotted relative to the observation number. plt . plot ( df . sales_value , 'o' , color = 'black' ) plt . title ( \"Index plot\" ) plt . xlabel ( 'Sales Value' ); From the above plot, we can infer that there are around 3000 observations for sales, and they are captured randomly along the data. Strip Plot/Strip Chart (univariate scatter diagram) \u00b6 Displays the values of a single variable as symbols plotted along a line. This is a basic plot where we can see the spread of the data. ax = sns . stripplot ( x = df . sales_value ) ax . set ( xlabel = 'Sales_value' , title = 'Strip Chart' ); Dot Plot/Dot Chart \u00b6 Dot plot displays the values plotted along a line. It is generally constructed after sorting the rows. This can help us in determining the distribution of the data. It can also help us identify the continuity of the data. plt . plot ( df . sort_values ( by = 'sales_value' ) . reset_index () . sales_value , 'o' , color = 'black' ) plt . title ( \"Dot plot\" ) plt . ylabel ( 'Sales Value' ); From looking at the plot, most of the data lies within 6-12 while the frequeny of the data decreases as we go away from the mean. The graph is also symmetric. This indicates the distribution could be Normal distribution. Univariate Summary Plots \u00b6 Summary plots display an object or a graph that gives a more concise expression of the location, dispersion, and distribution of a variable than an enumerative plot, but this comes at the expense of some loss of information: In a summary plot, it is no longer possible to retrieve the individual data value, but the gain usually matches this loss in understanding that results from the efficient representation of the data. Summary plots generally prove to be much better than the enumerative plots in revealing the distribution of the data. Box plot \u00b6 A simple way of representing statistical data on a plot in which a rectangle is drawn to represent the second and third quartiles, usually with a vertical line inside to indicate the median value. The lower and upper quartiles are shown as horizontal lines on either side of the rectangle. ax = sns . boxplot ( x = df . sales_value ) ax . set ( xlabel = 'Sales value' , title = 'Box Chart' ); Histograms \u00b6 The other summary plots are of various types: Histograms: Histograms are a type of bar chart that displays the counts or relative frequencies of values falling in different class intervals or ranges. Density Plots: A density plot is a plot of the local relative frequency or density of points along the number line or x-axis of a plot. Where points occur more frequently, this sum, and consequently the local density, will be greater. # For continuous data ax = df . sales_value . plot . hist ( alpha = 0.75 ) df . groupby ( 'sales_value' )[ 'sales_value' ] . count () . plot () ax . set ( xlabel = 'Sales value' , title = 'Histogram' ); Q-Q plot \u00b6 In statistics, a Q-Q (quantile-quantile) plot is a probability plot, which is a graphical method for comparing two probability distributions by plotting their quantiles against each other. If the two distributions being compared are similar, the points in the Q-Q plot will approximately lie on the line y = x. If the distributions are linearly related, the points in the Q-Q plot will approximately lie on a line, but not necessarily on the line y = x. Q-Q plots can also be used as a graphical means of estimating parameters in a location-scale family of distributions. A Q-Q plot is used to compare the shapes of distributions, providing a graphical view of how properties such as location, scale, and skewness are similar or different in the two distributions. Below is a Q-Q plot of the sales data with a normal distribution from scipy import stats stats . probplot ( df . sales_value , plot = sns . mpl . pyplot ); From the above plot, it is clear that the distribution is normal. Bar chart \u00b6 Whereas the above plots are applicable for continuous data, a simple bar chart can help us with categorical data. df . groupby ( 'region' )[ 'region' ] . count () . plot . bar () . set ( xlabel = 'Sales value' , title = 'Histogram' ); Combining the univariate EDA's \u00b6 The below code will do the following for all the columns in the dataset: 1. For continuous data, it will plot the scatter plot, box plot, histogram and q-q plot with normal distribution 2. For categorical data, it will plot the bar chart def univariate_analysis ( dataset ): # For catogorical data for i in ( dataset . select_dtypes ( exclude = [ 'int' , 'int64' , 'float' , 'float64' ]) . columns ): dataset . groupby ( i )[ i ] . count () . plot . bar () plt . show (); # For continuous data ## Selecting the columns that are continuous for i in ( dataset . select_dtypes ( include = [ 'int' , 'int64' , 'float' , 'float64' ]) . columns ): # Index plot plt . subplot ( 221 ) plt . plot ( dataset [ i ], 'o' , color = 'black' ) plt . xlabel ( i ) # q-q plot plt . subplot ( 222 ) stats . probplot ( dataset [ i ], plot = sns . mpl . pyplot ) #Box chart plt . title ( i ) plt . subplot ( 223 ) ax = sns . boxplot ( x = dataset [ i ]) # Histogram plt . subplot ( 224 ) ax2 = dataset [ i ] . plot . hist ( alpha = 0.75 ) dataset . groupby ( i )[ i ] . count () . plot () plt . show (); univariate_analysis ( df ) Bivariate analysis \u00b6 The bivariate analysis deals with visualisations between two variables. The bi-variate analysis is used to identify the relationship between dependant and independent variable. The dependent and independent variables can be of the following types: Problem Independent var Dependent var Classification Categorical Categorical Classification Continuous Categorical Classification Categorical Continuous Classification Continuous Continuous For all the four types, we want to identify the relation between the dependant variable and the independent variable. Classification Visualisations \u00b6 First, let us consider the classification problem. Let's say we have to predict the reporting status of the bid. We have three categorical independent variables and five continuous independent variables. Joint Histograms \u00b6 The five continuous variables are: 1. Strength in segment 2. Profit for customer 3. Sales Value 4. Profit percentage 5. joint bid portion For these variables, we can look at joint histograms. What we are trying to see is the overlap between the distributions for the two different classes. If the overlap between the two variables is small, then that variable can be a good predictor and vice versa. bi_con_cat = df . groupby ([ 'reporting_status' ])[ 'strength_in_segment' ] . plot . hist ( alpha = 0.5 ) plt . xlabel ( 'strength_in_segment' ) plt . legend ( df . groupby ([ 'reporting_status' ])[ 'strength_in_segment' ] . count () . axes [ 0 ] . tolist ()) plt . title ( 'Joint histogram' ); bi_con_cat = df . groupby ([ 'reporting_status' ])[ 'profit_for_customer' ] . plot . hist ( alpha = 0.5 ) plt . xlabel ( 'profit_for_customer' ) plt . legend ( df . groupby ([ 'reporting_status' ])[ 'profit_for_customer' ] . count () . axes [ 0 ] . tolist ()) plt . title ( 'Joint histogram' ); From the above graphs, we can see that profit for customer can explain the status of the bid when compared to strength of segment. We can also see the mean, variance and the distributions of the independent variables between the classes. In a decision tree, the tree will split with profit_for_customer > 1 as 'Lost' class and profit_for_customer < 1 as 'Won'. In logistic regression, the pseudo R-squared will be greater for profit_for_customer than for strength_in_segment. Similar thinking can be applied to SVM, Naive-Bates classifiers etc. Mosaic Plots \u00b6 The three categorical variables are: 1. Product 2. Industry 3. Region For these variables, a mosaic plot will be useful. In the mosaic plot, the area of the rectangles is proportional to the frequency of the class. In the x-axis, we have the dependant variables, and in the y-axis, we have the continuous variables. Using this, we can see the relative frequencies of the 'Won' and 'Lost' in each of the dependant variable classes. # from statsmodels.graphics.mosaicplot import mosaic mosaic ( df , [ 'product' , 'reporting_status' ]); For example, the ratio of Lost to won cases is same in products 'G', 'Li', 'P'. Product 'F' has more wins than normal, while product 'L' has more losses than normal. The products 'C' and 'Lo' are too small to be statistically significant. Intuitively, in logistic regression, the products 'G', 'Li', 'P' can be considered as base classes with 'F' having a positive slope value and 'L' having a negative slope value. In decision-trees, the products 'G', 'Li', 'P' will be part of one branch while products 'L' and 'F' will be part of different branches. Similar thinking can be applied to SVM, Naive-Bates classifiers etc. Combining the classification EDA's \u00b6 The below code will do the following for all the columns in the dataset: 1. For continuous data, it will plot the joint histograms 2. For categorical data, it will plot the mosaic plot def classification_bivariate_analysis ( dataset , dependant_variable ): # For continuous data for i in ( dataset . select_dtypes ( include = [ 'int' , 'int64' , 'float' , 'float64' ]) . columns ): bi_con_cat = dataset . groupby ([ dependant_variable ])[ i ] . plot . hist ( alpha = 0.5 ) plt . xlabel ( i ) plt . legend ( dataset . groupby ([ dependant_variable ])[ i ] . count () . axes [ 0 ] . tolist ()) plt . title ( i ) plt . show (); # For catogorical data for i in ( dataset . select_dtypes ( exclude = [ 'int' , 'int64' , 'float' , 'float64' ]) . columns ): if ( i != dependant_variable ): mosaic ( dataset , [ i , dependant_variable ]); plt . show (); classification_bivariate_analysis ( df , 'reporting_status' ) Bivariate Regression Visualisations \u00b6 Let us consider the regression problem. Let's say we have to predict sales_value of the successful bids. We have three categorical independent variables and four continuous independent variables. successful_bids = df [ df [ 'reporting_status' ] == 'Won' ] Scatter plots \u00b6 There are four continuous variables: 1. Strength in segment 2. Profit for customer 3. Profit percentage 4. Joint bid portion Scatter plots show how much and how one variable is affected by another. We can use them to identify how changing the independent variable changes the dependant variable. Using this, we can identify if we have to do any transformations to the variables. plt . scatter ( successful_bids [ 'joint_bid_portion' ], successful_bids [ 'sales_value' ]) plt . xlabel ( 'joint_bid_portion' ) plt . ylabel ( 'sales_value' ) plt . title ( 'Scatter plot' ); In the above plot, there seems to be no relation between joint_bid_portion and sales_value. We can also observe how joint bid portion behaves after 80. Box plots \u00b6 The three categorical variables are: 1. Product 2. Industry 3. Region For these variables, a box plot will be useful. While showing the relative means among the classes, we can also visualise the variations and distributions in the data. bi_variate_boxplot = sns . boxplot ( x = \"industry\" , y = \"sales_value\" , data = successful_bids ) bi_variate_boxplot . set ( title = 'Box Chart' ); From the above plot, the mean of sales for 'Sec', 'Air, 'Ban', 'Cap', 'Con', 'Oth', 'Def', 'Agr' are similar with similar distributions. The mean of 'Ins', 'OG', 'Gov', 'Hea','Whi' classes seems to be higher and the mean of 'Mob', 'Fin', 'Tel' is lower. In a linear regression, the following industries would be considered as base classes: 'Sec', 'Air, 'Ban', 'Cap', 'Con', 'Oth', 'Def', 'Agr' while 'Ins', 'OG', 'Gov', 'Hea','Whi' will have positive slope value and 'Mob', 'Fin', 'Tel' will have a negative slope. Combining the bivariate regression EDA's \u00b6 The below code will do the following for all the columns in the dataset: 1. For continuous data, it will plot the scatter plots 2. For categorical data, it will plot the bar charts def regression_bivariate_analysis ( dataset , dependant_variable ): # For continuous data for i in ( dataset . select_dtypes ( include = [ 'int' , 'int64' , 'float' , 'float64' ]) . columns ): if ( i != dependant_variable ): plt . scatter ( dataset [ i ], dataset [ dependant_variable ]) plt . show (); # For catogorical data for i in ( dataset . select_dtypes ( exclude = [ 'int' , 'int64' , 'float' , 'float64' ]) . columns ): bi_variate_boxplot = sns . boxplot ( x = i , y = dependant_variable , data = dataset ) bi_variate_boxplot . set ( title = i ) plt . show (); regression_bivariate_analysis ( successful_bids , 'sales_value' )","title":"Vizualising for predictive analytics (Python)"},{"location":"Python/Visualization%20for%20predictive%20analytics/#data-visualisation-for-predictive-analytics","text":"Author: Achyuthuni Sri Harsha Data visualisation can be performed in many ways. There are infinite ways to visualise the data, and what works is dependant on the patterns in the data. In this post, we are trying to categorise the visualisation of data for regression and classification problems. Every regression, classification and clustering problem has some or all of the following assumptions: 1. Change in independent variables changes the dependant variable. In other words, there is a relationship between the dependant variable and the independent variables. Before building a model, it is advised to visualise this relationship. 2. Assumptions on the distribution of the dependant or independent variable. For example, for Naive Bayes classifier, the independent variables should follow a normal distribution. 3. Assumptions of relationships between independent variables. For example, for linear regression, the independent variables should not be correlated. 4. Unbalanced dataset. The frequency of the smaller class should be significant when compared to the frequency of the larger class. 5. The time series of data/features are stationary. Apart from validating the assumptions and identifying trends in the data, data visualisation can also be used for gathering insights and feature engineering. The below example is from the marketing department of a consulting firm. The problem is to identify the projects that they can win. # Importing the necessary libraries import pandas as pd import numpy as np import os import matplotlib.pyplot as plt from statsmodels.graphics.mosaicplot import mosaic import seaborn as sns % matplotlib inline # loading th data path = \"data/marketing dept.csv\" df = pd . read_csv ( path ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } reporting_status product industry region strength_in_segment profit_for_customer sales_value profit_perc joint_bid_portion 0 Lost F Cap Oth 57 1.225 6.5 64 59 1 Lost L Def UK 51 1.469 9.9 56 58 2 Lost Lo Cli UK 79 0.887 7.0 59 48 3 Lost G Fin UK 55 1.316 8.9 34 41 4 Won G Sec UK 32 1.010 5.7 43 63","title":"Data visualisation for predictive analytics"},{"location":"Python/Visualization%20for%20predictive%20analytics/#univariate-analysis","text":"The univariate analysis deals with EDA on one variable alone. In describing or characterising the observations of an individual variable, three basic properties are of interest: 1. The location of observations, or how large or small the values of the individual observations are 2. The dispersion (sometimes called scale or spread) of the observations 3. The distribution of the observations Uni-variate plots provide one way to find out about those properties. There are two basic kinds of univariate plots: 1. Enumeration plots, or plots that show every observation 2. Summary plots that generalise the data into a simplified representation 3. Enumerative plots","title":"Univariate analysis"},{"location":"Python/Visualization%20for%20predictive%20analytics/#index-plotunivariate-scatter-diagram","text":"The most common enumerative plot is the index plot. It displays the values of a single variable for each observation using symbols plotted relative to the observation number. plt . plot ( df . sales_value , 'o' , color = 'black' ) plt . title ( \"Index plot\" ) plt . xlabel ( 'Sales Value' ); From the above plot, we can infer that there are around 3000 observations for sales, and they are captured randomly along the data.","title":"Index Plot/Univariate Scatter Diagram"},{"location":"Python/Visualization%20for%20predictive%20analytics/#strip-plotstrip-chart-univariate-scatter-diagram","text":"Displays the values of a single variable as symbols plotted along a line. This is a basic plot where we can see the spread of the data. ax = sns . stripplot ( x = df . sales_value ) ax . set ( xlabel = 'Sales_value' , title = 'Strip Chart' );","title":"Strip Plot/Strip Chart (univariate scatter diagram)"},{"location":"Python/Visualization%20for%20predictive%20analytics/#dot-plotdot-chart","text":"Dot plot displays the values plotted along a line. It is generally constructed after sorting the rows. This can help us in determining the distribution of the data. It can also help us identify the continuity of the data. plt . plot ( df . sort_values ( by = 'sales_value' ) . reset_index () . sales_value , 'o' , color = 'black' ) plt . title ( \"Dot plot\" ) plt . ylabel ( 'Sales Value' ); From looking at the plot, most of the data lies within 6-12 while the frequeny of the data decreases as we go away from the mean. The graph is also symmetric. This indicates the distribution could be Normal distribution.","title":"Dot Plot/Dot Chart"},{"location":"Python/Visualization%20for%20predictive%20analytics/#univariate-summary-plots","text":"Summary plots display an object or a graph that gives a more concise expression of the location, dispersion, and distribution of a variable than an enumerative plot, but this comes at the expense of some loss of information: In a summary plot, it is no longer possible to retrieve the individual data value, but the gain usually matches this loss in understanding that results from the efficient representation of the data. Summary plots generally prove to be much better than the enumerative plots in revealing the distribution of the data.","title":"Univariate Summary Plots"},{"location":"Python/Visualization%20for%20predictive%20analytics/#box-plot","text":"A simple way of representing statistical data on a plot in which a rectangle is drawn to represent the second and third quartiles, usually with a vertical line inside to indicate the median value. The lower and upper quartiles are shown as horizontal lines on either side of the rectangle. ax = sns . boxplot ( x = df . sales_value ) ax . set ( xlabel = 'Sales value' , title = 'Box Chart' );","title":"Box plot"},{"location":"Python/Visualization%20for%20predictive%20analytics/#histograms","text":"The other summary plots are of various types: Histograms: Histograms are a type of bar chart that displays the counts or relative frequencies of values falling in different class intervals or ranges. Density Plots: A density plot is a plot of the local relative frequency or density of points along the number line or x-axis of a plot. Where points occur more frequently, this sum, and consequently the local density, will be greater. # For continuous data ax = df . sales_value . plot . hist ( alpha = 0.75 ) df . groupby ( 'sales_value' )[ 'sales_value' ] . count () . plot () ax . set ( xlabel = 'Sales value' , title = 'Histogram' );","title":"Histograms"},{"location":"Python/Visualization%20for%20predictive%20analytics/#q-q-plot","text":"In statistics, a Q-Q (quantile-quantile) plot is a probability plot, which is a graphical method for comparing two probability distributions by plotting their quantiles against each other. If the two distributions being compared are similar, the points in the Q-Q plot will approximately lie on the line y = x. If the distributions are linearly related, the points in the Q-Q plot will approximately lie on a line, but not necessarily on the line y = x. Q-Q plots can also be used as a graphical means of estimating parameters in a location-scale family of distributions. A Q-Q plot is used to compare the shapes of distributions, providing a graphical view of how properties such as location, scale, and skewness are similar or different in the two distributions. Below is a Q-Q plot of the sales data with a normal distribution from scipy import stats stats . probplot ( df . sales_value , plot = sns . mpl . pyplot ); From the above plot, it is clear that the distribution is normal.","title":"Q-Q plot"},{"location":"Python/Visualization%20for%20predictive%20analytics/#bar-chart","text":"Whereas the above plots are applicable for continuous data, a simple bar chart can help us with categorical data. df . groupby ( 'region' )[ 'region' ] . count () . plot . bar () . set ( xlabel = 'Sales value' , title = 'Histogram' );","title":"Bar chart"},{"location":"Python/Visualization%20for%20predictive%20analytics/#combining-the-univariate-edas","text":"The below code will do the following for all the columns in the dataset: 1. For continuous data, it will plot the scatter plot, box plot, histogram and q-q plot with normal distribution 2. For categorical data, it will plot the bar chart def univariate_analysis ( dataset ): # For catogorical data for i in ( dataset . select_dtypes ( exclude = [ 'int' , 'int64' , 'float' , 'float64' ]) . columns ): dataset . groupby ( i )[ i ] . count () . plot . bar () plt . show (); # For continuous data ## Selecting the columns that are continuous for i in ( dataset . select_dtypes ( include = [ 'int' , 'int64' , 'float' , 'float64' ]) . columns ): # Index plot plt . subplot ( 221 ) plt . plot ( dataset [ i ], 'o' , color = 'black' ) plt . xlabel ( i ) # q-q plot plt . subplot ( 222 ) stats . probplot ( dataset [ i ], plot = sns . mpl . pyplot ) #Box chart plt . title ( i ) plt . subplot ( 223 ) ax = sns . boxplot ( x = dataset [ i ]) # Histogram plt . subplot ( 224 ) ax2 = dataset [ i ] . plot . hist ( alpha = 0.75 ) dataset . groupby ( i )[ i ] . count () . plot () plt . show (); univariate_analysis ( df )","title":"Combining the univariate EDA's"},{"location":"Python/Visualization%20for%20predictive%20analytics/#bivariate-analysis","text":"The bivariate analysis deals with visualisations between two variables. The bi-variate analysis is used to identify the relationship between dependant and independent variable. The dependent and independent variables can be of the following types: Problem Independent var Dependent var Classification Categorical Categorical Classification Continuous Categorical Classification Categorical Continuous Classification Continuous Continuous For all the four types, we want to identify the relation between the dependant variable and the independent variable.","title":"Bivariate analysis"},{"location":"Python/Visualization%20for%20predictive%20analytics/#classification-visualisations","text":"First, let us consider the classification problem. Let's say we have to predict the reporting status of the bid. We have three categorical independent variables and five continuous independent variables.","title":"Classification Visualisations"},{"location":"Python/Visualization%20for%20predictive%20analytics/#joint-histograms","text":"The five continuous variables are: 1. Strength in segment 2. Profit for customer 3. Sales Value 4. Profit percentage 5. joint bid portion For these variables, we can look at joint histograms. What we are trying to see is the overlap between the distributions for the two different classes. If the overlap between the two variables is small, then that variable can be a good predictor and vice versa. bi_con_cat = df . groupby ([ 'reporting_status' ])[ 'strength_in_segment' ] . plot . hist ( alpha = 0.5 ) plt . xlabel ( 'strength_in_segment' ) plt . legend ( df . groupby ([ 'reporting_status' ])[ 'strength_in_segment' ] . count () . axes [ 0 ] . tolist ()) plt . title ( 'Joint histogram' ); bi_con_cat = df . groupby ([ 'reporting_status' ])[ 'profit_for_customer' ] . plot . hist ( alpha = 0.5 ) plt . xlabel ( 'profit_for_customer' ) plt . legend ( df . groupby ([ 'reporting_status' ])[ 'profit_for_customer' ] . count () . axes [ 0 ] . tolist ()) plt . title ( 'Joint histogram' ); From the above graphs, we can see that profit for customer can explain the status of the bid when compared to strength of segment. We can also see the mean, variance and the distributions of the independent variables between the classes. In a decision tree, the tree will split with profit_for_customer > 1 as 'Lost' class and profit_for_customer < 1 as 'Won'. In logistic regression, the pseudo R-squared will be greater for profit_for_customer than for strength_in_segment. Similar thinking can be applied to SVM, Naive-Bates classifiers etc.","title":"Joint Histograms"},{"location":"Python/Visualization%20for%20predictive%20analytics/#mosaic-plots","text":"The three categorical variables are: 1. Product 2. Industry 3. Region For these variables, a mosaic plot will be useful. In the mosaic plot, the area of the rectangles is proportional to the frequency of the class. In the x-axis, we have the dependant variables, and in the y-axis, we have the continuous variables. Using this, we can see the relative frequencies of the 'Won' and 'Lost' in each of the dependant variable classes. # from statsmodels.graphics.mosaicplot import mosaic mosaic ( df , [ 'product' , 'reporting_status' ]); For example, the ratio of Lost to won cases is same in products 'G', 'Li', 'P'. Product 'F' has more wins than normal, while product 'L' has more losses than normal. The products 'C' and 'Lo' are too small to be statistically significant. Intuitively, in logistic regression, the products 'G', 'Li', 'P' can be considered as base classes with 'F' having a positive slope value and 'L' having a negative slope value. In decision-trees, the products 'G', 'Li', 'P' will be part of one branch while products 'L' and 'F' will be part of different branches. Similar thinking can be applied to SVM, Naive-Bates classifiers etc.","title":"Mosaic Plots"},{"location":"Python/Visualization%20for%20predictive%20analytics/#combining-the-classification-edas","text":"The below code will do the following for all the columns in the dataset: 1. For continuous data, it will plot the joint histograms 2. For categorical data, it will plot the mosaic plot def classification_bivariate_analysis ( dataset , dependant_variable ): # For continuous data for i in ( dataset . select_dtypes ( include = [ 'int' , 'int64' , 'float' , 'float64' ]) . columns ): bi_con_cat = dataset . groupby ([ dependant_variable ])[ i ] . plot . hist ( alpha = 0.5 ) plt . xlabel ( i ) plt . legend ( dataset . groupby ([ dependant_variable ])[ i ] . count () . axes [ 0 ] . tolist ()) plt . title ( i ) plt . show (); # For catogorical data for i in ( dataset . select_dtypes ( exclude = [ 'int' , 'int64' , 'float' , 'float64' ]) . columns ): if ( i != dependant_variable ): mosaic ( dataset , [ i , dependant_variable ]); plt . show (); classification_bivariate_analysis ( df , 'reporting_status' )","title":"Combining the classification EDA's"},{"location":"Python/Visualization%20for%20predictive%20analytics/#bivariate-regression-visualisations","text":"Let us consider the regression problem. Let's say we have to predict sales_value of the successful bids. We have three categorical independent variables and four continuous independent variables. successful_bids = df [ df [ 'reporting_status' ] == 'Won' ]","title":"Bivariate Regression Visualisations"},{"location":"Python/Visualization%20for%20predictive%20analytics/#scatter-plots","text":"There are four continuous variables: 1. Strength in segment 2. Profit for customer 3. Profit percentage 4. Joint bid portion Scatter plots show how much and how one variable is affected by another. We can use them to identify how changing the independent variable changes the dependant variable. Using this, we can identify if we have to do any transformations to the variables. plt . scatter ( successful_bids [ 'joint_bid_portion' ], successful_bids [ 'sales_value' ]) plt . xlabel ( 'joint_bid_portion' ) plt . ylabel ( 'sales_value' ) plt . title ( 'Scatter plot' ); In the above plot, there seems to be no relation between joint_bid_portion and sales_value. We can also observe how joint bid portion behaves after 80.","title":"Scatter plots"},{"location":"Python/Visualization%20for%20predictive%20analytics/#box-plots","text":"The three categorical variables are: 1. Product 2. Industry 3. Region For these variables, a box plot will be useful. While showing the relative means among the classes, we can also visualise the variations and distributions in the data. bi_variate_boxplot = sns . boxplot ( x = \"industry\" , y = \"sales_value\" , data = successful_bids ) bi_variate_boxplot . set ( title = 'Box Chart' ); From the above plot, the mean of sales for 'Sec', 'Air, 'Ban', 'Cap', 'Con', 'Oth', 'Def', 'Agr' are similar with similar distributions. The mean of 'Ins', 'OG', 'Gov', 'Hea','Whi' classes seems to be higher and the mean of 'Mob', 'Fin', 'Tel' is lower. In a linear regression, the following industries would be considered as base classes: 'Sec', 'Air, 'Ban', 'Cap', 'Con', 'Oth', 'Def', 'Agr' while 'Ins', 'OG', 'Gov', 'Hea','Whi' will have positive slope value and 'Mob', 'Fin', 'Tel' will have a negative slope.","title":"Box plots"},{"location":"Python/Visualization%20for%20predictive%20analytics/#combining-the-bivariate-regression-edas","text":"The below code will do the following for all the columns in the dataset: 1. For continuous data, it will plot the scatter plots 2. For categorical data, it will plot the bar charts def regression_bivariate_analysis ( dataset , dependant_variable ): # For continuous data for i in ( dataset . select_dtypes ( include = [ 'int' , 'int64' , 'float' , 'float64' ]) . columns ): if ( i != dependant_variable ): plt . scatter ( dataset [ i ], dataset [ dependant_variable ]) plt . show (); # For catogorical data for i in ( dataset . select_dtypes ( exclude = [ 'int' , 'int64' , 'float' , 'float64' ]) . columns ): bi_variate_boxplot = sns . boxplot ( x = i , y = dependant_variable , data = dataset ) bi_variate_boxplot . set ( title = i ) plt . show (); regression_bivariate_analysis ( successful_bids , 'sales_value' )","title":"Combining the bivariate regression EDA's"},{"location":"Python/Vizualisation%20using%20python%20Part%201/","text":"Visualizalising tabular data \u00b6 Author: Achyuthuni Sri Harsha In this visualistion, we look at various visualisation types on datatype tables. Matplotlib is the most popular library for viz in Python. Seaborn is built on top of it with integrated analysis, specialized plots, and pretty good integration with Pandas. Plotly express is another library for viz. Also see the full gallery of Seaborn or Matplotlib . #disable some annoying warnings import warnings warnings . filterwarnings ( 'ignore' , category = FutureWarning ) #plots the figures in place instead of a new window % matplotlib inline import matplotlib.pyplot as plt import seaborn as sns import plotly.express as px import pandas as pd import numpy as np In this blog, we are going to look into the Airbnb data for London. We will look at some trends, patterns and effect of seasonality on the data. First, let us ponder over the popularity of Airbnb over time. The popularity is proportional to the number of reviews. Importing the reviews dataset: reviews = pd . read_csv ( 'reviews.csv.gz' , parse_dates = [ 'date' ]) reviews . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } listing_id id date reviewer_id reviewer_name comments 0 11551 30672 2010-03-21 93896 Shar-Lyn The flat was bright, comfortable and clean and... 1 11551 32236 2010-03-29 97890 Zane We stayed with Adriano and Valerio for a week ... 2 11551 41044 2010-05-09 104133 Chase Adriano was a fantastic host. We felt very at ... 3 11551 48926 2010-06-01 122714 John & Sylvia We had a most wonderful stay with Adriano and ... 4 11551 58352 2010-06-28 111543 Monique I'm not sure which of us misunderstood the s... Scatterplot \u00b6 To find the number of reviews, we add the total reviews everyday and we then plot it across time as a scatterplot. fs , axs = plt . subplots ( 1 , figsize = ( 15 , 10 )) plt . title ( \"Airbnb London: Number of reviews across time\" , fontsize = 30 ) reviews . groupby ( 'date' ) . aggregate ({ 'listing_id' : 'count' }) . reset_index () . plot . scatter ( x = 'date' , y = 'listing_id' , alpha = 0.25 , ax = axs ) axs . set_ylabel ( 'Number of reviews' , fontsize = 20 ) axs . set_xlabel ( 'Time' , fontsize = 20 ) plt . show () From this plot, we can observe the following: 1. There is an exponential growth in the business pre-pandemic and this has a sudden drop after Covid related restrictions started. 2. Seasonality within every year is visible To expand n these trends, we should zoom in two sections of the plot. First we should find the seasonality and trend of the data, and then zoom into one of the pre-pandemic year to elaborate on the seasonality. Second, we can zoom into 2020-21 to identify the patterns from covid related lockdowns. # getting the total number of reviews per day reviews_time = reviews . groupby ( 'date' ) . aggregate ({ 'listing_id' : 'count' }) . reset_index () reviews_time [ 'year' ] = reviews_time . date . dt . year # Pre covid data reviews_time_proper = reviews_time [( reviews_time . year < 2020 )] We can find seasonality and trend within the pre-pandemic data by using decomposition (not explained in this blog). Splitting the data into trend, seasonal and random component, gives me the following: from statsmodels.tsa.seasonal import seasonal_decompose result = seasonal_decompose ( reviews_time_proper . listing_id , model = 'additive' , period = 365 ) print ( result . plot ()) We can see a exponential trend and a repeating constant seasonality within the data. Identifying and predicting pre-pandemic trend and predicting for 2020-21. y_values = result . trend [ 182 : 3136 ] x_values = range ( 182 , 3136 ) coeffs = np . polyfit ( x_values , y_values , 2 ) poly_eqn = np . poly1d ( coeffs ) y_hat = poly_eqn ( range ( 365 , len ( reviews_time [ reviews_time . year < 2020 ]))) y_hat1 = poly_eqn ( range ( len ( reviews_time [ reviews_time . year < 2020 ]), len ( reviews_time ))) Approximating the seasonality by using a polynomial equation. y_values = reviews_time [ reviews_time . year == 2019 ] . listing_id x_values = range ( 365 ) coeffs = np . polyfit ( x_values , y_values , 15 ) poly_eqn = np . poly1d ( coeffs ) y_hat_seasonal = poly_eqn ( range ( 365 )) Approximating the pattern in 2020-21 with a polynomial equation. reviews_covid = reviews_time [ reviews_time . year >= 2020 ] y_values = reviews_covid . listing_id x_values = range ( len ( reviews_covid . listing_id )) coeffs = np . polyfit ( x_values , y_values , 17 ) poly_eqn = np . poly1d ( coeffs ) y_hat_covid = poly_eqn ( range ( 25 , len ( reviews_covid . listing_id ) - 15 )) from mpl_toolkits.axes_grid1.inset_locator import mark_inset , inset_axes from matplotlib.patches import ConnectionPatch # making lines from top lot to below plot # Two plots, the main on the top with height 20 inches and the bottom one is 10 inches. fs , axs = plt . subplots ( 2 , figsize = ( 20 , 30 ), gridspec_kw = { 'height_ratios' : [ 2 , 1 ]}, constrained_layout = True ) # Title plt . suptitle ( \"Airbnb London: Number of reviews across time\" , fontsize = 30 ) # First plot, main scatterplot reviews . groupby ( 'date' ) . aggregate ({ 'listing_id' : 'count' }) . \\ reset_index () . plot . scatter ( x = 'date' , y = 'listing_id' , alpha = 0.25 , ax = axs [ 0 ]) # adding the trend lines axs [ 0 ] . plot ( reviews_time [ 365 : len ( reviews_time [ reviews_time . year < 2020 ])] . date , y_hat , color = 'red' ) axs [ 0 ] . plot ( reviews_time [ len ( reviews_time [ reviews_time . year < 2020 ]):] . date , y_hat1 , color = 'red' , linestyle = 'dashed' ) # Modifying the labels and title axs [ 0 ] . set_ylabel ( 'Number of reviews' , fontsize = 15 ) axs [ 0 ] . set_xlabel ( 'Time' , fontsize = 15 ) axs [ 0 ] . set_title ( 'Total reviews of all types of rooms across London. A trend line is plotted taking the exponential growth of the business before Covid 19 and projecting the same trend during Covid. \\n ' + 'Seasonality before Covid is shown by zooming for 2019 (sample year). The affect of covid related lockdowns is also shown by zooming from 2020 onwards.' , fontsize = 15 , loc = 'left' ) # Plotting the data within 2019 as a semantic zooming axins = inset_axes ( axs [ 0 ], 8 , 5 , loc = 2 , bbox_to_anchor = ( 0.15 , 0.925 ), bbox_transform = axs [ 0 ] . figure . transFigure ) # Semantic zooming plot, scatterplot reviews . groupby ( 'date' ) . aggregate ({ 'listing_id' : 'count' }) . reset_index () . \\ plot . scatter ( x = 'date' , y = 'listing_id' , alpha = 0.5 , ax = axins ) #adding the trend lines, labels and title plt . plot ( reviews_time [ reviews_time . year == 2019 ] . date , y_hat_seasonal , color = 'red' ) plt . ylabel ( 'Number of reviews' , fontsize = 15 ) plt . xlabel ( 'Date' , fontsize = 15 ) plt . title ( 'Sesonality in a year (before COVID 19)' , fontsize = 20 ) # Seasonality plot x and y limits x1 = min ( reviews_time [ reviews_time . year == 2019 ] . date ) x2 = max ( reviews_time [ reviews_time . year == 2019 ] . date ) axins . set_xlim ( x1 , x2 ) axins . set_ylim ( 0 , 2000 ) mark_inset ( axs [ 0 ], axins , loc1 = 1 , loc2 = 3 , fc = \"none\" , ec = \"0.5\" ) # Second plot x1 = min ( reviews_time [ reviews_time . year == 2020 ] . date ) x2 = max ( reviews_time [ reviews_time . year >= 2020 ] . date ) reviews_time [ reviews_time . year >= 2020 ] . plot . scatter ( x = 'date' , y = 'listing_id' , alpha = 0.75 , ax = axs [ 1 ]) axs [ 1 ] . set_ylabel ( 'Number of reviews' , fontsize = 15 ) axs [ 1 ] . set_xlabel ( 'Date' , fontsize = 15 ) axs [ 1 ] . set_ylim ( 0 , 1200 ) axs [ 1 ] . set_xlim ( x1 , x2 ) axs [ 1 ] . set_title ( 'Effect of Covid19 on number of reviews' , fontsize = 20 ) axs [ 1 ] . plot ( reviews_covid . date [ 25 : - 15 ], y_hat_covid , color = 'red' ) # Adding annotations in the plot axs [ 1 ] . annotate ( text = 'First Covid 19 advisory \\n 3-16-2020' , # the text xy = ( '3-16-2020' , 500 ), #what to annotate xytext = ( '3-16-2020' , 700 ), # where the text should be arrowprops = dict ( arrowstyle = \"->\" , connectionstyle = \"angle3,angleA=-90,angleB=0\" ), fontsize = 15 ) axs [ 1 ] . annotate ( 'First Lockdown \\n 3-23-2020' , xy = ( '3-23-2020' , 350 ), xytext = ( '5-10-2020' , 600 ), arrowprops = dict ( arrowstyle = \"->\" ), fontsize = 15 ) axs [ 1 ] . annotate ( 'Easing restrictions \\n 7-4-2020' , xy = ( '7-4-2020' , 50 ), xytext = ( '6-15-2020' , 700 ), arrowprops = dict ( arrowstyle = \"->\" ), fontsize = 15 ) axs [ 1 ] . annotate ( 'Restrictions eased further \\n 8-14-2020' , xy = ( '8-14-2020' , 250 ), xytext = ( '8-1-2020' , 600 ), arrowprops = dict ( arrowstyle = \"->\" ), fontsize = 15 ) axs [ 1 ] . annotate ( 'Second Lockdown \\n 10-31-2020' , xy = ( '10-31-2020' , 165 ), xytext = ( '10-1-2020' , 700 ), arrowprops = dict ( arrowstyle = \"->\" ), fontsize = 15 ) axs [ 1 ] . annotate ( 'Easing restrictions \\n 12-2-2020' , xy = ( '12-2-2020' , 120 ), xytext = ( '11-10-2020' , 600 ), arrowprops = dict ( arrowstyle = \"->\" ), fontsize = 15 ) axs [ 1 ] . annotate ( 'Christmas \\n 12-25-2020' , xy = ( '12-25-2020' , 160 ), xytext = ( '12-25-2020' , 700 ), arrowprops = dict ( arrowstyle = \"->\" ), fontsize = 15 ) axs [ 1 ] . annotate ( 'Third Lockdown \\n 1-6-2021' , xy = ( '1-6-2021' , 140 ), xytext = ( '1-20-2021' , 600 ), arrowprops = dict ( arrowstyle = \"->\" ), fontsize = 15 ) axs [ 1 ] . annotate ( 'Schools reopen \\n 3-8-2021' , xy = ( '3-8-2021' , 125 ), xytext = ( '2-25-2021' , 700 ), arrowprops = dict ( arrowstyle = \"->\" ), fontsize = 15 ) axs [ 1 ] . annotate ( 'All restrictions removed \\n 6-21-2021' , xy = ( '6-21-2021' , 400 ), xytext = ( '5-21-2021' , 700 ), arrowprops = dict ( arrowstyle = \"->\" ), fontsize = 15 ) axs [ 1 ] . annotate ( 'Non essentials reopen \\n 4-12-2021' , xy = ( '4-12-2021' , 270 ), xytext = ( '4-1-2021' , 600 ), arrowprops = dict ( arrowstyle = \"->\" ), fontsize = 15 ) # plotting the connections between the two plots con = ConnectionPatch ( xyA = ( x1 , - 105 ), xyB = ( x1 , 1200 ), coordsA = \"data\" , coordsB = \"data\" , axesA = axs [ 0 ], axesB = axs [ 1 ]) axs [ 1 ] . add_artist ( con ) # con = ConnectionPatch(axesA=axs[0], axesB=axs[1]) con = ConnectionPatch ( xyA = ( x2 , - 105 ), xyB = ( x2 , 1200 ), coordsA = \"data\" , coordsB = \"data\" , axesA = axs [ 0 ], axesB = axs [ 1 ]) axs [ 1 ] . add_artist ( con ) plt . show () From this plot, we can see the following: 1. The variation of the reviews across time and the trend before the pandemic are captured. The trend is extrapolated to 2020-21 to show the growth that could have happened if not for the pandemic. 2. Seasonality within the data is shown by semantic zooming into one sample year. We can see how Airbnb is more popular in July, September and January. 3. We can also see the effect of the panemic on the number of reviews. We can observe a sharp decline in the first few months of 2020, and then how lockdowns and openings have affected the total number of reviews. Sunburst and pie charts \u00b6 Let us now deep dive into the data and look at the type of listings and locations that have contributed to this growth. Importing the complete listings dataset. listing_detailed = pd . read_csv ( 'listings.csv.gz' ) pd . options . display . max_columns = None # to show all the columns listing_detailed . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id listing_url scrape_id last_scraped name description neighborhood_overview picture_url host_id host_url host_name host_since host_location host_about host_response_time host_response_rate host_acceptance_rate host_is_superhost host_thumbnail_url host_picture_url host_neighbourhood host_listings_count host_total_listings_count host_verifications host_has_profile_pic host_identity_verified neighbourhood neighbourhood_cleansed neighbourhood_group_cleansed latitude longitude property_type room_type accommodates bathrooms bathrooms_text bedrooms beds amenities price minimum_nights maximum_nights minimum_minimum_nights maximum_minimum_nights minimum_maximum_nights maximum_maximum_nights minimum_nights_avg_ntm maximum_nights_avg_ntm calendar_updated has_availability availability_30 availability_60 availability_90 availability_365 calendar_last_scraped number_of_reviews number_of_reviews_ltm number_of_reviews_l30d first_review last_review review_scores_rating review_scores_accuracy review_scores_cleanliness review_scores_checkin review_scores_communication review_scores_location review_scores_value license instant_bookable calculated_host_listings_count calculated_host_listings_count_entire_homes calculated_host_listings_count_private_rooms calculated_host_listings_count_shared_rooms reviews_per_month 0 11551 https://www.airbnb.com/rooms/11551 20210706215658 2021-07-08 Arty and Bright London Apartment in Zone 2 Unlike most rental apartments my flat gives yo... Not even 10 minutes by metro from Victoria Sta... https://a0.muscache.com/pictures/b7afccf4-18e5... 43039 https://www.airbnb.com/users/show/43039 Adriano 2009-10-03 London, England, United Kingdom Hello, I'm a friendly Italian man with a posit... within an hour 100% 85% f https://a0.muscache.com/im/pictures/user/5f182... https://a0.muscache.com/im/pictures/user/5f182... Brixton 0.0 0.0 ['email', 'phone', 'reviews', 'jumio', 'offlin... t t London, United Kingdom Lambeth NaN 51.46095 -0.11758 Entire apartment Entire home/apt 4 NaN 1 bath 1.0 3.0 [\"Hot water\", \"Hair dryer\", \"Smoke alarm\", \"Fi... $99.00 2 1125 2.0 2.0 1125.0 1125.0 2.0 1125.0 NaN t 0 30 58 290 2021-07-08 193 1 0 2011-10-11 2018-04-29 4.57 4.62 4.58 4.78 4.85 4.53 4.52 NaN f 3 3 0 0 1.63 1 13913 https://www.airbnb.com/rooms/13913 20210706215658 2021-07-08 Holiday London DB Room Let-on going My bright double bedroom with a large window h... Finsbury Park is a friendly melting pot commun... https://a0.muscache.com/pictures/miso/Hosting-... 54730 https://www.airbnb.com/users/show/54730 Alina 2009-11-16 London, England, United Kingdom I am a Multi-Media Visual Artist and Creative ... within a few hours 100% 100% f https://a0.muscache.com/im/users/54730/profile... https://a0.muscache.com/im/users/54730/profile... LB of Islington 3.0 3.0 ['email', 'phone', 'facebook', 'reviews', 'off... t t Islington, Greater London, United Kingdom Islington NaN 51.56861 -0.11270 Private room in apartment Private room 2 NaN 1 shared bath 1.0 0.0 [\"Host greets you\", \"Dryer\", \"Hot water\", \"Sha... $65.00 1 29 1.0 1.0 29.0 29.0 1.0 29.0 NaN t 30 60 90 365 2021-07-08 21 0 0 2011-07-11 2011-09-13 4.85 4.79 4.84 4.79 4.89 4.63 4.74 NaN f 2 1 1 0 0.17 2 15400 https://www.airbnb.com/rooms/15400 20210706215658 2021-07-08 Bright Chelsea Apartment. Chelsea! Lots of windows and light. St Luke's Gardens ... It is Chelsea. https://a0.muscache.com/pictures/428392/462d26... 60302 https://www.airbnb.com/users/show/60302 Philippa 2009-12-05 Kensington, England, United Kingdom English, grandmother, I have travelled quite ... NaN NaN NaN f https://a0.muscache.com/im/users/60302/profile... https://a0.muscache.com/im/users/60302/profile... Chelsea 1.0 1.0 ['email', 'phone', 'reviews', 'jumio', 'govern... t t London, United Kingdom Kensington and Chelsea NaN 51.48780 -0.16813 Entire apartment Entire home/apt 2 NaN 1 bath 1.0 1.0 [\"Dryer\", \"Hot water\", \"Shampoo\", \"Hair dryer\"... $75.00 10 50 10.0 10.0 50.0 50.0 10.0 50.0 NaN t 0 14 44 319 2021-07-08 89 0 0 2012-07-16 2019-08-10 4.79 4.84 4.88 4.87 4.82 4.93 4.73 NaN t 1 1 0 0 0.81 3 17402 https://www.airbnb.com/rooms/17402 20210706215658 2021-07-08 Superb 3-Bed/2 Bath & Wifi: Trendy W1 You'll have a wonderful stay in this superb mo... Location, location, location! You won't find b... https://a0.muscache.com/pictures/39d5309d-fba7... 67564 https://www.airbnb.com/users/show/67564 Liz 2010-01-04 Brighton and Hove, England, United Kingdom We are Liz and Jack. We manage a number of ho... within a day 70% 90% f https://a0.muscache.com/im/users/67564/profile... https://a0.muscache.com/im/users/67564/profile... Fitzrovia 18.0 18.0 ['email', 'phone', 'reviews', 'jumio', 'offlin... t t London, Fitzrovia, United Kingdom Westminster NaN 51.52195 -0.14094 Entire apartment Entire home/apt 6 NaN 2 baths 3.0 3.0 [\"Dryer\", \"Hot water\", \"Shampoo\", \"Hair dryer\"... $307.00 4 365 4.0 4.0 365.0 365.0 4.0 365.0 NaN t 6 6 17 218 2021-07-08 43 1 1 2011-09-18 2019-11-02 4.69 4.80 4.68 4.66 4.66 4.85 4.59 NaN f 15 15 0 0 0.36 4 17506 https://www.airbnb.com/rooms/17506 20210706215658 2021-07-08 Boutique Chelsea/Fulham Double bed 5-star ensuite Enjoy a chic stay in this elegant but fully mo... Fulham is 'villagey' and residential \u2013 a real ... https://a0.muscache.com/pictures/11901327/e63d... 67915 https://www.airbnb.com/users/show/67915 Charlotte 2010-01-05 London, England, United Kingdom Named best B&B by The Times. Easy going hosts,... NaN NaN NaN f https://a0.muscache.com/im/users/67915/profile... https://a0.muscache.com/im/users/67915/profile... Fulham 3.0 3.0 ['email', 'phone', 'jumio', 'selfie', 'governm... t t London, United Kingdom Hammersmith and Fulham NaN 51.47935 -0.19743 Private room in townhouse Private room 2 NaN 1 private bath 1.0 1.0 [\"Air conditioning\", \"Carbon monoxide alarm\", ... $150.00 3 21 3.0 3.0 21.0 21.0 3.0 21.0 NaN t 29 59 89 364 2021-07-08 0 0 0 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN f 2 0 2 0 NaN Looking at the number of reviews by room type and by location, we have: listing_detailed . groupby ( 'room_type' ) . \\ aggregate ({ 'number_of_reviews' : 'sum' }) . \\ plot . pie ( y = 'number_of_reviews' , figsize = ( 10 , 10 ), autopct = ' %1.1f%% ' # to add the percentages text ) plt . ylabel ( \"\" ) plt . title ( \"Airbnb London: Number of reviews by room type\" , fontsize = 20 ) plt . show () # Function to combine last few classes ito one class total_other_reviews = 0 def combine_last_n ( df ): df1 = df . copy () global total_other_reviews total_other_reviews = ( sum ( df [ df . number_of_reviews <= 5000 ] . number_of_reviews )) df1 . loc [ df . number_of_reviews <= 10000 , 'number_of_reviews' ] = 0 return df1 . number_of_reviews listing_detailed . groupby ( 'neighbourhood_cleansed' ) . \\ aggregate ({ 'number_of_reviews' : 'sum' }) . \\ sort_values ( 'number_of_reviews' , ascending = False ) . \\ assign ( no_reviews_alt = combine_last_n ) . \\ append ( pd . Series ({ 'number_of_reviews' : 0 , 'no_reviews_alt' : total_other_reviews }, name = 'Others' )) . \\ plot . pie ( y = 'no_reviews_alt' , figsize = ( 10 , 10 ), legend = None , rotatelabels = True , wedgeprops = dict ( width = .5 ) # for donut shape ) plt . ylabel ( \"\" ) We can see that private room is the most popular with the most number of reviews followed by entire home/apt. The others are insignificant. Similarly, Westminster and Camden are the top two locations in London. Using a sunburst chart, we can look at these two combined. listings_sunburst = listing_detailed . groupby ([ 'room_type' , 'neighbourhood_cleansed' ]) . \\ aggregate ({ 'number_of_reviews' : 'sum' }) . \\ sort_values ( 'number_of_reviews' , ascending = False ) . reset_index () fig = px . sunburst ( listings_sunburst , path = [ 'room_type' , 'neighbourhood_cleansed' ], values = 'number_of_reviews' , hover_data = [ 'room_type' , 'number_of_reviews' ], hover_name = 'neighbourhood_cleansed' , title = 'Airbnb London: Popularity sunburst chart' , width = 900 , height = 900 ) fig . show () Parallel Coordinates \u00b6 From this chart, we can see that Westminster is the most popular location for all the room types, the second and the third popular are different for different room types. Let us take Kensington and Chelsea for example, we can see the ranking of this area in every room type using a parallel cordinates plot. top_20_names = list ( listings_sunburst . sort_values ( 'number_of_reviews' , ascending = False ) . head ( 30 )[ 'neighbourhood_cleansed' ]) listings_sunburst_wide = listings_sunburst . \\ pivot ( index = 'neighbourhood_cleansed' , columns = 'room_type' , values = 'number_of_reviews' ) . reset_index () listings_sunburst_wide = listings_sunburst_wide . replace ( np . nan , 0 ) fig = px . parallel_coordinates ( listings_sunburst_wide ) # add the pink line to highlight Kensington fig . data [ 0 ][ 'dimensions' ][ 0 ][ 'constraintrange' ] = [ 50000 , 60000 ] fig . update_layout ( title_text = 'Kensington and Chelsea (in blue) popularity ranking across room types' , title_x = 0.7 , title_y = 0.05 ) fig . show () From this chart we can see how Kensington (highlighted in Blue) is in top 2 for Entire home Apartment while its not in the top 5 for shared room. This chart, along with the sunburst above shows what type of locations are popular for different room types. Bar chart and Steam graphs \u00b6 How does this ratio between the popularities change with time? One way to see this is using a stacked bar chart. reviews_detailed = pd . merge ( listing_detailed [[ 'id' , 'neighbourhood_cleansed' , 'room_type' ]], reviews , left_on = 'id' , right_on = 'listing_id' ) fs , axs = plt . subplots ( 1 , figsize = ( 15 , 10 )) reviews_detailed [ 'year' ] = reviews_detailed . date . dt . year reviews_detailed . groupby ([ 'year' , 'room_type' ]) . \\ aggregate ({ 'listing_id' : 'count' }) . \\ unstack () . reset_index () . \\ plot . bar ( x = 'year' , y = 'listing_id' , ax = axs , stacked = True ) plt . title ( 'Popularity of different rooms across the years' , fontsize = 25 ) plt . legend ( loc = 'upper right' , title = \"Type of room\" , fontsize = 'medium' , fancybox = True ) axs . set_ylabel ( 'Number of reviews' , fontsize = 20 ) axs . set_xlabel ( 'Years' , fontsize = 20 ) plt . show () From this bar chart we can see the same increase that we have seen in the scatter plot, that is an exponential increase till 2019, and a subsequent decrease due to the pandemic. Another cool way to look at this is by looking at streamgraphs. In streamgraph, we can see the effect of seasonality within the classes. fs , ax = plt . subplots ( 1 , figsize = ( 15 , 10 )) reviews_room = reviews_detailed . groupby ([ 'date' , 'room_type' ]) . \\ aggregate ({ 'listing_id' : 'count' }) . \\ unstack () . reset_index () reviews_room . columns = [ 'date' , 'Entire home/apt' , 'Hotel room' , 'Private room' , 'Shared room' ] ax . stackplot ( reviews_room . date , list ( reviews_room [[ 'Entire home/apt' , 'Hotel room' , 'Private room' , 'Shared room' ]] . fillna ( 0 ) . \\ to_numpy () . transpose ()), baseline = 'wiggle' ) plt . title ( 'Popularity of different rooms across the years' , fontsize = 20 , y = 1.05 , loc = 'left' ) ax . text ( \"2010\" , 1050 , 'Streamgraph of the number of reviews across time' , ha = 'left' , fontsize = 12 ) plt . legend ([ 'Entire home/apt' , 'Hotel room' , 'Private room' , 'Shared room' ], loc = 'upper left' , title = \"Type of room\" ) ax . set_xlabel ( 'Years' , fontsize = 20 ) plt . show () Heatmap \u00b6 Which locations are better, and which locations should improve on their rating? We can find the average rating across the location by averaging out the rating for each host within the location. location_rating = listing_detailed . groupby ([ 'neighbourhood_cleansed' , 'room_type' ]) . \\ aggregate ({ 'review_scores_rating' : 'mean' }) . unstack () . reset_index () location_rating . columns = [ 'neighbourhood' , 'Entire home/apt' , 'Hotel room' , 'Private room' , 'Shared room' ] location_rating . index = location_rating . neighbourhood One of the ways to visualise the average rating is using a heatmap. fig , ax = plt . subplots ( figsize = [ 20 , len ( location_rating ) / 3.3 ]) sns . heatmap ( data = location_rating [[ 'Entire home/apt' , 'Hotel room' , 'Private room' , 'Shared room' ]], annot = False , cbar_kws = { \"shrink\" : 0.5 , \"orientation\" : 'vertical' }, linewidths = 0.004 , linecolor = 'grey' , vmin = 0 , vmax = 5 , center = 0.25 ) plt . show () Although this heatmap presents us the with the average ratings per burough, we can further add the following details for clarity: 1. Location of the burough in London (eg: Central London) 2. Arranged from the best rated to the worst rated buroughs within each location 3. Proper color selection based on scale and human rating psychology : Average human ratings below 2.5 means bad rating and above 4.5 means very good rating (out of 5). It is more natural to use a diverging red-green scale for displaying negitive-positive relationship. def add_regions ( df , borough_col_name ): \"\"\" This function takes as input a dataframe with a column which includes London's borough name Then returns the same dataframw with sub regions names added for each borough \"\"\" central = [ 'Camden' , 'City of London' , 'Kensington and Chelsea' , 'Islington' , 'Lambeth' , 'Southwark' , 'Westminster' ] east = [ 'Barking and Dagenham' , 'Bexley' , 'Greenwich' , 'Hackney' , 'Havering' , 'Lewisham' , 'Newham' , 'Redbridge' , 'Tower Hamlets' , 'Waltham Forest' ] north = [ 'Barnet' , 'Enfield' , 'Haringey' ] south = [ 'Bromley' , 'Croydon' , 'Kingston upon Thames' , 'Merton' , 'Sutton' , 'Wandsworth' ] west = [ 'Brent' , 'Ealing' , 'Hammersmith and Fulham' , 'Harrow' , 'Richmond upon Thames' , 'Hillingdon' , 'Hounslow' ] df [ 'sub_regions' ] = np . where (( df [ borough_col_name ] == central [ 0 ]) | ( df [ borough_col_name ] == central [ 1 ]) | ( df [ borough_col_name ] == central [ 2 ]) | ( df [ borough_col_name ] == central [ 3 ]) | ( df [ borough_col_name ] == central [ 4 ]) | ( df [ borough_col_name ] == central [ 5 ]) | ( df [ borough_col_name ] == central [ 6 ]) , 'Central' , 'no' ) df [ 'sub_regions' ] = np . where (( df [ borough_col_name ] == east [ 0 ]) | ( df [ borough_col_name ] == east [ 1 ]) | ( df [ borough_col_name ] == east [ 2 ]) | ( df [ borough_col_name ] == east [ 3 ]) | ( df [ borough_col_name ] == east [ 4 ]) | ( df [ borough_col_name ] == east [ 5 ]) | ( df [ borough_col_name ] == east [ 6 ]) | ( df [ borough_col_name ] == east [ 7 ]) | ( df [ borough_col_name ] == east [ 8 ]) | ( df [ borough_col_name ] == east [ 9 ]) , 'East' , df [ 'sub_regions' ]) df [ 'sub_regions' ] = np . where (( df [ borough_col_name ] == north [ 0 ]) | ( df [ borough_col_name ] == north [ 1 ]) | ( df [ borough_col_name ] == north [ 2 ]) , 'North' , df [ 'sub_regions' ]) df [ 'sub_regions' ] = np . where (( df [ borough_col_name ] == south [ 0 ]) | ( df [ borough_col_name ] == south [ 1 ]) | ( df [ borough_col_name ] == south [ 2 ]) | ( df [ borough_col_name ] == south [ 3 ]) | ( df [ borough_col_name ] == south [ 4 ]) | ( df [ borough_col_name ] == south [ 5 ]) , 'South' , df [ 'sub_regions' ]) df [ 'sub_regions' ] = np . where (( df [ borough_col_name ] == west [ 0 ]) | ( df [ borough_col_name ] == west [ 1 ]) | ( df [ borough_col_name ] == west [ 2 ]) | ( df [ borough_col_name ] == west [ 3 ]) | ( df [ borough_col_name ] == west [ 4 ]) | ( df [ borough_col_name ] == west [ 5 ]) | ( df [ borough_col_name ] == west [ 6 ]) , 'West' , df [ 'sub_regions' ]) return df def sort_data ( df ): \"\"\" Groups the data by location and sorts the data based on average rating within the location. Different locations are sorted by average rating. \"\"\" df1 = df . copy () df1 [ 'average_rating' ] = ( df1 [ 'Entire home/apt' ] + df1 [ 'Hotel room' ] + df1 [ 'Private room' ] + df1 [ 'Shared room' ]) / 4 df1 [ 'location_average' ] = df1 . groupby ( 'sub_regions' )[ 'average_rating' ] . transform ( 'mean' ) df1 = df1 . sort_values ([ 'location_average' , 'average_rating' ], ascending = False ) return df1 [[ 'Private room' , 'Entire home/apt' , 'Hotel room' , 'Shared room' , 'sub_regions' ]] def prepare_reg_annotation_lists (): \"\"\" Creates the annotation in the form of groups within the data. Displays this on the right hand side of the heatmap. \"\"\" reg_sorted_list = location_rating . sub_regions . unique () reg_len = location_rating . sub_regions . value_counts () . to_dict () sorted_len = [] cum_len = [] arrow_style_str_list = [] # here we define the width of the bracket used, which is proportional to the number of boroughs within a sub region for i in range ( 5 ): if i == 0 : value = reg_len [ reg_sorted_list [ i ]] cum_value = value else : value = reg_len [ reg_sorted_list [ i ]] cum_value += value sorted_len . append ( value ) cum_len . append ( cum_value ) arrow_style_str_list . append ( '-[,widthB=' + str (( value / 1.2 ) - 0.5 ) + ',lengthB=0.7' ) # here we define ticks which represent the center location of each sub region relative to the heatmap Ticks = [] for i in range ( 5 ): if i == 0 : Ticks . append ( 1 - (( cum_len [ i ] / 2 ) / 33 )) else : Ticks . append ( 1 - (((( cum_len [ i ] - cum_len [ i - 1 ]) / 2 ) + cum_len [ i - 1 ]) / 33 )) return Ticks , arrow_style_str_list , reg_sorted_list location_rating = location_rating . replace ( np . nan , 2.5 ) location_rating = add_regions ( location_rating , 'neighbourhood' ) location_rating = sort_data ( location_rating ) Ticks_h , arrow , region = prepare_reg_annotation_lists () A diverging red-green palatte is chosen to represent good reviews and bad reviews. red_green_cmap = sns . diverging_palette ( 10 , 133 , as_cmap = True ) red_green_cmap fig , ax = plt . subplots ( figsize = [ 20 , len ( location_rating ) / 3.3 ]) sns . heatmap ( data = location_rating [[ 'Private room' , 'Entire home/apt' , 'Hotel room' , 'Shared room' ]], annot = False , cbar_kws = { \"shrink\" : 0.5 , \"orientation\" : 'vertical' }, linewidths = 0.004 , linecolor = 'grey' , vmin = 2.25 , vmax = 4.75 , cmap = red_green_cmap ) plt . title ( \"Average ratings for different locations in London\" , fontsize = 20 , y = 1.1 , loc = 'left' ) plt . text ( 0 , - 1 , 'Heatmap depicting the ratings among different locations in London. If no rating is available, minimum rating of 2.5 is assumed. \\n Good ratings are ratings above 3.5 while bad ratings are below. The data is grouped by location (right) and sorted by average rating.' , ha = 'left' , fontsize = 12 ) ax . set_ylabel ( '' ) #annotation for the borough for i in range ( 5 ): ax . annotate ( region [ i ], xy = ( 1.01 , Ticks_h [ i ]), xytext = ( 1.02 , Ticks_h [ i ]), xycoords = 'axes fraction' , ha = 'left' , va = 'center' , arrowprops = dict ( arrowstyle = arrow [ i ], lw = 1 )) We can see the ratings are good across the private rooms and entire home. The best location in each zone is: - West: Richmond upon Thames - Central: Camden - North: Enfield - East: Hackney - South: Croydon Treemap \u00b6 In this context, its not fair to compare ratings of different locations as we have seen that their popularities are different. So there could be 10 reviews in one location while 100 reviews in another. To combine them, we can use a treemap. reviews_treemap = listing_detailed . groupby ([ 'neighbourhood_cleansed' ]) . \\ aggregate ({ 'review_scores_rating' : 'mean' , 'number_of_reviews' : 'sum' }) . reset_index () reviews_treemap = add_regions ( reviews_treemap , 'neighbourhood_cleansed' ) #change col names for nice viz on hover reviews_treemap . columns = [ 'neighbourhood' , 'Average Reviews' , 'Number of reviews' , 'regions' ] fig = px . treemap ( reviews_treemap , path = [ px . Constant ( \"London\" ), 'regions' , 'neighbourhood' ], values = 'Number of reviews' , color = 'Average Reviews' , color_continuous_scale = 'RdBu' ) fig . update_layout ( margin = dict ( t = 50 , l = 25 , r = 25 , b = 25 )) fig . update_layout ( title_text = 'Airbnb London: Ratings overview' ) Wordcloud \u00b6 Now that we have classified the ratings into good ratings and bad ratings, let us look at the text in these ratings and identify if there are any patterns. reviews_detailed_text = pd . merge ( listing_detailed [[ 'id' , 'description' , 'neighborhood_overview' , 'host_about' , 'review_scores_rating' ]], reviews , left_on = 'id' , right_on = 'listing_id' ) reviews_detailed_positive_text = reviews_detailed_text [ reviews_detailed_text . review_scores_rating > 3.75 ] reviews_detailed_negative_text = reviews_detailed_text [ reviews_detailed_text . review_scores_rating <= 3.75 ] Selecting 100 random reviews each for positive and negative sets. pos_reviews_text = reviews_detailed_positive_text . sample ( n = 100 , random_state = 2 ) . comments . str . cat () neg_reviews_text = reviews_detailed_negative_text . sample ( n = 100 , random_state = 3 ) . comments . str . cat () Wordcloud for positive reviews # !pip install wordcloud from wordcloud import WordCloud , STOPWORDS , ImageColorGenerator from PIL import Image mask_pos = np . array ( Image . open ( 'thumbs-up-xxl.png' )) # word cloud, good vs bad ratings stop_words = [ \"https\" , \"co\" , \"RT\" , 'br' , '<br>' , '<br/>' , ' \\r ' , 'r' ] + list ( STOPWORDS ) wordcloud_pattern = WordCloud ( stopwords = stop_words , background_color = \"white\" , max_words = 2000 , max_font_size = 256 , random_state = 42 , mask = mask_pos , width = mask_pos . shape [ 1 ], height = mask_pos . shape [ 0 ]) wordcloud_positive = wordcloud_pattern . generate ( pos_reviews_text ) plt . imshow ( wordcloud_positive , interpolation = 'bilinear' ) plt . axis ( \"off\" ) plt . show () Wordcloud for negative reviews mask_neg = np . array ( Image . open ( 'thumbs-down-xxl.png' )) # word cloud, good vs bad ratings wordcloud_pattern = WordCloud ( stopwords = stop_words , background_color = \"white\" , max_words = 2000 , max_font_size = 256 , random_state = 42 , mask = mask_neg , width = mask_neg . shape [ 1 ], height = mask_neg . shape [ 0 ]) wordcloud_neg = wordcloud_pattern . generate ( neg_reviews_text ) plt . imshow ( wordcloud_neg , interpolation = 'bilinear' ) plt . axis ( \"off\" ) plt . show () Combining the positive and negative reviews in one plot to compare the differences: fs , axs = plt . subplots ( 1 , 2 , figsize = ( 20 , 10 )) plt . suptitle ( \"Airbnb London: Wordcloud of positive and negative reviews\" , fontsize = 20 ) plt . figtext ( 0.5 , 0.925 , 'Wordcloud derived from a random sample of 100 positive and 100 negative reviews.' , wrap = True , horizontalalignment = 'center' , fontsize = 12 ) axs [ 0 ] . imshow ( wordcloud_positive , interpolation = 'bilinear' ) axs [ 0 ] . axis ( \"off\" ) axs [ 1 ] . imshow ( wordcloud_neg , interpolation = 'bilinear' ) axs [ 1 ] . axis ( \"off\" ) plt . show () From these plots, we can see that automated postings, cancellations by hosts, and issues during arrival are the main issues that Airbnb should look into. Correlation matrix \u00b6 How are different parameters within the data related. How is ratings correlated with availability or maximum nights? This can be explained using a correlation plot. listing_detailed [ 'host_response_rate' ] = listing_detailed [ 'host_response_rate' ] . str . replace ( '%' , '' ) . astype ( float ) listing_detailed [ 'host_acceptance_rate' ] = listing_detailed [ 'host_acceptance_rate' ] . str . replace ( '%' , '' ) . astype ( float ) listing_detailed [ 'price' ] = listing_detailed [ 'price' ] . str . replace ( '$' , '' ) . str . replace ( ',' , '' ) . astype ( float ) col_for_corr = [ 'review_scores_rating' , 'review_scores_accuracy' , 'review_scores_cleanliness' , 'review_scores_checkin' , 'review_scores_communication' , 'review_scores_location' , 'review_scores_value' , 'number_of_reviews' , 'number_of_reviews_ltm' , 'number_of_reviews_l30d' , 'reviews_per_month' , 'availability_30' , 'availability_60' , 'availability_90' , 'availability_365' , 'minimum_nights' , 'maximum_nights' , 'minimum_minimum_nights' , 'maximum_minimum_nights' , 'minimum_maximum_nights' , 'maximum_maximum_nights' , 'minimum_nights_avg_ntm' , 'maximum_nights_avg_ntm' , 'bedrooms' , 'beds' , 'accommodates' , 'price' , 'host_response_rate' , 'host_acceptance_rate' , 'host_total_listings_count' ] f = plt . figure ( figsize = ( 20 , 20 )) plt . matshow ( listing_detailed [ col_for_corr ] . corr (), fignum = f , cmap = red_green_cmap , vmin =- 1 , vmax = 1 ) plt . xticks ( range ( listing_detailed [ col_for_corr ] . select_dtypes ([ 'number' ]) . shape [ 1 ]), listing_detailed [ col_for_corr ] . select_dtypes ([ 'number' ]) . columns , rotation = 90 , fontsize = 15 ) plt . yticks ( range ( listing_detailed [ col_for_corr ] . select_dtypes ([ 'number' ]) . shape [ 1 ]), listing_detailed [ col_for_corr ] . select_dtypes ([ 'number' ]) . columns , rotation = 0 , fontsize = 15 ) cb = plt . colorbar () cb . ax . tick_params ( labelsize = 14 ) plt . title ( \"Airbnb London: Correlation between different parameters\" , fontsize = 20 , loc = 'left' ) plt . text ( 0 , 32 , 'Correlation matrix displaying the different parameters within the data. DIverging green (+ve) red (-ve) scale is used to display the correlations. \\n \\ Features considered: Review scores, Number of reviews, availability, maximum and minimum days of stay, host and room parameters.' , ha = 'left' , fontsize = 12 ) plt . show () Scatterplot matrix \u00b6 While the above plot shows the correlation across various variables, I want to deep dive into change in ratings with price. I can use a scatterplot matrix to visualise this. Aditionally, I want the costliest properties, and the most popular yet cheap properties annotated. def annotate_plot ( x , y , ** kwargs ): if ( x . name == 'price' and y . name == 'review_scores_rating' ): ax = plt . gca () for index , obj in listing_detailed . nlargest ( 2 , 'price' ) . iterrows (): plt . annotate ( obj [ 'name' ], # the text xy = ( obj . price , obj . review_scores_rating ), xytext = ( 7500 , obj . review_scores_rating - 0.5 ), arrowprops = dict ( arrowstyle = \"->\" ) ) elif ( x . name == 'price' and y . name == 'number_of_reviews' ): ax = plt . gca () for index , obj in listing_detailed . nlargest ( 3 , 'number_of_reviews' ) . iterrows (): ax . text ( obj . price , obj . number_of_reviews , obj [ 'name' ]) col_for_pairplot = [ 'review_scores_rating' , 'number_of_reviews' , 'price' ] sns_plot = sns . pairplot ( listing_detailed , vars = col_for_pairplot , kind = 'scatter' , hue = 'room_type' , diag_kind = 'kde' ,) sns_plot . fig . set_size_inches ( 20 , 20 ) sns_plot . _legend . set_bbox_to_anchor (( 0.15 , 0.89 )) sns_plot . map_upper ( annotate_plot ) sns_plot . fig . suptitle ( \"Airbnb London: Scatterplot matrix\" , fontsize = 20 , y = 1 ) We can see that the two costliest properties are either historic apartments or a mansion. The three most popular yet cheap properties are small and quaint properties near popular destinations. Boxplot and Violin chart \u00b6 To look at the variation in ratings within the different room types, we could use either a boxplot or a Violin plot as shown. fs , axs = plt . subplots ( 1 , 2 , figsize = ( 20 , 20 )) listing_detailed . boxplot ( column = 'review_scores_rating' , by = 'room_type' , figsize = ( 10 , 20 ), ax = axs [ 0 ]) sns . violinplot ( 'room_type' , 'review_scores_rating' , data = listing_detailed , ax = axs [ 1 ]) plt . suptitle ( \"Airbnb London: Average rating across room types\" , fontsize = 20 , y = 0.95 ) plt . figtext ( 0.5 , 0.925 , 'Boxplot (left) and Violin plot (right) for the average review across room types' , wrap = True , horizontalalignment = 'center' , fontsize = 12 ) axs [ 0 ] . set_title ( '' ) for ax in axs : ax . set_ylim ( - 1 , 6 ) ax . set_ylabel ( 'Average Rating' , fontsize = 12 ) ax . set_xlabel ( 'Room types' , fontsize = 12 ) Cluster map \u00b6 If we wanted to cluster localities based on some features, then cluster map is teh ideal choice. In the below map, we cluster different locations based on one feature from each type. The features are also clustered to show the similarity between features. Finally we use a white-blue color palatte for displaying the variation within the data. from sklearn.preprocessing import MinMaxScaler import seaborn as sns col_for_corr = [ 'price' , 'review_scores_rating' , 'number_of_reviews' , 'availability_90' , 'minimum_nights_avg_ntm' , 'bedrooms' , 'host_response_rate' ] df_cluster = listing_detailed . groupby ( 'neighbourhood_cleansed' ) . aggregate ({ 'review_scores_rating' : 'mean' , 'number_of_reviews' : 'sum' , 'availability_90' : 'mean' , 'minimum_nights_avg_ntm' : 'mean' , 'bedrooms' : 'mean' , 'price' : 'mean' , 'host_response_rate' : 'mean' }) . reset_index () scaler = MinMaxScaler () df_cluster1 = pd . DataFrame ( scaler . fit_transform ( df_cluster [ col_for_corr ]), columns = col_for_corr ) df_cluster1 . index = df_cluster . neighbourhood_cleansed crest_cmap = sns . color_palette ( \"crest\" , as_cmap = True ) crest_cmap g = sns . clustermap ( df_cluster1 , cmap = crest_cmap , vmin = 0 , vmax = 1 ) plt . title ( \"Airbnb London: Clusters within London\" , fontsize = 20 , loc = 'left' , y = 2 , x = - 25 ) g . ax_cbar . set_position (( 1 , .2 , .03 , .4 )) g . ax_heatmap . set_ylabel ( \"\" ) plt . show () References \u00b6 Visualisation Analytics and Design, Tamara Munzner Class notes and asignments, Imperial College London Visual Analytics lab at JKU Linz","title":"Vizualizing tabular data (Python)"},{"location":"Python/Vizualisation%20using%20python%20Part%201/#visualizalising-tabular-data","text":"Author: Achyuthuni Sri Harsha In this visualistion, we look at various visualisation types on datatype tables. Matplotlib is the most popular library for viz in Python. Seaborn is built on top of it with integrated analysis, specialized plots, and pretty good integration with Pandas. Plotly express is another library for viz. Also see the full gallery of Seaborn or Matplotlib . #disable some annoying warnings import warnings warnings . filterwarnings ( 'ignore' , category = FutureWarning ) #plots the figures in place instead of a new window % matplotlib inline import matplotlib.pyplot as plt import seaborn as sns import plotly.express as px import pandas as pd import numpy as np In this blog, we are going to look into the Airbnb data for London. We will look at some trends, patterns and effect of seasonality on the data. First, let us ponder over the popularity of Airbnb over time. The popularity is proportional to the number of reviews. Importing the reviews dataset: reviews = pd . read_csv ( 'reviews.csv.gz' , parse_dates = [ 'date' ]) reviews . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } listing_id id date reviewer_id reviewer_name comments 0 11551 30672 2010-03-21 93896 Shar-Lyn The flat was bright, comfortable and clean and... 1 11551 32236 2010-03-29 97890 Zane We stayed with Adriano and Valerio for a week ... 2 11551 41044 2010-05-09 104133 Chase Adriano was a fantastic host. We felt very at ... 3 11551 48926 2010-06-01 122714 John & Sylvia We had a most wonderful stay with Adriano and ... 4 11551 58352 2010-06-28 111543 Monique I'm not sure which of us misunderstood the s...","title":"Visualizalising tabular data"},{"location":"Python/Vizualisation%20using%20python%20Part%201/#scatterplot","text":"To find the number of reviews, we add the total reviews everyday and we then plot it across time as a scatterplot. fs , axs = plt . subplots ( 1 , figsize = ( 15 , 10 )) plt . title ( \"Airbnb London: Number of reviews across time\" , fontsize = 30 ) reviews . groupby ( 'date' ) . aggregate ({ 'listing_id' : 'count' }) . reset_index () . plot . scatter ( x = 'date' , y = 'listing_id' , alpha = 0.25 , ax = axs ) axs . set_ylabel ( 'Number of reviews' , fontsize = 20 ) axs . set_xlabel ( 'Time' , fontsize = 20 ) plt . show () From this plot, we can observe the following: 1. There is an exponential growth in the business pre-pandemic and this has a sudden drop after Covid related restrictions started. 2. Seasonality within every year is visible To expand n these trends, we should zoom in two sections of the plot. First we should find the seasonality and trend of the data, and then zoom into one of the pre-pandemic year to elaborate on the seasonality. Second, we can zoom into 2020-21 to identify the patterns from covid related lockdowns. # getting the total number of reviews per day reviews_time = reviews . groupby ( 'date' ) . aggregate ({ 'listing_id' : 'count' }) . reset_index () reviews_time [ 'year' ] = reviews_time . date . dt . year # Pre covid data reviews_time_proper = reviews_time [( reviews_time . year < 2020 )] We can find seasonality and trend within the pre-pandemic data by using decomposition (not explained in this blog). Splitting the data into trend, seasonal and random component, gives me the following: from statsmodels.tsa.seasonal import seasonal_decompose result = seasonal_decompose ( reviews_time_proper . listing_id , model = 'additive' , period = 365 ) print ( result . plot ()) We can see a exponential trend and a repeating constant seasonality within the data. Identifying and predicting pre-pandemic trend and predicting for 2020-21. y_values = result . trend [ 182 : 3136 ] x_values = range ( 182 , 3136 ) coeffs = np . polyfit ( x_values , y_values , 2 ) poly_eqn = np . poly1d ( coeffs ) y_hat = poly_eqn ( range ( 365 , len ( reviews_time [ reviews_time . year < 2020 ]))) y_hat1 = poly_eqn ( range ( len ( reviews_time [ reviews_time . year < 2020 ]), len ( reviews_time ))) Approximating the seasonality by using a polynomial equation. y_values = reviews_time [ reviews_time . year == 2019 ] . listing_id x_values = range ( 365 ) coeffs = np . polyfit ( x_values , y_values , 15 ) poly_eqn = np . poly1d ( coeffs ) y_hat_seasonal = poly_eqn ( range ( 365 )) Approximating the pattern in 2020-21 with a polynomial equation. reviews_covid = reviews_time [ reviews_time . year >= 2020 ] y_values = reviews_covid . listing_id x_values = range ( len ( reviews_covid . listing_id )) coeffs = np . polyfit ( x_values , y_values , 17 ) poly_eqn = np . poly1d ( coeffs ) y_hat_covid = poly_eqn ( range ( 25 , len ( reviews_covid . listing_id ) - 15 )) from mpl_toolkits.axes_grid1.inset_locator import mark_inset , inset_axes from matplotlib.patches import ConnectionPatch # making lines from top lot to below plot # Two plots, the main on the top with height 20 inches and the bottom one is 10 inches. fs , axs = plt . subplots ( 2 , figsize = ( 20 , 30 ), gridspec_kw = { 'height_ratios' : [ 2 , 1 ]}, constrained_layout = True ) # Title plt . suptitle ( \"Airbnb London: Number of reviews across time\" , fontsize = 30 ) # First plot, main scatterplot reviews . groupby ( 'date' ) . aggregate ({ 'listing_id' : 'count' }) . \\ reset_index () . plot . scatter ( x = 'date' , y = 'listing_id' , alpha = 0.25 , ax = axs [ 0 ]) # adding the trend lines axs [ 0 ] . plot ( reviews_time [ 365 : len ( reviews_time [ reviews_time . year < 2020 ])] . date , y_hat , color = 'red' ) axs [ 0 ] . plot ( reviews_time [ len ( reviews_time [ reviews_time . year < 2020 ]):] . date , y_hat1 , color = 'red' , linestyle = 'dashed' ) # Modifying the labels and title axs [ 0 ] . set_ylabel ( 'Number of reviews' , fontsize = 15 ) axs [ 0 ] . set_xlabel ( 'Time' , fontsize = 15 ) axs [ 0 ] . set_title ( 'Total reviews of all types of rooms across London. A trend line is plotted taking the exponential growth of the business before Covid 19 and projecting the same trend during Covid. \\n ' + 'Seasonality before Covid is shown by zooming for 2019 (sample year). The affect of covid related lockdowns is also shown by zooming from 2020 onwards.' , fontsize = 15 , loc = 'left' ) # Plotting the data within 2019 as a semantic zooming axins = inset_axes ( axs [ 0 ], 8 , 5 , loc = 2 , bbox_to_anchor = ( 0.15 , 0.925 ), bbox_transform = axs [ 0 ] . figure . transFigure ) # Semantic zooming plot, scatterplot reviews . groupby ( 'date' ) . aggregate ({ 'listing_id' : 'count' }) . reset_index () . \\ plot . scatter ( x = 'date' , y = 'listing_id' , alpha = 0.5 , ax = axins ) #adding the trend lines, labels and title plt . plot ( reviews_time [ reviews_time . year == 2019 ] . date , y_hat_seasonal , color = 'red' ) plt . ylabel ( 'Number of reviews' , fontsize = 15 ) plt . xlabel ( 'Date' , fontsize = 15 ) plt . title ( 'Sesonality in a year (before COVID 19)' , fontsize = 20 ) # Seasonality plot x and y limits x1 = min ( reviews_time [ reviews_time . year == 2019 ] . date ) x2 = max ( reviews_time [ reviews_time . year == 2019 ] . date ) axins . set_xlim ( x1 , x2 ) axins . set_ylim ( 0 , 2000 ) mark_inset ( axs [ 0 ], axins , loc1 = 1 , loc2 = 3 , fc = \"none\" , ec = \"0.5\" ) # Second plot x1 = min ( reviews_time [ reviews_time . year == 2020 ] . date ) x2 = max ( reviews_time [ reviews_time . year >= 2020 ] . date ) reviews_time [ reviews_time . year >= 2020 ] . plot . scatter ( x = 'date' , y = 'listing_id' , alpha = 0.75 , ax = axs [ 1 ]) axs [ 1 ] . set_ylabel ( 'Number of reviews' , fontsize = 15 ) axs [ 1 ] . set_xlabel ( 'Date' , fontsize = 15 ) axs [ 1 ] . set_ylim ( 0 , 1200 ) axs [ 1 ] . set_xlim ( x1 , x2 ) axs [ 1 ] . set_title ( 'Effect of Covid19 on number of reviews' , fontsize = 20 ) axs [ 1 ] . plot ( reviews_covid . date [ 25 : - 15 ], y_hat_covid , color = 'red' ) # Adding annotations in the plot axs [ 1 ] . annotate ( text = 'First Covid 19 advisory \\n 3-16-2020' , # the text xy = ( '3-16-2020' , 500 ), #what to annotate xytext = ( '3-16-2020' , 700 ), # where the text should be arrowprops = dict ( arrowstyle = \"->\" , connectionstyle = \"angle3,angleA=-90,angleB=0\" ), fontsize = 15 ) axs [ 1 ] . annotate ( 'First Lockdown \\n 3-23-2020' , xy = ( '3-23-2020' , 350 ), xytext = ( '5-10-2020' , 600 ), arrowprops = dict ( arrowstyle = \"->\" ), fontsize = 15 ) axs [ 1 ] . annotate ( 'Easing restrictions \\n 7-4-2020' , xy = ( '7-4-2020' , 50 ), xytext = ( '6-15-2020' , 700 ), arrowprops = dict ( arrowstyle = \"->\" ), fontsize = 15 ) axs [ 1 ] . annotate ( 'Restrictions eased further \\n 8-14-2020' , xy = ( '8-14-2020' , 250 ), xytext = ( '8-1-2020' , 600 ), arrowprops = dict ( arrowstyle = \"->\" ), fontsize = 15 ) axs [ 1 ] . annotate ( 'Second Lockdown \\n 10-31-2020' , xy = ( '10-31-2020' , 165 ), xytext = ( '10-1-2020' , 700 ), arrowprops = dict ( arrowstyle = \"->\" ), fontsize = 15 ) axs [ 1 ] . annotate ( 'Easing restrictions \\n 12-2-2020' , xy = ( '12-2-2020' , 120 ), xytext = ( '11-10-2020' , 600 ), arrowprops = dict ( arrowstyle = \"->\" ), fontsize = 15 ) axs [ 1 ] . annotate ( 'Christmas \\n 12-25-2020' , xy = ( '12-25-2020' , 160 ), xytext = ( '12-25-2020' , 700 ), arrowprops = dict ( arrowstyle = \"->\" ), fontsize = 15 ) axs [ 1 ] . annotate ( 'Third Lockdown \\n 1-6-2021' , xy = ( '1-6-2021' , 140 ), xytext = ( '1-20-2021' , 600 ), arrowprops = dict ( arrowstyle = \"->\" ), fontsize = 15 ) axs [ 1 ] . annotate ( 'Schools reopen \\n 3-8-2021' , xy = ( '3-8-2021' , 125 ), xytext = ( '2-25-2021' , 700 ), arrowprops = dict ( arrowstyle = \"->\" ), fontsize = 15 ) axs [ 1 ] . annotate ( 'All restrictions removed \\n 6-21-2021' , xy = ( '6-21-2021' , 400 ), xytext = ( '5-21-2021' , 700 ), arrowprops = dict ( arrowstyle = \"->\" ), fontsize = 15 ) axs [ 1 ] . annotate ( 'Non essentials reopen \\n 4-12-2021' , xy = ( '4-12-2021' , 270 ), xytext = ( '4-1-2021' , 600 ), arrowprops = dict ( arrowstyle = \"->\" ), fontsize = 15 ) # plotting the connections between the two plots con = ConnectionPatch ( xyA = ( x1 , - 105 ), xyB = ( x1 , 1200 ), coordsA = \"data\" , coordsB = \"data\" , axesA = axs [ 0 ], axesB = axs [ 1 ]) axs [ 1 ] . add_artist ( con ) # con = ConnectionPatch(axesA=axs[0], axesB=axs[1]) con = ConnectionPatch ( xyA = ( x2 , - 105 ), xyB = ( x2 , 1200 ), coordsA = \"data\" , coordsB = \"data\" , axesA = axs [ 0 ], axesB = axs [ 1 ]) axs [ 1 ] . add_artist ( con ) plt . show () From this plot, we can see the following: 1. The variation of the reviews across time and the trend before the pandemic are captured. The trend is extrapolated to 2020-21 to show the growth that could have happened if not for the pandemic. 2. Seasonality within the data is shown by semantic zooming into one sample year. We can see how Airbnb is more popular in July, September and January. 3. We can also see the effect of the panemic on the number of reviews. We can observe a sharp decline in the first few months of 2020, and then how lockdowns and openings have affected the total number of reviews.","title":"Scatterplot"},{"location":"Python/Vizualisation%20using%20python%20Part%201/#sunburst-and-pie-charts","text":"Let us now deep dive into the data and look at the type of listings and locations that have contributed to this growth. Importing the complete listings dataset. listing_detailed = pd . read_csv ( 'listings.csv.gz' ) pd . options . display . max_columns = None # to show all the columns listing_detailed . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id listing_url scrape_id last_scraped name description neighborhood_overview picture_url host_id host_url host_name host_since host_location host_about host_response_time host_response_rate host_acceptance_rate host_is_superhost host_thumbnail_url host_picture_url host_neighbourhood host_listings_count host_total_listings_count host_verifications host_has_profile_pic host_identity_verified neighbourhood neighbourhood_cleansed neighbourhood_group_cleansed latitude longitude property_type room_type accommodates bathrooms bathrooms_text bedrooms beds amenities price minimum_nights maximum_nights minimum_minimum_nights maximum_minimum_nights minimum_maximum_nights maximum_maximum_nights minimum_nights_avg_ntm maximum_nights_avg_ntm calendar_updated has_availability availability_30 availability_60 availability_90 availability_365 calendar_last_scraped number_of_reviews number_of_reviews_ltm number_of_reviews_l30d first_review last_review review_scores_rating review_scores_accuracy review_scores_cleanliness review_scores_checkin review_scores_communication review_scores_location review_scores_value license instant_bookable calculated_host_listings_count calculated_host_listings_count_entire_homes calculated_host_listings_count_private_rooms calculated_host_listings_count_shared_rooms reviews_per_month 0 11551 https://www.airbnb.com/rooms/11551 20210706215658 2021-07-08 Arty and Bright London Apartment in Zone 2 Unlike most rental apartments my flat gives yo... Not even 10 minutes by metro from Victoria Sta... https://a0.muscache.com/pictures/b7afccf4-18e5... 43039 https://www.airbnb.com/users/show/43039 Adriano 2009-10-03 London, England, United Kingdom Hello, I'm a friendly Italian man with a posit... within an hour 100% 85% f https://a0.muscache.com/im/pictures/user/5f182... https://a0.muscache.com/im/pictures/user/5f182... Brixton 0.0 0.0 ['email', 'phone', 'reviews', 'jumio', 'offlin... t t London, United Kingdom Lambeth NaN 51.46095 -0.11758 Entire apartment Entire home/apt 4 NaN 1 bath 1.0 3.0 [\"Hot water\", \"Hair dryer\", \"Smoke alarm\", \"Fi... $99.00 2 1125 2.0 2.0 1125.0 1125.0 2.0 1125.0 NaN t 0 30 58 290 2021-07-08 193 1 0 2011-10-11 2018-04-29 4.57 4.62 4.58 4.78 4.85 4.53 4.52 NaN f 3 3 0 0 1.63 1 13913 https://www.airbnb.com/rooms/13913 20210706215658 2021-07-08 Holiday London DB Room Let-on going My bright double bedroom with a large window h... Finsbury Park is a friendly melting pot commun... https://a0.muscache.com/pictures/miso/Hosting-... 54730 https://www.airbnb.com/users/show/54730 Alina 2009-11-16 London, England, United Kingdom I am a Multi-Media Visual Artist and Creative ... within a few hours 100% 100% f https://a0.muscache.com/im/users/54730/profile... https://a0.muscache.com/im/users/54730/profile... LB of Islington 3.0 3.0 ['email', 'phone', 'facebook', 'reviews', 'off... t t Islington, Greater London, United Kingdom Islington NaN 51.56861 -0.11270 Private room in apartment Private room 2 NaN 1 shared bath 1.0 0.0 [\"Host greets you\", \"Dryer\", \"Hot water\", \"Sha... $65.00 1 29 1.0 1.0 29.0 29.0 1.0 29.0 NaN t 30 60 90 365 2021-07-08 21 0 0 2011-07-11 2011-09-13 4.85 4.79 4.84 4.79 4.89 4.63 4.74 NaN f 2 1 1 0 0.17 2 15400 https://www.airbnb.com/rooms/15400 20210706215658 2021-07-08 Bright Chelsea Apartment. Chelsea! Lots of windows and light. St Luke's Gardens ... It is Chelsea. https://a0.muscache.com/pictures/428392/462d26... 60302 https://www.airbnb.com/users/show/60302 Philippa 2009-12-05 Kensington, England, United Kingdom English, grandmother, I have travelled quite ... NaN NaN NaN f https://a0.muscache.com/im/users/60302/profile... https://a0.muscache.com/im/users/60302/profile... Chelsea 1.0 1.0 ['email', 'phone', 'reviews', 'jumio', 'govern... t t London, United Kingdom Kensington and Chelsea NaN 51.48780 -0.16813 Entire apartment Entire home/apt 2 NaN 1 bath 1.0 1.0 [\"Dryer\", \"Hot water\", \"Shampoo\", \"Hair dryer\"... $75.00 10 50 10.0 10.0 50.0 50.0 10.0 50.0 NaN t 0 14 44 319 2021-07-08 89 0 0 2012-07-16 2019-08-10 4.79 4.84 4.88 4.87 4.82 4.93 4.73 NaN t 1 1 0 0 0.81 3 17402 https://www.airbnb.com/rooms/17402 20210706215658 2021-07-08 Superb 3-Bed/2 Bath & Wifi: Trendy W1 You'll have a wonderful stay in this superb mo... Location, location, location! You won't find b... https://a0.muscache.com/pictures/39d5309d-fba7... 67564 https://www.airbnb.com/users/show/67564 Liz 2010-01-04 Brighton and Hove, England, United Kingdom We are Liz and Jack. We manage a number of ho... within a day 70% 90% f https://a0.muscache.com/im/users/67564/profile... https://a0.muscache.com/im/users/67564/profile... Fitzrovia 18.0 18.0 ['email', 'phone', 'reviews', 'jumio', 'offlin... t t London, Fitzrovia, United Kingdom Westminster NaN 51.52195 -0.14094 Entire apartment Entire home/apt 6 NaN 2 baths 3.0 3.0 [\"Dryer\", \"Hot water\", \"Shampoo\", \"Hair dryer\"... $307.00 4 365 4.0 4.0 365.0 365.0 4.0 365.0 NaN t 6 6 17 218 2021-07-08 43 1 1 2011-09-18 2019-11-02 4.69 4.80 4.68 4.66 4.66 4.85 4.59 NaN f 15 15 0 0 0.36 4 17506 https://www.airbnb.com/rooms/17506 20210706215658 2021-07-08 Boutique Chelsea/Fulham Double bed 5-star ensuite Enjoy a chic stay in this elegant but fully mo... Fulham is 'villagey' and residential \u2013 a real ... https://a0.muscache.com/pictures/11901327/e63d... 67915 https://www.airbnb.com/users/show/67915 Charlotte 2010-01-05 London, England, United Kingdom Named best B&B by The Times. Easy going hosts,... NaN NaN NaN f https://a0.muscache.com/im/users/67915/profile... https://a0.muscache.com/im/users/67915/profile... Fulham 3.0 3.0 ['email', 'phone', 'jumio', 'selfie', 'governm... t t London, United Kingdom Hammersmith and Fulham NaN 51.47935 -0.19743 Private room in townhouse Private room 2 NaN 1 private bath 1.0 1.0 [\"Air conditioning\", \"Carbon monoxide alarm\", ... $150.00 3 21 3.0 3.0 21.0 21.0 3.0 21.0 NaN t 29 59 89 364 2021-07-08 0 0 0 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN f 2 0 2 0 NaN Looking at the number of reviews by room type and by location, we have: listing_detailed . groupby ( 'room_type' ) . \\ aggregate ({ 'number_of_reviews' : 'sum' }) . \\ plot . pie ( y = 'number_of_reviews' , figsize = ( 10 , 10 ), autopct = ' %1.1f%% ' # to add the percentages text ) plt . ylabel ( \"\" ) plt . title ( \"Airbnb London: Number of reviews by room type\" , fontsize = 20 ) plt . show () # Function to combine last few classes ito one class total_other_reviews = 0 def combine_last_n ( df ): df1 = df . copy () global total_other_reviews total_other_reviews = ( sum ( df [ df . number_of_reviews <= 5000 ] . number_of_reviews )) df1 . loc [ df . number_of_reviews <= 10000 , 'number_of_reviews' ] = 0 return df1 . number_of_reviews listing_detailed . groupby ( 'neighbourhood_cleansed' ) . \\ aggregate ({ 'number_of_reviews' : 'sum' }) . \\ sort_values ( 'number_of_reviews' , ascending = False ) . \\ assign ( no_reviews_alt = combine_last_n ) . \\ append ( pd . Series ({ 'number_of_reviews' : 0 , 'no_reviews_alt' : total_other_reviews }, name = 'Others' )) . \\ plot . pie ( y = 'no_reviews_alt' , figsize = ( 10 , 10 ), legend = None , rotatelabels = True , wedgeprops = dict ( width = .5 ) # for donut shape ) plt . ylabel ( \"\" ) We can see that private room is the most popular with the most number of reviews followed by entire home/apt. The others are insignificant. Similarly, Westminster and Camden are the top two locations in London. Using a sunburst chart, we can look at these two combined. listings_sunburst = listing_detailed . groupby ([ 'room_type' , 'neighbourhood_cleansed' ]) . \\ aggregate ({ 'number_of_reviews' : 'sum' }) . \\ sort_values ( 'number_of_reviews' , ascending = False ) . reset_index () fig = px . sunburst ( listings_sunburst , path = [ 'room_type' , 'neighbourhood_cleansed' ], values = 'number_of_reviews' , hover_data = [ 'room_type' , 'number_of_reviews' ], hover_name = 'neighbourhood_cleansed' , title = 'Airbnb London: Popularity sunburst chart' , width = 900 , height = 900 ) fig . show ()","title":"Sunburst and pie charts"},{"location":"Python/Vizualisation%20using%20python%20Part%201/#parallel-coordinates","text":"From this chart, we can see that Westminster is the most popular location for all the room types, the second and the third popular are different for different room types. Let us take Kensington and Chelsea for example, we can see the ranking of this area in every room type using a parallel cordinates plot. top_20_names = list ( listings_sunburst . sort_values ( 'number_of_reviews' , ascending = False ) . head ( 30 )[ 'neighbourhood_cleansed' ]) listings_sunburst_wide = listings_sunburst . \\ pivot ( index = 'neighbourhood_cleansed' , columns = 'room_type' , values = 'number_of_reviews' ) . reset_index () listings_sunburst_wide = listings_sunburst_wide . replace ( np . nan , 0 ) fig = px . parallel_coordinates ( listings_sunburst_wide ) # add the pink line to highlight Kensington fig . data [ 0 ][ 'dimensions' ][ 0 ][ 'constraintrange' ] = [ 50000 , 60000 ] fig . update_layout ( title_text = 'Kensington and Chelsea (in blue) popularity ranking across room types' , title_x = 0.7 , title_y = 0.05 ) fig . show () From this chart we can see how Kensington (highlighted in Blue) is in top 2 for Entire home Apartment while its not in the top 5 for shared room. This chart, along with the sunburst above shows what type of locations are popular for different room types.","title":"Parallel Coordinates"},{"location":"Python/Vizualisation%20using%20python%20Part%201/#bar-chart-and-steam-graphs","text":"How does this ratio between the popularities change with time? One way to see this is using a stacked bar chart. reviews_detailed = pd . merge ( listing_detailed [[ 'id' , 'neighbourhood_cleansed' , 'room_type' ]], reviews , left_on = 'id' , right_on = 'listing_id' ) fs , axs = plt . subplots ( 1 , figsize = ( 15 , 10 )) reviews_detailed [ 'year' ] = reviews_detailed . date . dt . year reviews_detailed . groupby ([ 'year' , 'room_type' ]) . \\ aggregate ({ 'listing_id' : 'count' }) . \\ unstack () . reset_index () . \\ plot . bar ( x = 'year' , y = 'listing_id' , ax = axs , stacked = True ) plt . title ( 'Popularity of different rooms across the years' , fontsize = 25 ) plt . legend ( loc = 'upper right' , title = \"Type of room\" , fontsize = 'medium' , fancybox = True ) axs . set_ylabel ( 'Number of reviews' , fontsize = 20 ) axs . set_xlabel ( 'Years' , fontsize = 20 ) plt . show () From this bar chart we can see the same increase that we have seen in the scatter plot, that is an exponential increase till 2019, and a subsequent decrease due to the pandemic. Another cool way to look at this is by looking at streamgraphs. In streamgraph, we can see the effect of seasonality within the classes. fs , ax = plt . subplots ( 1 , figsize = ( 15 , 10 )) reviews_room = reviews_detailed . groupby ([ 'date' , 'room_type' ]) . \\ aggregate ({ 'listing_id' : 'count' }) . \\ unstack () . reset_index () reviews_room . columns = [ 'date' , 'Entire home/apt' , 'Hotel room' , 'Private room' , 'Shared room' ] ax . stackplot ( reviews_room . date , list ( reviews_room [[ 'Entire home/apt' , 'Hotel room' , 'Private room' , 'Shared room' ]] . fillna ( 0 ) . \\ to_numpy () . transpose ()), baseline = 'wiggle' ) plt . title ( 'Popularity of different rooms across the years' , fontsize = 20 , y = 1.05 , loc = 'left' ) ax . text ( \"2010\" , 1050 , 'Streamgraph of the number of reviews across time' , ha = 'left' , fontsize = 12 ) plt . legend ([ 'Entire home/apt' , 'Hotel room' , 'Private room' , 'Shared room' ], loc = 'upper left' , title = \"Type of room\" ) ax . set_xlabel ( 'Years' , fontsize = 20 ) plt . show ()","title":"Bar chart and Steam graphs"},{"location":"Python/Vizualisation%20using%20python%20Part%201/#heatmap","text":"Which locations are better, and which locations should improve on their rating? We can find the average rating across the location by averaging out the rating for each host within the location. location_rating = listing_detailed . groupby ([ 'neighbourhood_cleansed' , 'room_type' ]) . \\ aggregate ({ 'review_scores_rating' : 'mean' }) . unstack () . reset_index () location_rating . columns = [ 'neighbourhood' , 'Entire home/apt' , 'Hotel room' , 'Private room' , 'Shared room' ] location_rating . index = location_rating . neighbourhood One of the ways to visualise the average rating is using a heatmap. fig , ax = plt . subplots ( figsize = [ 20 , len ( location_rating ) / 3.3 ]) sns . heatmap ( data = location_rating [[ 'Entire home/apt' , 'Hotel room' , 'Private room' , 'Shared room' ]], annot = False , cbar_kws = { \"shrink\" : 0.5 , \"orientation\" : 'vertical' }, linewidths = 0.004 , linecolor = 'grey' , vmin = 0 , vmax = 5 , center = 0.25 ) plt . show () Although this heatmap presents us the with the average ratings per burough, we can further add the following details for clarity: 1. Location of the burough in London (eg: Central London) 2. Arranged from the best rated to the worst rated buroughs within each location 3. Proper color selection based on scale and human rating psychology : Average human ratings below 2.5 means bad rating and above 4.5 means very good rating (out of 5). It is more natural to use a diverging red-green scale for displaying negitive-positive relationship. def add_regions ( df , borough_col_name ): \"\"\" This function takes as input a dataframe with a column which includes London's borough name Then returns the same dataframw with sub regions names added for each borough \"\"\" central = [ 'Camden' , 'City of London' , 'Kensington and Chelsea' , 'Islington' , 'Lambeth' , 'Southwark' , 'Westminster' ] east = [ 'Barking and Dagenham' , 'Bexley' , 'Greenwich' , 'Hackney' , 'Havering' , 'Lewisham' , 'Newham' , 'Redbridge' , 'Tower Hamlets' , 'Waltham Forest' ] north = [ 'Barnet' , 'Enfield' , 'Haringey' ] south = [ 'Bromley' , 'Croydon' , 'Kingston upon Thames' , 'Merton' , 'Sutton' , 'Wandsworth' ] west = [ 'Brent' , 'Ealing' , 'Hammersmith and Fulham' , 'Harrow' , 'Richmond upon Thames' , 'Hillingdon' , 'Hounslow' ] df [ 'sub_regions' ] = np . where (( df [ borough_col_name ] == central [ 0 ]) | ( df [ borough_col_name ] == central [ 1 ]) | ( df [ borough_col_name ] == central [ 2 ]) | ( df [ borough_col_name ] == central [ 3 ]) | ( df [ borough_col_name ] == central [ 4 ]) | ( df [ borough_col_name ] == central [ 5 ]) | ( df [ borough_col_name ] == central [ 6 ]) , 'Central' , 'no' ) df [ 'sub_regions' ] = np . where (( df [ borough_col_name ] == east [ 0 ]) | ( df [ borough_col_name ] == east [ 1 ]) | ( df [ borough_col_name ] == east [ 2 ]) | ( df [ borough_col_name ] == east [ 3 ]) | ( df [ borough_col_name ] == east [ 4 ]) | ( df [ borough_col_name ] == east [ 5 ]) | ( df [ borough_col_name ] == east [ 6 ]) | ( df [ borough_col_name ] == east [ 7 ]) | ( df [ borough_col_name ] == east [ 8 ]) | ( df [ borough_col_name ] == east [ 9 ]) , 'East' , df [ 'sub_regions' ]) df [ 'sub_regions' ] = np . where (( df [ borough_col_name ] == north [ 0 ]) | ( df [ borough_col_name ] == north [ 1 ]) | ( df [ borough_col_name ] == north [ 2 ]) , 'North' , df [ 'sub_regions' ]) df [ 'sub_regions' ] = np . where (( df [ borough_col_name ] == south [ 0 ]) | ( df [ borough_col_name ] == south [ 1 ]) | ( df [ borough_col_name ] == south [ 2 ]) | ( df [ borough_col_name ] == south [ 3 ]) | ( df [ borough_col_name ] == south [ 4 ]) | ( df [ borough_col_name ] == south [ 5 ]) , 'South' , df [ 'sub_regions' ]) df [ 'sub_regions' ] = np . where (( df [ borough_col_name ] == west [ 0 ]) | ( df [ borough_col_name ] == west [ 1 ]) | ( df [ borough_col_name ] == west [ 2 ]) | ( df [ borough_col_name ] == west [ 3 ]) | ( df [ borough_col_name ] == west [ 4 ]) | ( df [ borough_col_name ] == west [ 5 ]) | ( df [ borough_col_name ] == west [ 6 ]) , 'West' , df [ 'sub_regions' ]) return df def sort_data ( df ): \"\"\" Groups the data by location and sorts the data based on average rating within the location. Different locations are sorted by average rating. \"\"\" df1 = df . copy () df1 [ 'average_rating' ] = ( df1 [ 'Entire home/apt' ] + df1 [ 'Hotel room' ] + df1 [ 'Private room' ] + df1 [ 'Shared room' ]) / 4 df1 [ 'location_average' ] = df1 . groupby ( 'sub_regions' )[ 'average_rating' ] . transform ( 'mean' ) df1 = df1 . sort_values ([ 'location_average' , 'average_rating' ], ascending = False ) return df1 [[ 'Private room' , 'Entire home/apt' , 'Hotel room' , 'Shared room' , 'sub_regions' ]] def prepare_reg_annotation_lists (): \"\"\" Creates the annotation in the form of groups within the data. Displays this on the right hand side of the heatmap. \"\"\" reg_sorted_list = location_rating . sub_regions . unique () reg_len = location_rating . sub_regions . value_counts () . to_dict () sorted_len = [] cum_len = [] arrow_style_str_list = [] # here we define the width of the bracket used, which is proportional to the number of boroughs within a sub region for i in range ( 5 ): if i == 0 : value = reg_len [ reg_sorted_list [ i ]] cum_value = value else : value = reg_len [ reg_sorted_list [ i ]] cum_value += value sorted_len . append ( value ) cum_len . append ( cum_value ) arrow_style_str_list . append ( '-[,widthB=' + str (( value / 1.2 ) - 0.5 ) + ',lengthB=0.7' ) # here we define ticks which represent the center location of each sub region relative to the heatmap Ticks = [] for i in range ( 5 ): if i == 0 : Ticks . append ( 1 - (( cum_len [ i ] / 2 ) / 33 )) else : Ticks . append ( 1 - (((( cum_len [ i ] - cum_len [ i - 1 ]) / 2 ) + cum_len [ i - 1 ]) / 33 )) return Ticks , arrow_style_str_list , reg_sorted_list location_rating = location_rating . replace ( np . nan , 2.5 ) location_rating = add_regions ( location_rating , 'neighbourhood' ) location_rating = sort_data ( location_rating ) Ticks_h , arrow , region = prepare_reg_annotation_lists () A diverging red-green palatte is chosen to represent good reviews and bad reviews. red_green_cmap = sns . diverging_palette ( 10 , 133 , as_cmap = True ) red_green_cmap fig , ax = plt . subplots ( figsize = [ 20 , len ( location_rating ) / 3.3 ]) sns . heatmap ( data = location_rating [[ 'Private room' , 'Entire home/apt' , 'Hotel room' , 'Shared room' ]], annot = False , cbar_kws = { \"shrink\" : 0.5 , \"orientation\" : 'vertical' }, linewidths = 0.004 , linecolor = 'grey' , vmin = 2.25 , vmax = 4.75 , cmap = red_green_cmap ) plt . title ( \"Average ratings for different locations in London\" , fontsize = 20 , y = 1.1 , loc = 'left' ) plt . text ( 0 , - 1 , 'Heatmap depicting the ratings among different locations in London. If no rating is available, minimum rating of 2.5 is assumed. \\n Good ratings are ratings above 3.5 while bad ratings are below. The data is grouped by location (right) and sorted by average rating.' , ha = 'left' , fontsize = 12 ) ax . set_ylabel ( '' ) #annotation for the borough for i in range ( 5 ): ax . annotate ( region [ i ], xy = ( 1.01 , Ticks_h [ i ]), xytext = ( 1.02 , Ticks_h [ i ]), xycoords = 'axes fraction' , ha = 'left' , va = 'center' , arrowprops = dict ( arrowstyle = arrow [ i ], lw = 1 )) We can see the ratings are good across the private rooms and entire home. The best location in each zone is: - West: Richmond upon Thames - Central: Camden - North: Enfield - East: Hackney - South: Croydon","title":"Heatmap"},{"location":"Python/Vizualisation%20using%20python%20Part%201/#treemap","text":"In this context, its not fair to compare ratings of different locations as we have seen that their popularities are different. So there could be 10 reviews in one location while 100 reviews in another. To combine them, we can use a treemap. reviews_treemap = listing_detailed . groupby ([ 'neighbourhood_cleansed' ]) . \\ aggregate ({ 'review_scores_rating' : 'mean' , 'number_of_reviews' : 'sum' }) . reset_index () reviews_treemap = add_regions ( reviews_treemap , 'neighbourhood_cleansed' ) #change col names for nice viz on hover reviews_treemap . columns = [ 'neighbourhood' , 'Average Reviews' , 'Number of reviews' , 'regions' ] fig = px . treemap ( reviews_treemap , path = [ px . Constant ( \"London\" ), 'regions' , 'neighbourhood' ], values = 'Number of reviews' , color = 'Average Reviews' , color_continuous_scale = 'RdBu' ) fig . update_layout ( margin = dict ( t = 50 , l = 25 , r = 25 , b = 25 )) fig . update_layout ( title_text = 'Airbnb London: Ratings overview' )","title":"Treemap"},{"location":"Python/Vizualisation%20using%20python%20Part%201/#wordcloud","text":"Now that we have classified the ratings into good ratings and bad ratings, let us look at the text in these ratings and identify if there are any patterns. reviews_detailed_text = pd . merge ( listing_detailed [[ 'id' , 'description' , 'neighborhood_overview' , 'host_about' , 'review_scores_rating' ]], reviews , left_on = 'id' , right_on = 'listing_id' ) reviews_detailed_positive_text = reviews_detailed_text [ reviews_detailed_text . review_scores_rating > 3.75 ] reviews_detailed_negative_text = reviews_detailed_text [ reviews_detailed_text . review_scores_rating <= 3.75 ] Selecting 100 random reviews each for positive and negative sets. pos_reviews_text = reviews_detailed_positive_text . sample ( n = 100 , random_state = 2 ) . comments . str . cat () neg_reviews_text = reviews_detailed_negative_text . sample ( n = 100 , random_state = 3 ) . comments . str . cat () Wordcloud for positive reviews # !pip install wordcloud from wordcloud import WordCloud , STOPWORDS , ImageColorGenerator from PIL import Image mask_pos = np . array ( Image . open ( 'thumbs-up-xxl.png' )) # word cloud, good vs bad ratings stop_words = [ \"https\" , \"co\" , \"RT\" , 'br' , '<br>' , '<br/>' , ' \\r ' , 'r' ] + list ( STOPWORDS ) wordcloud_pattern = WordCloud ( stopwords = stop_words , background_color = \"white\" , max_words = 2000 , max_font_size = 256 , random_state = 42 , mask = mask_pos , width = mask_pos . shape [ 1 ], height = mask_pos . shape [ 0 ]) wordcloud_positive = wordcloud_pattern . generate ( pos_reviews_text ) plt . imshow ( wordcloud_positive , interpolation = 'bilinear' ) plt . axis ( \"off\" ) plt . show () Wordcloud for negative reviews mask_neg = np . array ( Image . open ( 'thumbs-down-xxl.png' )) # word cloud, good vs bad ratings wordcloud_pattern = WordCloud ( stopwords = stop_words , background_color = \"white\" , max_words = 2000 , max_font_size = 256 , random_state = 42 , mask = mask_neg , width = mask_neg . shape [ 1 ], height = mask_neg . shape [ 0 ]) wordcloud_neg = wordcloud_pattern . generate ( neg_reviews_text ) plt . imshow ( wordcloud_neg , interpolation = 'bilinear' ) plt . axis ( \"off\" ) plt . show () Combining the positive and negative reviews in one plot to compare the differences: fs , axs = plt . subplots ( 1 , 2 , figsize = ( 20 , 10 )) plt . suptitle ( \"Airbnb London: Wordcloud of positive and negative reviews\" , fontsize = 20 ) plt . figtext ( 0.5 , 0.925 , 'Wordcloud derived from a random sample of 100 positive and 100 negative reviews.' , wrap = True , horizontalalignment = 'center' , fontsize = 12 ) axs [ 0 ] . imshow ( wordcloud_positive , interpolation = 'bilinear' ) axs [ 0 ] . axis ( \"off\" ) axs [ 1 ] . imshow ( wordcloud_neg , interpolation = 'bilinear' ) axs [ 1 ] . axis ( \"off\" ) plt . show () From these plots, we can see that automated postings, cancellations by hosts, and issues during arrival are the main issues that Airbnb should look into.","title":"Wordcloud"},{"location":"Python/Vizualisation%20using%20python%20Part%201/#correlation-matrix","text":"How are different parameters within the data related. How is ratings correlated with availability or maximum nights? This can be explained using a correlation plot. listing_detailed [ 'host_response_rate' ] = listing_detailed [ 'host_response_rate' ] . str . replace ( '%' , '' ) . astype ( float ) listing_detailed [ 'host_acceptance_rate' ] = listing_detailed [ 'host_acceptance_rate' ] . str . replace ( '%' , '' ) . astype ( float ) listing_detailed [ 'price' ] = listing_detailed [ 'price' ] . str . replace ( '$' , '' ) . str . replace ( ',' , '' ) . astype ( float ) col_for_corr = [ 'review_scores_rating' , 'review_scores_accuracy' , 'review_scores_cleanliness' , 'review_scores_checkin' , 'review_scores_communication' , 'review_scores_location' , 'review_scores_value' , 'number_of_reviews' , 'number_of_reviews_ltm' , 'number_of_reviews_l30d' , 'reviews_per_month' , 'availability_30' , 'availability_60' , 'availability_90' , 'availability_365' , 'minimum_nights' , 'maximum_nights' , 'minimum_minimum_nights' , 'maximum_minimum_nights' , 'minimum_maximum_nights' , 'maximum_maximum_nights' , 'minimum_nights_avg_ntm' , 'maximum_nights_avg_ntm' , 'bedrooms' , 'beds' , 'accommodates' , 'price' , 'host_response_rate' , 'host_acceptance_rate' , 'host_total_listings_count' ] f = plt . figure ( figsize = ( 20 , 20 )) plt . matshow ( listing_detailed [ col_for_corr ] . corr (), fignum = f , cmap = red_green_cmap , vmin =- 1 , vmax = 1 ) plt . xticks ( range ( listing_detailed [ col_for_corr ] . select_dtypes ([ 'number' ]) . shape [ 1 ]), listing_detailed [ col_for_corr ] . select_dtypes ([ 'number' ]) . columns , rotation = 90 , fontsize = 15 ) plt . yticks ( range ( listing_detailed [ col_for_corr ] . select_dtypes ([ 'number' ]) . shape [ 1 ]), listing_detailed [ col_for_corr ] . select_dtypes ([ 'number' ]) . columns , rotation = 0 , fontsize = 15 ) cb = plt . colorbar () cb . ax . tick_params ( labelsize = 14 ) plt . title ( \"Airbnb London: Correlation between different parameters\" , fontsize = 20 , loc = 'left' ) plt . text ( 0 , 32 , 'Correlation matrix displaying the different parameters within the data. DIverging green (+ve) red (-ve) scale is used to display the correlations. \\n \\ Features considered: Review scores, Number of reviews, availability, maximum and minimum days of stay, host and room parameters.' , ha = 'left' , fontsize = 12 ) plt . show ()","title":"Correlation matrix"},{"location":"Python/Vizualisation%20using%20python%20Part%201/#scatterplot-matrix","text":"While the above plot shows the correlation across various variables, I want to deep dive into change in ratings with price. I can use a scatterplot matrix to visualise this. Aditionally, I want the costliest properties, and the most popular yet cheap properties annotated. def annotate_plot ( x , y , ** kwargs ): if ( x . name == 'price' and y . name == 'review_scores_rating' ): ax = plt . gca () for index , obj in listing_detailed . nlargest ( 2 , 'price' ) . iterrows (): plt . annotate ( obj [ 'name' ], # the text xy = ( obj . price , obj . review_scores_rating ), xytext = ( 7500 , obj . review_scores_rating - 0.5 ), arrowprops = dict ( arrowstyle = \"->\" ) ) elif ( x . name == 'price' and y . name == 'number_of_reviews' ): ax = plt . gca () for index , obj in listing_detailed . nlargest ( 3 , 'number_of_reviews' ) . iterrows (): ax . text ( obj . price , obj . number_of_reviews , obj [ 'name' ]) col_for_pairplot = [ 'review_scores_rating' , 'number_of_reviews' , 'price' ] sns_plot = sns . pairplot ( listing_detailed , vars = col_for_pairplot , kind = 'scatter' , hue = 'room_type' , diag_kind = 'kde' ,) sns_plot . fig . set_size_inches ( 20 , 20 ) sns_plot . _legend . set_bbox_to_anchor (( 0.15 , 0.89 )) sns_plot . map_upper ( annotate_plot ) sns_plot . fig . suptitle ( \"Airbnb London: Scatterplot matrix\" , fontsize = 20 , y = 1 ) We can see that the two costliest properties are either historic apartments or a mansion. The three most popular yet cheap properties are small and quaint properties near popular destinations.","title":"Scatterplot matrix"},{"location":"Python/Vizualisation%20using%20python%20Part%201/#boxplot-and-violin-chart","text":"To look at the variation in ratings within the different room types, we could use either a boxplot or a Violin plot as shown. fs , axs = plt . subplots ( 1 , 2 , figsize = ( 20 , 20 )) listing_detailed . boxplot ( column = 'review_scores_rating' , by = 'room_type' , figsize = ( 10 , 20 ), ax = axs [ 0 ]) sns . violinplot ( 'room_type' , 'review_scores_rating' , data = listing_detailed , ax = axs [ 1 ]) plt . suptitle ( \"Airbnb London: Average rating across room types\" , fontsize = 20 , y = 0.95 ) plt . figtext ( 0.5 , 0.925 , 'Boxplot (left) and Violin plot (right) for the average review across room types' , wrap = True , horizontalalignment = 'center' , fontsize = 12 ) axs [ 0 ] . set_title ( '' ) for ax in axs : ax . set_ylim ( - 1 , 6 ) ax . set_ylabel ( 'Average Rating' , fontsize = 12 ) ax . set_xlabel ( 'Room types' , fontsize = 12 )","title":"Boxplot and Violin chart"},{"location":"Python/Vizualisation%20using%20python%20Part%201/#cluster-map","text":"If we wanted to cluster localities based on some features, then cluster map is teh ideal choice. In the below map, we cluster different locations based on one feature from each type. The features are also clustered to show the similarity between features. Finally we use a white-blue color palatte for displaying the variation within the data. from sklearn.preprocessing import MinMaxScaler import seaborn as sns col_for_corr = [ 'price' , 'review_scores_rating' , 'number_of_reviews' , 'availability_90' , 'minimum_nights_avg_ntm' , 'bedrooms' , 'host_response_rate' ] df_cluster = listing_detailed . groupby ( 'neighbourhood_cleansed' ) . aggregate ({ 'review_scores_rating' : 'mean' , 'number_of_reviews' : 'sum' , 'availability_90' : 'mean' , 'minimum_nights_avg_ntm' : 'mean' , 'bedrooms' : 'mean' , 'price' : 'mean' , 'host_response_rate' : 'mean' }) . reset_index () scaler = MinMaxScaler () df_cluster1 = pd . DataFrame ( scaler . fit_transform ( df_cluster [ col_for_corr ]), columns = col_for_corr ) df_cluster1 . index = df_cluster . neighbourhood_cleansed crest_cmap = sns . color_palette ( \"crest\" , as_cmap = True ) crest_cmap g = sns . clustermap ( df_cluster1 , cmap = crest_cmap , vmin = 0 , vmax = 1 ) plt . title ( \"Airbnb London: Clusters within London\" , fontsize = 20 , loc = 'left' , y = 2 , x = - 25 ) g . ax_cbar . set_position (( 1 , .2 , .03 , .4 )) g . ax_heatmap . set_ylabel ( \"\" ) plt . show ()","title":"Cluster map"},{"location":"Python/Vizualisation%20using%20python%20Part%201/#references","text":"Visualisation Analytics and Design, Tamara Munzner Class notes and asignments, Imperial College London Visual Analytics lab at JKU Linz","title":"References"},{"location":"R/ARIMA/","text":"MathJax.Hub.Config({ extensions: [\"tex2jax.js\",\"TeX/AMSmath.js\",\"TeX/AMSsymbols.js\"], jax: [\"input/TeX\", \"output/HTML-CSS\"], tex2jax: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ], }, \"HTML-CSS\": { availableFonts: [\"TeX\"] } }); Arima model \u00b6 ARIMA, short for 'Auto Regressive Integrated Moving Average' is a very popular forecasting model. ARIMA models are basically regression models: auto-regression simply means regression of a variable on itself measured at different time periods. AR : Autoregression. A model that uses the dependent relationship between an observation and some number of lagged observations I : Integrated. The use of differencing of raw observations (i.e. subtracting an observation from an observation at the previous time step) in order to make the time series stationary MA : Moving Average. A model that uses the dependency between an observation and residual errors from a moving average model applied to lagged observations Stationary \u00b6 The assumption of AR model is that the time series is assumed to be a stationary process. If a time-series data, \\(Y_t\\) , is stationary, then it satisfies the following conditions: 1. The mean values of \\(Y_t\\) at different values of t are constant 2. The variances of \\(Y_t\\) at different time periods are constant (Homoscedasticity) 3. The covariances of \\(Y_t\\) and \\(Y_{t-k}\\) for different lags depend only on k and not on time t Box-Jenkins method \u00b6 The initial ARMA and ARIMA models were developed by Box and Jenkins in 1970. The authors also suggested a process for identifying, estimating, and checking models for any specific time series data-set. It contains three steps Model form selection 1.1 Evaluate stationarity 1.2 Selection of the differencing level (d) \u2013 to fix stationarity problems 1.3 Selection of the AR level (p) 1.4 Selection of the MA level (q) Parameter estimation Model checking Model form selection \u00b6 Evaluate stationarity \u00b6 A stationary time series is one whose properties do not depend on the time at which the series is observed. Time series with trends, or with seasonality, are not stationary. A white noise series is stationary \u2014 it does not matter when you observe it, it should look much the same at any point in time. The presence of stationarity can be found in many ways among which the most popular three are: 1. ACF plot: When the data is non-stationary, the auto-correlation function will not be cut-off to zero quickly 2. Dickey\u2212Fuller or augmented Dickey\u2212Fuller tests 3. KPSS test In the below example, I will use a sample from my attendance data set described in EDA blogs. (Actual data is not shown for privacy reasons. This is mock data which is very similar to the actual one. The analysis will be the same) The time plot for the same is shown below: By looking at the plot, I can clearly see that the series is not stationary as the trend is visible and variance seems to be decreasing with time. The ACF of this time series is: From the above plot, I can identify that the time series is not stationary. Augmented Dickey\u2212Fuller Test \u00b6 Augmented Dickey\u2212Fuller test is a hypothesis test in which the null hypothesis and alternative hypothesis are given by \\(H_0\\) : \\(\\gamma\\) = 0 (the time series is non-stationary) \\(H_1\\) : \\(\\gamma\\) < 0 (the time series is stationary) Where \\( \\(\\Delta y_t = \\alpha + \\beta t + \\gamma y_{t-1} + \\delta_1 \\Delta y_{t-1} + \\delta_2 \\Delta y_{t-2} + \\dots\\) \\) ## ## Augmented Dickey-Fuller Test ## ## data: time.series ## Dickey-Fuller = -2.8819, Lag order = 6, p-value = 0.2045 ## alternative hypothesis: stationary As p value is greater than the cutoff $ \\alpha = 5\\% $, retaining the null hypothesis that the time series is non-stationary. KPSS Test \u00b6 The null hypothesis for the KPSS test is that the data are stationary. For this test, we do NOT want to reject the null hypothesis. ## ## KPSS Test for Trend Stationarity ## ## data: time.series ## KPSS Trend = 0.41839, Truncation lag parameter = 5, p-value = 0.01 As p-value is less than 5%, rejecting the null hypothesis that the data is stationary. Selection of differencing parameter d \u00b6 The attendance data (in time) have failed both tests for the stationary, the Augmented Dickey-Fuller and the KPSS test. Differencing is used to convert the data to a stationary model. Differencing is nothing but computing the differences between consecutive observations. The above tests for first difference for in-time looks like the following: ## ## Augmented Dickey-Fuller Test ## ## data: time.series.diff ## Dickey-Fuller = -10.851, Lag order = 6, p-value = 0.01 ## alternative hypothesis: stationary ## ## KPSS Test for Trend Stationarity ## ## data: time.series %>% diff() ## KPSS Trend = 0.01895, Truncation lag parameter = 5, p-value = 0.1 After differencing by d = 1, The ACF of the differenced in-time looks just like that of a white noise series. In the ADF test the p-value is lower than cutoff rejecting the Null hypothesis that time series is non stationary while the KPSS test p-value is greater than 5% retaining the null hypothesis that the data is stationary. This suggests that after differencing by one time, the in-time is essentially a random amount and is stationary. ## [1] \"The ideal differencing parameter is 1\" ## ## ####################### ## # KPSS Unit Root Test # ## ####################### ## ## Test is of type: mu with 5 lags. ## ## Value of test-statistic is: 0.0188 ## ## Critical value for a significance level of: ## 10pct 5pct 2.5pct 1pct ## critical values 0.347 0.463 0.574 0.739 Selection of AR(p) and MA(q) parameters \u00b6 One of the important tasks in using auto-regressive model in forecasting is the model identification, which is, identifying the value of p and q (the number of lags). Selection of AR(p) and MA(q) lags can be done by two methods: 1. ACF and PACF functions 2. AIC or BIC coefficients ACF and PACF coefficients \u00b6 Auto-correlation is the correlation between \\(Y_t\\) measured at different time periods (for example, \\(Y_t\\) and \\(Y_{t-1}\\) or \\(Y_t\\) and \\(Y_{t-k}\\) ). A plot of auto-correlation for different values of k is called auto-correlation function (ACF) or correlogram. Partial auto-correlation of lag k is the correlation between \\(Y_t\\) and \\(Y_{t-k}\\) when the influence of all intermediate values ( \\(Y_{t-1}\\) , \\(Y_{t-2}\\) ... \\(Y_{t-k+1}\\) ) is removed (partial out) from both \\(Y_t\\) and \\(Y_{t-k}\\) . A plot of partial auto-correlation for different values of k is called partial auto-correlation function (PACF). Hypothesis tests can be carried out to check whether the auto-correlation and partial auto-correlation values are different from zero. The corresponding null and alternative hypotheses are \\(H_0: r_k = 0\\) and \\(H_A: r_k \\neq 0\\) , where \\(r_k\\) is the auto-correlation of order k \\(H_0: r_{pk} = 0\\) and \\(H_A: r_{pk} \\neq 0\\) , where \\(r_{pk}\\) is the partial auto-correlation of order k The null hypothesis is rejected when \\(|r_k| > \\frac{1 96}{\\sqrt{n}}\\) and \\(|r_{pk}| > \\frac{1 96}{\\sqrt{n}}\\) . In the ACF and PACF plots, these cutoff are shown as dotted blue lines. The values of p and q in a ARMA process can be identified using the following thumb rule: 1. Auto-correlation value, \\(r_p > cutoff\\) for first q values (first q lags) and cuts off to zero 2. Partial auto-correlation function, \\(r_{pk} > cutoff\\) for first p values and cuts off to zero After differencing, the ACF and PACF plots of in0time are as follows: From the ACF and PACF plots. the auto-correlations cuts off to zero after the first lag. The PACF value cuts off to zero after 2 lags. So, the appropriate model could be ARMA(2, 1) process. Combining differencing parameter from previous section(d=1), The most appropriate model would be ARIMA(2, 1, 1) AIC and BIC coefficients \u00b6 Akaike\u2019s Information Criterion (AIC) and Bayesian Information Criterion (BIC), which were useful in selecting predictors for regression, are also useful for determining the order of an ARIMA model. Best estimates of AR and MA orders will minimize AIC or BIC. ## ## Fitting models using approximations to speed things up... ## ## ARIMA(2,1,1) with drift : 2503.652 ## ARIMA(0,1,0) with drift : 2656.365 ## ARIMA(1,1,0) with drift : 2569.569 ## ARIMA(0,1,1) with drift : 2511.579 ## ARIMA(0,1,0) : 2654.343 ## ARIMA(1,1,1) with drift : 2510.969 ## ARIMA(2,1,0) with drift : 2526.337 ## ARIMA(3,1,1) with drift : 2506.575 ## ARIMA(3,1,0) with drift : 2523.835 ## ARIMA(2,1,1) : 2504.125 ## ## Now re-fitting the best model(s) without approximations... ## ## ARIMA(2,1,1) with drift : 2516.48 ## ## Best model: ARIMA(2,1,1) with drift ## Series: time.series ## ARIMA(2,1,1) with drift ## ## Coefficients: ## ar1 ar2 ma1 drift ## -0.0530 0.0200 -0.8174 -0.3120 ## s.e. 0.0811 0.0751 0.0539 0.2595 ## ## sigma^2 estimated as 574.2: log likelihood=-1253.13 ## AIC=2516.26 AICc=2516.48 BIC=2534.3 The model selected using AIC coefficient is ARIMA(2,1,1) which is same as the one selected using ACF and PACF. ARIMA(2, 1, 1) is the final model as selected from both the methods. Parameter estimation \u00b6 Once the model order has been identified (i.e., the values of p, d and q), we need to estimate the model parameters. Using a regression model to identify the parameters: ## Series: time.series ## ARIMA(2,1,1) with drift ## ## Coefficients: ## ar1 ar2 ma1 drift ## -0.0530 0.0200 -0.8174 -0.3120 ## s.e. 0.0811 0.0751 0.0539 0.2595 ## ## sigma^2 estimated as 574.2: log likelihood=-1253.13 ## AIC=2516.26 AICc=2516.48 BIC=2534.3 Model testing \u00b6 Residuals \u00b6 The \u201cresiduals\u201d in a time series model are what is left over after fitting a model. Residuals are useful in checking whether a model has adequately captured the information in the data. A good forecasting method will yield residuals with the following properties: 1. The residuals are uncorrelated. If there are correlations between residuals, then there is information left in the residuals which should be used in computing forecasts. 2. The residuals have zero mean. If the residuals have a mean other than zero, then the forecasts are biased. Portmanteau tests for auto-correlation \u00b6 When we look at the ACF plot to see whether each spike is within the required limits, we are implicitly carrying out multiple hypothesis tests, each one with a small probability of giving a false positive. When enough of these tests are done, it is likely that at least one will give a false positive, and so we may conclude that the residuals have some remaining auto-correlation, when in fact they do not. In order to overcome this problem, we test whether the first h auto-correlations are significantly different from what would be expected from a white noise process. A test for a group of auto-correlations is called a portmanteau test. One such test is the Ljung-Box test. Ljung\u2212Box Test for Auto-Correlations \u00b6 Ljung\u2212Box is a test of lack of fit of the forecasting model and checks whether the auto-correlations for the errors are different from zero. The null and alternative hypotheses are given by \\(H_0\\) : The model does not show lack of fit \\(H_1\\) : The model exhibits lack of fit The Ljung\u2212Box statistic (Q-Statistic) is given by \\( \\(Q(m) = n(n+2) \\sum_{k=1}^{m}\\frac{\\rho_k^2}{n-k}\\) \\) where n is the number of observations in the time series, k is the number of lag, \\(r_k\\) is the auto-correlation of lag k, and m is the total number of lags. Q-statistic is an approximate chi-square distribution with m \u2013 p \u2013 q degrees of freedom where p and q are the AR and MA lags. ## ## Ljung-Box test ## ## data: Residuals from ARIMA(2,1,1) with drift ## Q* = 15.754, df = 6, p-value = 0.01513 ## ## Model df: 4. Total lags used: 10 From the above tests we can conclude that the model is a good fit of the data. References \u00b6 Business Analytics: The Science of Data-Driven Decision Making - Dinesh Kumar Forecasting: Principles and Practice - Rob J Hyndman and George Athanasopoulos - Online SAS for Forecasting Time Series, Third Edition - Dickey Applied Time Series Analysis for Fisheries and Environmental Sciences - E. E. Holmes, M. D. Scheuerell, and E. J. Ward - Online The Box-Jenkins Method - NCSS Statistical Software - Online Box-Jenkins modelling - Rob J Hyndman - Online Basic Ecnometrics - Damodar N Gujarati Time Series Analysis: Forecasting and Control - Box and Jenkins","title":"ARIMA in R"},{"location":"R/ARIMA/#arima-model","text":"ARIMA, short for 'Auto Regressive Integrated Moving Average' is a very popular forecasting model. ARIMA models are basically regression models: auto-regression simply means regression of a variable on itself measured at different time periods. AR : Autoregression. A model that uses the dependent relationship between an observation and some number of lagged observations I : Integrated. The use of differencing of raw observations (i.e. subtracting an observation from an observation at the previous time step) in order to make the time series stationary MA : Moving Average. A model that uses the dependency between an observation and residual errors from a moving average model applied to lagged observations","title":"Arima model"},{"location":"R/ARIMA/#stationary","text":"The assumption of AR model is that the time series is assumed to be a stationary process. If a time-series data, \\(Y_t\\) , is stationary, then it satisfies the following conditions: 1. The mean values of \\(Y_t\\) at different values of t are constant 2. The variances of \\(Y_t\\) at different time periods are constant (Homoscedasticity) 3. The covariances of \\(Y_t\\) and \\(Y_{t-k}\\) for different lags depend only on k and not on time t","title":"Stationary"},{"location":"R/ARIMA/#box-jenkins-method","text":"The initial ARMA and ARIMA models were developed by Box and Jenkins in 1970. The authors also suggested a process for identifying, estimating, and checking models for any specific time series data-set. It contains three steps Model form selection 1.1 Evaluate stationarity 1.2 Selection of the differencing level (d) \u2013 to fix stationarity problems 1.3 Selection of the AR level (p) 1.4 Selection of the MA level (q) Parameter estimation Model checking","title":"Box-Jenkins method"},{"location":"R/ARIMA/#model-form-selection","text":"","title":"Model form selection"},{"location":"R/ARIMA/#evaluate-stationarity","text":"A stationary time series is one whose properties do not depend on the time at which the series is observed. Time series with trends, or with seasonality, are not stationary. A white noise series is stationary \u2014 it does not matter when you observe it, it should look much the same at any point in time. The presence of stationarity can be found in many ways among which the most popular three are: 1. ACF plot: When the data is non-stationary, the auto-correlation function will not be cut-off to zero quickly 2. Dickey\u2212Fuller or augmented Dickey\u2212Fuller tests 3. KPSS test In the below example, I will use a sample from my attendance data set described in EDA blogs. (Actual data is not shown for privacy reasons. This is mock data which is very similar to the actual one. The analysis will be the same) The time plot for the same is shown below: By looking at the plot, I can clearly see that the series is not stationary as the trend is visible and variance seems to be decreasing with time. The ACF of this time series is: From the above plot, I can identify that the time series is not stationary.","title":"Evaluate stationarity"},{"location":"R/ARIMA/#augmented-dickeyfuller-test","text":"Augmented Dickey\u2212Fuller test is a hypothesis test in which the null hypothesis and alternative hypothesis are given by \\(H_0\\) : \\(\\gamma\\) = 0 (the time series is non-stationary) \\(H_1\\) : \\(\\gamma\\) < 0 (the time series is stationary) Where \\( \\(\\Delta y_t = \\alpha + \\beta t + \\gamma y_{t-1} + \\delta_1 \\Delta y_{t-1} + \\delta_2 \\Delta y_{t-2} + \\dots\\) \\) ## ## Augmented Dickey-Fuller Test ## ## data: time.series ## Dickey-Fuller = -2.8819, Lag order = 6, p-value = 0.2045 ## alternative hypothesis: stationary As p value is greater than the cutoff $ \\alpha = 5\\% $, retaining the null hypothesis that the time series is non-stationary.","title":"Augmented Dickey\u2212Fuller Test"},{"location":"R/ARIMA/#kpss-test","text":"The null hypothesis for the KPSS test is that the data are stationary. For this test, we do NOT want to reject the null hypothesis. ## ## KPSS Test for Trend Stationarity ## ## data: time.series ## KPSS Trend = 0.41839, Truncation lag parameter = 5, p-value = 0.01 As p-value is less than 5%, rejecting the null hypothesis that the data is stationary.","title":"KPSS Test"},{"location":"R/ARIMA/#selection-of-differencing-parameter-d","text":"The attendance data (in time) have failed both tests for the stationary, the Augmented Dickey-Fuller and the KPSS test. Differencing is used to convert the data to a stationary model. Differencing is nothing but computing the differences between consecutive observations. The above tests for first difference for in-time looks like the following: ## ## Augmented Dickey-Fuller Test ## ## data: time.series.diff ## Dickey-Fuller = -10.851, Lag order = 6, p-value = 0.01 ## alternative hypothesis: stationary ## ## KPSS Test for Trend Stationarity ## ## data: time.series %>% diff() ## KPSS Trend = 0.01895, Truncation lag parameter = 5, p-value = 0.1 After differencing by d = 1, The ACF of the differenced in-time looks just like that of a white noise series. In the ADF test the p-value is lower than cutoff rejecting the Null hypothesis that time series is non stationary while the KPSS test p-value is greater than 5% retaining the null hypothesis that the data is stationary. This suggests that after differencing by one time, the in-time is essentially a random amount and is stationary. ## [1] \"The ideal differencing parameter is 1\" ## ## ####################### ## # KPSS Unit Root Test # ## ####################### ## ## Test is of type: mu with 5 lags. ## ## Value of test-statistic is: 0.0188 ## ## Critical value for a significance level of: ## 10pct 5pct 2.5pct 1pct ## critical values 0.347 0.463 0.574 0.739","title":"Selection of differencing parameter d"},{"location":"R/ARIMA/#selection-of-arp-and-maq-parameters","text":"One of the important tasks in using auto-regressive model in forecasting is the model identification, which is, identifying the value of p and q (the number of lags). Selection of AR(p) and MA(q) lags can be done by two methods: 1. ACF and PACF functions 2. AIC or BIC coefficients","title":"Selection of AR(p) and MA(q) parameters"},{"location":"R/ARIMA/#acf-and-pacf-coefficients","text":"Auto-correlation is the correlation between \\(Y_t\\) measured at different time periods (for example, \\(Y_t\\) and \\(Y_{t-1}\\) or \\(Y_t\\) and \\(Y_{t-k}\\) ). A plot of auto-correlation for different values of k is called auto-correlation function (ACF) or correlogram. Partial auto-correlation of lag k is the correlation between \\(Y_t\\) and \\(Y_{t-k}\\) when the influence of all intermediate values ( \\(Y_{t-1}\\) , \\(Y_{t-2}\\) ... \\(Y_{t-k+1}\\) ) is removed (partial out) from both \\(Y_t\\) and \\(Y_{t-k}\\) . A plot of partial auto-correlation for different values of k is called partial auto-correlation function (PACF). Hypothesis tests can be carried out to check whether the auto-correlation and partial auto-correlation values are different from zero. The corresponding null and alternative hypotheses are \\(H_0: r_k = 0\\) and \\(H_A: r_k \\neq 0\\) , where \\(r_k\\) is the auto-correlation of order k \\(H_0: r_{pk} = 0\\) and \\(H_A: r_{pk} \\neq 0\\) , where \\(r_{pk}\\) is the partial auto-correlation of order k The null hypothesis is rejected when \\(|r_k| > \\frac{1 96}{\\sqrt{n}}\\) and \\(|r_{pk}| > \\frac{1 96}{\\sqrt{n}}\\) . In the ACF and PACF plots, these cutoff are shown as dotted blue lines. The values of p and q in a ARMA process can be identified using the following thumb rule: 1. Auto-correlation value, \\(r_p > cutoff\\) for first q values (first q lags) and cuts off to zero 2. Partial auto-correlation function, \\(r_{pk} > cutoff\\) for first p values and cuts off to zero After differencing, the ACF and PACF plots of in0time are as follows: From the ACF and PACF plots. the auto-correlations cuts off to zero after the first lag. The PACF value cuts off to zero after 2 lags. So, the appropriate model could be ARMA(2, 1) process. Combining differencing parameter from previous section(d=1), The most appropriate model would be ARIMA(2, 1, 1)","title":"ACF and PACF coefficients"},{"location":"R/ARIMA/#aic-and-bic-coefficients","text":"Akaike\u2019s Information Criterion (AIC) and Bayesian Information Criterion (BIC), which were useful in selecting predictors for regression, are also useful for determining the order of an ARIMA model. Best estimates of AR and MA orders will minimize AIC or BIC. ## ## Fitting models using approximations to speed things up... ## ## ARIMA(2,1,1) with drift : 2503.652 ## ARIMA(0,1,0) with drift : 2656.365 ## ARIMA(1,1,0) with drift : 2569.569 ## ARIMA(0,1,1) with drift : 2511.579 ## ARIMA(0,1,0) : 2654.343 ## ARIMA(1,1,1) with drift : 2510.969 ## ARIMA(2,1,0) with drift : 2526.337 ## ARIMA(3,1,1) with drift : 2506.575 ## ARIMA(3,1,0) with drift : 2523.835 ## ARIMA(2,1,1) : 2504.125 ## ## Now re-fitting the best model(s) without approximations... ## ## ARIMA(2,1,1) with drift : 2516.48 ## ## Best model: ARIMA(2,1,1) with drift ## Series: time.series ## ARIMA(2,1,1) with drift ## ## Coefficients: ## ar1 ar2 ma1 drift ## -0.0530 0.0200 -0.8174 -0.3120 ## s.e. 0.0811 0.0751 0.0539 0.2595 ## ## sigma^2 estimated as 574.2: log likelihood=-1253.13 ## AIC=2516.26 AICc=2516.48 BIC=2534.3 The model selected using AIC coefficient is ARIMA(2,1,1) which is same as the one selected using ACF and PACF. ARIMA(2, 1, 1) is the final model as selected from both the methods.","title":"AIC and BIC coefficients"},{"location":"R/ARIMA/#parameter-estimation","text":"Once the model order has been identified (i.e., the values of p, d and q), we need to estimate the model parameters. Using a regression model to identify the parameters: ## Series: time.series ## ARIMA(2,1,1) with drift ## ## Coefficients: ## ar1 ar2 ma1 drift ## -0.0530 0.0200 -0.8174 -0.3120 ## s.e. 0.0811 0.0751 0.0539 0.2595 ## ## sigma^2 estimated as 574.2: log likelihood=-1253.13 ## AIC=2516.26 AICc=2516.48 BIC=2534.3","title":"Parameter estimation"},{"location":"R/ARIMA/#model-testing","text":"","title":"Model testing"},{"location":"R/ARIMA/#residuals","text":"The \u201cresiduals\u201d in a time series model are what is left over after fitting a model. Residuals are useful in checking whether a model has adequately captured the information in the data. A good forecasting method will yield residuals with the following properties: 1. The residuals are uncorrelated. If there are correlations between residuals, then there is information left in the residuals which should be used in computing forecasts. 2. The residuals have zero mean. If the residuals have a mean other than zero, then the forecasts are biased.","title":"Residuals"},{"location":"R/ARIMA/#portmanteau-tests-for-auto-correlation","text":"When we look at the ACF plot to see whether each spike is within the required limits, we are implicitly carrying out multiple hypothesis tests, each one with a small probability of giving a false positive. When enough of these tests are done, it is likely that at least one will give a false positive, and so we may conclude that the residuals have some remaining auto-correlation, when in fact they do not. In order to overcome this problem, we test whether the first h auto-correlations are significantly different from what would be expected from a white noise process. A test for a group of auto-correlations is called a portmanteau test. One such test is the Ljung-Box test.","title":"Portmanteau tests for auto-correlation"},{"location":"R/ARIMA/#ljungbox-test-for-auto-correlations","text":"Ljung\u2212Box is a test of lack of fit of the forecasting model and checks whether the auto-correlations for the errors are different from zero. The null and alternative hypotheses are given by \\(H_0\\) : The model does not show lack of fit \\(H_1\\) : The model exhibits lack of fit The Ljung\u2212Box statistic (Q-Statistic) is given by \\( \\(Q(m) = n(n+2) \\sum_{k=1}^{m}\\frac{\\rho_k^2}{n-k}\\) \\) where n is the number of observations in the time series, k is the number of lag, \\(r_k\\) is the auto-correlation of lag k, and m is the total number of lags. Q-statistic is an approximate chi-square distribution with m \u2013 p \u2013 q degrees of freedom where p and q are the AR and MA lags. ## ## Ljung-Box test ## ## data: Residuals from ARIMA(2,1,1) with drift ## Q* = 15.754, df = 6, p-value = 0.01513 ## ## Model df: 4. Total lags used: 10 From the above tests we can conclude that the model is a good fit of the data.","title":"Ljung\u2212Box Test for Auto-Correlations"},{"location":"R/ARIMA/#references","text":"Business Analytics: The Science of Data-Driven Decision Making - Dinesh Kumar Forecasting: Principles and Practice - Rob J Hyndman and George Athanasopoulos - Online SAS for Forecasting Time Series, Third Edition - Dickey Applied Time Series Analysis for Fisheries and Environmental Sciences - E. E. Holmes, M. D. Scheuerell, and E. J. Ward - Online The Box-Jenkins Method - NCSS Statistical Software - Online Box-Jenkins modelling - Rob J Hyndman - Online Basic Ecnometrics - Damodar N Gujarati Time Series Analysis: Forecasting and Control - Box and Jenkins","title":"References"},{"location":"R/Seasonal-Time-Series/","text":"MathJax.Hub.Config({ extensions: [\"tex2jax.js\",\"TeX/AMSmath.js\",\"TeX/AMSsymbols.js\"], jax: [\"input/TeX\", \"output/HTML-CSS\"], tex2jax: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ], }, \"HTML-CSS\": { availableFonts: [\"TeX\"] } }); Data and context \u00b6 This blog is inspired by the work done at www.andreasgeorgopoulos.com for predicting the demand for lettuce in a particular store (46673) fast food chain of restaurants. The data and EDA for the same can be found in the above link. You can find the same in my git repository. The historical demand for Lettuce for the first ten days is shown below. Demand of lettuce for the first ten days date IngredientId quantity_lettuce 15-03-05 27 152 15-03-06 27 100 15-03-07 27 54 15-03-08 27 199 15-03-09 27 166 15-03-10 27 143 15-03-11 27 162 15-03-12 27 116 15-03-13 27 136 15-03-14 27 68 Demand forecasting with Seasonality \u00b6 The Lettuce in store 46673 shows a clear seasonal pattern. The quantity of Lettuce used seems to depend on the day of the week. From this plot, there seems to be no discernible trend in the quantity of Lettuce used. Visualising seasonality \u00b6 We can visualize this seasonality using seasonal plots. From these plots, we can see that the demand for Lettuce is least on Tuesday while highest on Wednesdays. Thus, a clear weekly seasonality in the data can be observed in the data. The same is more evident in polar coordinates. We can see the similarity in demand within weekdays by looking at the seasonal sub-series plots where data for each weekday is collected together. The mean and the variance across weeks for each of the weekdays can be seen. SARIMA \u00b6 As the data is seasonal, we will need to use SARIMA (Seasonal ARIMA). By looking at these plots, I suspect that the series is stationary. This can be validated using Dickey-Fuller and KPSS tests. Dickey\u2212Fuller Test \u00b6 Dickey\u2212Fuller test is a hypothesis test in which the null hypothesis and alternative hypothesis are given by \\(H_0\\) : \\(\\gamma\\) = 0 (the time series is non-stationary) \\(H_1\\) : \\(\\gamma\\) < 0 (the time series is stationary) Where \\( \\(\\Delta y_t = \\alpha + \\beta t + \\gamma y_{t-1} + \\delta_1 \\Delta y_{t-1} + \\delta_2 \\Delta y_{t-2} + \\dots\\) \\) ## ## Augmented Dickey-Fuller Test ## ## data: time.series ## Dickey-Fuller = -8.4973, Lag order = 4, p-value = 0.01 ## alternative hypothesis: stationary As p value is less than the cutoff \\(\\alpha = 5\\%\\) , rejecting the null hypothesis that the time series is non-stationary. The time series is stationary. KPSS Test \u00b6 The null hypothesis for the KPSS test is that the data is stationary. ## ## KPSS Test for Trend Stationarity ## ## data: time.series ## KPSS Trend = 0.10896, Truncation lag parameter = 4, p-value = 0.1 As the p-value is greater than the cutoff $ \\alpha = 5\\% $, accepting the null hypothesis that the series is stationary. ACF and PACF plots \u00b6 The ACF and PACF plots of this time series are: From the ACF plot, I can see a correlation within multiples of 7, indicating a weekly seasonality. However, the PACF shows no such trend after the first iteration. Stationarity in the seasonal component \u00b6 To test if the seasonal component is also stationary, we can look at the time series after differencing with period 7 (which will remove the seasonal non-stationarity if it exists). After differencing once with period 7, we can determine if the data is still stationary using the KPSS test. We can also use the nsdiff function in R to find the differencing factor to make the data stationary. library ( urca ) diff ( time.series , 7 ) %>% ur.kpss %>% summary () ## ## ####################### ## # KPSS Unit Root Test # ## ####################### ## ## Test is of type: mu with 3 lags. ## ## Value of test-statistic is: 0.0759 ## ## Critical value for a significance level of: ## 10pct 5pct 2.5pct 1pct ## critical values 0.347 0.463 0.574 0.739 # Number of differencing necessary on the data to bring to stationarity time.series %>% nsdiffs () ## [1] 1 # After differencing with period 7, we are checking for stationarity time.series %>% diff ( lag = 7 ) %>% nsdiffs () ## [1] 0 We can see that the seasonality component is not stationary and can be made stationary by differencing once (with period 7). Auto Sarima \u00b6 We can minimize AIC and BIC to find the optimal (p, d, q) coefficients, as well as the seasonal (p, d, q) components. To reduce over-fitting, we can reduce the data into training and test datasets. We will use the train dataset to tran, and compare the accuracy on the test dataset. As we have around 15 weeks of data (15 th week is not full), we can use the first 13 weeks for training and the remaining 14 th and 15 th week for testing. ## ## ARIMA(2,0,2)(1,1,1)[7] with drift : 810.332 ## ARIMA(0,0,0)(0,1,0)[7] with drift : 835.2064 ## ARIMA(1,0,0)(1,1,0)[7] with drift : 814.9145 ## ARIMA(0,0,1)(0,1,1)[7] with drift : 803.1689 ## ARIMA(0,0,0)(0,1,0)[7] : 833.1092 ## ARIMA(0,0,1)(0,1,0)[7] with drift : 837.2882 ## ARIMA(0,0,1)(1,1,1)[7] with drift : 805.3521 ## ARIMA(0,0,1)(0,1,2)[7] with drift : 805.3646 ## ARIMA(0,0,1)(1,1,0)[7] with drift : 814.7505 ## ARIMA(0,0,1)(1,1,2)[7] with drift : Inf ## ARIMA(0,0,0)(0,1,1)[7] with drift : 803.6079 ## ARIMA(1,0,1)(0,1,1)[7] with drift : 805.3438 ## ARIMA(0,0,2)(0,1,1)[7] with drift : 805.3646 ## ARIMA(1,0,0)(0,1,1)[7] with drift : 803.3737 ## ARIMA(1,0,2)(0,1,1)[7] with drift : 807.6387 ## ARIMA(0,0,1)(0,1,1)[7] : 801.7491 ## ARIMA(0,0,1)(0,1,0)[7] : 835.1395 ## ARIMA(0,0,1)(1,1,1)[7] : 803.949 ## ARIMA(0,0,1)(0,1,2)[7] : 803.9496 ## ARIMA(0,0,1)(1,1,0)[7] : 812.5485 ## ARIMA(0,0,1)(1,1,2)[7] : 806.0826 ## ARIMA(0,0,0)(0,1,1)[7] : 802.3558 ## ARIMA(1,0,1)(0,1,1)[7] : 803.897 ## ARIMA(0,0,2)(0,1,1)[7] : 803.916 ## ARIMA(1,0,0)(0,1,1)[7] : 801.9277 ## ARIMA(1,0,2)(0,1,1)[7] : 806.1242 ## ## Best model: ARIMA(0,0,1)(0,1,1)[7] ## Series: train.time.series ## ARIMA(0,0,1)(0,1,1)[7] ## ## Coefficients: ## ma1 sma1 ## 0.1883 -0.7976 ## s.e. 0.1107 0.1208 ## ## sigma^2 estimated as 639.5: log likelihood=-397.73 ## AIC=801.45 AICc=801.75 BIC=808.78 ## ## Training set error measures: ## ME RMSE MAE MPE MAPE MASE ## Training set -0.3200893 24.02014 17.54746 -2.825856 13.15411 0.6661607 ## ACF1 ## Training set 0.001434103 The model selected using AIC coefficient is ARIMA(0, 0, 1) with a seasonal component (0, 1, 1) with frequency 7. The residual plots (on the training data) show that the errors are normally distributed and look like random noise. ## ## Ljung-Box test ## ## data: Residuals from ARIMA(0,0,1)(0,1,1)[7] ## Q* = 15.185, df = 12, p-value = 0.2315 ## ## Model df: 2. Total lags used: 14 Holts Winter model \u00b6 The Holt-Winters model has three components, level ( \\(\\alpha\\) ), trend ( \\(\\beta\\) ) and seasonality ( \\(\\gamma\\) . In each of these components, there can be additive, multiplicative or no effects. Assuming that the series only has seasonal component, the below plots use additive, multiplicative and damped-multiplicative methods for prediction. The accuracy of the three models are ## [1] \"Aditive Model\" ## ME RMSE MAE MPE MAPE MASE ACF1 ## Training set -1.35 22.94 18.3 -3.66 14.01 0.69 0.08 ## [1] \"Multiplicative Model\" ## ME RMSE MAE MPE MAPE MASE ACF1 ## Training set -1.45 22.96 18.35 -4.37 14.33 0.7 0.08 ## [1] \"Damped multiplicative Model\" ## ME RMSE MAE MPE MAPE MASE ACF1 ## Training set 0.76 22.67 18.46 -2.13 14.06 0.7 0.12 Estimating ETS models \u00b6 Estimating the \\(\\alpha, \\beta\\) and \\(\\gamma\\) parameters by minimizing the sum of squared errors, we get that the additive error type, No trend and additive seasonality. ## ETS(A,N,A) ## ## Call: ## ets(y = train.time.series, model = \"ZZZ\") ## ## Smoothing parameters: ## alpha = 0.0967 ## gamma = 1e-04 ## ## Initial states: ## l = 143.9203 ## s = 31.7796 17.6211 26.946 20.0456 -69.8942 -46.3021 ## 19.804 ## ## sigma: 23.9411 ## ## AIC AICc BIC ## 1010.843 1013.560 1036.061 ## ## Training set error measures: ## ME RMSE MAE MPE MAPE MASE ## Training set -0.7342076 22.73998 18.15336 -3.504596 13.98764 0.6891627 ## ACF1 ## Training set 0.08178856 The residuals (on the training data) are normally distributed and look like random noise. ## ## Ljung-Box test ## ## data: Residuals from ETS(A,N,A) ## Q* = 14.79, df = 5, p-value = 0.0113 ## ## Model df: 9. Total lags used: 14 ## NULL Comparision between SARIMA and ETS \u00b6 The accuracy metrics on the test data for both the models are show below Comparision ME RMSE MAE MPE MAPE MASE ACF1 Theil.s.U Holt-Winters Train -0.73 22.74 18.15 -3.50 13.99 0.69 0.08 NA Holt-Winters Test 15.54 42.83 33.50 7.71 21.65 1.27 0.06 0.62 SARIMA Train -0.32 24.02 17.55 -2.83 13.15 0.67 0.00 NA SARIMA Test 13.02 44.40 34.28 5.28 21.71 1.30 0.12 0.64 Holts-Winter has lower error rate (RMSE, MAE) than SARIMA. Therefore Holts-winter is the better model. References \u00b6 Forecasting Principles and practice by Rob J Hyndman and George Athanasopoulos Blogpost: Stationary tests by Achyuthuni Sri Harsha Blogpost: ARIMA by Achyuthuni Sri Harsha Github code samples by HarshaAsh Projects on www.andreasgeorgopoulos.com by Andreas Georgopoulos","title":"Seasonal time series (R)"},{"location":"R/Seasonal-Time-Series/#data-and-context","text":"This blog is inspired by the work done at www.andreasgeorgopoulos.com for predicting the demand for lettuce in a particular store (46673) fast food chain of restaurants. The data and EDA for the same can be found in the above link. You can find the same in my git repository. The historical demand for Lettuce for the first ten days is shown below. Demand of lettuce for the first ten days date IngredientId quantity_lettuce 15-03-05 27 152 15-03-06 27 100 15-03-07 27 54 15-03-08 27 199 15-03-09 27 166 15-03-10 27 143 15-03-11 27 162 15-03-12 27 116 15-03-13 27 136 15-03-14 27 68","title":"Data and context"},{"location":"R/Seasonal-Time-Series/#demand-forecasting-with-seasonality","text":"The Lettuce in store 46673 shows a clear seasonal pattern. The quantity of Lettuce used seems to depend on the day of the week. From this plot, there seems to be no discernible trend in the quantity of Lettuce used.","title":"Demand forecasting with Seasonality"},{"location":"R/Seasonal-Time-Series/#visualising-seasonality","text":"We can visualize this seasonality using seasonal plots. From these plots, we can see that the demand for Lettuce is least on Tuesday while highest on Wednesdays. Thus, a clear weekly seasonality in the data can be observed in the data. The same is more evident in polar coordinates. We can see the similarity in demand within weekdays by looking at the seasonal sub-series plots where data for each weekday is collected together. The mean and the variance across weeks for each of the weekdays can be seen.","title":"Visualising seasonality"},{"location":"R/Seasonal-Time-Series/#sarima","text":"As the data is seasonal, we will need to use SARIMA (Seasonal ARIMA). By looking at these plots, I suspect that the series is stationary. This can be validated using Dickey-Fuller and KPSS tests.","title":"SARIMA"},{"location":"R/Seasonal-Time-Series/#dickeyfuller-test","text":"Dickey\u2212Fuller test is a hypothesis test in which the null hypothesis and alternative hypothesis are given by \\(H_0\\) : \\(\\gamma\\) = 0 (the time series is non-stationary) \\(H_1\\) : \\(\\gamma\\) < 0 (the time series is stationary) Where \\( \\(\\Delta y_t = \\alpha + \\beta t + \\gamma y_{t-1} + \\delta_1 \\Delta y_{t-1} + \\delta_2 \\Delta y_{t-2} + \\dots\\) \\) ## ## Augmented Dickey-Fuller Test ## ## data: time.series ## Dickey-Fuller = -8.4973, Lag order = 4, p-value = 0.01 ## alternative hypothesis: stationary As p value is less than the cutoff \\(\\alpha = 5\\%\\) , rejecting the null hypothesis that the time series is non-stationary. The time series is stationary.","title":"Dickey\u2212Fuller Test"},{"location":"R/Seasonal-Time-Series/#kpss-test","text":"The null hypothesis for the KPSS test is that the data is stationary. ## ## KPSS Test for Trend Stationarity ## ## data: time.series ## KPSS Trend = 0.10896, Truncation lag parameter = 4, p-value = 0.1 As the p-value is greater than the cutoff $ \\alpha = 5\\% $, accepting the null hypothesis that the series is stationary.","title":"KPSS Test"},{"location":"R/Seasonal-Time-Series/#acf-and-pacf-plots","text":"The ACF and PACF plots of this time series are: From the ACF plot, I can see a correlation within multiples of 7, indicating a weekly seasonality. However, the PACF shows no such trend after the first iteration.","title":"ACF and PACF plots"},{"location":"R/Seasonal-Time-Series/#stationarity-in-the-seasonal-component","text":"To test if the seasonal component is also stationary, we can look at the time series after differencing with period 7 (which will remove the seasonal non-stationarity if it exists). After differencing once with period 7, we can determine if the data is still stationary using the KPSS test. We can also use the nsdiff function in R to find the differencing factor to make the data stationary. library ( urca ) diff ( time.series , 7 ) %>% ur.kpss %>% summary () ## ## ####################### ## # KPSS Unit Root Test # ## ####################### ## ## Test is of type: mu with 3 lags. ## ## Value of test-statistic is: 0.0759 ## ## Critical value for a significance level of: ## 10pct 5pct 2.5pct 1pct ## critical values 0.347 0.463 0.574 0.739 # Number of differencing necessary on the data to bring to stationarity time.series %>% nsdiffs () ## [1] 1 # After differencing with period 7, we are checking for stationarity time.series %>% diff ( lag = 7 ) %>% nsdiffs () ## [1] 0 We can see that the seasonality component is not stationary and can be made stationary by differencing once (with period 7).","title":"Stationarity in the seasonal component"},{"location":"R/Seasonal-Time-Series/#auto-sarima","text":"We can minimize AIC and BIC to find the optimal (p, d, q) coefficients, as well as the seasonal (p, d, q) components. To reduce over-fitting, we can reduce the data into training and test datasets. We will use the train dataset to tran, and compare the accuracy on the test dataset. As we have around 15 weeks of data (15 th week is not full), we can use the first 13 weeks for training and the remaining 14 th and 15 th week for testing. ## ## ARIMA(2,0,2)(1,1,1)[7] with drift : 810.332 ## ARIMA(0,0,0)(0,1,0)[7] with drift : 835.2064 ## ARIMA(1,0,0)(1,1,0)[7] with drift : 814.9145 ## ARIMA(0,0,1)(0,1,1)[7] with drift : 803.1689 ## ARIMA(0,0,0)(0,1,0)[7] : 833.1092 ## ARIMA(0,0,1)(0,1,0)[7] with drift : 837.2882 ## ARIMA(0,0,1)(1,1,1)[7] with drift : 805.3521 ## ARIMA(0,0,1)(0,1,2)[7] with drift : 805.3646 ## ARIMA(0,0,1)(1,1,0)[7] with drift : 814.7505 ## ARIMA(0,0,1)(1,1,2)[7] with drift : Inf ## ARIMA(0,0,0)(0,1,1)[7] with drift : 803.6079 ## ARIMA(1,0,1)(0,1,1)[7] with drift : 805.3438 ## ARIMA(0,0,2)(0,1,1)[7] with drift : 805.3646 ## ARIMA(1,0,0)(0,1,1)[7] with drift : 803.3737 ## ARIMA(1,0,2)(0,1,1)[7] with drift : 807.6387 ## ARIMA(0,0,1)(0,1,1)[7] : 801.7491 ## ARIMA(0,0,1)(0,1,0)[7] : 835.1395 ## ARIMA(0,0,1)(1,1,1)[7] : 803.949 ## ARIMA(0,0,1)(0,1,2)[7] : 803.9496 ## ARIMA(0,0,1)(1,1,0)[7] : 812.5485 ## ARIMA(0,0,1)(1,1,2)[7] : 806.0826 ## ARIMA(0,0,0)(0,1,1)[7] : 802.3558 ## ARIMA(1,0,1)(0,1,1)[7] : 803.897 ## ARIMA(0,0,2)(0,1,1)[7] : 803.916 ## ARIMA(1,0,0)(0,1,1)[7] : 801.9277 ## ARIMA(1,0,2)(0,1,1)[7] : 806.1242 ## ## Best model: ARIMA(0,0,1)(0,1,1)[7] ## Series: train.time.series ## ARIMA(0,0,1)(0,1,1)[7] ## ## Coefficients: ## ma1 sma1 ## 0.1883 -0.7976 ## s.e. 0.1107 0.1208 ## ## sigma^2 estimated as 639.5: log likelihood=-397.73 ## AIC=801.45 AICc=801.75 BIC=808.78 ## ## Training set error measures: ## ME RMSE MAE MPE MAPE MASE ## Training set -0.3200893 24.02014 17.54746 -2.825856 13.15411 0.6661607 ## ACF1 ## Training set 0.001434103 The model selected using AIC coefficient is ARIMA(0, 0, 1) with a seasonal component (0, 1, 1) with frequency 7. The residual plots (on the training data) show that the errors are normally distributed and look like random noise. ## ## Ljung-Box test ## ## data: Residuals from ARIMA(0,0,1)(0,1,1)[7] ## Q* = 15.185, df = 12, p-value = 0.2315 ## ## Model df: 2. Total lags used: 14","title":"Auto Sarima"},{"location":"R/Seasonal-Time-Series/#holts-winter-model","text":"The Holt-Winters model has three components, level ( \\(\\alpha\\) ), trend ( \\(\\beta\\) ) and seasonality ( \\(\\gamma\\) . In each of these components, there can be additive, multiplicative or no effects. Assuming that the series only has seasonal component, the below plots use additive, multiplicative and damped-multiplicative methods for prediction. The accuracy of the three models are ## [1] \"Aditive Model\" ## ME RMSE MAE MPE MAPE MASE ACF1 ## Training set -1.35 22.94 18.3 -3.66 14.01 0.69 0.08 ## [1] \"Multiplicative Model\" ## ME RMSE MAE MPE MAPE MASE ACF1 ## Training set -1.45 22.96 18.35 -4.37 14.33 0.7 0.08 ## [1] \"Damped multiplicative Model\" ## ME RMSE MAE MPE MAPE MASE ACF1 ## Training set 0.76 22.67 18.46 -2.13 14.06 0.7 0.12","title":"Holts Winter model"},{"location":"R/Seasonal-Time-Series/#estimating-ets-models","text":"Estimating the \\(\\alpha, \\beta\\) and \\(\\gamma\\) parameters by minimizing the sum of squared errors, we get that the additive error type, No trend and additive seasonality. ## ETS(A,N,A) ## ## Call: ## ets(y = train.time.series, model = \"ZZZ\") ## ## Smoothing parameters: ## alpha = 0.0967 ## gamma = 1e-04 ## ## Initial states: ## l = 143.9203 ## s = 31.7796 17.6211 26.946 20.0456 -69.8942 -46.3021 ## 19.804 ## ## sigma: 23.9411 ## ## AIC AICc BIC ## 1010.843 1013.560 1036.061 ## ## Training set error measures: ## ME RMSE MAE MPE MAPE MASE ## Training set -0.7342076 22.73998 18.15336 -3.504596 13.98764 0.6891627 ## ACF1 ## Training set 0.08178856 The residuals (on the training data) are normally distributed and look like random noise. ## ## Ljung-Box test ## ## data: Residuals from ETS(A,N,A) ## Q* = 14.79, df = 5, p-value = 0.0113 ## ## Model df: 9. Total lags used: 14 ## NULL","title":"Estimating ETS models"},{"location":"R/Seasonal-Time-Series/#comparision-between-sarima-and-ets","text":"The accuracy metrics on the test data for both the models are show below Comparision ME RMSE MAE MPE MAPE MASE ACF1 Theil.s.U Holt-Winters Train -0.73 22.74 18.15 -3.50 13.99 0.69 0.08 NA Holt-Winters Test 15.54 42.83 33.50 7.71 21.65 1.27 0.06 0.62 SARIMA Train -0.32 24.02 17.55 -2.83 13.15 0.67 0.00 NA SARIMA Test 13.02 44.40 34.28 5.28 21.71 1.30 0.12 0.64 Holts-Winter has lower error rate (RMSE, MAE) than SARIMA. Therefore Holts-winter is the better model.","title":"Comparision between SARIMA and ETS"},{"location":"R/Seasonal-Time-Series/#references","text":"Forecasting Principles and practice by Rob J Hyndman and George Athanasopoulos Blogpost: Stationary tests by Achyuthuni Sri Harsha Blogpost: ARIMA by Achyuthuni Sri Harsha Github code samples by HarshaAsh Projects on www.andreasgeorgopoulos.com by Andreas Georgopoulos","title":"References"},{"location":"R/Stationarity-tests/","text":"MathJax.Hub.Config({ extensions: [\"tex2jax.js\",\"TeX/AMSmath.js\",\"TeX/AMSsymbols.js\"], jax: [\"input/TeX\", \"output/HTML-CSS\"], tex2jax: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ], }, \"HTML-CSS\": { availableFonts: [\"TeX\"] } }); Tests of stationarity \u00b6 This is the second post on ARIMA time series forecasting method. In the first post, we discussed about stationarity, random walk and other concepts. In this blog, we are going to discuss about various ways one can test for stationarity. Two types of tests are introduced in this blog 1. Unit root tests 2. Independence tests Data \u00b6 For the following blog, we will use a sample from attendance data set described in EDA blogs. From the time series and ACF plots, one can observe non-stationarity and decreasing trend. Unit root tests \u00b6 As discussed in the previous blog, unit root stochastic process is another name for Random walk process. A random walk process can be written as $$ Y_t=\\rho \\times Y_{t\u22121} + \\epsilon_t $$ Where \\(\\rho = 1\\) . If \\(|\\rho|<1\\) then the process represents Markov first order auto regressive model which is stationary. Only for \\(\\rho=1\\) we get non-stationary. The above equation can be alternatively written as $$ Y_t - Y_{t-1} = \\Delta Y_t = \\delta \\times Y_{t-1} + \\epsilon_t $$ where \\(\\delta = \\rho -1\\) . For non-stationarity, the condition now becomes \\(\\delta = 0\\) the alternative hypothesis being \\(\\delta < 0\\) . The null and alternate hypothesis are: $$ H_0: \\delta = 0 $$ $$ H_1 : \\delta < 0 $$ Under this null hypothesis, \\(Y_{t-1}\\) does not follow a normal distribution(or t-distribution). Dickey and Fuller have shown that for the above null and alternate hypothesis, the the estimated test statistic follows the \\(\\tau\\) statistic. If the hypothesis that \\(\\delta=0\\) is rejected, that is if the series is stationary, then we can use the t-test for further analysis. Dickey Fuller tests \u00b6 The Dickey fuller tests contains two steps. 1. Test if the series is stationary 2. If the series is not stationary, test what kind of non-stationarity is present As non-stationarity can exist in three ways, the dickey fuller test is estimated in three different forms \\(Y_t\\) is a random walk : \\(\\Delta Y_t = \\delta Y_{t-1} + \\epsilon_t\\) \\(Y_t\\) is a random walk with drift : \\(\\Delta Y_t = \\beta_1 + \\delta Y_{t-1} + \\epsilon_t\\) \\(Y_t\\) is a random walk with drift around a deterministic trend : \\(\\Delta Y_t = \\beta_1 + \\beta_2 t +\\delta Y_{t-1} + \\epsilon_t\\) In each case, the null hypothesis is that \\(\\delta = 0\\) , i.e., there is a unit root\u2014the time series is non-stationary. The alternative hypothesis is that \\(\\delta < 0\\) that is, the time series is stationary. If the null hypothesis is rejected , it means the following in the three scenarios: 1. \\(Y_t\\) is a stationary time series with zero mean in the case of random walk 2. \\(Y_t\\) is stationary with a nonzero mean in the case of random walk with drift 3. \\(Y_t\\) is stationary around a deterministic trend in case of random walk with drift around a deterministic trend The actual estimation procedure is as follows: 1. Perform the tests from backwards, i.e., estimate deterministic trend first, then random walk with drift and then random walk. This is to ensure we are not committing specification error 2. For the three tests, estimate the \\(\\tau\\) statistic and compare with the (MacKinnon) critical tau values, If the computed absolute value of the tau statistic ( \\(|\\tau|\\) ) exceeds the critical tau values, we reject the Null hypothesis in which case the time series is stationary 3. If any of the ( \\(\\tau\\) ) values are less than the critical tau value, then we retain the Null hypothesis in which case the time series is non-stationary. The critical \\(\\tau\\) values can vary between the three tests For the attendance data, the Dickey fuller tests give the following results: ## ======================================================================== ## At the 5pct level: ## The test is for type random walk ## delta=0: The null hypothesis is not rejected, the series is not stationary ## ======================================================================== ## ======================================================================== ## At the 5pct level: ## The test is for type random walk with drift ## delta=0: The first null hypothesis is not rejected, the series is not stationary ## delta=0 and beta1=0: The second null hypothesis is rejected, the series is not stationary ## and there is drift. ## ======================================================================== ## ======================================================================== ## At the 5pct level: ## The test is for type random walk with drift and deterministic trend ## delta=0: The first null hypothesis is not rejected, the series is not stationary ## delta=0 and beta2=0: The second null hypothesis is rejected, the series is not stationary ## and there is trend ## delta=0 and beta1=0 and beta2=0: The third null hypothesis is rejected, the series is not stationary ## there is trend, and there may or may not be drift ## Warning in interp_urdf(ur.df(time.series, type = \"trend\")): Presence of drift is inconclusive. ## ======================================================================== The above result should be interpreted as follows (read backwards): 3. There might be a deterministic trend in the series there may or may not be a drift coefficient 2. The series is non-stationary and there is drift coefficient 1. The series is non-stationary Therefore the series is not stationary. Augmented Dickey\u2212Fuller Test \u00b6 In conducting the DF test, it was assumed that the error term was uncorrelated. But in case the \\(\\epsilon_t\\) are correlated, Dickey and Fuller have developed a test, known as the augmented Dickey\u2013Fuller (ADF) test. This test is conducted by \u201caugmenting\u201d the three equations by adding the lagged values of the dependent variable. The null hypothesis and alternative hypothesis are given by \\(H_0\\) : \\(\\gamma\\) = 0 (the time series is non-stationary) \\(H_1\\) : \\(\\gamma\\) < 0 (the time series is stationary) Where \\( \\(\\Delta y_t = \\alpha + \\beta t + \\gamma y_{t-1} + \\delta_1 \\Delta y_{t-1} + \\delta_2 \\Delta y_{t-2} + \\dots\\) \\) Ljung\u2013Box independence test \u00b6 Ljung\u2212Box is a test of lack of fit of the forecasting model and checks whether the auto-correlations for the errors are different from zero. The null and alternative hypotheses are given by \\(H_0\\) : The model does not show lack of fit \\(H_1\\) : The model exhibits lack of fit The Ljung\u2212Box statistic (Q-Statistic) is given by \\( \\(Q(m) = n(n+2) \\sum_{k=1}^{m}\\frac{\\rho_k^2}{n-k}\\) \\) where n is the number of observations in the time series, k is the number of lag, \\(r_k\\) is the auto-correlation of lag k, and m is the total number of lags. Q-statistic is an approximate chi-square distribution with m \u2013 p \u2013 q degrees of freedom where p and q are the AR and MA lags. ## ## Box-Ljung test ## ## data: time.series ## X-squared = 1070.9, df = 10, p-value < 2.2e-16 Therefore from Ljung box test, we can conclude that the model shows a lack of fit with stationary process. References: \u00b6 Basic Ecnometrics - Damodar N Gujarati SAS for Forecasting Time Series, Third Edition - Dickey Customer Analytics at Flipkart.Com - Naveen Bhansali (case study in Harvard business review) This discussion on stack overflow","title":"Stationary Tests (R)"},{"location":"R/Stationarity-tests/#tests-of-stationarity","text":"This is the second post on ARIMA time series forecasting method. In the first post, we discussed about stationarity, random walk and other concepts. In this blog, we are going to discuss about various ways one can test for stationarity. Two types of tests are introduced in this blog 1. Unit root tests 2. Independence tests","title":"Tests of stationarity"},{"location":"R/Stationarity-tests/#data","text":"For the following blog, we will use a sample from attendance data set described in EDA blogs. From the time series and ACF plots, one can observe non-stationarity and decreasing trend.","title":"Data"},{"location":"R/Stationarity-tests/#unit-root-tests","text":"As discussed in the previous blog, unit root stochastic process is another name for Random walk process. A random walk process can be written as $$ Y_t=\\rho \\times Y_{t\u22121} + \\epsilon_t $$ Where \\(\\rho = 1\\) . If \\(|\\rho|<1\\) then the process represents Markov first order auto regressive model which is stationary. Only for \\(\\rho=1\\) we get non-stationary. The above equation can be alternatively written as $$ Y_t - Y_{t-1} = \\Delta Y_t = \\delta \\times Y_{t-1} + \\epsilon_t $$ where \\(\\delta = \\rho -1\\) . For non-stationarity, the condition now becomes \\(\\delta = 0\\) the alternative hypothesis being \\(\\delta < 0\\) . The null and alternate hypothesis are: $$ H_0: \\delta = 0 $$ $$ H_1 : \\delta < 0 $$ Under this null hypothesis, \\(Y_{t-1}\\) does not follow a normal distribution(or t-distribution). Dickey and Fuller have shown that for the above null and alternate hypothesis, the the estimated test statistic follows the \\(\\tau\\) statistic. If the hypothesis that \\(\\delta=0\\) is rejected, that is if the series is stationary, then we can use the t-test for further analysis.","title":"Unit root tests"},{"location":"R/Stationarity-tests/#dickey-fuller-tests","text":"The Dickey fuller tests contains two steps. 1. Test if the series is stationary 2. If the series is not stationary, test what kind of non-stationarity is present As non-stationarity can exist in three ways, the dickey fuller test is estimated in three different forms \\(Y_t\\) is a random walk : \\(\\Delta Y_t = \\delta Y_{t-1} + \\epsilon_t\\) \\(Y_t\\) is a random walk with drift : \\(\\Delta Y_t = \\beta_1 + \\delta Y_{t-1} + \\epsilon_t\\) \\(Y_t\\) is a random walk with drift around a deterministic trend : \\(\\Delta Y_t = \\beta_1 + \\beta_2 t +\\delta Y_{t-1} + \\epsilon_t\\) In each case, the null hypothesis is that \\(\\delta = 0\\) , i.e., there is a unit root\u2014the time series is non-stationary. The alternative hypothesis is that \\(\\delta < 0\\) that is, the time series is stationary. If the null hypothesis is rejected , it means the following in the three scenarios: 1. \\(Y_t\\) is a stationary time series with zero mean in the case of random walk 2. \\(Y_t\\) is stationary with a nonzero mean in the case of random walk with drift 3. \\(Y_t\\) is stationary around a deterministic trend in case of random walk with drift around a deterministic trend The actual estimation procedure is as follows: 1. Perform the tests from backwards, i.e., estimate deterministic trend first, then random walk with drift and then random walk. This is to ensure we are not committing specification error 2. For the three tests, estimate the \\(\\tau\\) statistic and compare with the (MacKinnon) critical tau values, If the computed absolute value of the tau statistic ( \\(|\\tau|\\) ) exceeds the critical tau values, we reject the Null hypothesis in which case the time series is stationary 3. If any of the ( \\(\\tau\\) ) values are less than the critical tau value, then we retain the Null hypothesis in which case the time series is non-stationary. The critical \\(\\tau\\) values can vary between the three tests For the attendance data, the Dickey fuller tests give the following results: ## ======================================================================== ## At the 5pct level: ## The test is for type random walk ## delta=0: The null hypothesis is not rejected, the series is not stationary ## ======================================================================== ## ======================================================================== ## At the 5pct level: ## The test is for type random walk with drift ## delta=0: The first null hypothesis is not rejected, the series is not stationary ## delta=0 and beta1=0: The second null hypothesis is rejected, the series is not stationary ## and there is drift. ## ======================================================================== ## ======================================================================== ## At the 5pct level: ## The test is for type random walk with drift and deterministic trend ## delta=0: The first null hypothesis is not rejected, the series is not stationary ## delta=0 and beta2=0: The second null hypothesis is rejected, the series is not stationary ## and there is trend ## delta=0 and beta1=0 and beta2=0: The third null hypothesis is rejected, the series is not stationary ## there is trend, and there may or may not be drift ## Warning in interp_urdf(ur.df(time.series, type = \"trend\")): Presence of drift is inconclusive. ## ======================================================================== The above result should be interpreted as follows (read backwards): 3. There might be a deterministic trend in the series there may or may not be a drift coefficient 2. The series is non-stationary and there is drift coefficient 1. The series is non-stationary Therefore the series is not stationary.","title":"Dickey Fuller tests"},{"location":"R/Stationarity-tests/#augmented-dickeyfuller-test","text":"In conducting the DF test, it was assumed that the error term was uncorrelated. But in case the \\(\\epsilon_t\\) are correlated, Dickey and Fuller have developed a test, known as the augmented Dickey\u2013Fuller (ADF) test. This test is conducted by \u201caugmenting\u201d the three equations by adding the lagged values of the dependent variable. The null hypothesis and alternative hypothesis are given by \\(H_0\\) : \\(\\gamma\\) = 0 (the time series is non-stationary) \\(H_1\\) : \\(\\gamma\\) < 0 (the time series is stationary) Where \\( \\(\\Delta y_t = \\alpha + \\beta t + \\gamma y_{t-1} + \\delta_1 \\Delta y_{t-1} + \\delta_2 \\Delta y_{t-2} + \\dots\\) \\)","title":"Augmented Dickey\u2212Fuller Test"},{"location":"R/Stationarity-tests/#ljungbox-independence-test","text":"Ljung\u2212Box is a test of lack of fit of the forecasting model and checks whether the auto-correlations for the errors are different from zero. The null and alternative hypotheses are given by \\(H_0\\) : The model does not show lack of fit \\(H_1\\) : The model exhibits lack of fit The Ljung\u2212Box statistic (Q-Statistic) is given by \\( \\(Q(m) = n(n+2) \\sum_{k=1}^{m}\\frac{\\rho_k^2}{n-k}\\) \\) where n is the number of observations in the time series, k is the number of lag, \\(r_k\\) is the auto-correlation of lag k, and m is the total number of lags. Q-statistic is an approximate chi-square distribution with m \u2013 p \u2013 q degrees of freedom where p and q are the AR and MA lags. ## ## Box-Ljung test ## ## data: time.series ## X-squared = 1070.9, df = 10, p-value < 2.2e-16 Therefore from Ljung box test, we can conclude that the model shows a lack of fit with stationary process.","title":"Ljung\u2013Box independence test"},{"location":"R/Stationarity-tests/#references","text":"Basic Ecnometrics - Damodar N Gujarati SAS for Forecasting Time Series, Third Edition - Dickey Customer Analytics at Flipkart.Com - Naveen Bhansali (case study in Harvard business review) This discussion on stack overflow","title":"References:"},{"location":"R/Univariate-analysis/","text":"Introduction \u00b6 A univariate analysis is the simplest form of EDA. \"Uni\" means \"one\", so in other words, this analysis is on only one variable. It doesn't deal with causes or relationships, and its primary purpose is to describe; it takes data, summarizes that data and finds patterns in it. In describing or characterizing the observations of an individual variable, three basic properties are of interest: 1. The location of observations, or how large or small the values of the individual observations are 2. The dispersion (sometimes called scale or spread) of the observations 3. The distribution of the observations Uni-variate plots provide one way to find out about those properties. There are two basic kinds of univariate plots: 1. Enumeration plots, or plots that show every observation 2. Summary plots that generalize the data into a simplified representation. For the current tutorial, I will be using my office attendance data set. The data set contains the time when I swiped into the office and when I swiped out of the office. Data from 4 th October 2017 to 29 th November 2018. After some data set manipulation, I will get the difference between policy out-time and my actual out-time. I can leave 15 minutes before the policy out time. After manipulation, a sample of the data is as follows: (Actual data is not shown for security reasons. This is mock data that is very similar to the actual one.) ## Attendance.Date diff.in.time diff.out.time ## 1 2018-03-22 18 mins 226 mins ## 2 2018-08-14 -9 mins 5 mins ## 3 2017-12-04 42 mins 11 mins ## 4 2018-03-01 26 mins -6 mins ## 5 2018-01-23 35 mins -4 mins Summary Statistics \u00b6 Before further analysis, some basic summary statistics would show me the mean and standard deviation of the data. For this tutorial, I will use diff.in.time (difference between actual in-time and policy in-time) mean ( as.numeric ( attendance $ diff.out.time )) # Mean in minutes ## [1] 20.3227 sd ( as.numeric ( attendance $ diff.out.time )) # Standard Deviation in minutes ## [1] 69.06549 nrow ( attendance ) # Length of the data set ## [1] 282 Enumerative plots \u00b6 \"Enumerative plots\" are called such because they enumerate or show every individual data point. Index Plot/Univariate Scatter Diagram \u00b6 Displays the values of a single variable for each observation using symbols plotted relative to the observation number. ggplot ( attendance , aes ( x = Attendance.Date , y = as.numeric ( diff.out.time ), color = ( diff.out.time >= -15 ))) + geom_point ( show.legend = FALSE ) + labs ( x = 'Time' , y = 'Out-time difference (Minutes)' ) + theme_minimal () Just looking at this plot, I can say the following: 1. I could cluster into three parts. - One cluster would be before December 2017, where I used to leave the office way after my out-time. - The second cluster would be from December 2017 to June 2018, where I used to leave the office 15 minutes before my out time. - The third cluster was after June 2018, when I was leaving way after my out-time. 2. The red dots indicate the days when I came to the office after 15 minutes from in-time. They are anomalies, days when I took half days etc. We can exclude them from our current analysis. Y Zero High-Density Plot \u00b6 Another way to look at the same data is by using a Y Zero High-Density Plot. It displays the values of a single variable plotted as thin vertical lines. Here the magnitude of the observations iss highlighted. ggplot ( attendance , aes ( x = Attendance.Date , y = 0 , color = ( diff.out.time >= -15 ), xend = Attendance.Date , yend = as.numeric ( diff.out.time ))) + geom_segment ( show.legend = FALSE ) + labs ( x = 'Time' , y = 'Out-time difference (Minutes)' ) + theme_minimal () Removing half-days as outliers attendance <- attendance %>% filter ( diff.out.time >= -15 ) Strip Plot/Strip Chart (univariate scatter diagram) \u00b6 Displays the values of a single variable as symbols plotted along a line. This is a basic plot where we can see the spread of the data. stripchart ( x = as.numeric ( attendance $ diff.out.time ), xlab = 'Out-time difference (minutes)' ) Sometimes it is more visually apparent when the points are stacked. ggplot ( attendance , aes ( x = as.numeric ( diff.out.time ), y = ..density.. )) + geom_dotplot ( binwidth = 3 , method = 'histodot' ) + labs ( x = 'Out-time difference (minutes)' ) + theme_minimal () We can observe that the number of observations is high and slowly tends to drop as time progresses. Dot Plot/Dot Chart \u00b6 Displays the values of a single variable as symbols plotted along a line. With a separate line for each observation, it is generally constructed after sorting the rows of the data table. df = attendance %>% arrange ( as.numeric ( diff.out.time )) ggplot ( df , aes ( x = as.numeric ( row.names ( df )), y = as.numeric ( diff.out.time ))) + geom_point () + labs ( x = 'count' , y = 'Out time difference (min)' ) + theme_minimal () From the graph, I can observe that the distribution initially seems to be exponential. A sample normal distribution is plotted for reference. We can see that the distribution looks nowhere like a normal distribution. Instead, I suspect that it is close to an exponential distribution. Univariate Summary Plots \u00b6 Summary plots display an object or graph that gives a more concise expression of a variable's location, dispersion, and distribution than an enumerative plot. This comes at the expense of some loss of information: In a summary plot, it is no longer possible to retrieve the individual data value, but the gain usually matches this loss in understanding that results from the efficient representation of the data. Summary plots generally prove to be much better than the enumerative plots in revealing the data distribution. Box plot \u00b6 A simple way of representing statistical data on a plot in which a rectangle is drawn to represent the second and third quartiles, usually with a vertical line inside to indicate the median value. The lower and upper quartiles are shown as horizontal lines on either side of the rectangle. ggplot ( attendance , aes ( x = \"Box Plot\" , y = as.numeric ( diff.out.time ), group = 123 )) + geom_boxplot () + labs ( y = 'Out time difference (min)' ) + theme_minimal () Histograms \u00b6 The other summary plots are of various types: Histograms: Histograms are bar charts that display the counts or relative frequencies of values falling in different class intervals or ranges. Density Plots: A density plot plots the local relative frequency or density of points along the number line or x-axis of a plot. The local density is determined by summing the individual \"kernel\" densities for each point. Where points occur more frequently, this sum, and consequently the local density, will be greater. legendcols <- c ( \"Normal distribution\" = \"darkred\" , \"Density\" = \"darkBlue\" , \"Histogram\" = \"lightBlue\" ) ggplot ( attendance , aes ( x = as.numeric ( diff.out.time ))) + geom_histogram ( aes ( y = ..density.. , fill = \"Histogram\" ), bins = 50 ) + stat_function ( fun = dnorm , aes ( color = \"Normal distribution\" ), size = 1 , args = list ( mean = mean ( as.numeric ( attendance $ diff.out.time )), sd = sd ( as.numeric ( attendance $ diff.out.time )) )) + geom_density ( aes ( y = ..density.. , color = \"Density\" ), size = 1 ) + scale_colour_manual ( name = \"Distribution\" , values = legendcols ) + scale_fill_manual ( name = \"Bar\" , values = legendcols ) + labs ( x = 'Out-time difference (minutes)' , y = 'Density' ) + theme_minimal () + theme ( legend.position = \"bottom\" ) In the above graph, the red line is a normal distribution(with the same mean and standard deviation) while the blue line is the density plot of in-time. Q-Q plot \u00b6 In statistics, a Q-Q (quantile-quantile) plot is a probability plot, a graphical method for comparing two probability distributions by plotting their quantiles against each other. If the two distributions being compared are similar, the points in the Q-Q plot will approximately lie on the line y = x. If the distributions are linearly related, the points in the Q-Q plot will approximately lie on a line, but not necessarily on the line y = x. Q-Q plots can also be used as a graphical means of estimating parameters in a location-scale family of distributions. A Q-Q plot is used to compare the shapes of distributions, providing a graphical view of how properties such as location, scale, and skewness are similar or different in the two distributions. Below is a Q-Q plot with a normal distribution ggplot ( attendance , aes ( sample = as.numeric ( diff.out.time ))) + stat_qq () + stat_qq_line () + ggtitle ( \"Normal distribution Q-Q plot\" ) + theme_minimal () We can see that the distribution is not normal. Trying to check with exponential distribution params <- as.list ( fitdistr ( rexp ( nrow ( attendance ), rate = 2 ), \"exponential\" ) $ estimate ) ggplot ( attendance , aes ( sample = as.numeric ( diff.out.time ))) + stat_qq ( distribution = qexp , dparams = params ) + stat_qq_line ( distribution = qexp , dparams = params ) + ggtitle ( \"Exponential distribution Q-Q plot\" ) + theme_minimal () From the above graph, I am approximating my distribution to an exponential distribution. lamda <- 1 / mean ( sd ( as.numeric ( attendance $ diff.out.time )), mean ( as.numeric ( attendance $ diff.out.time ))) exp.curve <- function ( x ){ lamda * exp ( - lamda * ( x +15 )) } legendcols <- c ( \"Exponential distribution\" = \"darkred\" , \"Density\" = \"darkBlue\" , \"Histogram\" = \"lightBlue\" ) ggplot ( attendance , aes ( x = as.numeric ( diff.out.time ))) + geom_histogram ( aes ( y = ..density.. , fill = \"Histogram\" ), bins = 50 ) + stat_function ( fun = exp.curve , aes ( color = \"Exponential distribution\" ), size = 1 ) + geom_density ( aes ( y = ..density.. , color = \"Density\" ), size = 1 ) + scale_colour_manual ( name = \"Distribution\" , values = legendcols ) + scale_fill_manual ( name = \"Bar\" , values = legendcols ) + labs ( x = 'Out-time difference (minutes)' , y = 'Density' ) + theme_minimal () + theme ( legend.position = \"bottom\" ) In the above graph, the red line is exponential distribution, while the blue line is the density plot of in-time. Created using RMarkdown.","title":"Univariate Analysis (R)"},{"location":"R/Univariate-analysis/#introduction","text":"A univariate analysis is the simplest form of EDA. \"Uni\" means \"one\", so in other words, this analysis is on only one variable. It doesn't deal with causes or relationships, and its primary purpose is to describe; it takes data, summarizes that data and finds patterns in it. In describing or characterizing the observations of an individual variable, three basic properties are of interest: 1. The location of observations, or how large or small the values of the individual observations are 2. The dispersion (sometimes called scale or spread) of the observations 3. The distribution of the observations Uni-variate plots provide one way to find out about those properties. There are two basic kinds of univariate plots: 1. Enumeration plots, or plots that show every observation 2. Summary plots that generalize the data into a simplified representation. For the current tutorial, I will be using my office attendance data set. The data set contains the time when I swiped into the office and when I swiped out of the office. Data from 4 th October 2017 to 29 th November 2018. After some data set manipulation, I will get the difference between policy out-time and my actual out-time. I can leave 15 minutes before the policy out time. After manipulation, a sample of the data is as follows: (Actual data is not shown for security reasons. This is mock data that is very similar to the actual one.) ## Attendance.Date diff.in.time diff.out.time ## 1 2018-03-22 18 mins 226 mins ## 2 2018-08-14 -9 mins 5 mins ## 3 2017-12-04 42 mins 11 mins ## 4 2018-03-01 26 mins -6 mins ## 5 2018-01-23 35 mins -4 mins","title":"Introduction"},{"location":"R/Univariate-analysis/#summary-statistics","text":"Before further analysis, some basic summary statistics would show me the mean and standard deviation of the data. For this tutorial, I will use diff.in.time (difference between actual in-time and policy in-time) mean ( as.numeric ( attendance $ diff.out.time )) # Mean in minutes ## [1] 20.3227 sd ( as.numeric ( attendance $ diff.out.time )) # Standard Deviation in minutes ## [1] 69.06549 nrow ( attendance ) # Length of the data set ## [1] 282","title":"Summary Statistics"},{"location":"R/Univariate-analysis/#enumerative-plots","text":"\"Enumerative plots\" are called such because they enumerate or show every individual data point.","title":"Enumerative plots"},{"location":"R/Univariate-analysis/#index-plotunivariate-scatter-diagram","text":"Displays the values of a single variable for each observation using symbols plotted relative to the observation number. ggplot ( attendance , aes ( x = Attendance.Date , y = as.numeric ( diff.out.time ), color = ( diff.out.time >= -15 ))) + geom_point ( show.legend = FALSE ) + labs ( x = 'Time' , y = 'Out-time difference (Minutes)' ) + theme_minimal () Just looking at this plot, I can say the following: 1. I could cluster into three parts. - One cluster would be before December 2017, where I used to leave the office way after my out-time. - The second cluster would be from December 2017 to June 2018, where I used to leave the office 15 minutes before my out time. - The third cluster was after June 2018, when I was leaving way after my out-time. 2. The red dots indicate the days when I came to the office after 15 minutes from in-time. They are anomalies, days when I took half days etc. We can exclude them from our current analysis.","title":"Index Plot/Univariate Scatter Diagram"},{"location":"R/Univariate-analysis/#y-zero-high-density-plot","text":"Another way to look at the same data is by using a Y Zero High-Density Plot. It displays the values of a single variable plotted as thin vertical lines. Here the magnitude of the observations iss highlighted. ggplot ( attendance , aes ( x = Attendance.Date , y = 0 , color = ( diff.out.time >= -15 ), xend = Attendance.Date , yend = as.numeric ( diff.out.time ))) + geom_segment ( show.legend = FALSE ) + labs ( x = 'Time' , y = 'Out-time difference (Minutes)' ) + theme_minimal () Removing half-days as outliers attendance <- attendance %>% filter ( diff.out.time >= -15 )","title":"Y Zero High-Density Plot"},{"location":"R/Univariate-analysis/#strip-plotstrip-chart-univariate-scatter-diagram","text":"Displays the values of a single variable as symbols plotted along a line. This is a basic plot where we can see the spread of the data. stripchart ( x = as.numeric ( attendance $ diff.out.time ), xlab = 'Out-time difference (minutes)' ) Sometimes it is more visually apparent when the points are stacked. ggplot ( attendance , aes ( x = as.numeric ( diff.out.time ), y = ..density.. )) + geom_dotplot ( binwidth = 3 , method = 'histodot' ) + labs ( x = 'Out-time difference (minutes)' ) + theme_minimal () We can observe that the number of observations is high and slowly tends to drop as time progresses.","title":"Strip Plot/Strip Chart (univariate scatter diagram)"},{"location":"R/Univariate-analysis/#dot-plotdot-chart","text":"Displays the values of a single variable as symbols plotted along a line. With a separate line for each observation, it is generally constructed after sorting the rows of the data table. df = attendance %>% arrange ( as.numeric ( diff.out.time )) ggplot ( df , aes ( x = as.numeric ( row.names ( df )), y = as.numeric ( diff.out.time ))) + geom_point () + labs ( x = 'count' , y = 'Out time difference (min)' ) + theme_minimal () From the graph, I can observe that the distribution initially seems to be exponential. A sample normal distribution is plotted for reference. We can see that the distribution looks nowhere like a normal distribution. Instead, I suspect that it is close to an exponential distribution.","title":"Dot Plot/Dot Chart"},{"location":"R/Univariate-analysis/#univariate-summary-plots","text":"Summary plots display an object or graph that gives a more concise expression of a variable's location, dispersion, and distribution than an enumerative plot. This comes at the expense of some loss of information: In a summary plot, it is no longer possible to retrieve the individual data value, but the gain usually matches this loss in understanding that results from the efficient representation of the data. Summary plots generally prove to be much better than the enumerative plots in revealing the data distribution.","title":"Univariate Summary Plots"},{"location":"R/Univariate-analysis/#box-plot","text":"A simple way of representing statistical data on a plot in which a rectangle is drawn to represent the second and third quartiles, usually with a vertical line inside to indicate the median value. The lower and upper quartiles are shown as horizontal lines on either side of the rectangle. ggplot ( attendance , aes ( x = \"Box Plot\" , y = as.numeric ( diff.out.time ), group = 123 )) + geom_boxplot () + labs ( y = 'Out time difference (min)' ) + theme_minimal ()","title":"Box plot"},{"location":"R/Univariate-analysis/#histograms","text":"The other summary plots are of various types: Histograms: Histograms are bar charts that display the counts or relative frequencies of values falling in different class intervals or ranges. Density Plots: A density plot plots the local relative frequency or density of points along the number line or x-axis of a plot. The local density is determined by summing the individual \"kernel\" densities for each point. Where points occur more frequently, this sum, and consequently the local density, will be greater. legendcols <- c ( \"Normal distribution\" = \"darkred\" , \"Density\" = \"darkBlue\" , \"Histogram\" = \"lightBlue\" ) ggplot ( attendance , aes ( x = as.numeric ( diff.out.time ))) + geom_histogram ( aes ( y = ..density.. , fill = \"Histogram\" ), bins = 50 ) + stat_function ( fun = dnorm , aes ( color = \"Normal distribution\" ), size = 1 , args = list ( mean = mean ( as.numeric ( attendance $ diff.out.time )), sd = sd ( as.numeric ( attendance $ diff.out.time )) )) + geom_density ( aes ( y = ..density.. , color = \"Density\" ), size = 1 ) + scale_colour_manual ( name = \"Distribution\" , values = legendcols ) + scale_fill_manual ( name = \"Bar\" , values = legendcols ) + labs ( x = 'Out-time difference (minutes)' , y = 'Density' ) + theme_minimal () + theme ( legend.position = \"bottom\" ) In the above graph, the red line is a normal distribution(with the same mean and standard deviation) while the blue line is the density plot of in-time.","title":"Histograms"},{"location":"R/Univariate-analysis/#q-q-plot","text":"In statistics, a Q-Q (quantile-quantile) plot is a probability plot, a graphical method for comparing two probability distributions by plotting their quantiles against each other. If the two distributions being compared are similar, the points in the Q-Q plot will approximately lie on the line y = x. If the distributions are linearly related, the points in the Q-Q plot will approximately lie on a line, but not necessarily on the line y = x. Q-Q plots can also be used as a graphical means of estimating parameters in a location-scale family of distributions. A Q-Q plot is used to compare the shapes of distributions, providing a graphical view of how properties such as location, scale, and skewness are similar or different in the two distributions. Below is a Q-Q plot with a normal distribution ggplot ( attendance , aes ( sample = as.numeric ( diff.out.time ))) + stat_qq () + stat_qq_line () + ggtitle ( \"Normal distribution Q-Q plot\" ) + theme_minimal () We can see that the distribution is not normal. Trying to check with exponential distribution params <- as.list ( fitdistr ( rexp ( nrow ( attendance ), rate = 2 ), \"exponential\" ) $ estimate ) ggplot ( attendance , aes ( sample = as.numeric ( diff.out.time ))) + stat_qq ( distribution = qexp , dparams = params ) + stat_qq_line ( distribution = qexp , dparams = params ) + ggtitle ( \"Exponential distribution Q-Q plot\" ) + theme_minimal () From the above graph, I am approximating my distribution to an exponential distribution. lamda <- 1 / mean ( sd ( as.numeric ( attendance $ diff.out.time )), mean ( as.numeric ( attendance $ diff.out.time ))) exp.curve <- function ( x ){ lamda * exp ( - lamda * ( x +15 )) } legendcols <- c ( \"Exponential distribution\" = \"darkred\" , \"Density\" = \"darkBlue\" , \"Histogram\" = \"lightBlue\" ) ggplot ( attendance , aes ( x = as.numeric ( diff.out.time ))) + geom_histogram ( aes ( y = ..density.. , fill = \"Histogram\" ), bins = 50 ) + stat_function ( fun = exp.curve , aes ( color = \"Exponential distribution\" ), size = 1 ) + geom_density ( aes ( y = ..density.. , color = \"Density\" ), size = 1 ) + scale_colour_manual ( name = \"Distribution\" , values = legendcols ) + scale_fill_manual ( name = \"Bar\" , values = legendcols ) + labs ( x = 'Out-time difference (minutes)' , y = 'Density' ) + theme_minimal () + theme ( legend.position = \"bottom\" ) In the above graph, the red line is exponential distribution, while the blue line is the density plot of in-time. Created using RMarkdown.","title":"Q-Q plot"},{"location":"R/multivariateAnalysis/","text":"Introduction \u00b6 Multivariate EDA techniques generally show the relationship between two or more variables with the depandant variable in the form of either cross-tabulation, statistics or visually. In the current problem it will help us look at relationships between our data. This blog is a part of in-time analysis problem . I want to analyse my entry time at office and understand what factors effect it. After integrating Google maps data with attendence dataset , I currently have the factors 1. date (month / week day / season etc) 2. main_activity (means of transport) 3. hours.worked (of the previous day) 4. travelling.time (time it took to travel from house to office) 5. home.addr (the place of residence) The dependent variable is diff.in.time (difference between my actual in time vs policy in-time) A sample of the data is shown Sample Data diff.in.time date main_activity hours.worked travelling.time home.addr diff.out.time -9 2018-08-14 IN_VEHICLE 8.933333 900.719 Old House 5 17 2018-03-16 ON_FOOT 9.116667 930.126 Old House -10 -14 2018-09-10 ON_FOOT 4.583333 1179.873 Old House -251 -7 2018-10-19 ON_BICYCLE 9.583333 1501.060 New House 42 -9 2018-06-28 IN_VEHICLE 9.783333 670.700 Old House 56 Cross-tabulation \u00b6 For categorical data cross-tabulation is very useful. For two variables, cross-tabulation is performed by making a two-way table with column headings that match the levels of one variable and row headings that match the levels of the other variable, then filling in the counts of all subjects that share a pair of levels. The two variables might be both explanatory, both outcome, or one of each. I am using Kable to make cool tables. cross_table <- travel %>% group_by ( home.addr , main_activity ) %>% summarise ( avg.travel.time = mean ( travelling.time ), avg.in.time.diff = mean ( diff.in.time ), median.in.time.diff = median ( diff.in.time )) %>% arrange ( home.addr , main_activity ) library ( kableExtra ) kable ( cross_table , caption = 'Cross Tabulation' ) %>% kable_styling ( full_width = F ) %>% column_spec ( 1 , bold = T ) %>% collapse_rows ( columns = 1 : 2 , valign = \"middle\" ) %>% scroll_box () Cross Tabulation home.addr main_activity avg.travel.time avg.in.time.diff median.in.time.diff New House IN_VEHICLE 1285.0264 -1.800000 -3 New House ON_BICYCLE 1547.5557 -4.000000 -6 New House ON_FOOT 1695.7091 5.285714 5 Old House IN_VEHICLE 771.1752 2.857143 -4 Old House ON_BICYCLE 1029.6329 14.941176 18 Old House ON_FOOT 1170.4783 17.433628 17 Scatter plots \u00b6 Scatter plots show how much one variable is affected by another. To see how travelling time affects in-time ggplot ( travel , aes ( x = diff.in.time , y = travelling.time , color = main_activity )) + geom_point ( show.legend = TRUE ) + labs ( x = 'In-time difference (Minutes)' , y = 'Travelling time (seconds)' , title = \"Travelling time vs in-time\" , color = 'Mode of transport' ) + theme_minimal () + theme ( legend.position = \"bottom\" ) From the above graph, I can see that: 1. For bicycle, as travelling time decreases(low traffic) in-time difference increases(coming earlier to office) 2. There seems to be no relationship between travelling time (traffic) and in-time difference when on foot. 3. Travelling time has little affect on it-time difference when travelling on vehicle. To see how hours worked(on previous day) affects in-time From the above graph, I can observe that irrespective of mode of transport, my in-time difference increases (coming earlier to office) as hours worked on the previous day increases. Box plots \u00b6 Similarly, I want to see how mode of transport affects in-time difference. For categorical variable, box plots display this information in the most ideal manner. ggplot ( travel , aes ( x = main_activity , y = diff.in.time , group = main_activity )) + geom_boxplot () + labs ( x = 'Mode of transport' , y = 'In time difference (min)' ) + theme_minimal () From the above graph, I can observe that: 1. On vehicle, I went to office on average, ~12 minutes after the policy in-time (in-time difference is -12) 2. On cycle, I went to office almost close to the policy in-time 3. While walking, I was almost always before the policy in-time Similarly for place of residence. From this graph, I can understand that from New house I was close to ~5 minutes after the policy in-time while I used to be on-time while living in Old house. Created using R Markdown. Credits: Thinkstats Experimental Design and Analysis","title":"Multivariate Analysis (R)"},{"location":"R/multivariateAnalysis/#introduction","text":"Multivariate EDA techniques generally show the relationship between two or more variables with the depandant variable in the form of either cross-tabulation, statistics or visually. In the current problem it will help us look at relationships between our data. This blog is a part of in-time analysis problem . I want to analyse my entry time at office and understand what factors effect it. After integrating Google maps data with attendence dataset , I currently have the factors 1. date (month / week day / season etc) 2. main_activity (means of transport) 3. hours.worked (of the previous day) 4. travelling.time (time it took to travel from house to office) 5. home.addr (the place of residence) The dependent variable is diff.in.time (difference between my actual in time vs policy in-time) A sample of the data is shown Sample Data diff.in.time date main_activity hours.worked travelling.time home.addr diff.out.time -9 2018-08-14 IN_VEHICLE 8.933333 900.719 Old House 5 17 2018-03-16 ON_FOOT 9.116667 930.126 Old House -10 -14 2018-09-10 ON_FOOT 4.583333 1179.873 Old House -251 -7 2018-10-19 ON_BICYCLE 9.583333 1501.060 New House 42 -9 2018-06-28 IN_VEHICLE 9.783333 670.700 Old House 56","title":"Introduction"},{"location":"R/multivariateAnalysis/#cross-tabulation","text":"For categorical data cross-tabulation is very useful. For two variables, cross-tabulation is performed by making a two-way table with column headings that match the levels of one variable and row headings that match the levels of the other variable, then filling in the counts of all subjects that share a pair of levels. The two variables might be both explanatory, both outcome, or one of each. I am using Kable to make cool tables. cross_table <- travel %>% group_by ( home.addr , main_activity ) %>% summarise ( avg.travel.time = mean ( travelling.time ), avg.in.time.diff = mean ( diff.in.time ), median.in.time.diff = median ( diff.in.time )) %>% arrange ( home.addr , main_activity ) library ( kableExtra ) kable ( cross_table , caption = 'Cross Tabulation' ) %>% kable_styling ( full_width = F ) %>% column_spec ( 1 , bold = T ) %>% collapse_rows ( columns = 1 : 2 , valign = \"middle\" ) %>% scroll_box () Cross Tabulation home.addr main_activity avg.travel.time avg.in.time.diff median.in.time.diff New House IN_VEHICLE 1285.0264 -1.800000 -3 New House ON_BICYCLE 1547.5557 -4.000000 -6 New House ON_FOOT 1695.7091 5.285714 5 Old House IN_VEHICLE 771.1752 2.857143 -4 Old House ON_BICYCLE 1029.6329 14.941176 18 Old House ON_FOOT 1170.4783 17.433628 17","title":"Cross-tabulation"},{"location":"R/multivariateAnalysis/#scatter-plots","text":"Scatter plots show how much one variable is affected by another. To see how travelling time affects in-time ggplot ( travel , aes ( x = diff.in.time , y = travelling.time , color = main_activity )) + geom_point ( show.legend = TRUE ) + labs ( x = 'In-time difference (Minutes)' , y = 'Travelling time (seconds)' , title = \"Travelling time vs in-time\" , color = 'Mode of transport' ) + theme_minimal () + theme ( legend.position = \"bottom\" ) From the above graph, I can see that: 1. For bicycle, as travelling time decreases(low traffic) in-time difference increases(coming earlier to office) 2. There seems to be no relationship between travelling time (traffic) and in-time difference when on foot. 3. Travelling time has little affect on it-time difference when travelling on vehicle. To see how hours worked(on previous day) affects in-time From the above graph, I can observe that irrespective of mode of transport, my in-time difference increases (coming earlier to office) as hours worked on the previous day increases.","title":"Scatter plots"},{"location":"R/multivariateAnalysis/#box-plots","text":"Similarly, I want to see how mode of transport affects in-time difference. For categorical variable, box plots display this information in the most ideal manner. ggplot ( travel , aes ( x = main_activity , y = diff.in.time , group = main_activity )) + geom_boxplot () + labs ( x = 'Mode of transport' , y = 'In time difference (min)' ) + theme_minimal () From the above graph, I can observe that: 1. On vehicle, I went to office on average, ~12 minutes after the policy in-time (in-time difference is -12) 2. On cycle, I went to office almost close to the policy in-time 3. While walking, I was almost always before the policy in-time Similarly for place of residence. From this graph, I can understand that from New house I was close to ~5 minutes after the policy in-time while I used to be on-time while living in Old house. Created using R Markdown. Credits: Thinkstats Experimental Design and Analysis","title":"Box plots"},{"location":"R/time-series/","text":"MathJax.Hub.Config({ extensions: [\"tex2jax.js\",\"TeX/AMSmath.js\",\"TeX/AMSsymbols.js\"], jax: [\"input/TeX\", \"output/HTML-CSS\"], tex2jax: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ], }, \"HTML-CSS\": { availableFonts: [\"TeX\"] } }); Time Series \u00b6 A time series is a series of data points captured in time order. Most commonly, a time series is a sequence taken at successive equally spaced points in time. This post is the first in a series of blogs on time series methods and forecasting. In this blog, we will discuss about stationarity, random walk, deterministic drift and other vocabulary which form as foundation to time series: Stochastic processes \u00b6 A random or stochastic process is a collection of random variables ordered in time. It is denoted as \\(Y_t\\) . For example, in-time of an employee is a stochastic process. How is in-time a stochastic process? Consider the in-time on a particular day is 9:00 AM. In theory, the in-time could be any particular value which depends on many factors like traffic, work load, weather etc. The figure 9:00 AM is a particular realization of many such possibilities. Therefore we can say that in-time is a stochastic process where as the actual values observed are a particular realization (sample) of the process. Stationary Processes \u00b6 A stochastic process is said to be stationary if the following conditions are met: 1. Mean is constant over time 2. Variance is constant over time 3. Value of the co-variance between two time periods depends only on the distance or gap or lag between the two time periods and not the actual time at which the co variance is computed This type of process is also called weakly stationary, or co variance stationary, or second-order stationary or wide sense stationary process. Written mathematically, the conditions are: $$ Mean: E(Y_t) = \\mu $$ $$ Variance: var(Y_t) = E(Y_t-\\mu)^2 = \\sigma^2 $$ $$ Covariance: \\gamma_k = E[(Y_y - \\mu)(Y_{t+k} - \\mu)] $$ Purely random or white noise process \u00b6 A stochastic process is purely random if it has zero mean, constant variance, and is serially uncorrelated. An example of white noise is the error term in a linear regression which has zero mean, constant standard deviation and no auto-correlation. Simulation \u00b6 For simulating a stationary process, I am creating 100 realizations(samples) and comparing their mean, variance and co-variance. The data for 6 days and 5 realizations is shown: Samples of Stationary process date realization_1 realization_2 realization_25 realization_50 realization_100 1 2021-12-28 0.3409607 0.5713826 0.2313986 0.6050719 0.5335372 2 2021-12-29 0.5554507 0.5244803 0.4288635 0.9073932 0.6350137 3 2021-12-30 0.1281935 0.1139629 0.2330727 0.8417148 0.8781020 10 2022-01-06 0.1901487 0.7607555 0.5620072 0.2611821 0.4575932 15 2022-01-11 0.8317412 0.6043582 0.0995929 0.9609510 0.2208680 30 2022-01-26 0.3612965 0.5961108 0.5965198 0.3048035 0.7668487 The mean, variance and co-variance between the samples (realizations) across are as follows: For a stationary process, the mean, variance and co variance are constant. Non-stationary Processes \u00b6 If a time series is not stationary, it is called a non-stationary time series. In other words, a non-stationary time series will have a time-varying mean or a time-varying variance or both. Random walk, random walk with drift etc are examples of non-stationary processes. Random walk \u00b6 Suppose \\(\\epsilon_t\\) is a white noise error term with mean 0 and variance \\(\u03c3_2\\) . Then the series \\(Y_t\\) is said to be a random walk if $$ Y_t = Y_{t\u22121} + \\epsilon_t $$ In the random walk model, the value of Y at time t is equal to its value at time (t \u2212 1) plus a random shock. For a random walk, $$ Y_1 = Y_0 + \\epsilon_1 $$ $$ Y_2 = Y_1 + \\epsilon_2 = Y_0 + \\epsilon_1 + \\epsilon_2 $$ $$ Y_3 = Y_2 + \\epsilon_3 = Y_0 + \\epsilon_1 + \\epsilon_2 + \\epsilon_3 $$ and so on.. In general we could write $$ Y_t = Y_0 + \\sum \\epsilon_t $$ As $$ E(Y_t) = E(Y_0 + \\sum \\epsilon_t) = Y_0 $$ $$ var(Y_t) = t\\times \\sigma^2 $$ Although the mean is constant with time, the variance is proportional to time. For simulating a random walk process, I am creating 100 realizations(samples) and comparing their mean, variance and co-variance. The data for 6 days of 5 realizations (samples) is shown: Samples of Random walk process date realization_1 realization_2 realization_25 realization_50 realization_100 1 2021-12-28 4.000000 4.0000000 4.000000 4.000000 4.000000 2 2021-12-29 3.215170 4.9727559 4.981838 2.677480 4.209128 3 2021-12-30 2.400451 4.2477385 6.266374 3.249609 5.545876 10 2022-01-06 2.510370 4.1251187 8.500313 4.559066 7.634846 15 2022-01-11 6.286410 5.3430478 9.441353 2.147137 7.098887 30 2022-01-26 2.985008 0.2757552 5.219005 3.402089 4.125985 The mean, variance and covariances between the samples (realizations) across time would look like follows: From the above plot, the mean of Y is equal to its initial, or starting value, which is constant, but as t increases, its variance increases indefinitely, thus violating a condition of stationarity. A random walk process is also called as a unit root process . Random walk with drift \u00b6 If the random walk model predicts that the value at time t will equal the last period's value plus a constant, or drift ( \\(\\delta\\) ), and a white noise term ( \\(\u03b5_t\\) ), then the process is random walk with a drift. $$ Y_t = \\delta + Y_{t\u22121} + \\epsilon_t $$ The mean $$ E(Y_t) = E(Y_0 + \\sum \\epsilon_t + \\delta) = Y_0 + t\\times\\delta $$ so mean is dependent on time and the variance $$ var(Y_t) = t\\times \\sigma^2 $$ is also dependent on time. As random walk with drift violates the conditions of stationary process, it is a non-stationary process. Samples of Random walk with drift process date realization_1 realization_2 realization_25 realization_50 realization_100 1 2021-12-28 4.000000 4.000000 4.000000 4.000000 4.000000 2 2021-12-29 6.707957 4.783724 5.082320 4.050322 6.047140 3 2021-12-30 6.154817 6.034937 6.593877 5.690097 6.443667 10 2022-01-06 3.092089 13.488318 13.143434 11.613472 8.216818 15 2022-01-11 4.827608 16.137101 12.706459 14.614712 12.535962 30 2022-01-26 8.567962 19.017960 20.586592 19.409629 14.157457 The mean, variance and the co-variance are all dependent on time. Unit root stochastic process \u00b6 Unit root stochastic process is another name for Random walk process. A random walk process can be written as $$ Y_t = \\rho \\times Y_{t\u22121} + \\epsilon_t $$ Where \\(\\rho = 1\\) . If \\(|\\rho| < 1\\) then the process represents Markov first order auto regressive model which is stationary. Only for \\(\\rho = 1\\) we get non-stationary. The distribution of mean, variance and co-variance for \\(\\rho =0.5\\) is Deterministic trend process \u00b6 In the above random walk and random walk with drift, the trend component is stochastic in nature. If instead the trend is deterministic in nature, it will follow a deterministic trend process. $$ Y_t = \u03b2_1 + \u03b2_2\\times t + \\epsilon_t$$ In a deterministic trend process, the mean is \\(\u03b2_1 + \u03b2_2\\times t\\) which is proportional with time but the variance is constant. This type of process is also called as trend seasonality as subtracting mean of \\(Y_t\\) from \\(Y_t\\) will give us a stationary process. This procedure is called de-trending. Samples of Deterministic trend process date realization_1 realization_2 realization_25 realization_50 realization_100 1 2021-12-28 0.2435772 0.266316 1.634834 1.501271 -0.2332093 2 2021-12-29 1.7185437 1.974812 1.128986 2.605209 1.0183324 3 2021-12-30 3.0196971 2.321355 3.529886 3.100916 3.2666808 10 2022-01-06 11.8821817 9.759775 11.575552 9.727393 9.3407779 15 2022-01-11 13.3588365 15.525071 15.037742 15.931198 14.2090916 30 2022-01-26 30.2218724 30.342918 30.405570 29.090780 29.6063424 A combination of deterministic and stochastic trend could also exist in a process. Comparison. \u00b6 A comparison of all the processes is shown below: References \u00b6 Basic Ecnometrics - Damodar N Gujarati (textbook for reference) Business Analytics: The Science of Data-Driven Decision Making - Dinesh Kumar (textbook for reference) Customer Analytics at Flipkart.Com - Naveen Bhansali (case study in Harvard business review)","title":"Introduction to stationarity (R)"},{"location":"R/time-series/#time-series","text":"A time series is a series of data points captured in time order. Most commonly, a time series is a sequence taken at successive equally spaced points in time. This post is the first in a series of blogs on time series methods and forecasting. In this blog, we will discuss about stationarity, random walk, deterministic drift and other vocabulary which form as foundation to time series:","title":"Time Series"},{"location":"R/time-series/#stochastic-processes","text":"A random or stochastic process is a collection of random variables ordered in time. It is denoted as \\(Y_t\\) . For example, in-time of an employee is a stochastic process. How is in-time a stochastic process? Consider the in-time on a particular day is 9:00 AM. In theory, the in-time could be any particular value which depends on many factors like traffic, work load, weather etc. The figure 9:00 AM is a particular realization of many such possibilities. Therefore we can say that in-time is a stochastic process where as the actual values observed are a particular realization (sample) of the process.","title":"Stochastic processes"},{"location":"R/time-series/#stationary-processes","text":"A stochastic process is said to be stationary if the following conditions are met: 1. Mean is constant over time 2. Variance is constant over time 3. Value of the co-variance between two time periods depends only on the distance or gap or lag between the two time periods and not the actual time at which the co variance is computed This type of process is also called weakly stationary, or co variance stationary, or second-order stationary or wide sense stationary process. Written mathematically, the conditions are: $$ Mean: E(Y_t) = \\mu $$ $$ Variance: var(Y_t) = E(Y_t-\\mu)^2 = \\sigma^2 $$ $$ Covariance: \\gamma_k = E[(Y_y - \\mu)(Y_{t+k} - \\mu)] $$","title":"Stationary Processes"},{"location":"R/time-series/#purely-random-or-white-noise-process","text":"A stochastic process is purely random if it has zero mean, constant variance, and is serially uncorrelated. An example of white noise is the error term in a linear regression which has zero mean, constant standard deviation and no auto-correlation.","title":"Purely random or white noise process"},{"location":"R/time-series/#simulation","text":"For simulating a stationary process, I am creating 100 realizations(samples) and comparing their mean, variance and co-variance. The data for 6 days and 5 realizations is shown: Samples of Stationary process date realization_1 realization_2 realization_25 realization_50 realization_100 1 2021-12-28 0.3409607 0.5713826 0.2313986 0.6050719 0.5335372 2 2021-12-29 0.5554507 0.5244803 0.4288635 0.9073932 0.6350137 3 2021-12-30 0.1281935 0.1139629 0.2330727 0.8417148 0.8781020 10 2022-01-06 0.1901487 0.7607555 0.5620072 0.2611821 0.4575932 15 2022-01-11 0.8317412 0.6043582 0.0995929 0.9609510 0.2208680 30 2022-01-26 0.3612965 0.5961108 0.5965198 0.3048035 0.7668487 The mean, variance and co-variance between the samples (realizations) across are as follows: For a stationary process, the mean, variance and co variance are constant.","title":"Simulation"},{"location":"R/time-series/#non-stationary-processes","text":"If a time series is not stationary, it is called a non-stationary time series. In other words, a non-stationary time series will have a time-varying mean or a time-varying variance or both. Random walk, random walk with drift etc are examples of non-stationary processes.","title":"Non-stationary Processes"},{"location":"R/time-series/#random-walk","text":"Suppose \\(\\epsilon_t\\) is a white noise error term with mean 0 and variance \\(\u03c3_2\\) . Then the series \\(Y_t\\) is said to be a random walk if $$ Y_t = Y_{t\u22121} + \\epsilon_t $$ In the random walk model, the value of Y at time t is equal to its value at time (t \u2212 1) plus a random shock. For a random walk, $$ Y_1 = Y_0 + \\epsilon_1 $$ $$ Y_2 = Y_1 + \\epsilon_2 = Y_0 + \\epsilon_1 + \\epsilon_2 $$ $$ Y_3 = Y_2 + \\epsilon_3 = Y_0 + \\epsilon_1 + \\epsilon_2 + \\epsilon_3 $$ and so on.. In general we could write $$ Y_t = Y_0 + \\sum \\epsilon_t $$ As $$ E(Y_t) = E(Y_0 + \\sum \\epsilon_t) = Y_0 $$ $$ var(Y_t) = t\\times \\sigma^2 $$ Although the mean is constant with time, the variance is proportional to time. For simulating a random walk process, I am creating 100 realizations(samples) and comparing their mean, variance and co-variance. The data for 6 days of 5 realizations (samples) is shown: Samples of Random walk process date realization_1 realization_2 realization_25 realization_50 realization_100 1 2021-12-28 4.000000 4.0000000 4.000000 4.000000 4.000000 2 2021-12-29 3.215170 4.9727559 4.981838 2.677480 4.209128 3 2021-12-30 2.400451 4.2477385 6.266374 3.249609 5.545876 10 2022-01-06 2.510370 4.1251187 8.500313 4.559066 7.634846 15 2022-01-11 6.286410 5.3430478 9.441353 2.147137 7.098887 30 2022-01-26 2.985008 0.2757552 5.219005 3.402089 4.125985 The mean, variance and covariances between the samples (realizations) across time would look like follows: From the above plot, the mean of Y is equal to its initial, or starting value, which is constant, but as t increases, its variance increases indefinitely, thus violating a condition of stationarity. A random walk process is also called as a unit root process .","title":"Random walk"},{"location":"R/time-series/#random-walk-with-drift","text":"If the random walk model predicts that the value at time t will equal the last period's value plus a constant, or drift ( \\(\\delta\\) ), and a white noise term ( \\(\u03b5_t\\) ), then the process is random walk with a drift. $$ Y_t = \\delta + Y_{t\u22121} + \\epsilon_t $$ The mean $$ E(Y_t) = E(Y_0 + \\sum \\epsilon_t + \\delta) = Y_0 + t\\times\\delta $$ so mean is dependent on time and the variance $$ var(Y_t) = t\\times \\sigma^2 $$ is also dependent on time. As random walk with drift violates the conditions of stationary process, it is a non-stationary process. Samples of Random walk with drift process date realization_1 realization_2 realization_25 realization_50 realization_100 1 2021-12-28 4.000000 4.000000 4.000000 4.000000 4.000000 2 2021-12-29 6.707957 4.783724 5.082320 4.050322 6.047140 3 2021-12-30 6.154817 6.034937 6.593877 5.690097 6.443667 10 2022-01-06 3.092089 13.488318 13.143434 11.613472 8.216818 15 2022-01-11 4.827608 16.137101 12.706459 14.614712 12.535962 30 2022-01-26 8.567962 19.017960 20.586592 19.409629 14.157457 The mean, variance and the co-variance are all dependent on time.","title":"Random walk with drift"},{"location":"R/time-series/#unit-root-stochastic-process","text":"Unit root stochastic process is another name for Random walk process. A random walk process can be written as $$ Y_t = \\rho \\times Y_{t\u22121} + \\epsilon_t $$ Where \\(\\rho = 1\\) . If \\(|\\rho| < 1\\) then the process represents Markov first order auto regressive model which is stationary. Only for \\(\\rho = 1\\) we get non-stationary. The distribution of mean, variance and co-variance for \\(\\rho =0.5\\) is","title":"Unit root stochastic process"},{"location":"R/time-series/#deterministic-trend-process","text":"In the above random walk and random walk with drift, the trend component is stochastic in nature. If instead the trend is deterministic in nature, it will follow a deterministic trend process. $$ Y_t = \u03b2_1 + \u03b2_2\\times t + \\epsilon_t$$ In a deterministic trend process, the mean is \\(\u03b2_1 + \u03b2_2\\times t\\) which is proportional with time but the variance is constant. This type of process is also called as trend seasonality as subtracting mean of \\(Y_t\\) from \\(Y_t\\) will give us a stationary process. This procedure is called de-trending. Samples of Deterministic trend process date realization_1 realization_2 realization_25 realization_50 realization_100 1 2021-12-28 0.2435772 0.266316 1.634834 1.501271 -0.2332093 2 2021-12-29 1.7185437 1.974812 1.128986 2.605209 1.0183324 3 2021-12-30 3.0196971 2.321355 3.529886 3.100916 3.2666808 10 2022-01-06 11.8821817 9.759775 11.575552 9.727393 9.3407779 15 2022-01-11 13.3588365 15.525071 15.037742 15.931198 14.2090916 30 2022-01-26 30.2218724 30.342918 30.405570 29.090780 29.6063424 A combination of deterministic and stochastic trend could also exist in a process.","title":"Deterministic trend process"},{"location":"R/time-series/#comparison","text":"A comparison of all the processes is shown below:","title":"Comparison."},{"location":"R/time-series/#references","text":"Basic Ecnometrics - Damodar N Gujarati (textbook for reference) Business Analytics: The Science of Data-Driven Decision Making - Dinesh Kumar (textbook for reference) Customer Analytics at Flipkart.Com - Naveen Bhansali (case study in Harvard business review)","title":"References"}]}