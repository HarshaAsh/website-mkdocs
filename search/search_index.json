{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"this is a sample page","title":"Home"},{"location":"Python/Bipartite%20matching/","text":"Matching algorithms \u00b6 Stock markets, housing and labor markets, dating and so forth are examples of matching tasks. Let us take suppliers an buyers as an example. in a matching problem, our job is to match the supliers to buyers so that both sides/ either side are satisfied. Matching problems can be considered as network problems. In network terms, a matching is a subset of edges where every node in one group goes through only one node in the other group. There should be only one edge from each node. # Import networkx library and rename it as nx. import networkx as nx # Other packages required import numpy as np import pandas as pd import matplotlib.pyplot as plt Unweighted bipartite mapping \u00b6 Let us take a simple example of mapping students and dorm rooms. In this problem, students give a list of rooms they are willing to stay at. We represent students on side as nodes of a bipartite graph and rooms on the other side as nodes, and we put an edge between students and rooms as per this list. We want to map students and rooms. We want to identify a subset where we match one student to exactly one other room (no roommates). The students give what rooms are acceptable, and many solutions are possible. Consider the below problem where we have students (A, B, C and D) and we want to match them to rooms (1,2,3,4). The list of acceptable rooms for each student is given below. edgelist_df = pd . DataFrame ({ 'node1' :[ 'A' , 'A' , 'B' , 'B' , 'C' , 'C' , 'D' , 'D' ], 'node2' :[ 1 , 2 , 2 , 3 , 3 , 4 , 4 , 1 ]}) edgelist_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } node1 node2 0 A 1 1 A 2 2 B 2 3 B 3 4 C 3 5 C 4 6 D 4 7 D 1 This can be represented as a bipartite graph as follows g = nx . Graph () for i , elrow in edgelist_df . iterrows (): g . add_edge ( elrow [ 0 ], elrow [ 1 ]) # Make two sets in bipartite and get positions for the same left , right = nx . bipartite . sets ( g ) pos = {} # Update position for node from each group for i , node in enumerate ( sorted ( list ( left ))): g . add_node ( node , pos = ( 0 , i )) for i , node in enumerate ( sorted ( list ( right ))): g . add_node ( node , pos = ( 1 , i )) nx . draw ( g , pos = nx . get_node_attributes ( g , 'pos' ), with_labels = True ) Visually, we can find a couple of solutions, for example: - A:1, B:2, C:3, D:4 - A:2, B:3, C:4, D:1 In large graphs visual analysis might be difficult, and in such situations Halls theorem is useful to identify if matching is possible. Halls theorem \u00b6 But before Halls theorem, let us look at constricted set. Constricted set : A constricted set is a subset of edges (on either side) whose neighbours are smaller than the subset. For examle, if we have two students (subset of students) who give only one room (same room) in the list, then the size of the students is 2 and the size of the rooms is one, and no matching can be done. Halls theorem states that for a mapping to exist, there should be no constricted set. Augmenting paths 1. Select any random matching of unmatched nodes. 2. Switch to the augumented paths if it exists. If it doesnt exist, then we have a constricted set and we cannot do matching. 3. Repeat until all left nodes are matched to one right node. This is implemented in NetworkX as follows: # Select random edges selected_edges = [] for left_node in left : # For a left node, select a random node in the right list_of_nodes = list ( g . edges ( left_node )) random_node = np . random . randint ( len ( list_of_nodes )) selected_edges . append ( list_of_nodes [ random_node ]) selected_edges [('D', 4), ('C', 4), ('A', 2), ('B', 3)] Selected edges are shown in green for edge in g . edges : if edge in selected_edges or ( edge [ 1 ], edge [ 0 ]) in selected_edges : g . add_edge ( edge [ 0 ], edge [ 1 ], color = 'g' , weight = 10 ) else : g . add_edge ( edge [ 0 ], edge [ 1 ], color = 'b' , weight = 0.1 ) nx . draw ( g , pos = nx . get_node_attributes ( g , 'pos' ), with_labels = True , edge_color = nx . get_edge_attributes ( g , 'color' ) . values ()) We can see that this is not matching as node C and D are mapped to 4. This can be resolved by moving through the augumented paths for the C-4 node. For the C-4 node, moving thru the augumented path selects the C-3 and D-4 node. Following the smae process with with B-3 and A-3 nodes, we get: selected_edges = {( 1 , 'A' ), ( 2 , 'B' ), ( 3 , 'C' ), ( 4 , 'D' )} for edge in g . edges : if edge in selected_edges or ( edge [ 1 ], edge [ 0 ]) in selected_edges : g . add_edge ( edge [ 0 ], edge [ 1 ], color = 'g' , weight = 10 ) else : g . add_edge ( edge [ 0 ], edge [ 1 ], color = 'b' , weight = 0.1 ) nx . draw ( g , pos = nx . get_node_attributes ( g , 'pos' ), with_labels = True , edge_color = nx . get_edge_attributes ( g , 'color' ) . values ()) The final mapping is shown in green nx . algorithms . bipartite . matching . hopcroft_karp_matching ( g , top_nodes = list ( set ( edgelist_df . node1 ))) {'D': 4, 'C': 3, 'B': 2, 'A': 1, 1: 'A', 2: 'B', 3: 'C', 4: 'D'} Weighted bipartite mapping \u00b6 In the previous problem, we tried to find a perfect maching in an unweighted graph. What if every edge in the graph has certain weight attached to it. The weights could be quality index (for student-dorm matching) or valuations in a market etc. Consider the suppliers-buyers example for a housing market as shown below. We have three suppliers, A, B and C and three buyers (x, y and z). The valuation for each of the sellers is also given. For example, buyer x values house A with 12, house B with 4 and house C with 2. sellers = [ 'A' , 'B' , 'C' ] buyers = [ 'x' , 'y' , 'z' ] valuations_for_buyers = [[ 12 , 4 , 2 ], [ 8 , 7 , 6 ], [ 7 , 5 , 2 ]] sellers_price = [ 0 , 0 , 0 ] g = nx . Graph () pos = {} # Update position for node from each group for i , node in enumerate ( sellers ): g . add_node ( node , pos = ( 0 , len ( sellers ) - i )) for i , node in enumerate ( buyers ): g . add_node ( node , pos = ( 1 , len ( buyers ) - i )) # Plot text for the buyers for i , buyer in enumerate ( buyers ): plt . text ( 1.1 , len ( buyers ) - i , s = valuations_for_buyers [ i ], horizontalalignment = 'left' ) # Plot text for the sellers for i , buyer in enumerate ( buyers ): plt . text ( - 0.1 , len ( buyers ) - i , s = sellers_price [ i ], horizontalalignment = 'right' ) nx . draw ( g , pos = nx . get_node_attributes ( g , 'pos' ), with_labels = True ) What we want to achieve is clearing of the market. Clearing happens when all houses are sold to one buyer, and every buyer bought one house. This can be done using an auction algorithm. 1. Sellers quote a price 2. Buyers calculate utility: Net valuation (payoff) = Gross Valuation - Price charged by seller 3. Buyers select the object that has highest payoff 4. If the market is not cleared, the sellers who have more than one offer (overdetermined) will increase the price by one unit, and the process is repeated. # Function to pick the supplier with the maximumm utility def match_to_maximum_utility ( sellers , buyers , valuation , price ): max_utility_sellers = {} for buyer_index in range ( len ( buyers )): max_utility = 0 for seller_index in range ( len ( sellers )): if ( max_utility < valuation [ buyer_index ][ seller_index ] - price [ seller_index ]): max_utility = valuation [ buyer_index ][ seller_index ] - price [ seller_index ] max_utility_sellers [ buyers [ buyer_index ]] = [ sellers [ seller_index ]] elif ( max_utility == valuation [ buyer_index ][ seller_index ] - price [ seller_index ]): max_utility_sellers [ buyers [ buyer_index ]] . append ( sellers [ seller_index ]) return max_utility_sellers Assuming that the initial price set by the seller is zero (scaled to zero - displayed beside the node), buyer x calculates the following utility: - For A: 12-0 = 12 - For B: 4-0 = 4 - For C: 2-0 = 2 As the utility of A is the highest, x will chose A. Similarly, B and C will also choose A. max_util = match_to_maximum_utility ( sellers , buyers , valuations_for_buyers , sellers_price ) max_util {'x': ['A'], 'y': ['A'], 'z': ['A']} Plotting the selection, we see that A is overdetermined. def plot_max_utility_graph ( sellers , buyers , valuations_for_buyers , sellers_price , max_util ): g = nx . Graph () pos = {} # Update position for node from each group for i , node in enumerate ( sellers ): g . add_node ( node , pos = ( 0 , len ( sellers ) - i )) for i , node in enumerate ( buyers ): g . add_node ( node , pos = ( 1 , len ( buyers ) - i )) # Make edges for key , values in max_util . items (): for value in values : g . add_edge ( key , value ) # Plot text for the buyers for i , buyer in enumerate ( buyers ): plt . text ( 1.1 , len ( buyers ) - i , s = valuations_for_buyers [ i ], horizontalalignment = 'left' ) # Plot text for the sellers for i , buyer in enumerate ( buyers ): plt . text ( - 0.1 , len ( buyers ) - i , s = sellers_price [ i ], horizontalalignment = 'right' ) nx . draw ( g , pos = nx . get_node_attributes ( g , 'pos' ), with_labels = True ) plt . show () plot_max_utility_graph ( sellers , buyers , valuations_for_buyers , sellers_price , max_util ) For the overdetermined edge A, we increase the price by one unit. def get_overdetermined_edge_and_increase_price ( sellers , sellers_price , max_util ): from collections import Counter counts = dict ( Counter ( sum ( max_util . values (), []))) over_determined_list = [] for key , value in counts . items (): if ( value > 1 ): over_determined_list . append ( key ) sellers_price [ sellers . index ( key )] += 1 print ( 'Nodes' , over_determined_list , 'are over determined. Added 1 to the price for the suppliers' ) return counts get_overdetermined_edge_and_increase_price ( sellers , sellers_price , max_util ) Nodes ['A'] are over determined. Added 1 to the price for the suppliers {'A': 3} We then contiue this process until the market is cleared. no_of_sellers_selected = 0 while ( no_of_sellers_selected != len ( sellers )): max_util = match_to_maximum_utility ( sellers , buyers , valuations_for_buyers , sellers_price ) plot_max_utility_graph ( sellers , buyers , valuations_for_buyers , sellers_price , max_util ) counts = get_overdetermined_edge_and_increase_price ( sellers , sellers_price , max_util ) no_of_sellers_selected = len ( counts ) # need not always be the case, check Nodes ['A'] are over determined. Added 1 to the price for the suppliers Nodes ['A', 'B'] are over determined. Added 1 to the price for the suppliers Nodes ['A', 'B'] are over determined. Added 1 to the price for the suppliers We can see that for costs (A:3, B:1, C:0), the market can be cleared with buyer x choosing A, y choosing C and z choosing B. This is the maximum weight perfect matching. Matching with preferences \u00b6 In the previous scenario, we had weights on the edges which indicated the utility. In this case we will look at matching where we have preferences in a ranked order. This is more natural way in many scenarios, like students preference to universities/universities selecting students, dating scenarios etc. This was originally implemented by Al Roth for matching hospitals and residency. Our goal is to clear the market, but also have a stable matching. So, what is a stable matching? Stable matching : Stability is an eqvilibrium when no pair on ether side has an incentive to devaite from the mapping. Let us understand this using an example. Take the dating scenario for example. On the left hand side we have men and on the right hand side we have women. All men rank women in strict order and all women rank men in strict order. In a stable matching, no pair of nodes (male-female) prefers each other to their curently assigned partners. Gale Shapley Algorithm \u00b6 Let us say that are n players on both sides with males(m) on one side and women(w) on another side. The algorithm is as follows: 1. Every unmatched male (m) proposses to their first preference available. 2. If the proposed women (w) is unmatched, w accepts. If the women is already matched and the m has higher preference for w, w switches. Else, previous mapping remains. 3. This process continues until there is stability Lets take an example with three males and three females. The preferences are mentioned at the side of the node in a list. For example, m2 has a preference w2, followed by w3 and then w1. Similarly w2 has a preference of m3, followed by m1 and then m2. males = [ 'm1' , 'm2' , 'm3' ] females = [ 'w1' , 'w2' , 'w3' ] male_preferences = [[ 'w1' , 'w2' , 'w3' ], [ 'w2' , 'w3' , 'w1' ], [ 'w2' , 'w3' , 'w1' ]] female_preferences = [[ 'm2' , 'm3' , 'm1' ], [ 'm3' , 'm1' , 'm2' ], [ 'm1' , 'm2' , 'm3' ]] def match_next_male ( male_index , males , females , male_preferences , female_preferences ): for female in male_preferences [ male_index ]: if ( female not in current_mapping . values ()): current_mapping [ males [ male_index ]] = female return current_mapping elif ( female in current_mapping . values ()): current_mapping_inverse = dict ( zip ( current_mapping . values (), current_mapping . keys ())) current_male_for_the_female = current_mapping_inverse [ female ] if ( female_preferences [ males . index ( current_male_for_the_female )] > female_preferences [ male_index ]): current_mapping [ males [ male_index ]] = female current_mapping . pop ( current_male_for_the_female ) return current_mapping def plot_max_utility_graph ( males , females , male_preferences , female_preferences , current_mapping ): g = nx . Graph () pos = {} # Update position for node from each group for i , node in enumerate ( males ): g . add_node ( node , pos = ( 0 , len ( males ) - i )) for i , node in enumerate ( females ): g . add_node ( node , pos = ( 1 , len ( females ) - i )) # Make edges for key , value in current_mapping . items (): g . add_edge ( key , value ) # Plot text for the males for i , male in enumerate ( males ): plt . text ( - 0.1 , len ( males ) - i , s = male_preferences [ i ], horizontalalignment = 'right' ) # Plot text for the females for i , female in enumerate ( females ): plt . text ( 1.1 , len ( females ) - i , s = female_preferences [ i ], horizontalalignment = 'left' ) nx . draw ( g , pos = nx . get_node_attributes ( g , 'pos' ), with_labels = True ) plt . show () print ( '_____________________________________________________________________________' ) current_mapping = {} while ( len ( current_mapping ) != len ( males )): for male_index in range ( len ( males )): if ( current_mapping . get ( males [ male_index ]) is None ): current_mapping = match_next_male ( male_index , males , females , male_preferences , female_preferences ) plot_max_utility_graph ( males , females , male_preferences , female_preferences , current_mapping ) _____________________________________________________________________________ _____________________________________________________________________________ _____________________________________________________________________________ _____________________________________________________________________________ The process goes on as follows: 1. m1 proposes to w1 as w1 has maximum rank and as w1 is unselected, w1 accepts. We create an edge between them. 2. m2 proposes to w2 as w2 has maximum rank, and as w2 is unselected, w2 accepts. We crreate a edge between them. 3. m3 also proposes to w2. As w2 is already selected, it checks the preference of the current selection (m2) to m3. As m3 has better preference, w2 switches from m2 to m3. The edge between m2 and w2 is reoved and a new edge between m3 and w2 is created. 4. m2 is currently unmapped, and selects the next best preference, which is w3. As w3 is unselected, w3 accepts. 5. This clears the market and the process stops. This is stable mapping (from the men's side). These are the common matching techniques that exist.","title":"Bipartite matching (Python)"},{"location":"Python/Bipartite%20matching/#matching-algorithms","text":"Stock markets, housing and labor markets, dating and so forth are examples of matching tasks. Let us take suppliers an buyers as an example. in a matching problem, our job is to match the supliers to buyers so that both sides/ either side are satisfied. Matching problems can be considered as network problems. In network terms, a matching is a subset of edges where every node in one group goes through only one node in the other group. There should be only one edge from each node. # Import networkx library and rename it as nx. import networkx as nx # Other packages required import numpy as np import pandas as pd import matplotlib.pyplot as plt","title":"Matching algorithms"},{"location":"Python/Bipartite%20matching/#unweighted-bipartite-mapping","text":"Let us take a simple example of mapping students and dorm rooms. In this problem, students give a list of rooms they are willing to stay at. We represent students on side as nodes of a bipartite graph and rooms on the other side as nodes, and we put an edge between students and rooms as per this list. We want to map students and rooms. We want to identify a subset where we match one student to exactly one other room (no roommates). The students give what rooms are acceptable, and many solutions are possible. Consider the below problem where we have students (A, B, C and D) and we want to match them to rooms (1,2,3,4). The list of acceptable rooms for each student is given below. edgelist_df = pd . DataFrame ({ 'node1' :[ 'A' , 'A' , 'B' , 'B' , 'C' , 'C' , 'D' , 'D' ], 'node2' :[ 1 , 2 , 2 , 3 , 3 , 4 , 4 , 1 ]}) edgelist_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } node1 node2 0 A 1 1 A 2 2 B 2 3 B 3 4 C 3 5 C 4 6 D 4 7 D 1 This can be represented as a bipartite graph as follows g = nx . Graph () for i , elrow in edgelist_df . iterrows (): g . add_edge ( elrow [ 0 ], elrow [ 1 ]) # Make two sets in bipartite and get positions for the same left , right = nx . bipartite . sets ( g ) pos = {} # Update position for node from each group for i , node in enumerate ( sorted ( list ( left ))): g . add_node ( node , pos = ( 0 , i )) for i , node in enumerate ( sorted ( list ( right ))): g . add_node ( node , pos = ( 1 , i )) nx . draw ( g , pos = nx . get_node_attributes ( g , 'pos' ), with_labels = True ) Visually, we can find a couple of solutions, for example: - A:1, B:2, C:3, D:4 - A:2, B:3, C:4, D:1 In large graphs visual analysis might be difficult, and in such situations Halls theorem is useful to identify if matching is possible.","title":"Unweighted bipartite mapping"},{"location":"Python/Bipartite%20matching/#halls-theorem","text":"But before Halls theorem, let us look at constricted set. Constricted set : A constricted set is a subset of edges (on either side) whose neighbours are smaller than the subset. For examle, if we have two students (subset of students) who give only one room (same room) in the list, then the size of the students is 2 and the size of the rooms is one, and no matching can be done. Halls theorem states that for a mapping to exist, there should be no constricted set. Augmenting paths 1. Select any random matching of unmatched nodes. 2. Switch to the augumented paths if it exists. If it doesnt exist, then we have a constricted set and we cannot do matching. 3. Repeat until all left nodes are matched to one right node. This is implemented in NetworkX as follows: # Select random edges selected_edges = [] for left_node in left : # For a left node, select a random node in the right list_of_nodes = list ( g . edges ( left_node )) random_node = np . random . randint ( len ( list_of_nodes )) selected_edges . append ( list_of_nodes [ random_node ]) selected_edges [('D', 4), ('C', 4), ('A', 2), ('B', 3)] Selected edges are shown in green for edge in g . edges : if edge in selected_edges or ( edge [ 1 ], edge [ 0 ]) in selected_edges : g . add_edge ( edge [ 0 ], edge [ 1 ], color = 'g' , weight = 10 ) else : g . add_edge ( edge [ 0 ], edge [ 1 ], color = 'b' , weight = 0.1 ) nx . draw ( g , pos = nx . get_node_attributes ( g , 'pos' ), with_labels = True , edge_color = nx . get_edge_attributes ( g , 'color' ) . values ()) We can see that this is not matching as node C and D are mapped to 4. This can be resolved by moving through the augumented paths for the C-4 node. For the C-4 node, moving thru the augumented path selects the C-3 and D-4 node. Following the smae process with with B-3 and A-3 nodes, we get: selected_edges = {( 1 , 'A' ), ( 2 , 'B' ), ( 3 , 'C' ), ( 4 , 'D' )} for edge in g . edges : if edge in selected_edges or ( edge [ 1 ], edge [ 0 ]) in selected_edges : g . add_edge ( edge [ 0 ], edge [ 1 ], color = 'g' , weight = 10 ) else : g . add_edge ( edge [ 0 ], edge [ 1 ], color = 'b' , weight = 0.1 ) nx . draw ( g , pos = nx . get_node_attributes ( g , 'pos' ), with_labels = True , edge_color = nx . get_edge_attributes ( g , 'color' ) . values ()) The final mapping is shown in green nx . algorithms . bipartite . matching . hopcroft_karp_matching ( g , top_nodes = list ( set ( edgelist_df . node1 ))) {'D': 4, 'C': 3, 'B': 2, 'A': 1, 1: 'A', 2: 'B', 3: 'C', 4: 'D'}","title":"Halls theorem"},{"location":"Python/Bipartite%20matching/#weighted-bipartite-mapping","text":"In the previous problem, we tried to find a perfect maching in an unweighted graph. What if every edge in the graph has certain weight attached to it. The weights could be quality index (for student-dorm matching) or valuations in a market etc. Consider the suppliers-buyers example for a housing market as shown below. We have three suppliers, A, B and C and three buyers (x, y and z). The valuation for each of the sellers is also given. For example, buyer x values house A with 12, house B with 4 and house C with 2. sellers = [ 'A' , 'B' , 'C' ] buyers = [ 'x' , 'y' , 'z' ] valuations_for_buyers = [[ 12 , 4 , 2 ], [ 8 , 7 , 6 ], [ 7 , 5 , 2 ]] sellers_price = [ 0 , 0 , 0 ] g = nx . Graph () pos = {} # Update position for node from each group for i , node in enumerate ( sellers ): g . add_node ( node , pos = ( 0 , len ( sellers ) - i )) for i , node in enumerate ( buyers ): g . add_node ( node , pos = ( 1 , len ( buyers ) - i )) # Plot text for the buyers for i , buyer in enumerate ( buyers ): plt . text ( 1.1 , len ( buyers ) - i , s = valuations_for_buyers [ i ], horizontalalignment = 'left' ) # Plot text for the sellers for i , buyer in enumerate ( buyers ): plt . text ( - 0.1 , len ( buyers ) - i , s = sellers_price [ i ], horizontalalignment = 'right' ) nx . draw ( g , pos = nx . get_node_attributes ( g , 'pos' ), with_labels = True ) What we want to achieve is clearing of the market. Clearing happens when all houses are sold to one buyer, and every buyer bought one house. This can be done using an auction algorithm. 1. Sellers quote a price 2. Buyers calculate utility: Net valuation (payoff) = Gross Valuation - Price charged by seller 3. Buyers select the object that has highest payoff 4. If the market is not cleared, the sellers who have more than one offer (overdetermined) will increase the price by one unit, and the process is repeated. # Function to pick the supplier with the maximumm utility def match_to_maximum_utility ( sellers , buyers , valuation , price ): max_utility_sellers = {} for buyer_index in range ( len ( buyers )): max_utility = 0 for seller_index in range ( len ( sellers )): if ( max_utility < valuation [ buyer_index ][ seller_index ] - price [ seller_index ]): max_utility = valuation [ buyer_index ][ seller_index ] - price [ seller_index ] max_utility_sellers [ buyers [ buyer_index ]] = [ sellers [ seller_index ]] elif ( max_utility == valuation [ buyer_index ][ seller_index ] - price [ seller_index ]): max_utility_sellers [ buyers [ buyer_index ]] . append ( sellers [ seller_index ]) return max_utility_sellers Assuming that the initial price set by the seller is zero (scaled to zero - displayed beside the node), buyer x calculates the following utility: - For A: 12-0 = 12 - For B: 4-0 = 4 - For C: 2-0 = 2 As the utility of A is the highest, x will chose A. Similarly, B and C will also choose A. max_util = match_to_maximum_utility ( sellers , buyers , valuations_for_buyers , sellers_price ) max_util {'x': ['A'], 'y': ['A'], 'z': ['A']} Plotting the selection, we see that A is overdetermined. def plot_max_utility_graph ( sellers , buyers , valuations_for_buyers , sellers_price , max_util ): g = nx . Graph () pos = {} # Update position for node from each group for i , node in enumerate ( sellers ): g . add_node ( node , pos = ( 0 , len ( sellers ) - i )) for i , node in enumerate ( buyers ): g . add_node ( node , pos = ( 1 , len ( buyers ) - i )) # Make edges for key , values in max_util . items (): for value in values : g . add_edge ( key , value ) # Plot text for the buyers for i , buyer in enumerate ( buyers ): plt . text ( 1.1 , len ( buyers ) - i , s = valuations_for_buyers [ i ], horizontalalignment = 'left' ) # Plot text for the sellers for i , buyer in enumerate ( buyers ): plt . text ( - 0.1 , len ( buyers ) - i , s = sellers_price [ i ], horizontalalignment = 'right' ) nx . draw ( g , pos = nx . get_node_attributes ( g , 'pos' ), with_labels = True ) plt . show () plot_max_utility_graph ( sellers , buyers , valuations_for_buyers , sellers_price , max_util ) For the overdetermined edge A, we increase the price by one unit. def get_overdetermined_edge_and_increase_price ( sellers , sellers_price , max_util ): from collections import Counter counts = dict ( Counter ( sum ( max_util . values (), []))) over_determined_list = [] for key , value in counts . items (): if ( value > 1 ): over_determined_list . append ( key ) sellers_price [ sellers . index ( key )] += 1 print ( 'Nodes' , over_determined_list , 'are over determined. Added 1 to the price for the suppliers' ) return counts get_overdetermined_edge_and_increase_price ( sellers , sellers_price , max_util ) Nodes ['A'] are over determined. Added 1 to the price for the suppliers {'A': 3} We then contiue this process until the market is cleared. no_of_sellers_selected = 0 while ( no_of_sellers_selected != len ( sellers )): max_util = match_to_maximum_utility ( sellers , buyers , valuations_for_buyers , sellers_price ) plot_max_utility_graph ( sellers , buyers , valuations_for_buyers , sellers_price , max_util ) counts = get_overdetermined_edge_and_increase_price ( sellers , sellers_price , max_util ) no_of_sellers_selected = len ( counts ) # need not always be the case, check Nodes ['A'] are over determined. Added 1 to the price for the suppliers Nodes ['A', 'B'] are over determined. Added 1 to the price for the suppliers Nodes ['A', 'B'] are over determined. Added 1 to the price for the suppliers We can see that for costs (A:3, B:1, C:0), the market can be cleared with buyer x choosing A, y choosing C and z choosing B. This is the maximum weight perfect matching.","title":"Weighted bipartite mapping"},{"location":"Python/Bipartite%20matching/#matching-with-preferences","text":"In the previous scenario, we had weights on the edges which indicated the utility. In this case we will look at matching where we have preferences in a ranked order. This is more natural way in many scenarios, like students preference to universities/universities selecting students, dating scenarios etc. This was originally implemented by Al Roth for matching hospitals and residency. Our goal is to clear the market, but also have a stable matching. So, what is a stable matching? Stable matching : Stability is an eqvilibrium when no pair on ether side has an incentive to devaite from the mapping. Let us understand this using an example. Take the dating scenario for example. On the left hand side we have men and on the right hand side we have women. All men rank women in strict order and all women rank men in strict order. In a stable matching, no pair of nodes (male-female) prefers each other to their curently assigned partners.","title":"Matching with preferences"},{"location":"Python/Bipartite%20matching/#gale-shapley-algorithm","text":"Let us say that are n players on both sides with males(m) on one side and women(w) on another side. The algorithm is as follows: 1. Every unmatched male (m) proposses to their first preference available. 2. If the proposed women (w) is unmatched, w accepts. If the women is already matched and the m has higher preference for w, w switches. Else, previous mapping remains. 3. This process continues until there is stability Lets take an example with three males and three females. The preferences are mentioned at the side of the node in a list. For example, m2 has a preference w2, followed by w3 and then w1. Similarly w2 has a preference of m3, followed by m1 and then m2. males = [ 'm1' , 'm2' , 'm3' ] females = [ 'w1' , 'w2' , 'w3' ] male_preferences = [[ 'w1' , 'w2' , 'w3' ], [ 'w2' , 'w3' , 'w1' ], [ 'w2' , 'w3' , 'w1' ]] female_preferences = [[ 'm2' , 'm3' , 'm1' ], [ 'm3' , 'm1' , 'm2' ], [ 'm1' , 'm2' , 'm3' ]] def match_next_male ( male_index , males , females , male_preferences , female_preferences ): for female in male_preferences [ male_index ]: if ( female not in current_mapping . values ()): current_mapping [ males [ male_index ]] = female return current_mapping elif ( female in current_mapping . values ()): current_mapping_inverse = dict ( zip ( current_mapping . values (), current_mapping . keys ())) current_male_for_the_female = current_mapping_inverse [ female ] if ( female_preferences [ males . index ( current_male_for_the_female )] > female_preferences [ male_index ]): current_mapping [ males [ male_index ]] = female current_mapping . pop ( current_male_for_the_female ) return current_mapping def plot_max_utility_graph ( males , females , male_preferences , female_preferences , current_mapping ): g = nx . Graph () pos = {} # Update position for node from each group for i , node in enumerate ( males ): g . add_node ( node , pos = ( 0 , len ( males ) - i )) for i , node in enumerate ( females ): g . add_node ( node , pos = ( 1 , len ( females ) - i )) # Make edges for key , value in current_mapping . items (): g . add_edge ( key , value ) # Plot text for the males for i , male in enumerate ( males ): plt . text ( - 0.1 , len ( males ) - i , s = male_preferences [ i ], horizontalalignment = 'right' ) # Plot text for the females for i , female in enumerate ( females ): plt . text ( 1.1 , len ( females ) - i , s = female_preferences [ i ], horizontalalignment = 'left' ) nx . draw ( g , pos = nx . get_node_attributes ( g , 'pos' ), with_labels = True ) plt . show () print ( '_____________________________________________________________________________' ) current_mapping = {} while ( len ( current_mapping ) != len ( males )): for male_index in range ( len ( males )): if ( current_mapping . get ( males [ male_index ]) is None ): current_mapping = match_next_male ( male_index , males , females , male_preferences , female_preferences ) plot_max_utility_graph ( males , females , male_preferences , female_preferences , current_mapping ) _____________________________________________________________________________ _____________________________________________________________________________ _____________________________________________________________________________ _____________________________________________________________________________ The process goes on as follows: 1. m1 proposes to w1 as w1 has maximum rank and as w1 is unselected, w1 accepts. We create an edge between them. 2. m2 proposes to w2 as w2 has maximum rank, and as w2 is unselected, w2 accepts. We crreate a edge between them. 3. m3 also proposes to w2. As w2 is already selected, it checks the preference of the current selection (m2) to m3. As m3 has better preference, w2 switches from m2 to m3. The edge between m2 and w2 is reoved and a new edge between m3 and w2 is created. 4. m2 is currently unmapped, and selects the next best preference, which is w3. As w3 is unselected, w3 accepts. 5. This clears the market and the process stops. This is stable mapping (from the men's side). These are the common matching techniques that exist.","title":"Gale Shapley Algorithm"},{"location":"Python/Community%20detection/","text":"MathJax.Hub.Config({ extensions: [\"tex2jax.js\",\"TeX/AMSmath.js\",\"TeX/AMSsymbols.js\"], jax: [\"input/TeX\", \"output/HTML-CSS\"], tex2jax: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ], }, \"HTML-CSS\": { availableFonts: [\"TeX\"] } }); Community detection \u00b6 A community is a group of people who are homogeneous in some features. Marketers want to find communities so that they can target them. It is not trivial to identify communities from a given network. Community detection deals with identifying networks based only on the network topology. nodes in the community are closer than the nodes outside the community. What is closeness? To define closeness, we need to 1. Define distance metric based on network topology 2. Quantify nodes in a community are closer wth each other than nodes outside the community There are two approaches, bottom-up and top-down. Girwan Newman Algorithm \u00b6 The most popular algorithm for network community detection is the Girvan-Newman algorithm. It is a top down approach where we take the whole network and try to break it into two communities. This can be continued till the bottom. Steps: 1. Define betweenneess mesure for each edge 2. Find the edge with maximum betweenness and remove it (The edge most in-between in the network connects up most number of pairs of nodes) 3. Recalculate and repeat The example that we are using in this blog is the Zachary Karate club . It contains 34 members, and after an internal conflict, the club split into two. Our goal is to find if we can predict out how they split based on communities. import matplotlib.pyplot as plt % matplotlib inline import networkx as nx G = nx . karate_club_graph () nx . draw_kamada_kawai ( G , with_labels = True ) # Calculating the betweenness centrality btw_centrality = nx . algorithms . centrality . edge_betweenness_centrality ( G ) # Sorting based on the betweenness centraliy and displaying the first 10 edges. sorted ( btw_centrality . items (), key = lambda item : item [ 1 ], reverse = True )[ 0 : 10 ] [((0, 31), 0.1272599949070537), ((0, 6), 0.07813428401663695), ((0, 5), 0.07813428401663694), ((0, 2), 0.0777876807288572), ((0, 8), 0.07423959482783014), ((2, 32), 0.06898678663384543), ((13, 33), 0.06782389723566191), ((19, 33), 0.05938233879410351), ((0, 11), 0.058823529411764705), ((26, 33), 0.0542908072319837)] def girwan_newman ( G , no_of_components_to_split ): while ( no_of_components_to_split > nx . algorithms . components . number_connected_components ( G )): # Calculate the betweenness centrality btw_centrality = nx . algorithms . centrality . edge_betweenness_centrality ( G ) # sort based on betweenness centrality sorted_edges = sorted ( btw_centrality . items (), key = lambda item : item [ 1 ], reverse = True )[ 0 ] print ( 'Removing the edge' , sorted_edges ) # remove edge which has highest centrality G . remove_edge ( * sorted_edges [ 0 ]) # Check if graph is split if ( no_of_components_to_split <= nx . algorithms . components . number_connected_components ( G )): # Plot the graph with both the nodes having different colors nx . draw_spring ( G , with_labels = True ) # return list of nodes in each community list_of_nodes = [ c for c in sorted ( nx . connected_components ( G ), key = len , reverse = True )] return list_of_nodes G = nx . karate_club_graph () communities = girwan_newman ( G , 2 ) communities Removing the edge ((0, 31), 0.1272599949070537) Removing the edge ((0, 2), 0.11924273983097515) Removing the edge ((0, 8), 0.13782067605597018) Removing the edge ((13, 33), 0.14617273782105492) Removing the edge ((19, 33), 0.21966651886437982) Removing the edge ((2, 32), 0.1786195286195287) Removing the edge ((1, 30), 0.25601957954899124) Removing the edge ((1, 2), 0.1947415329768271) Removing the edge ((2, 3), 0.19191919191919182) Removing the edge ((2, 7), 0.25445632798573975) Removing the edge ((2, 13), 0.5080213903743315) [{2, 8, 9, 14, 15, 18, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33}, {0, 1, 3, 4, 5, 6, 7, 10, 11, 12, 13, 16, 17, 19, 21}] We can see that after removing 11 edges based on the maximum betweenness values, the club splits into two parts. This can be seen below. These groups matches with what actually happened in the karate club. # Reinstanciste G if necessary G = nx . karate_club_graph () # Colors for edges colors_for_edges = 'rbgoy' pos = nx . spring_layout ( G ) for i , nodes in enumerate ( communities ): for node in nodes : G . add_node ( node , node_color = colors_for_edges [ i ]) nx . draw_kamada_kawai ( G , with_labels = True , node_color = list ( nx . get_node_attributes ( G , 'node_color' ) . values ())) NetworkX has a inbuilt function to calculate the same split. karate_club_split = nx . algorithms . community . centrality . girvan_newman ( G ) karate_club_split_tuple = tuple ( sorted ( c ) for c in next ( karate_club_split )) karate_club_split_tuple ([0, 1, 3, 4, 5, 6, 7, 10, 11, 12, 13, 16, 17, 19, 21], [2, 8, 9, 14, 15, 18, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]) colors = 'rgb' for i , nodes in enumerate ( karate_club_split_tuple ): nx . draw_networkx_nodes ( G , pos = pos , nodelist = nodes , node_color = colors [ i ]) nx . draw_networkx_edges ( G , pos = pos ) Ratio cut method \u00b6 A more complex and advanced method is the ratio cut method. A cut in a graph is a set of edges, such that if we remove them, the network breaks up into two components. The procedure is as follows: 1. Find the minimum cut among all the pair of nodes 2. Repeat over each of the two parts cut The minimum cut can be found using Ratio Cut parameter. Minimum cut wil have the least ratio cut among all the other cuts. $$ Ratio\\,Cut\\, (P_{Red}) = \\frac{1}{number\\, of\\, partitions}\\times (\\frac{no\\, of\\, edges\\, in\\, partition1}{no\\, of\\, nodes\\, in\\, partition1} + \\frac{no\\, of\\, edges\\, in\\, partition2}{no\\, of\\, nodes\\, in\\, partition2} + ..) $$ This can be done quickly using an inbuilt function in NetworkX from networkx.algorithms import community communities = community . kernighan_lin_bisection ( G , max_iter = 100 ) pos = nx . spring_layout ( G ) nx . draw ( G , pos , with_labels = True , node_size = 100 , node_color = 'w' , node_shape = '.' ) for i in range ( len ( communities )): nx . draw_networkx_nodes ( G , pos , nodelist = communities [ i ], node_color = colors [ i ]) Other methods \u00b6 There are many other methods that can be used to identify communities, one of which is based on modularity. Modularity is defined as the \"distance\" from a random graph. essentially we will be comparing the number of edges with a community and the expected number of edges with a random graph. In NetworkX, we have a inbuilt function to implement this. # How many ideal nodes: Louvain package from community import community_louvain # https://en.wikipedia.org/wiki/Louvain_method community_louvain . best_partition ( G ) {0: 0, 1: 0, 2: 0, 3: 0, 4: 3, 5: 3, 6: 3, 7: 0, 8: 1, 9: 0, 10: 3, 11: 0, 12: 0, 13: 0, 14: 1, 15: 1, 16: 3, 17: 0, 18: 1, 19: 0, 20: 1, 21: 0, 22: 1, 23: 1, 24: 2, 25: 2, 26: 1, 27: 1, 28: 2, 29: 1, 30: 1, 31: 2, 32: 1, 33: 1}","title":"Community detection (Python)"},{"location":"Python/Community%20detection/#community-detection","text":"A community is a group of people who are homogeneous in some features. Marketers want to find communities so that they can target them. It is not trivial to identify communities from a given network. Community detection deals with identifying networks based only on the network topology. nodes in the community are closer than the nodes outside the community. What is closeness? To define closeness, we need to 1. Define distance metric based on network topology 2. Quantify nodes in a community are closer wth each other than nodes outside the community There are two approaches, bottom-up and top-down.","title":"Community detection"},{"location":"Python/Community%20detection/#girwan-newman-algorithm","text":"The most popular algorithm for network community detection is the Girvan-Newman algorithm. It is a top down approach where we take the whole network and try to break it into two communities. This can be continued till the bottom. Steps: 1. Define betweenneess mesure for each edge 2. Find the edge with maximum betweenness and remove it (The edge most in-between in the network connects up most number of pairs of nodes) 3. Recalculate and repeat The example that we are using in this blog is the Zachary Karate club . It contains 34 members, and after an internal conflict, the club split into two. Our goal is to find if we can predict out how they split based on communities. import matplotlib.pyplot as plt % matplotlib inline import networkx as nx G = nx . karate_club_graph () nx . draw_kamada_kawai ( G , with_labels = True ) # Calculating the betweenness centrality btw_centrality = nx . algorithms . centrality . edge_betweenness_centrality ( G ) # Sorting based on the betweenness centraliy and displaying the first 10 edges. sorted ( btw_centrality . items (), key = lambda item : item [ 1 ], reverse = True )[ 0 : 10 ] [((0, 31), 0.1272599949070537), ((0, 6), 0.07813428401663695), ((0, 5), 0.07813428401663694), ((0, 2), 0.0777876807288572), ((0, 8), 0.07423959482783014), ((2, 32), 0.06898678663384543), ((13, 33), 0.06782389723566191), ((19, 33), 0.05938233879410351), ((0, 11), 0.058823529411764705), ((26, 33), 0.0542908072319837)] def girwan_newman ( G , no_of_components_to_split ): while ( no_of_components_to_split > nx . algorithms . components . number_connected_components ( G )): # Calculate the betweenness centrality btw_centrality = nx . algorithms . centrality . edge_betweenness_centrality ( G ) # sort based on betweenness centrality sorted_edges = sorted ( btw_centrality . items (), key = lambda item : item [ 1 ], reverse = True )[ 0 ] print ( 'Removing the edge' , sorted_edges ) # remove edge which has highest centrality G . remove_edge ( * sorted_edges [ 0 ]) # Check if graph is split if ( no_of_components_to_split <= nx . algorithms . components . number_connected_components ( G )): # Plot the graph with both the nodes having different colors nx . draw_spring ( G , with_labels = True ) # return list of nodes in each community list_of_nodes = [ c for c in sorted ( nx . connected_components ( G ), key = len , reverse = True )] return list_of_nodes G = nx . karate_club_graph () communities = girwan_newman ( G , 2 ) communities Removing the edge ((0, 31), 0.1272599949070537) Removing the edge ((0, 2), 0.11924273983097515) Removing the edge ((0, 8), 0.13782067605597018) Removing the edge ((13, 33), 0.14617273782105492) Removing the edge ((19, 33), 0.21966651886437982) Removing the edge ((2, 32), 0.1786195286195287) Removing the edge ((1, 30), 0.25601957954899124) Removing the edge ((1, 2), 0.1947415329768271) Removing the edge ((2, 3), 0.19191919191919182) Removing the edge ((2, 7), 0.25445632798573975) Removing the edge ((2, 13), 0.5080213903743315) [{2, 8, 9, 14, 15, 18, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33}, {0, 1, 3, 4, 5, 6, 7, 10, 11, 12, 13, 16, 17, 19, 21}] We can see that after removing 11 edges based on the maximum betweenness values, the club splits into two parts. This can be seen below. These groups matches with what actually happened in the karate club. # Reinstanciste G if necessary G = nx . karate_club_graph () # Colors for edges colors_for_edges = 'rbgoy' pos = nx . spring_layout ( G ) for i , nodes in enumerate ( communities ): for node in nodes : G . add_node ( node , node_color = colors_for_edges [ i ]) nx . draw_kamada_kawai ( G , with_labels = True , node_color = list ( nx . get_node_attributes ( G , 'node_color' ) . values ())) NetworkX has a inbuilt function to calculate the same split. karate_club_split = nx . algorithms . community . centrality . girvan_newman ( G ) karate_club_split_tuple = tuple ( sorted ( c ) for c in next ( karate_club_split )) karate_club_split_tuple ([0, 1, 3, 4, 5, 6, 7, 10, 11, 12, 13, 16, 17, 19, 21], [2, 8, 9, 14, 15, 18, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]) colors = 'rgb' for i , nodes in enumerate ( karate_club_split_tuple ): nx . draw_networkx_nodes ( G , pos = pos , nodelist = nodes , node_color = colors [ i ]) nx . draw_networkx_edges ( G , pos = pos )","title":"Girwan Newman Algorithm"},{"location":"Python/Community%20detection/#ratio-cut-method","text":"A more complex and advanced method is the ratio cut method. A cut in a graph is a set of edges, such that if we remove them, the network breaks up into two components. The procedure is as follows: 1. Find the minimum cut among all the pair of nodes 2. Repeat over each of the two parts cut The minimum cut can be found using Ratio Cut parameter. Minimum cut wil have the least ratio cut among all the other cuts. $$ Ratio\\,Cut\\, (P_{Red}) = \\frac{1}{number\\, of\\, partitions}\\times (\\frac{no\\, of\\, edges\\, in\\, partition1}{no\\, of\\, nodes\\, in\\, partition1} + \\frac{no\\, of\\, edges\\, in\\, partition2}{no\\, of\\, nodes\\, in\\, partition2} + ..) $$ This can be done quickly using an inbuilt function in NetworkX from networkx.algorithms import community communities = community . kernighan_lin_bisection ( G , max_iter = 100 ) pos = nx . spring_layout ( G ) nx . draw ( G , pos , with_labels = True , node_size = 100 , node_color = 'w' , node_shape = '.' ) for i in range ( len ( communities )): nx . draw_networkx_nodes ( G , pos , nodelist = communities [ i ], node_color = colors [ i ])","title":"Ratio cut method"},{"location":"Python/Community%20detection/#other-methods","text":"There are many other methods that can be used to identify communities, one of which is based on modularity. Modularity is defined as the \"distance\" from a random graph. essentially we will be comparing the number of edges with a community and the expected number of edges with a random graph. In NetworkX, we have a inbuilt function to implement this. # How many ideal nodes: Louvain package from community import community_louvain # https://en.wikipedia.org/wiki/Louvain_method community_louvain . best_partition ( G ) {0: 0, 1: 0, 2: 0, 3: 0, 4: 3, 5: 3, 6: 3, 7: 0, 8: 1, 9: 0, 10: 3, 11: 0, 12: 0, 13: 0, 14: 1, 15: 1, 16: 3, 17: 0, 18: 1, 19: 0, 20: 1, 21: 0, 22: 1, 23: 1, 24: 2, 25: 2, 26: 1, 27: 1, 28: 2, 29: 1, 30: 1, 31: 2, 32: 1, 33: 1}","title":"Other methods"},{"location":"Python/Demonstrating%20online%20learning/","text":"MathJax.Hub.Config({ extensions: [\"tex2jax.js\",\"TeX/AMSmath.js\",\"TeX/AMSsymbols.js\"], jax: [\"input/TeX\", \"output/HTML-CSS\"], tex2jax: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ], }, \"HTML-CSS\": { availableFonts: [\"TeX\"] } }); Streaming machine learning \u00b6 Batch learning : In batch machine learning, we use one dataset to train a model, and we deploy the model to predict on new data. This assumes that the dataset in which the model is trained is a proper representative sample of the population. This model is assumed as a static object. In order to learn from new data, the model has to be re-trained from scratch. This is the most common form of deploying models. Online learning : Some of the machine learning models that we know can be modified to learn on a single datapoint (row). When we can learn from a single datapoint, we can learn incrementally from new datapoints. Data is considered as a stream. Once the model is trained, we need not store the historic training set. The model is also more up to date. If the data's distribution happens over time, the model will be able to handle it (drift)[1]. Where can we use them? They are most useful in scenarios where new data and patterns are constantly arriving. eg: 1. Spam filtering 2. Recommendation engines (news feed predictions) 3. Financial transactions 4. Low compute power (only one data point exists in memory as we train using one data point only) Issues 1. New and not a lot of experience 2. Very few tools and packages 3. All algorithms do not have an online version. Kernel SVMs are impossible to fit on a streaming dataset. Likewise, CART and ID3 decision trees can\u2019t be trained online. However, lesser-known online approximations exist, such as random Fourier features for SVMs and Hoeffding trees for decision trees. 4. Slower than batch learning to reach steady state in real life (It is computationally faster by more than an order of magnitude) 5. Do not guarantee that models learnt are similar to the ones obtained in batch mode(some models). Some models do not guarentee of achieving steady state. 6. Overfitting Similarities We have the same limitations of machine learning, such as: 1. We need to do proper preprocessing 2. We need to do feature engineering as usual 3. The concepts of ensembles, feature extraction, feature selection, imbalanced classes, multiclass etc are same Preprocessing steps \u00b6 How do we preprocess data when we are streaming? How do we impute the null values by mean if we do not have complete data? How do we identify outliers when working on one row at a time? How can we do one-hot encoding when we don't know what classes are present overall? The package river is a handy package for online learning. It has a lot of pre-defined preprocessing functions. Let us look at some of them: from river import preprocessing dir ( preprocessing )[ 0 : 12 ] ['AdaptiveStandardScaler', 'Binarizer', 'FeatureHasher', 'LDA', 'MaxAbsScaler', 'MinMaxScaler', 'Normalizer', 'OneHotEncoder', 'PreviousImputer', 'RobustScaler', 'StandardScaler', 'StatImputer'] There are six functions for scaling and normalizing data. They are: 1. AdaptiveStandardScalar 2. MaxAbsScalar 3. MinMaxScalar 4. Normalizer 5. RobustScalar 6. StandardScaler For example, let us look at the documentation for Standard Scaler It scales the data to have zero mean and unit variance. Under the hood, a running mean and a running variance are maintained. The scaling is slightly different than when scaling the data in batch because the exact means and variances are not known in advance. However, this doesn't have a detrimental impact on performance in the long run. Let us look at an example: import numpy as np import matplotlib.pyplot as plt import pandas as pd import random random . seed ( 1995 ) from sklearn.datasets import load_iris iris = load_iris () data1 = pd . DataFrame ( data = np . c_ [ iris [ 'data' ], iris [ 'target' ]], columns = iris [ 'feature_names' ] + [ 'target' ]) data1 = data1 . query ( 'target < 2' ) . sample ( frac = 1 ) def data_feed ( df_datafeed ): # Generator function to give the next candidate for _ctr in range ( len ( df_datafeed )): yield df_datafeed . iloc [ _ctr ] from river import compose from river import linear_model from river import preprocessing preprocessing_model = compose . Pipeline ( preprocessing . StandardScaler (), linear_model . LogisticRegression ()) data_stream = data_feed ( data1 . loc [:, data1 . columns != 'target' ]) for n in range ( 10 ): data_point = next ( data_stream ) . to_frame () . transpose () transformed_data = preprocessing_model . transform_one ( data_point . iloc [ 0 ,:]) print ( '------------------------' ) print ( data_point ) print ( transformed_data ) ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 67 5.8 2.7 4.1 1.0 {'sepal length (cm)': 0.0, 'sepal width (cm)': 0.0, 'petal length (cm)': 0.0, 'petal width (cm)': 0.0} ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 96 5.7 2.9 4.2 1.3 {'sepal length (cm)': -1.0, 'sepal width (cm)': 1.000000000000001, 'petal length (cm)': 0.9999999999999956, 'petal width (cm)': 1.0000000000000004} ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 19 5.1 3.8 1.5 0.3 {'sepal length (cm)': -1.4018260516446994, 'sepal width (cm)': 1.3934660285832352, 'petal length (cm)': -1.4134589797160622, 'petal width (cm)': -1.352447383098741} ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 5 5.4 3.9 1.7 0.4 {'sepal length (cm)': -0.36514837167010933, 'sepal width (cm)': 1.0830277015004253, 'petal length (cm)': -0.9198021534721369, 'petal width (cm)': -0.8427009716003844} ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 64 5.6 2.9 3.6 1.3 {'sepal length (cm)': 0.32232918561015234, 'sepal width (cm)': -0.6740938478604231, 'petal length (cm)': 0.49202037860731096, 'petal width (cm)': 1.019130320146575} ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 87 6.3 2.3 4.4 1.3 {'sepal length (cm)': 1.7636409634199253, 'sepal width (cm)': -1.3539553245018423, 'petal length (cm)': 0.9642101587457326, 'petal width (cm)': 0.8589556903873334} ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 80 5.5 2.4 3.8 1.1 {'sepal length (cm)': -0.37242264987106416, 'sepal width (cm)': -0.9985160994941403, 'petal length (cm)': 0.4205955120960296, 'petal width (cm)': 0.3575992699260759} ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 98 5.1 2.5 3.0 1.1 {'sepal length (cm)': -1.259494647504126, 'sepal width (cm)': -0.743358098059264, 'petal length (cm)': -0.27274857904612027, 'petal width (cm)': 0.331861655799986} ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 15 5.7 4.4 1.5 0.4 {'sepal length (cm)': 0.3503113654141663, 'sepal width (cm)': 1.8442002991885438, 'petal length (cm)': -1.3918304919158082, 'petal width (cm)': -1.282736189026269} ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 23 5.1 3.3 1.7 0.5 {'sepal length (cm)': -1.1921469919530685, 'sepal width (cm)': 0.28047526828011227, 'petal length (cm)': -1.077226017628646, 'petal width (cm)': -0.9305415914315355} Apart from preprocessing, using river, we can perform 1. Feature extraction/selection 2. Ensembles 3. Storing running statistics 4. Building regression and classification models 5. Time series 6. Anamoly detection 7. Clustering There are 6 types of machine learning models that we can build using river. They are: 1. Linear based (linear and logistic regression) 2. Tree based (Decision trees, Hoeffding trees) 3. Nearest neighbours based 4. Bayesian models 5. Neural Networks 6. Ensemble based models 7. Others from river import linear_model , naive_bayes , tree , neural_net , neighbors , expert , ensemble print ( 'Linear models' , dir ( linear_model )[ 0 : 7 ]) print ( 'Tree based modles' , dir ( tree )[ 0 : 6 ]) print ( 'Bayesian models' , dir ( naive_bayes )[ 0 : 4 ]) print ( 'Specialised models' , dir ( expert )[ 0 : 6 ]) print ( 'Ensemble mmodels' , dir ( ensemble )[ 0 : 8 ]) Linear models ['ALMAClassifier', 'LinearRegression', 'LogisticRegression', 'PAClassifier', 'PARegressor', 'Perceptron', 'SoftmaxRegression'] Tree based modles ['ExtremelyFastDecisionTreeClassifier', 'HoeffdingAdaptiveTreeClassifier', 'HoeffdingAdaptiveTreeRegressor', 'HoeffdingTreeClassifier', 'HoeffdingTreeRegressor', 'LabelCombinationHoeffdingTreeClassifier'] Bayesian models ['BernoulliNB', 'ComplementNB', 'GaussianNB', 'MultinomialNB'] Specialised models ['EWARegressor', 'EpsilonGreedyRegressor', 'StackingClassifier', 'SuccessiveHalvingClassifier', 'SuccessiveHalvingRegressor', 'UCBRegressor'] Ensemble mmodels ['ADWINBaggingClassifier', 'AdaBoostClassifier', 'AdaptiveRandomForestClassifier', 'AdaptiveRandomForestRegressor', 'BaggingClassifier', 'BaggingRegressor', 'LeveragingBaggingClassifier', 'SRPClassifier'] Modelling (under the hood) \u00b6 There are two types of streaming models, those which are entirely streaming and pseudo-online models. Pseudo-online models use a small batch of data to build the models, while completely online models build the models using only one data point.[3] Pseudo online models \u00b6 There are many theorems in statistics which can help us to bound the error of a metric between two variables. Hoeffding bound is one such theorem. \"Consider a real-valued random variable r whose range is R (e.g., for a probability the range is one, and for an information gain the range is log c, where c is the number of classes). Suppose we have made n independent observations of this variable, and computed their mean \\(\\bar r\\) . The Hoeffding bound states that, with probability 1 \u2212 \u03b4, the true mean of the variable is at least \\(\\bar r \u2212 \\epsilon\\) , where \\(\\epsilon = \\sqrt{\\frac{R^2ln(\\frac{1}{\\lambda})}{2n}}\\) .\" This is useful while creating decision trees. Using hoeffding bound, we can identify which feature should we use to split the tree. We can find if a variable has sufficient gini index (or other metrics) greater (lesser) than other variables with a probability below a cutoff probability and split the tree based on that metric. This can be acheived with very few data points which can be deleted after splitting and creating child nodes. This algorithm is called Hoeffding trees algorithm. Completely online models \u00b6 How can we update a model using only one data point? Let us look at Gradient Descent to understand this. In gradient descent, we want to minimise a convex loss function(MSE, regret, etc). As an example, consider the function \\(h(x) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ..\\) (or any convex function). The mean squared error is \\(J(\\theta) = \\frac{1}{2} \\sum_{i=1}^{m} (y_{(i)}-h_{\\theta}(x_{(i)}))^{2}\\) . In gradient descent, we find the \\(\\beta_i\\) that minimises \\(J(\\theta)\\) . In batch model, we consider all the data points that exist to identfy the optimal solution. In streaming learning, we initialise the \\(\\beta_i\\) as 0 and keep incrementally changing the \\(\\beta_i\\) 's using \\( \\(\\beta_{i, t} = \\beta_{i, t-1} + \\alpha \\times \\frac{\\partial}{\\partial \\theta_{j}} J(\\theta)\\) \\) Steps in updating the model \u00b6 In online learning, there are 4 steps[3]. For every new datapoint, we will recursively run the following steps. For \\({\\displaystyle t=1,2,...,T}\\) Learner receives input \\({\\displaystyle x_{t}}\\) Learner outputs \\({\\displaystyle w_{t}}\\) from a fixed convex set \\({\\displaystyle S}\\) Nature sends back a convex loss function \\({\\displaystyle v_{t}:S\\rightarrow \\mathbb {R} }\\) . Learner suffers loss \\({\\displaystyle v_{t}(w_{t})}\\) and updates its model. Implimentation using river \u00b6 Every online machine learning model has the following basic 5 functions: dir ( linear_model . LogisticRegression )[ 50 : 55 ] ['learn_many', 'learn_one', 'predict_many', 'predict_one', 'predict_proba_many'] As the name mentions, learn_one and predict_one learn and predict from one data point, learn_many, predict_many and predict_prob_many learn and predict using multiple data points. River is the result of a merger between creme and scikit-multiflow, and the remaining functions in the library follow a similar pattern to the same. Building a model \u00b6 Using the same data as above, let us build a sample model using river for streaming. from river import compose river_model = compose . Pipeline ( preprocessing . StandardScaler (), tree . HoeffdingTreeClassifier () ) from sklearn.metrics import accuracy_score acc_scores = [] cols_x = [ 'sepal length (cm)' , 'sepal width (cm)' , 'petal length (cm)' , 'petal width (cm)' ] def compute_accuracy ( data , model , truth_col ): predict_all = data . apply ( lambda row : model . predict_one ( row ), axis = 1 ) acc_scores . append ( accuracy_score ( np . array ( predict_all ), data [ truth_col ])) print ( 'Accuracy is ' , acc_scores [ - 1 ]) data_stream = data_feed ( data1 ) for n in range ( 20 ): data_point = next ( data_stream ) print ( data_point ) if ( n > 1 ): predict_one = river_model . predict_one ( data_point [ cols_x ]) print ( 'Current_prediction' , predict_one , data_point [ 'target' ]) river_model . learn_one ( data_point [ cols_x ], data_point [ 'target' ]) compute_accuracy ( data1 , river_model , 'target' ) print ( '------------------------' ) sepal length (cm) 5.8 sepal width (cm) 2.7 petal length (cm) 4.1 petal width (cm) 1.0 target 1.0 Name: 67, dtype: float64 Accuracy is 0.5 ------------------------ sepal length (cm) 5.7 sepal width (cm) 2.9 petal length (cm) 4.2 petal width (cm) 1.3 target 1.0 Name: 96, dtype: float64 Accuracy is 0.5 ------------------------ sepal length (cm) 5.1 sepal width (cm) 3.8 petal length (cm) 1.5 petal width (cm) 0.3 target 0.0 Name: 19, dtype: float64 Current_prediction 1.0 0.0 Accuracy is 0.5 ------------------------ sepal length (cm) 5.4 sepal width (cm) 3.9 petal length (cm) 1.7 petal width (cm) 0.4 target 0.0 Name: 5, dtype: float64 Current_prediction 1.0 0.0 Accuracy is 0.71 ------------------------ sepal length (cm) 5.6 sepal width (cm) 2.9 petal length (cm) 3.6 petal width (cm) 1.3 target 1.0 Name: 64, dtype: float64 Current_prediction 1.0 1.0 Accuracy is 0.79 ------------------------ sepal length (cm) 6.3 sepal width (cm) 2.3 petal length (cm) 4.4 petal width (cm) 1.3 target 1.0 Name: 87, dtype: float64 Current_prediction 1.0 1.0 Accuracy is 0.69 ------------------------ sepal length (cm) 5.5 sepal width (cm) 2.4 petal length (cm) 3.8 petal width (cm) 1.1 target 1.0 Name: 80, dtype: float64 Current_prediction 1.0 1.0 Accuracy is 0.69 ------------------------ sepal length (cm) 5.1 sepal width (cm) 2.5 petal length (cm) 3.0 petal width (cm) 1.1 target 1.0 Name: 98, dtype: float64 Current_prediction 1.0 1.0 Accuracy is 0.69 ------------------------ sepal length (cm) 5.7 sepal width (cm) 4.4 petal length (cm) 1.5 petal width (cm) 0.4 target 0.0 Name: 15, dtype: float64 Current_prediction 1.0 0.0 Accuracy is 0.99 ------------------------ sepal length (cm) 5.1 sepal width (cm) 3.3 petal length (cm) 1.7 petal width (cm) 0.5 target 0.0 Name: 23, dtype: float64 Current_prediction 0.0 0.0 Accuracy is 1.0 ------------------------ sepal length (cm) 4.8 sepal width (cm) 3.1 petal length (cm) 1.6 petal width (cm) 0.2 target 0.0 Name: 30, dtype: float64 Current_prediction 0.0 0.0 Accuracy is 1.0 ------------------------ sepal length (cm) 5.5 sepal width (cm) 2.5 petal length (cm) 4.0 petal width (cm) 1.3 target 1.0 Name: 89, dtype: float64 Current_prediction 1.0 1.0 Accuracy is 1.0 ------------------------ sepal length (cm) 4.5 sepal width (cm) 2.3 petal length (cm) 1.3 petal width (cm) 0.3 target 0.0 Name: 41, dtype: float64 Current_prediction 0.0 0.0 Accuracy is 1.0 ------------------------ sepal length (cm) 6.1 sepal width (cm) 3.0 petal length (cm) 4.6 petal width (cm) 1.4 target 1.0 Name: 91, dtype: float64 Current_prediction 1.0 1.0 Accuracy is 1.0 ------------------------ sepal length (cm) 5.2 sepal width (cm) 3.5 petal length (cm) 1.5 petal width (cm) 0.2 target 0.0 Name: 27, dtype: float64 Current_prediction 0.0 0.0 Accuracy is 1.0 ------------------------ sepal length (cm) 7.0 sepal width (cm) 3.2 petal length (cm) 4.7 petal width (cm) 1.4 target 1.0 Name: 50, dtype: float64 Current_prediction 1.0 1.0 Accuracy is 1.0 ------------------------ sepal length (cm) 5.5 sepal width (cm) 4.2 petal length (cm) 1.4 petal width (cm) 0.2 target 0.0 Name: 33, dtype: float64 Current_prediction 0.0 0.0 Accuracy is 1.0 ------------------------ sepal length (cm) 5.9 sepal width (cm) 3.2 petal length (cm) 4.8 petal width (cm) 1.8 target 1.0 Name: 70, dtype: float64 Current_prediction 1.0 1.0 Accuracy is 1.0 ------------------------ sepal length (cm) 6.6 sepal width (cm) 3.0 petal length (cm) 4.4 petal width (cm) 1.4 target 1.0 Name: 75, dtype: float64 Current_prediction 1.0 1.0 Accuracy is 1.0 ------------------------ sepal length (cm) 4.4 sepal width (cm) 3.0 petal length (cm) 1.3 petal width (cm) 0.2 target 0.0 Name: 38, dtype: float64 Current_prediction 0.0 0.0 Accuracy is 1.0 ------------------------ import matplotlib.pyplot as plt % matplotlib inline plt . plot ( acc_scores ) plt . ylabel ( 'Accuracy score' ) plt . xlabel ( 'Sample #' ) <matplotlib.text.Text at 0x340c5208> river_model2 = compose . Pipeline ( preprocessing . StandardScaler (), linear_model . LogisticRegression () ) acc_scores1 = [] def compute_accuracy ( data , model , truth_col ): predict_all = data . apply ( lambda row : model . predict_one ( row ), axis = 1 ) acc_scores1 . append ( accuracy_score ( np . array ( predict_all ), data [ truth_col ])) print ( 'Accuracy is ' , acc_scores1 [ - 1 ]) data_stream = data_feed ( data1 ) for n in range ( 10 ): data_point = next ( data_stream ) print ( data_point ) if ( n > 1 ): predict_one = river_model2 . predict_one ( data_point [ cols_x ]) print ( 'Current_prediction' , predict_one , data_point [ 'target' ]) river_model2 . learn_one ( data_point [ cols_x ], data_point [ 'target' ]) compute_accuracy ( data1 , river_model2 , 'target' ) print ( '------------------------' ) sepal length (cm) 5.8 sepal width (cm) 2.7 petal length (cm) 4.1 petal width (cm) 1.0 target 1.0 Name: 67, dtype: float64 Accuracy is 0.5 ------------------------ sepal length (cm) 5.7 sepal width (cm) 2.9 petal length (cm) 4.2 petal width (cm) 1.3 target 1.0 Name: 96, dtype: float64 Accuracy is 0.94 ------------------------ sepal length (cm) 5.1 sepal width (cm) 3.8 petal length (cm) 1.5 petal width (cm) 0.3 target 0.0 Name: 19, dtype: float64 Current_prediction False 0.0 Accuracy is 1.0 ------------------------ sepal length (cm) 5.4 sepal width (cm) 3.9 petal length (cm) 1.7 petal width (cm) 0.4 target 0.0 Name: 5, dtype: float64 Current_prediction False 0.0 Accuracy is 1.0 ------------------------ sepal length (cm) 5.6 sepal width (cm) 2.9 petal length (cm) 3.6 petal width (cm) 1.3 target 1.0 Name: 64, dtype: float64 Current_prediction True 1.0 Accuracy is 1.0 ------------------------ sepal length (cm) 6.3 sepal width (cm) 2.3 petal length (cm) 4.4 petal width (cm) 1.3 target 1.0 Name: 87, dtype: float64 Current_prediction True 1.0 Accuracy is 1.0 ------------------------ sepal length (cm) 5.5 sepal width (cm) 2.4 petal length (cm) 3.8 petal width (cm) 1.1 target 1.0 Name: 80, dtype: float64 Current_prediction True 1.0 Accuracy is 0.99 ------------------------ sepal length (cm) 5.1 sepal width (cm) 2.5 petal length (cm) 3.0 petal width (cm) 1.1 target 1.0 Name: 98, dtype: float64 Current_prediction True 1.0 Accuracy is 0.99 ------------------------ sepal length (cm) 5.7 sepal width (cm) 4.4 petal length (cm) 1.5 petal width (cm) 0.4 target 0.0 Name: 15, dtype: float64 Current_prediction False 0.0 Accuracy is 0.99 ------------------------ sepal length (cm) 5.1 sepal width (cm) 3.3 petal length (cm) 1.7 petal width (cm) 0.5 target 0.0 Name: 23, dtype: float64 Current_prediction False 0.0 Accuracy is 0.99 ------------------------ import matplotlib.pyplot as plt % matplotlib inline plt . plot ( acc_scores1 ) plt . ylabel ( 'Accuracy score' ) plt . xlabel ( 'Sample #' ) <matplotlib.text.Text at 0x34d304a8> References \u00b6 Introductory material: https://towardsdatascience.com/machine-learning-for-streaming-data-with-creme-dacf5fb469df Hoeffding Trees: https://homes.cs.washington.edu/~pedrod/papers/kdd00.pdf Modelling under the hood: https://en.wikipedia.org/wiki/Online_machine_learning River Git: https://github.com/online-ml/river River installation steps: https://riverml.xyz/dev/getting-started/installation/ River documentation: https://riverml.xyz/dev/api/overview/ Batch decision trees: https://www.harshaash.com/cart-classification/","title":"Streaming Machine Learning (Python)"},{"location":"Python/Demonstrating%20online%20learning/#streaming-machine-learning","text":"Batch learning : In batch machine learning, we use one dataset to train a model, and we deploy the model to predict on new data. This assumes that the dataset in which the model is trained is a proper representative sample of the population. This model is assumed as a static object. In order to learn from new data, the model has to be re-trained from scratch. This is the most common form of deploying models. Online learning : Some of the machine learning models that we know can be modified to learn on a single datapoint (row). When we can learn from a single datapoint, we can learn incrementally from new datapoints. Data is considered as a stream. Once the model is trained, we need not store the historic training set. The model is also more up to date. If the data's distribution happens over time, the model will be able to handle it (drift)[1]. Where can we use them? They are most useful in scenarios where new data and patterns are constantly arriving. eg: 1. Spam filtering 2. Recommendation engines (news feed predictions) 3. Financial transactions 4. Low compute power (only one data point exists in memory as we train using one data point only) Issues 1. New and not a lot of experience 2. Very few tools and packages 3. All algorithms do not have an online version. Kernel SVMs are impossible to fit on a streaming dataset. Likewise, CART and ID3 decision trees can\u2019t be trained online. However, lesser-known online approximations exist, such as random Fourier features for SVMs and Hoeffding trees for decision trees. 4. Slower than batch learning to reach steady state in real life (It is computationally faster by more than an order of magnitude) 5. Do not guarantee that models learnt are similar to the ones obtained in batch mode(some models). Some models do not guarentee of achieving steady state. 6. Overfitting Similarities We have the same limitations of machine learning, such as: 1. We need to do proper preprocessing 2. We need to do feature engineering as usual 3. The concepts of ensembles, feature extraction, feature selection, imbalanced classes, multiclass etc are same","title":"Streaming machine learning"},{"location":"Python/Demonstrating%20online%20learning/#preprocessing-steps","text":"How do we preprocess data when we are streaming? How do we impute the null values by mean if we do not have complete data? How do we identify outliers when working on one row at a time? How can we do one-hot encoding when we don't know what classes are present overall? The package river is a handy package for online learning. It has a lot of pre-defined preprocessing functions. Let us look at some of them: from river import preprocessing dir ( preprocessing )[ 0 : 12 ] ['AdaptiveStandardScaler', 'Binarizer', 'FeatureHasher', 'LDA', 'MaxAbsScaler', 'MinMaxScaler', 'Normalizer', 'OneHotEncoder', 'PreviousImputer', 'RobustScaler', 'StandardScaler', 'StatImputer'] There are six functions for scaling and normalizing data. They are: 1. AdaptiveStandardScalar 2. MaxAbsScalar 3. MinMaxScalar 4. Normalizer 5. RobustScalar 6. StandardScaler For example, let us look at the documentation for Standard Scaler It scales the data to have zero mean and unit variance. Under the hood, a running mean and a running variance are maintained. The scaling is slightly different than when scaling the data in batch because the exact means and variances are not known in advance. However, this doesn't have a detrimental impact on performance in the long run. Let us look at an example: import numpy as np import matplotlib.pyplot as plt import pandas as pd import random random . seed ( 1995 ) from sklearn.datasets import load_iris iris = load_iris () data1 = pd . DataFrame ( data = np . c_ [ iris [ 'data' ], iris [ 'target' ]], columns = iris [ 'feature_names' ] + [ 'target' ]) data1 = data1 . query ( 'target < 2' ) . sample ( frac = 1 ) def data_feed ( df_datafeed ): # Generator function to give the next candidate for _ctr in range ( len ( df_datafeed )): yield df_datafeed . iloc [ _ctr ] from river import compose from river import linear_model from river import preprocessing preprocessing_model = compose . Pipeline ( preprocessing . StandardScaler (), linear_model . LogisticRegression ()) data_stream = data_feed ( data1 . loc [:, data1 . columns != 'target' ]) for n in range ( 10 ): data_point = next ( data_stream ) . to_frame () . transpose () transformed_data = preprocessing_model . transform_one ( data_point . iloc [ 0 ,:]) print ( '------------------------' ) print ( data_point ) print ( transformed_data ) ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 67 5.8 2.7 4.1 1.0 {'sepal length (cm)': 0.0, 'sepal width (cm)': 0.0, 'petal length (cm)': 0.0, 'petal width (cm)': 0.0} ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 96 5.7 2.9 4.2 1.3 {'sepal length (cm)': -1.0, 'sepal width (cm)': 1.000000000000001, 'petal length (cm)': 0.9999999999999956, 'petal width (cm)': 1.0000000000000004} ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 19 5.1 3.8 1.5 0.3 {'sepal length (cm)': -1.4018260516446994, 'sepal width (cm)': 1.3934660285832352, 'petal length (cm)': -1.4134589797160622, 'petal width (cm)': -1.352447383098741} ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 5 5.4 3.9 1.7 0.4 {'sepal length (cm)': -0.36514837167010933, 'sepal width (cm)': 1.0830277015004253, 'petal length (cm)': -0.9198021534721369, 'petal width (cm)': -0.8427009716003844} ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 64 5.6 2.9 3.6 1.3 {'sepal length (cm)': 0.32232918561015234, 'sepal width (cm)': -0.6740938478604231, 'petal length (cm)': 0.49202037860731096, 'petal width (cm)': 1.019130320146575} ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 87 6.3 2.3 4.4 1.3 {'sepal length (cm)': 1.7636409634199253, 'sepal width (cm)': -1.3539553245018423, 'petal length (cm)': 0.9642101587457326, 'petal width (cm)': 0.8589556903873334} ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 80 5.5 2.4 3.8 1.1 {'sepal length (cm)': -0.37242264987106416, 'sepal width (cm)': -0.9985160994941403, 'petal length (cm)': 0.4205955120960296, 'petal width (cm)': 0.3575992699260759} ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 98 5.1 2.5 3.0 1.1 {'sepal length (cm)': -1.259494647504126, 'sepal width (cm)': -0.743358098059264, 'petal length (cm)': -0.27274857904612027, 'petal width (cm)': 0.331861655799986} ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 15 5.7 4.4 1.5 0.4 {'sepal length (cm)': 0.3503113654141663, 'sepal width (cm)': 1.8442002991885438, 'petal length (cm)': -1.3918304919158082, 'petal width (cm)': -1.282736189026269} ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 23 5.1 3.3 1.7 0.5 {'sepal length (cm)': -1.1921469919530685, 'sepal width (cm)': 0.28047526828011227, 'petal length (cm)': -1.077226017628646, 'petal width (cm)': -0.9305415914315355} Apart from preprocessing, using river, we can perform 1. Feature extraction/selection 2. Ensembles 3. Storing running statistics 4. Building regression and classification models 5. Time series 6. Anamoly detection 7. Clustering There are 6 types of machine learning models that we can build using river. They are: 1. Linear based (linear and logistic regression) 2. Tree based (Decision trees, Hoeffding trees) 3. Nearest neighbours based 4. Bayesian models 5. Neural Networks 6. Ensemble based models 7. Others from river import linear_model , naive_bayes , tree , neural_net , neighbors , expert , ensemble print ( 'Linear models' , dir ( linear_model )[ 0 : 7 ]) print ( 'Tree based modles' , dir ( tree )[ 0 : 6 ]) print ( 'Bayesian models' , dir ( naive_bayes )[ 0 : 4 ]) print ( 'Specialised models' , dir ( expert )[ 0 : 6 ]) print ( 'Ensemble mmodels' , dir ( ensemble )[ 0 : 8 ]) Linear models ['ALMAClassifier', 'LinearRegression', 'LogisticRegression', 'PAClassifier', 'PARegressor', 'Perceptron', 'SoftmaxRegression'] Tree based modles ['ExtremelyFastDecisionTreeClassifier', 'HoeffdingAdaptiveTreeClassifier', 'HoeffdingAdaptiveTreeRegressor', 'HoeffdingTreeClassifier', 'HoeffdingTreeRegressor', 'LabelCombinationHoeffdingTreeClassifier'] Bayesian models ['BernoulliNB', 'ComplementNB', 'GaussianNB', 'MultinomialNB'] Specialised models ['EWARegressor', 'EpsilonGreedyRegressor', 'StackingClassifier', 'SuccessiveHalvingClassifier', 'SuccessiveHalvingRegressor', 'UCBRegressor'] Ensemble mmodels ['ADWINBaggingClassifier', 'AdaBoostClassifier', 'AdaptiveRandomForestClassifier', 'AdaptiveRandomForestRegressor', 'BaggingClassifier', 'BaggingRegressor', 'LeveragingBaggingClassifier', 'SRPClassifier']","title":"Preprocessing steps"},{"location":"Python/Demonstrating%20online%20learning/#modelling-under-the-hood","text":"There are two types of streaming models, those which are entirely streaming and pseudo-online models. Pseudo-online models use a small batch of data to build the models, while completely online models build the models using only one data point.[3]","title":"Modelling (under the hood)"},{"location":"Python/Demonstrating%20online%20learning/#pseudo-online-models","text":"There are many theorems in statistics which can help us to bound the error of a metric between two variables. Hoeffding bound is one such theorem. \"Consider a real-valued random variable r whose range is R (e.g., for a probability the range is one, and for an information gain the range is log c, where c is the number of classes). Suppose we have made n independent observations of this variable, and computed their mean \\(\\bar r\\) . The Hoeffding bound states that, with probability 1 \u2212 \u03b4, the true mean of the variable is at least \\(\\bar r \u2212 \\epsilon\\) , where \\(\\epsilon = \\sqrt{\\frac{R^2ln(\\frac{1}{\\lambda})}{2n}}\\) .\" This is useful while creating decision trees. Using hoeffding bound, we can identify which feature should we use to split the tree. We can find if a variable has sufficient gini index (or other metrics) greater (lesser) than other variables with a probability below a cutoff probability and split the tree based on that metric. This can be acheived with very few data points which can be deleted after splitting and creating child nodes. This algorithm is called Hoeffding trees algorithm.","title":"Pseudo online models"},{"location":"Python/Demonstrating%20online%20learning/#completely-online-models","text":"How can we update a model using only one data point? Let us look at Gradient Descent to understand this. In gradient descent, we want to minimise a convex loss function(MSE, regret, etc). As an example, consider the function \\(h(x) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ..\\) (or any convex function). The mean squared error is \\(J(\\theta) = \\frac{1}{2} \\sum_{i=1}^{m} (y_{(i)}-h_{\\theta}(x_{(i)}))^{2}\\) . In gradient descent, we find the \\(\\beta_i\\) that minimises \\(J(\\theta)\\) . In batch model, we consider all the data points that exist to identfy the optimal solution. In streaming learning, we initialise the \\(\\beta_i\\) as 0 and keep incrementally changing the \\(\\beta_i\\) 's using \\( \\(\\beta_{i, t} = \\beta_{i, t-1} + \\alpha \\times \\frac{\\partial}{\\partial \\theta_{j}} J(\\theta)\\) \\)","title":"Completely online models"},{"location":"Python/Demonstrating%20online%20learning/#steps-in-updating-the-model","text":"In online learning, there are 4 steps[3]. For every new datapoint, we will recursively run the following steps. For \\({\\displaystyle t=1,2,...,T}\\) Learner receives input \\({\\displaystyle x_{t}}\\) Learner outputs \\({\\displaystyle w_{t}}\\) from a fixed convex set \\({\\displaystyle S}\\) Nature sends back a convex loss function \\({\\displaystyle v_{t}:S\\rightarrow \\mathbb {R} }\\) . Learner suffers loss \\({\\displaystyle v_{t}(w_{t})}\\) and updates its model.","title":"Steps in updating the model"},{"location":"Python/Demonstrating%20online%20learning/#implimentation-using-river","text":"Every online machine learning model has the following basic 5 functions: dir ( linear_model . LogisticRegression )[ 50 : 55 ] ['learn_many', 'learn_one', 'predict_many', 'predict_one', 'predict_proba_many'] As the name mentions, learn_one and predict_one learn and predict from one data point, learn_many, predict_many and predict_prob_many learn and predict using multiple data points. River is the result of a merger between creme and scikit-multiflow, and the remaining functions in the library follow a similar pattern to the same.","title":"Implimentation using river"},{"location":"Python/Demonstrating%20online%20learning/#building-a-model","text":"Using the same data as above, let us build a sample model using river for streaming. from river import compose river_model = compose . Pipeline ( preprocessing . StandardScaler (), tree . HoeffdingTreeClassifier () ) from sklearn.metrics import accuracy_score acc_scores = [] cols_x = [ 'sepal length (cm)' , 'sepal width (cm)' , 'petal length (cm)' , 'petal width (cm)' ] def compute_accuracy ( data , model , truth_col ): predict_all = data . apply ( lambda row : model . predict_one ( row ), axis = 1 ) acc_scores . append ( accuracy_score ( np . array ( predict_all ), data [ truth_col ])) print ( 'Accuracy is ' , acc_scores [ - 1 ]) data_stream = data_feed ( data1 ) for n in range ( 20 ): data_point = next ( data_stream ) print ( data_point ) if ( n > 1 ): predict_one = river_model . predict_one ( data_point [ cols_x ]) print ( 'Current_prediction' , predict_one , data_point [ 'target' ]) river_model . learn_one ( data_point [ cols_x ], data_point [ 'target' ]) compute_accuracy ( data1 , river_model , 'target' ) print ( '------------------------' ) sepal length (cm) 5.8 sepal width (cm) 2.7 petal length (cm) 4.1 petal width (cm) 1.0 target 1.0 Name: 67, dtype: float64 Accuracy is 0.5 ------------------------ sepal length (cm) 5.7 sepal width (cm) 2.9 petal length (cm) 4.2 petal width (cm) 1.3 target 1.0 Name: 96, dtype: float64 Accuracy is 0.5 ------------------------ sepal length (cm) 5.1 sepal width (cm) 3.8 petal length (cm) 1.5 petal width (cm) 0.3 target 0.0 Name: 19, dtype: float64 Current_prediction 1.0 0.0 Accuracy is 0.5 ------------------------ sepal length (cm) 5.4 sepal width (cm) 3.9 petal length (cm) 1.7 petal width (cm) 0.4 target 0.0 Name: 5, dtype: float64 Current_prediction 1.0 0.0 Accuracy is 0.71 ------------------------ sepal length (cm) 5.6 sepal width (cm) 2.9 petal length (cm) 3.6 petal width (cm) 1.3 target 1.0 Name: 64, dtype: float64 Current_prediction 1.0 1.0 Accuracy is 0.79 ------------------------ sepal length (cm) 6.3 sepal width (cm) 2.3 petal length (cm) 4.4 petal width (cm) 1.3 target 1.0 Name: 87, dtype: float64 Current_prediction 1.0 1.0 Accuracy is 0.69 ------------------------ sepal length (cm) 5.5 sepal width (cm) 2.4 petal length (cm) 3.8 petal width (cm) 1.1 target 1.0 Name: 80, dtype: float64 Current_prediction 1.0 1.0 Accuracy is 0.69 ------------------------ sepal length (cm) 5.1 sepal width (cm) 2.5 petal length (cm) 3.0 petal width (cm) 1.1 target 1.0 Name: 98, dtype: float64 Current_prediction 1.0 1.0 Accuracy is 0.69 ------------------------ sepal length (cm) 5.7 sepal width (cm) 4.4 petal length (cm) 1.5 petal width (cm) 0.4 target 0.0 Name: 15, dtype: float64 Current_prediction 1.0 0.0 Accuracy is 0.99 ------------------------ sepal length (cm) 5.1 sepal width (cm) 3.3 petal length (cm) 1.7 petal width (cm) 0.5 target 0.0 Name: 23, dtype: float64 Current_prediction 0.0 0.0 Accuracy is 1.0 ------------------------ sepal length (cm) 4.8 sepal width (cm) 3.1 petal length (cm) 1.6 petal width (cm) 0.2 target 0.0 Name: 30, dtype: float64 Current_prediction 0.0 0.0 Accuracy is 1.0 ------------------------ sepal length (cm) 5.5 sepal width (cm) 2.5 petal length (cm) 4.0 petal width (cm) 1.3 target 1.0 Name: 89, dtype: float64 Current_prediction 1.0 1.0 Accuracy is 1.0 ------------------------ sepal length (cm) 4.5 sepal width (cm) 2.3 petal length (cm) 1.3 petal width (cm) 0.3 target 0.0 Name: 41, dtype: float64 Current_prediction 0.0 0.0 Accuracy is 1.0 ------------------------ sepal length (cm) 6.1 sepal width (cm) 3.0 petal length (cm) 4.6 petal width (cm) 1.4 target 1.0 Name: 91, dtype: float64 Current_prediction 1.0 1.0 Accuracy is 1.0 ------------------------ sepal length (cm) 5.2 sepal width (cm) 3.5 petal length (cm) 1.5 petal width (cm) 0.2 target 0.0 Name: 27, dtype: float64 Current_prediction 0.0 0.0 Accuracy is 1.0 ------------------------ sepal length (cm) 7.0 sepal width (cm) 3.2 petal length (cm) 4.7 petal width (cm) 1.4 target 1.0 Name: 50, dtype: float64 Current_prediction 1.0 1.0 Accuracy is 1.0 ------------------------ sepal length (cm) 5.5 sepal width (cm) 4.2 petal length (cm) 1.4 petal width (cm) 0.2 target 0.0 Name: 33, dtype: float64 Current_prediction 0.0 0.0 Accuracy is 1.0 ------------------------ sepal length (cm) 5.9 sepal width (cm) 3.2 petal length (cm) 4.8 petal width (cm) 1.8 target 1.0 Name: 70, dtype: float64 Current_prediction 1.0 1.0 Accuracy is 1.0 ------------------------ sepal length (cm) 6.6 sepal width (cm) 3.0 petal length (cm) 4.4 petal width (cm) 1.4 target 1.0 Name: 75, dtype: float64 Current_prediction 1.0 1.0 Accuracy is 1.0 ------------------------ sepal length (cm) 4.4 sepal width (cm) 3.0 petal length (cm) 1.3 petal width (cm) 0.2 target 0.0 Name: 38, dtype: float64 Current_prediction 0.0 0.0 Accuracy is 1.0 ------------------------ import matplotlib.pyplot as plt % matplotlib inline plt . plot ( acc_scores ) plt . ylabel ( 'Accuracy score' ) plt . xlabel ( 'Sample #' ) <matplotlib.text.Text at 0x340c5208> river_model2 = compose . Pipeline ( preprocessing . StandardScaler (), linear_model . LogisticRegression () ) acc_scores1 = [] def compute_accuracy ( data , model , truth_col ): predict_all = data . apply ( lambda row : model . predict_one ( row ), axis = 1 ) acc_scores1 . append ( accuracy_score ( np . array ( predict_all ), data [ truth_col ])) print ( 'Accuracy is ' , acc_scores1 [ - 1 ]) data_stream = data_feed ( data1 ) for n in range ( 10 ): data_point = next ( data_stream ) print ( data_point ) if ( n > 1 ): predict_one = river_model2 . predict_one ( data_point [ cols_x ]) print ( 'Current_prediction' , predict_one , data_point [ 'target' ]) river_model2 . learn_one ( data_point [ cols_x ], data_point [ 'target' ]) compute_accuracy ( data1 , river_model2 , 'target' ) print ( '------------------------' ) sepal length (cm) 5.8 sepal width (cm) 2.7 petal length (cm) 4.1 petal width (cm) 1.0 target 1.0 Name: 67, dtype: float64 Accuracy is 0.5 ------------------------ sepal length (cm) 5.7 sepal width (cm) 2.9 petal length (cm) 4.2 petal width (cm) 1.3 target 1.0 Name: 96, dtype: float64 Accuracy is 0.94 ------------------------ sepal length (cm) 5.1 sepal width (cm) 3.8 petal length (cm) 1.5 petal width (cm) 0.3 target 0.0 Name: 19, dtype: float64 Current_prediction False 0.0 Accuracy is 1.0 ------------------------ sepal length (cm) 5.4 sepal width (cm) 3.9 petal length (cm) 1.7 petal width (cm) 0.4 target 0.0 Name: 5, dtype: float64 Current_prediction False 0.0 Accuracy is 1.0 ------------------------ sepal length (cm) 5.6 sepal width (cm) 2.9 petal length (cm) 3.6 petal width (cm) 1.3 target 1.0 Name: 64, dtype: float64 Current_prediction True 1.0 Accuracy is 1.0 ------------------------ sepal length (cm) 6.3 sepal width (cm) 2.3 petal length (cm) 4.4 petal width (cm) 1.3 target 1.0 Name: 87, dtype: float64 Current_prediction True 1.0 Accuracy is 1.0 ------------------------ sepal length (cm) 5.5 sepal width (cm) 2.4 petal length (cm) 3.8 petal width (cm) 1.1 target 1.0 Name: 80, dtype: float64 Current_prediction True 1.0 Accuracy is 0.99 ------------------------ sepal length (cm) 5.1 sepal width (cm) 2.5 petal length (cm) 3.0 petal width (cm) 1.1 target 1.0 Name: 98, dtype: float64 Current_prediction True 1.0 Accuracy is 0.99 ------------------------ sepal length (cm) 5.7 sepal width (cm) 4.4 petal length (cm) 1.5 petal width (cm) 0.4 target 0.0 Name: 15, dtype: float64 Current_prediction False 0.0 Accuracy is 0.99 ------------------------ sepal length (cm) 5.1 sepal width (cm) 3.3 petal length (cm) 1.7 petal width (cm) 0.5 target 0.0 Name: 23, dtype: float64 Current_prediction False 0.0 Accuracy is 0.99 ------------------------ import matplotlib.pyplot as plt % matplotlib inline plt . plot ( acc_scores1 ) plt . ylabel ( 'Accuracy score' ) plt . xlabel ( 'Sample #' ) <matplotlib.text.Text at 0x34d304a8>","title":"Building a model"},{"location":"Python/Demonstrating%20online%20learning/#references","text":"Introductory material: https://towardsdatascience.com/machine-learning-for-streaming-data-with-creme-dacf5fb469df Hoeffding Trees: https://homes.cs.washington.edu/~pedrod/papers/kdd00.pdf Modelling under the hood: https://en.wikipedia.org/wiki/Online_machine_learning River Git: https://github.com/online-ml/river River installation steps: https://riverml.xyz/dev/getting-started/installation/ River documentation: https://riverml.xyz/dev/api/overview/ Batch decision trees: https://www.harshaash.com/cart-classification/","title":"References"},{"location":"Python/Diffusion%20on%20networks/","text":"MathJax.Hub.Config({ extensions: [\"tex2jax.js\",\"TeX/AMSmath.js\",\"TeX/AMSsymbols.js\"], jax: [\"input/TeX\", \"output/HTML-CSS\"], tex2jax: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ], }, \"HTML-CSS\": { availableFonts: [\"TeX\"] } }); Forecasting adoption of a new product \u00b6 Harsha Achyuthuni Introduction \u00b6 Forecasting new adoptions after a product introduction is an important marketing problem. I want to use a forecasting model developed by Frank Bass that has proven to be effective in forecasting the adoption of innovative and new technologies. I am going to use Nonlinear programming to estimate the parameters of the Bass forecasting model. Bass Forecasting model \u00b6 The model has three parameters that must be estimated. parameter explanation m the number of people estimated to eventually adopt the new product q the coefficient of imitation p the coefficient of innovation The coefficient of imitation (q) is a parameter that measures the likelihood of adoption due to a potential adopter being influenced by someone who has already adopted the product. It measures the \u201cword-of-mouth\u201d effect influencing purchases. The coefficient of innovation (p) measures the likelihood of adoption, assuming no influence from someone who has already purchased (adopted) the product. It is the likelihood of someone adopting the product due to her or his own interest in the innovation. If \\(C_{t\u22121}\\) is the number of people that adopted the product by time t-1, then the number of new adopters during time t is given by Bass forecasting model and it is: $$ F_t=(p+q[\\frac{C_{t\u22121}}{m}])(m\u2212C_{t\u22121}) $$ If \\(c_{t} = C_t/m\\) , then $$ c_{t} - c_{t-1} = (p+qc_{t-1})(1-c_{t-1})$$ Doing some maths, instead of one time period, we could consider \\(\\Delta t\\) time period, we can write as: \\[ c_{t+ \\Delta t} - c_{t} = (p+qc_{t})(1-c_{t}) \\Delta t \\] \\[ \\frac{c_{t+ \\Delta t} - c_{t}}{\\Delta t} = (p+qc_{t})(1-c_{t}) \\] \\[ \\frac{d}{dt}c_{t} = (p+qc_{t})(1-c_{t}) \\] Solving we get $$ c(t) = \\frac{1-e {-(p+q)t}}{1+\\frac{q}{p}e } $$ As an example, consider the following revenues for a product. import pandas as pd data = pd . DataFrame ({ 'week' : [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 ], 'revenues' : [ 0.1 , 3 , 5.2 , 7 , 5.25 , 4.9 , 3 , 2.4 , 1.9 , 1.3 , 0.8 , 0.6 ]}) data [ 'cum_sum' ] = data [ 'revenues' ] . cumsum () data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } week revenues cum_sum 0 1 0.10 0.10 1 2 3.00 3.10 2 3 5.20 8.30 3 4 7.00 15.30 4 5 5.25 20.55 5 6 4.90 25.45 6 7 3.00 28.45 7 8 2.40 30.85 8 9 1.90 32.75 9 10 1.30 34.05 10 11 0.80 34.85 11 12 0.60 35.45 Optimising for the ideal p, q and m values, we get from scipy.optimize import curve_fit def c_t ( x , p , q , m ): return ( p + ( q / m ) * ( x )) * ( m - x ) popt , pcov = curve_fit ( c_t , data . cum_sum [ 0 : 11 ], data . revenues [ 1 : 12 ]) popt array([ 0.11467648, 0.37950562, 35.22906717]) The optimal p,d and q are:0.11467648, 0.37950562, 35.22906717. We can use these to predict the future revenues of the product.","title":"Bass Forecasting model (Python)"},{"location":"Python/Diffusion%20on%20networks/#forecasting-adoption-of-a-new-product","text":"Harsha Achyuthuni","title":"Forecasting adoption of a new product"},{"location":"Python/Diffusion%20on%20networks/#introduction","text":"Forecasting new adoptions after a product introduction is an important marketing problem. I want to use a forecasting model developed by Frank Bass that has proven to be effective in forecasting the adoption of innovative and new technologies. I am going to use Nonlinear programming to estimate the parameters of the Bass forecasting model.","title":"Introduction"},{"location":"Python/Diffusion%20on%20networks/#bass-forecasting-model","text":"The model has three parameters that must be estimated. parameter explanation m the number of people estimated to eventually adopt the new product q the coefficient of imitation p the coefficient of innovation The coefficient of imitation (q) is a parameter that measures the likelihood of adoption due to a potential adopter being influenced by someone who has already adopted the product. It measures the \u201cword-of-mouth\u201d effect influencing purchases. The coefficient of innovation (p) measures the likelihood of adoption, assuming no influence from someone who has already purchased (adopted) the product. It is the likelihood of someone adopting the product due to her or his own interest in the innovation. If \\(C_{t\u22121}\\) is the number of people that adopted the product by time t-1, then the number of new adopters during time t is given by Bass forecasting model and it is: $$ F_t=(p+q[\\frac{C_{t\u22121}}{m}])(m\u2212C_{t\u22121}) $$ If \\(c_{t} = C_t/m\\) , then $$ c_{t} - c_{t-1} = (p+qc_{t-1})(1-c_{t-1})$$ Doing some maths, instead of one time period, we could consider \\(\\Delta t\\) time period, we can write as: \\[ c_{t+ \\Delta t} - c_{t} = (p+qc_{t})(1-c_{t}) \\Delta t \\] \\[ \\frac{c_{t+ \\Delta t} - c_{t}}{\\Delta t} = (p+qc_{t})(1-c_{t}) \\] \\[ \\frac{d}{dt}c_{t} = (p+qc_{t})(1-c_{t}) \\] Solving we get $$ c(t) = \\frac{1-e {-(p+q)t}}{1+\\frac{q}{p}e } $$ As an example, consider the following revenues for a product. import pandas as pd data = pd . DataFrame ({ 'week' : [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 ], 'revenues' : [ 0.1 , 3 , 5.2 , 7 , 5.25 , 4.9 , 3 , 2.4 , 1.9 , 1.3 , 0.8 , 0.6 ]}) data [ 'cum_sum' ] = data [ 'revenues' ] . cumsum () data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } week revenues cum_sum 0 1 0.10 0.10 1 2 3.00 3.10 2 3 5.20 8.30 3 4 7.00 15.30 4 5 5.25 20.55 5 6 4.90 25.45 6 7 3.00 28.45 7 8 2.40 30.85 8 9 1.90 32.75 9 10 1.30 34.05 10 11 0.80 34.85 11 12 0.60 35.45 Optimising for the ideal p, q and m values, we get from scipy.optimize import curve_fit def c_t ( x , p , q , m ): return ( p + ( q / m ) * ( x )) * ( m - x ) popt , pcov = curve_fit ( c_t , data . cum_sum [ 0 : 11 ], data . revenues [ 1 : 12 ]) popt array([ 0.11467648, 0.37950562, 35.22906717]) The optimal p,d and q are:0.11467648, 0.37950562, 35.22906717. We can use these to predict the future revenues of the product.","title":"Bass Forecasting model"},{"location":"Python/Introduction%20to%20Networkx/","text":"Networks in python \u00b6 Networks play an important role in data science, with Google (page rank), Uber (route optimisation), Amazon (supply chian optimisation) and other companies becoming technology giants using network and data related optimisations. This article is an introduction to using networks in python using networkx package. # Import networkx library and rename it as nx. import networkx as nx # Other packages required import numpy as np import pandas as pd import matplotlib.pyplot as plt Undirected graphs \u00b6 A network can be represented in many ways. Edgelist represents graphs as a list of edges. Graphs can be undirected or directed. Consider the below edgelist where there is a relation between edge 1 and edge 2, and the weight for that edge is also provided. edgelist_df = pd . DataFrame ({ 'node1' :[ 1 , 1 , 1 , 2 , 2 , 3 , 4 , 4 ], 'node2' :[ 2 , 3 , 4 , 4 , 5 , 5 , 3 , 5 ], 'weights' :[ 5 , 2 , 1 , 3 , 1 , 7 , 1 , 4 ]}) edgelist_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } node1 node2 weights 0 1 2 5 1 1 3 2 2 1 4 1 3 2 4 3 4 2 5 1 5 3 5 7 6 4 3 1 7 4 5 4 One way to create a graph is to create an empty graph and add edges (and nodes) to the graph. g = nx . Graph () for i , elrow in edgelist_df . iterrows (): g . add_edge ( elrow [ 0 ], elrow [ 1 ], weight = elrow [ 2 ]) g . add_node ( 1 , pos = ( 0 , 5 )) g . edges ( data = True ) EdgeDataView([(1, 2, {'weight': 5}), (1, 3, {'weight': 2}), (1, 4, {'weight': 1}), (2, 4, {'weight': 3}), (2, 5, {'weight': 1}), (3, 5, {'weight': 7}), (3, 4, {'weight': 1}), (4, 5, {'weight': 4})]) # for each node we are trying to fix the coordinates g . add_node ( 1 , pos = ( 0 , 5 )) g . add_node ( 2 , pos = ( 5 , 10 )) g . add_node ( 3 , pos = ( 5 , 0 )) g . add_node ( 4 , pos = ( 10 , 5 )) g . add_node ( 5 , pos = ( 15 , 5 )) g . nodes ( data = True ) NodeDataView({1: {'pos': (0, 5)}, 2: {'pos': (5, 10)}, 3: {'pos': (5, 0)}, 4: {'pos': (10, 5)}, 5: {'pos': (15, 5)}}) This graph can be visualised as follows: # This function gets the coordinates for nodes pos = nx . get_node_attributes ( g , 'pos' ) # This function gets the weights for the edges weight = nx . get_edge_attributes ( g , 'weight' ) # The simplest way to draw a graph is by using nx.draw. nx . draw ( g , pos , with_labels = True ) # This adds edge lables nx . draw_networkx_edge_labels ( g , pos , edge_labels = weight ) plt . show () Mathematically this grah can be described in many ways. The most important are the adjacency matrix and incidence matrix. The adjacency matrix shows the relationship between the nodes in a matrix format where the nodes represent rows and columns. An edge between two nodes is represented by a positve number in the adjacency matrix, and the magnitude represents the weight. nx . linalg . graphmatrix . adj_matrix ( g ) . toarray () array([[0, 5, 2, 1, 0], [5, 0, 0, 3, 1], [2, 0, 0, 1, 7], [1, 3, 1, 0, 4], [0, 1, 7, 4, 0]], dtype=int64) The concurrency matrix represents the relationship between the nodes and edges. We have 8 edges as shown, and the relationship between the 5 nodes and 8 edges is shown below. g . edges EdgeView([(1, 2), (1, 3), (1, 4), (2, 4), (2, 5), (3, 5), (3, 4), (4, 5)]) nx . linalg . graphmatrix . incidence_matrix ( g ) . toarray () array([[1., 1., 1., 0., 0., 0., 0., 0.], [1., 0., 0., 1., 1., 0., 0., 0.], [0., 1., 0., 0., 0., 1., 1., 0.], [0., 0., 1., 1., 0., 0., 1., 1.], [0., 0., 0., 0., 1., 1., 0., 1.]]) Directed graph. \u00b6 A directed graph has edges which are in a particular direction. In this example, we are creating a DiGraph and reading a file where the edgelist is defined. An edge from a to b with a weight W exists when a line \"a b W\" can be found in the file. G1 = nx . DiGraph () # Read an directed graph from a list of edges, need to specify that using the parameter \u2018create_using\u2019 G1 = nx . read_edgelist ( \"edgelist1.txt\" , nodetype = str , create_using = nx . DiGraph (), data = [( 'weight' , int )]) print ( 'Edgelist' ) import sys nx . write_weighted_edgelist ( G1 , sys . stdout ) print ( 'nodes' ) print ( G1 . nodes ( data = True )) print ( 'edges' ) print ( G1 . edges ( data = True )) Edgelist 0 1 4 0 3 4 0 4 2 1 0 4 1 2 9 1 4 1 2 3 4 3 1 8 3 4 7 nodes [('0', {}), ('1', {}), ('2', {}), ('3', {}), ('4', {})] edges [('0', '1', {'weight': 4}), ('0', '3', {'weight': 4}), ('0', '4', {'weight': 2}), ('1', '0', {'weight': 4}), ('1', '2', {'weight': 9}), ('1', '4', {'weight': 1}), ('2', '3', {'weight': 4}), ('3', '1', {'weight': 8}), ('3', '4', {'weight': 7})] This graph can be visualised in many ways. Some of the visualisation patterns are (And these visualisations are same for directed or undirected graphs) 1. Spring layout (The length of edges is proportional to the weights) 2. Circular layout (The nodes are present in a circle) 3. Random layout All the three layouts are shown below nx . draw_spring ( G1 , with_labels = True , node_color = 'skyblue' , node_size = 200 , edge_color = 'black' ) nx . draw_circular ( G1 , with_labels = True , node_color = 'red' , node_size = 300 , edge_color = 'black' ) nx . draw_random ( G1 , with_labels = True , node_color = 'red' , node_size = 300 , edge_color = 'black' ) The concurrency matrix looks differently for directed graphs. If a edge is leaving a node it is represented by -1 and if it is incoming to a node i is +1. nx . linalg . graphmatrix . incidence_matrix ( G1 , oriented = True ) . toarray () array([[-1., -1., -1., 1., 0., 0., 0., 0., 0.], [ 1., 0., 0., -1., -1., -1., 0., 1., 0.], [ 0., 0., 0., 0., 1., 0., -1., 0., 0.], [ 0., 1., 0., 0., 0., 0., 1., -1., -1.], [ 0., 0., 1., 0., 0., 1., 0., 0., 1.]]) We can read from not just edgelists but also from adjacency matrix. For instance, we can create a new graph using the adjacency matrix from the directed graph. # Adjacencey matrix of the graph G1 M1 = nx . linalg . graphmatrix . adj_matrix ( G1 ) . toarray () print ( 'Adjacency matrix of G1' ) print ( M1 ) # convert the matrix into a graph G2 = nx . from_numpy_matrix ( M1 ) # Print the graph information print ( \"Printing G2\" ) nx . write_weighted_edgelist ( G2 , sys . stdout ) Adjacency matrix of G1 [[0 4 0 4 2] [4 0 9 0 1] [0 0 0 4 0] [0 8 0 0 7] [0 0 0 0 0]] Printing G2 0 1 4 0 3 4 0 4 2 1 2 9 1 4 1 1 3 8 2 3 4 3 4 7 Visualisation of graphs \u00b6 We can display the graphs using a variety of methods. The weights among the edges can be represented as thickness (or color) of the edges, the importance of nodes can be defined, etc. This will provide additional information of the graph. # get the outward degree for each node and store them as a list of (node_number, degree) out_deg = G1 . out_degree () # Making all the required parameters out_deg = [ int ( out_deg [ node ]) * 1000 for node in G1 . nodes ()] color = [ 'g' if val > np . mean ( out_deg ) else 'r' for val in out_deg ] weight = list ( nx . get_edge_attributes ( G1 , 'weight' ) . values ()) degree = dict ( G1 . degree ) # create the plot and title plt . subplots ( figsize = ( 10 , 10 )) plt . title ( 'Graph containing everything' ) nx . draw_circular ( G1 , with_labels = True , edge_color = color , width = weight , node_size = out_deg ) Anther different type of graph is the bipartite graph, which has a visualisation of its own. In the below example, we take an inbilt biparite graph to show how it can be visualised. B = nx . bipartite . gnmk_random_graph ( 3 , 5 , 10 , seed = 123 ) top = nx . bipartite . sets ( B )[ 0 ] pos = nx . bipartite_layout ( B , top ) nx . draw_networkx ( B , pos = nx . drawing . layout . bipartite_layout ( B , top ))","title":"Introduction to NetworkX (Python)"},{"location":"Python/Introduction%20to%20Networkx/#networks-in-python","text":"Networks play an important role in data science, with Google (page rank), Uber (route optimisation), Amazon (supply chian optimisation) and other companies becoming technology giants using network and data related optimisations. This article is an introduction to using networks in python using networkx package. # Import networkx library and rename it as nx. import networkx as nx # Other packages required import numpy as np import pandas as pd import matplotlib.pyplot as plt","title":"Networks in python"},{"location":"Python/Introduction%20to%20Networkx/#undirected-graphs","text":"A network can be represented in many ways. Edgelist represents graphs as a list of edges. Graphs can be undirected or directed. Consider the below edgelist where there is a relation between edge 1 and edge 2, and the weight for that edge is also provided. edgelist_df = pd . DataFrame ({ 'node1' :[ 1 , 1 , 1 , 2 , 2 , 3 , 4 , 4 ], 'node2' :[ 2 , 3 , 4 , 4 , 5 , 5 , 3 , 5 ], 'weights' :[ 5 , 2 , 1 , 3 , 1 , 7 , 1 , 4 ]}) edgelist_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } node1 node2 weights 0 1 2 5 1 1 3 2 2 1 4 1 3 2 4 3 4 2 5 1 5 3 5 7 6 4 3 1 7 4 5 4 One way to create a graph is to create an empty graph and add edges (and nodes) to the graph. g = nx . Graph () for i , elrow in edgelist_df . iterrows (): g . add_edge ( elrow [ 0 ], elrow [ 1 ], weight = elrow [ 2 ]) g . add_node ( 1 , pos = ( 0 , 5 )) g . edges ( data = True ) EdgeDataView([(1, 2, {'weight': 5}), (1, 3, {'weight': 2}), (1, 4, {'weight': 1}), (2, 4, {'weight': 3}), (2, 5, {'weight': 1}), (3, 5, {'weight': 7}), (3, 4, {'weight': 1}), (4, 5, {'weight': 4})]) # for each node we are trying to fix the coordinates g . add_node ( 1 , pos = ( 0 , 5 )) g . add_node ( 2 , pos = ( 5 , 10 )) g . add_node ( 3 , pos = ( 5 , 0 )) g . add_node ( 4 , pos = ( 10 , 5 )) g . add_node ( 5 , pos = ( 15 , 5 )) g . nodes ( data = True ) NodeDataView({1: {'pos': (0, 5)}, 2: {'pos': (5, 10)}, 3: {'pos': (5, 0)}, 4: {'pos': (10, 5)}, 5: {'pos': (15, 5)}}) This graph can be visualised as follows: # This function gets the coordinates for nodes pos = nx . get_node_attributes ( g , 'pos' ) # This function gets the weights for the edges weight = nx . get_edge_attributes ( g , 'weight' ) # The simplest way to draw a graph is by using nx.draw. nx . draw ( g , pos , with_labels = True ) # This adds edge lables nx . draw_networkx_edge_labels ( g , pos , edge_labels = weight ) plt . show () Mathematically this grah can be described in many ways. The most important are the adjacency matrix and incidence matrix. The adjacency matrix shows the relationship between the nodes in a matrix format where the nodes represent rows and columns. An edge between two nodes is represented by a positve number in the adjacency matrix, and the magnitude represents the weight. nx . linalg . graphmatrix . adj_matrix ( g ) . toarray () array([[0, 5, 2, 1, 0], [5, 0, 0, 3, 1], [2, 0, 0, 1, 7], [1, 3, 1, 0, 4], [0, 1, 7, 4, 0]], dtype=int64) The concurrency matrix represents the relationship between the nodes and edges. We have 8 edges as shown, and the relationship between the 5 nodes and 8 edges is shown below. g . edges EdgeView([(1, 2), (1, 3), (1, 4), (2, 4), (2, 5), (3, 5), (3, 4), (4, 5)]) nx . linalg . graphmatrix . incidence_matrix ( g ) . toarray () array([[1., 1., 1., 0., 0., 0., 0., 0.], [1., 0., 0., 1., 1., 0., 0., 0.], [0., 1., 0., 0., 0., 1., 1., 0.], [0., 0., 1., 1., 0., 0., 1., 1.], [0., 0., 0., 0., 1., 1., 0., 1.]])","title":"Undirected graphs"},{"location":"Python/Introduction%20to%20Networkx/#directed-graph","text":"A directed graph has edges which are in a particular direction. In this example, we are creating a DiGraph and reading a file where the edgelist is defined. An edge from a to b with a weight W exists when a line \"a b W\" can be found in the file. G1 = nx . DiGraph () # Read an directed graph from a list of edges, need to specify that using the parameter \u2018create_using\u2019 G1 = nx . read_edgelist ( \"edgelist1.txt\" , nodetype = str , create_using = nx . DiGraph (), data = [( 'weight' , int )]) print ( 'Edgelist' ) import sys nx . write_weighted_edgelist ( G1 , sys . stdout ) print ( 'nodes' ) print ( G1 . nodes ( data = True )) print ( 'edges' ) print ( G1 . edges ( data = True )) Edgelist 0 1 4 0 3 4 0 4 2 1 0 4 1 2 9 1 4 1 2 3 4 3 1 8 3 4 7 nodes [('0', {}), ('1', {}), ('2', {}), ('3', {}), ('4', {})] edges [('0', '1', {'weight': 4}), ('0', '3', {'weight': 4}), ('0', '4', {'weight': 2}), ('1', '0', {'weight': 4}), ('1', '2', {'weight': 9}), ('1', '4', {'weight': 1}), ('2', '3', {'weight': 4}), ('3', '1', {'weight': 8}), ('3', '4', {'weight': 7})] This graph can be visualised in many ways. Some of the visualisation patterns are (And these visualisations are same for directed or undirected graphs) 1. Spring layout (The length of edges is proportional to the weights) 2. Circular layout (The nodes are present in a circle) 3. Random layout All the three layouts are shown below nx . draw_spring ( G1 , with_labels = True , node_color = 'skyblue' , node_size = 200 , edge_color = 'black' ) nx . draw_circular ( G1 , with_labels = True , node_color = 'red' , node_size = 300 , edge_color = 'black' ) nx . draw_random ( G1 , with_labels = True , node_color = 'red' , node_size = 300 , edge_color = 'black' ) The concurrency matrix looks differently for directed graphs. If a edge is leaving a node it is represented by -1 and if it is incoming to a node i is +1. nx . linalg . graphmatrix . incidence_matrix ( G1 , oriented = True ) . toarray () array([[-1., -1., -1., 1., 0., 0., 0., 0., 0.], [ 1., 0., 0., -1., -1., -1., 0., 1., 0.], [ 0., 0., 0., 0., 1., 0., -1., 0., 0.], [ 0., 1., 0., 0., 0., 0., 1., -1., -1.], [ 0., 0., 1., 0., 0., 1., 0., 0., 1.]]) We can read from not just edgelists but also from adjacency matrix. For instance, we can create a new graph using the adjacency matrix from the directed graph. # Adjacencey matrix of the graph G1 M1 = nx . linalg . graphmatrix . adj_matrix ( G1 ) . toarray () print ( 'Adjacency matrix of G1' ) print ( M1 ) # convert the matrix into a graph G2 = nx . from_numpy_matrix ( M1 ) # Print the graph information print ( \"Printing G2\" ) nx . write_weighted_edgelist ( G2 , sys . stdout ) Adjacency matrix of G1 [[0 4 0 4 2] [4 0 9 0 1] [0 0 0 4 0] [0 8 0 0 7] [0 0 0 0 0]] Printing G2 0 1 4 0 3 4 0 4 2 1 2 9 1 4 1 1 3 8 2 3 4 3 4 7","title":"Directed graph."},{"location":"Python/Introduction%20to%20Networkx/#visualisation-of-graphs","text":"We can display the graphs using a variety of methods. The weights among the edges can be represented as thickness (or color) of the edges, the importance of nodes can be defined, etc. This will provide additional information of the graph. # get the outward degree for each node and store them as a list of (node_number, degree) out_deg = G1 . out_degree () # Making all the required parameters out_deg = [ int ( out_deg [ node ]) * 1000 for node in G1 . nodes ()] color = [ 'g' if val > np . mean ( out_deg ) else 'r' for val in out_deg ] weight = list ( nx . get_edge_attributes ( G1 , 'weight' ) . values ()) degree = dict ( G1 . degree ) # create the plot and title plt . subplots ( figsize = ( 10 , 10 )) plt . title ( 'Graph containing everything' ) nx . draw_circular ( G1 , with_labels = True , edge_color = color , width = weight , node_size = out_deg ) Anther different type of graph is the bipartite graph, which has a visualisation of its own. In the below example, we take an inbilt biparite graph to show how it can be visualised. B = nx . bipartite . gnmk_random_graph ( 3 , 5 , 10 , seed = 123 ) top = nx . bipartite . sets ( B )[ 0 ] pos = nx . bipartite_layout ( B , top ) nx . draw_networkx ( B , pos = nx . drawing . layout . bipartite_layout ( B , top ))","title":"Visualisation of graphs"},{"location":"Python/Network%20Flow%20problems/","text":"MathJax.Hub.Config({ extensions: [\"tex2jax.js\",\"TeX/AMSmath.js\",\"TeX/AMSsymbols.js\"], jax: [\"input/TeX\", \"output/HTML-CSS\"], tex2jax: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ], }, \"HTML-CSS\": { availableFonts: [\"TeX\"] } }); Flow maximisation problems \u00b6 A directed graph has the capacities on all the edges and our job is to find the maximum amount of flow that can happen from one node (starting node) to another node(outflow node). There are two types of flow maximisation problems: 1. Max flow: What is the maximum flow that can be sent from source to outfow without exceeding capacity. 2. Min cost flow: We have the cost along wih capacities on each edge. We want to find the minimum cost path of sending f uits of flow from source to outflow. We can use integer programming to solve both these problems. import networkx as nx # Other packages for manupulating data import numpy as np import pandas as pd # packages for plotting import matplotlib.pyplot as plt Maximum flow problem \u00b6 Let us first consider the max flow problem. Consider the below graph. edgelist_df = pd . DataFrame ({ 'node1' :[ 's' , 's' , 'u' , 'u' , 'v' ], 'node2' :[ 'u' , 'v' , 'v' , 't' , 't' ], 'weights' :[ 20 , 10 , 30 , 10 , 20 ] }) edgelist_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } node1 node2 weights 0 s u 20 1 s v 10 2 u v 30 3 u t 10 4 v t 20 g = nx . DiGraph () for i , elrow in edgelist_df . iterrows (): g . add_edge ( elrow [ 0 ], elrow [ 1 ], weight = elrow [ 2 ]) g . edges ( data = True ) OutEdgeDataView([('s', 'u', {'weight': 20}), ('s', 'v', {'weight': 10}), ('u', 'v', {'weight': 30}), ('u', 't', {'weight': 10}), ('v', 't', {'weight': 20})]) # for each node we are trying to fix the coordinates g . add_node ( 's' , pos = ( 0 , 1 )) g . add_node ( 'u' , pos = ( 1 , 2 )) g . add_node ( 'v' , pos = ( 1 , 0 )) g . add_node ( 't' , pos = ( 2 , 1 )) g . nodes ( data = True ) NodeDataView({'s': {'pos': (0, 1)}, 'u': {'pos': (1, 2)}, 'v': {'pos': (1, 0)}, 't': {'pos': (2, 1)}}) pos = nx . get_node_attributes ( g , 'pos' ) weight = nx . get_edge_attributes ( g , 'weight' ) nx . draw ( g , pos , with_labels = True ) nx . draw_networkx_edge_labels ( g , pos , edge_labels = weight ) plt . show () We want to find the maximum flow between s and t. This can be formulated as an integer programming problem, with Decision variable 1. Integer variable \\(edge_{i,j}\\) representing the amount of flow in edge ij. 2. Integer variable f representing the maximum flow from ortools.sat.python import cp_model max_flow_model = cp_model . CpModel () # Creating one integer decision variable for each edge edge_int_vars = {} for edge in g . edges : edge_int_vars [ edge [ 0 ], edge [ 1 ]] = max_flow_model . NewIntVar ( 0 , 100 , 'edge_ %s _ %s ' % edge ) print ( 'Creating the integer variable ' , edge_int_vars [ edge [ 0 ], edge [ 1 ]], 'representing the amount to flow in edge' , ( edge [ 0 ], edge [ 1 ])) Creating the integer variable edge_s_u representing the amount to flow in edge ('s', 'u') Creating the integer variable edge_s_v representing the amount to flow in edge ('s', 'v') Creating the integer variable edge_u_v representing the amount to flow in edge ('u', 'v') Creating the integer variable edge_u_t representing the amount to flow in edge ('u', 't') Creating the integer variable edge_v_t representing the amount to flow in edge ('v', 't') Constraints The first set of constraints are flow constraints, 1. At the input node, the net flow should be -f 2. At the output node, the net flow should be +f 3. The net flow in all other nodes should be 0 input_node = 's' output_node = 't' # Adding constraints on the nodes flow = max_flow_model . NewIntVar ( 0 , 100 , 'flow' ) # Initialising flow this will be maximised later for node in g . nodes : in_edges = g . in_edges ( node ) out_edges = g . out_edges ( node ) print ( 'Adding the constraint on node ' , node ) equation_at_this_edge = sum ( edge_int_vars [ edge [ 0 ], edge [ 1 ]] for edge in in_edges ) - \\ sum ( edge_int_vars [ edge [ 0 ], edge [ 1 ]] for edge in out_edges ) if ( node == input_node ): max_flow_model . Add ( equation_at_this_edge == - flow ) print ( equation_at_this_edge == - flow ) elif ( node == output_node ): max_flow_model . Add ( equation_at_this_edge == flow ) print ( equation_at_this_edge == flow ) else : max_flow_model . Add ( equation_at_this_edge == 0 ) print ( equation_at_this_edge , '== 0' ) print ( '' ) Adding the constraint on node s ((-((edge_s_u) + edge_s_v)) + (1 * flow)) == 0 Adding the constraint on node u ((edge_s_u) + -((edge_u_v) + edge_u_t)) == 0 Adding the constraint on node v (((edge_s_v) + edge_u_v) + -(edge_v_t)) == 0 Adding the constraint on node t ((((edge_u_t) + edge_v_t)) + -flow) == 0 Another set of constraints are the capacity restrictions on every edge. # Adding constraints on the edges for edge in g . edges : print ( 'Adding constraint on edge ' , edge ) max_flow_in_edge = g . get_edge_data ( * edge )[ 'weight' ] max_flow_model . Add ( edge_int_vars [ edge [ 0 ], edge [ 1 ]] <= max_flow_in_edge ) print ( edge_int_vars [ edge [ 0 ], edge [ 1 ]] <= max_flow_in_edge ) Adding constraint on edge ('s', 'u') edge_s_u <= 20 Adding constraint on edge ('s', 'v') edge_s_v <= 10 Adding constraint on edge ('u', 'v') edge_u_v <= 30 Adding constraint on edge ('u', 't') edge_u_t <= 10 Adding constraint on edge ('v', 't') edge_v_t <= 20 Objective The objective is to maximise flow. # The objective is to maximise flow max_flow_model . Maximize ( flow ) Solving the problem, we get # Solving the problem solver = cp_model . CpSolver () solution_printer = cp_model . ObjectiveSolutionPrinter () status = solver . SolveWithSolutionCallback ( max_flow_model , solution_printer ) Solution 0, time = 0.44 s, objective = 30 cp_model . OPTIMAL == status True result_edges = {} for edge in g . edges : result_edges [ edge [ 0 ], edge [ 1 ]] = solver . Value ( edge_int_vars [ edge [ 0 ], edge [ 1 ]]) result_edges {('s', 'u'): 20, ('s', 'v'): 10, ('u', 'v'): 10, ('u', 't'): 10, ('v', 't'): 20} The flow in each of the nodes is shown in the figure pos = nx . get_node_attributes ( g , 'pos' ) nx . draw ( g , pos , with_labels = True ) nx . draw_networkx_edge_labels ( g , pos , edge_labels = result_edges ) plt . show () Minimum cost flow problems \u00b6 We want to find the minimum cost to transfer a fixed amount of flow from one edge to another. The costs are given int he table bwlow. The costs are also displayed in the network below. edgelist_df = pd . DataFrame ({ 'node1' :[ 's' , 's' , 'u' , 'u' , 'v' ], 'node2' :[ 'u' , 'v' , 'v' , 't' , 't' ], 'weights' :[ 20 , 10 , 30 , 10 , 20 ], 'costs' :[ 3 , 2 , 0.7 , 1 , 7 ] }) edgelist_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } node1 node2 weights costs 0 s u 20 3.0 1 s v 10 2.0 2 u v 30 0.7 3 u t 10 1.0 4 v t 20 7.0 g = nx . DiGraph () for i , elrow in edgelist_df . iterrows (): g . add_edge ( elrow [ 0 ], elrow [ 1 ], weight = elrow [ 2 ], cost = elrow [ 3 ]) g . edges ( data = True ) OutEdgeDataView([('s', 'u', {'weight': 20, 'cost': 3.0}), ('s', 'v', {'weight': 10, 'cost': 2.0}), ('u', 'v', {'weight': 30, 'cost': 0.7}), ('u', 't', {'weight': 10, 'cost': 1.0}), ('v', 't', {'weight': 20, 'cost': 7.0})]) # for each node we are trying to fix the coordinates g . add_node ( 's' , pos = ( 0 , 1 )) g . add_node ( 'u' , pos = ( 1 , 2 )) g . add_node ( 'v' , pos = ( 1 , 0 )) g . add_node ( 't' , pos = ( 2 , 1 )) g . nodes ( data = True ) NodeDataView({'s': {'pos': (0, 1)}, 'u': {'pos': (1, 2)}, 'v': {'pos': (1, 0)}, 't': {'pos': (2, 1)}}) pos = nx . get_node_attributes ( g , 'pos' ) weight = nx . get_edge_attributes ( g , 'weight' ) cost = nx . get_edge_attributes ( g , 'cost' ) res = { key : str ( weight [ key ]) + '/$' + str ( cost . get ( key , '' )) for key in cost . keys ()} nx . draw ( g , pos , with_labels = True ) nx . draw_networkx_edge_labels ( g , pos , edge_labels = res ) plt . show () Just like maximum flow, we can use integer programming to find out the solution in this scenario also. Decision variables Integer variables \\(edge_{i,j}\\) representing the amount of flow in edge (i,j) Constraints Capacity restrictions on every edge Objective The objective is to minimise the overall cost from ortools.sat.python import cp_model min_cost_flow_model = cp_model . CpModel () # Creating one integer decision variable for each edge edge_int_vars = {} for edge in g . edges : edge_int_vars [ edge [ 0 ], edge [ 1 ]] = min_cost_flow_model . NewIntVar ( 0 , 100 , 'edge_ %s _ %s ' % edge ) print ( 'Creating the integer variable ' , edge_int_vars [ edge [ 0 ], edge [ 1 ]], 'representing the amount to flow in edge' , ( edge [ 0 ], edge [ 1 ])) Creating the integer variable edge_s_u representing the amount to flow in edge ('s', 'u') Creating the integer variable edge_s_v representing the amount to flow in edge ('s', 'v') Creating the integer variable edge_u_v representing the amount to flow in edge ('u', 'v') Creating the integer variable edge_u_t representing the amount to flow in edge ('u', 't') Creating the integer variable edge_v_t representing the amount to flow in edge ('v', 't') input_node = 's' output_node = 't' flow = 20 # Assuming a constant flow of 20 units # Adding constraints on the nodes for node in g . nodes : in_edges = g . in_edges ( node ) out_edges = g . out_edges ( node ) print ( 'Adding the constraint on node ' , node ) print ( 'This node has %i in-edges and %i out-edges' % ( len ( in_edges ), len ( out_edges ))) equation_at_this_edge = sum ( edge_int_vars [ edge [ 0 ], edge [ 1 ]] for edge in in_edges ) - \\ sum ( edge_int_vars [ edge [ 0 ], edge [ 1 ]] for edge in out_edges ) if ( node == input_node ): min_cost_flow_model . Add ( equation_at_this_edge == - flow ) print ( equation_at_this_edge == - flow ) elif ( node == output_node ): min_cost_flow_model . Add ( equation_at_this_edge == flow ) print ( equation_at_this_edge == flow ) else : min_cost_flow_model . Add ( equation_at_this_edge == 0 ) print ( equation_at_this_edge , '== 0' ) print ( '' ) Adding the constraint on node s This node has 0 in-edges and 2 out-edges (-((edge_s_u) + edge_s_v)) == -20 Adding the constraint on node u This node has 1 in-edges and 2 out-edges ((edge_s_u) + -((edge_u_v) + edge_u_t)) == 0 Adding the constraint on node v This node has 2 in-edges and 1 out-edges (((edge_s_v) + edge_u_v) + -(edge_v_t)) == 0 Adding the constraint on node t This node has 2 in-edges and 0 out-edges (((edge_u_t) + edge_v_t)) == 20 # Adding constraints on the edges for edge in g . edges : print ( 'Adding constraint on edge ' , edge ) max_flow_in_edge = g . get_edge_data ( * edge )[ 'weight' ] min_cost_flow_model . Add ( edge_int_vars [ edge [ 0 ], edge [ 1 ]] <= max_flow_in_edge ) print ( edge_int_vars [ edge [ 0 ], edge [ 1 ]] <= max_flow_in_edge ) Adding constraint on edge ('s', 'u') edge_s_u <= 20 Adding constraint on edge ('s', 'v') edge_s_v <= 10 Adding constraint on edge ('u', 'v') edge_u_v <= 30 Adding constraint on edge ('u', 't') edge_u_t <= 10 Adding constraint on edge ('v', 't') edge_v_t <= 20 # factor to make everything including costs integer factor_to_int = 10 # The objective is to maximise flow total_cost = sum ( int ( g . get_edge_data ( * edge )[ 'cost' ] * factor_to_int ) * edge_int_vars [ edge [ 0 ], edge [ 1 ]] for edge in g . edges ) print ( 'Objective is to optimise cost' ) print ( total_cost ) min_cost_flow_model . Minimize ( total_cost ) Objective is to optimise cost ((((((30 * edge_s_u)) + (20 * edge_s_v)) + (7 * edge_u_v)) + (10 * edge_u_t)) + (70 * edge_v_t)) # Solving the problem solver = cp_model . CpSolver () solution_printer = cp_model . ObjectiveSolutionPrinter () status = solver . SolveWithSolutionCallback ( min_cost_flow_model , solution_printer ) Solution 0, time = 0.24 s, objective = 1300 cp_model . OPTIMAL == status True The solution to the minimum flow problem is result_edges = {} for edge in g . edges : result_edges [ edge [ 0 ], edge [ 1 ]] = solver . Value ( edge_int_vars [ edge [ 0 ], edge [ 1 ]]) result_edges {('s', 'u'): 10, ('s', 'v'): 10, ('u', 'v'): 0, ('u', 't'): 10, ('v', 't'): 10} # This function gets the coordinates for nodes pos = nx . get_node_attributes ( g , 'pos' ) # The simplest way to draw a graph is by using nx.draw. nx . draw ( g , pos , with_labels = True ) # This adds edge lables nx . draw_networkx_edge_labels ( g , pos , edge_labels = result_edges ) plt . show ()","title":"Network flow problems (Python)"},{"location":"Python/Network%20Flow%20problems/#flow-maximisation-problems","text":"A directed graph has the capacities on all the edges and our job is to find the maximum amount of flow that can happen from one node (starting node) to another node(outflow node). There are two types of flow maximisation problems: 1. Max flow: What is the maximum flow that can be sent from source to outfow without exceeding capacity. 2. Min cost flow: We have the cost along wih capacities on each edge. We want to find the minimum cost path of sending f uits of flow from source to outflow. We can use integer programming to solve both these problems. import networkx as nx # Other packages for manupulating data import numpy as np import pandas as pd # packages for plotting import matplotlib.pyplot as plt","title":"Flow maximisation problems"},{"location":"Python/Network%20Flow%20problems/#maximum-flow-problem","text":"Let us first consider the max flow problem. Consider the below graph. edgelist_df = pd . DataFrame ({ 'node1' :[ 's' , 's' , 'u' , 'u' , 'v' ], 'node2' :[ 'u' , 'v' , 'v' , 't' , 't' ], 'weights' :[ 20 , 10 , 30 , 10 , 20 ] }) edgelist_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } node1 node2 weights 0 s u 20 1 s v 10 2 u v 30 3 u t 10 4 v t 20 g = nx . DiGraph () for i , elrow in edgelist_df . iterrows (): g . add_edge ( elrow [ 0 ], elrow [ 1 ], weight = elrow [ 2 ]) g . edges ( data = True ) OutEdgeDataView([('s', 'u', {'weight': 20}), ('s', 'v', {'weight': 10}), ('u', 'v', {'weight': 30}), ('u', 't', {'weight': 10}), ('v', 't', {'weight': 20})]) # for each node we are trying to fix the coordinates g . add_node ( 's' , pos = ( 0 , 1 )) g . add_node ( 'u' , pos = ( 1 , 2 )) g . add_node ( 'v' , pos = ( 1 , 0 )) g . add_node ( 't' , pos = ( 2 , 1 )) g . nodes ( data = True ) NodeDataView({'s': {'pos': (0, 1)}, 'u': {'pos': (1, 2)}, 'v': {'pos': (1, 0)}, 't': {'pos': (2, 1)}}) pos = nx . get_node_attributes ( g , 'pos' ) weight = nx . get_edge_attributes ( g , 'weight' ) nx . draw ( g , pos , with_labels = True ) nx . draw_networkx_edge_labels ( g , pos , edge_labels = weight ) plt . show () We want to find the maximum flow between s and t. This can be formulated as an integer programming problem, with Decision variable 1. Integer variable \\(edge_{i,j}\\) representing the amount of flow in edge ij. 2. Integer variable f representing the maximum flow from ortools.sat.python import cp_model max_flow_model = cp_model . CpModel () # Creating one integer decision variable for each edge edge_int_vars = {} for edge in g . edges : edge_int_vars [ edge [ 0 ], edge [ 1 ]] = max_flow_model . NewIntVar ( 0 , 100 , 'edge_ %s _ %s ' % edge ) print ( 'Creating the integer variable ' , edge_int_vars [ edge [ 0 ], edge [ 1 ]], 'representing the amount to flow in edge' , ( edge [ 0 ], edge [ 1 ])) Creating the integer variable edge_s_u representing the amount to flow in edge ('s', 'u') Creating the integer variable edge_s_v representing the amount to flow in edge ('s', 'v') Creating the integer variable edge_u_v representing the amount to flow in edge ('u', 'v') Creating the integer variable edge_u_t representing the amount to flow in edge ('u', 't') Creating the integer variable edge_v_t representing the amount to flow in edge ('v', 't') Constraints The first set of constraints are flow constraints, 1. At the input node, the net flow should be -f 2. At the output node, the net flow should be +f 3. The net flow in all other nodes should be 0 input_node = 's' output_node = 't' # Adding constraints on the nodes flow = max_flow_model . NewIntVar ( 0 , 100 , 'flow' ) # Initialising flow this will be maximised later for node in g . nodes : in_edges = g . in_edges ( node ) out_edges = g . out_edges ( node ) print ( 'Adding the constraint on node ' , node ) equation_at_this_edge = sum ( edge_int_vars [ edge [ 0 ], edge [ 1 ]] for edge in in_edges ) - \\ sum ( edge_int_vars [ edge [ 0 ], edge [ 1 ]] for edge in out_edges ) if ( node == input_node ): max_flow_model . Add ( equation_at_this_edge == - flow ) print ( equation_at_this_edge == - flow ) elif ( node == output_node ): max_flow_model . Add ( equation_at_this_edge == flow ) print ( equation_at_this_edge == flow ) else : max_flow_model . Add ( equation_at_this_edge == 0 ) print ( equation_at_this_edge , '== 0' ) print ( '' ) Adding the constraint on node s ((-((edge_s_u) + edge_s_v)) + (1 * flow)) == 0 Adding the constraint on node u ((edge_s_u) + -((edge_u_v) + edge_u_t)) == 0 Adding the constraint on node v (((edge_s_v) + edge_u_v) + -(edge_v_t)) == 0 Adding the constraint on node t ((((edge_u_t) + edge_v_t)) + -flow) == 0 Another set of constraints are the capacity restrictions on every edge. # Adding constraints on the edges for edge in g . edges : print ( 'Adding constraint on edge ' , edge ) max_flow_in_edge = g . get_edge_data ( * edge )[ 'weight' ] max_flow_model . Add ( edge_int_vars [ edge [ 0 ], edge [ 1 ]] <= max_flow_in_edge ) print ( edge_int_vars [ edge [ 0 ], edge [ 1 ]] <= max_flow_in_edge ) Adding constraint on edge ('s', 'u') edge_s_u <= 20 Adding constraint on edge ('s', 'v') edge_s_v <= 10 Adding constraint on edge ('u', 'v') edge_u_v <= 30 Adding constraint on edge ('u', 't') edge_u_t <= 10 Adding constraint on edge ('v', 't') edge_v_t <= 20 Objective The objective is to maximise flow. # The objective is to maximise flow max_flow_model . Maximize ( flow ) Solving the problem, we get # Solving the problem solver = cp_model . CpSolver () solution_printer = cp_model . ObjectiveSolutionPrinter () status = solver . SolveWithSolutionCallback ( max_flow_model , solution_printer ) Solution 0, time = 0.44 s, objective = 30 cp_model . OPTIMAL == status True result_edges = {} for edge in g . edges : result_edges [ edge [ 0 ], edge [ 1 ]] = solver . Value ( edge_int_vars [ edge [ 0 ], edge [ 1 ]]) result_edges {('s', 'u'): 20, ('s', 'v'): 10, ('u', 'v'): 10, ('u', 't'): 10, ('v', 't'): 20} The flow in each of the nodes is shown in the figure pos = nx . get_node_attributes ( g , 'pos' ) nx . draw ( g , pos , with_labels = True ) nx . draw_networkx_edge_labels ( g , pos , edge_labels = result_edges ) plt . show ()","title":"Maximum flow problem"},{"location":"Python/Network%20Flow%20problems/#minimum-cost-flow-problems","text":"We want to find the minimum cost to transfer a fixed amount of flow from one edge to another. The costs are given int he table bwlow. The costs are also displayed in the network below. edgelist_df = pd . DataFrame ({ 'node1' :[ 's' , 's' , 'u' , 'u' , 'v' ], 'node2' :[ 'u' , 'v' , 'v' , 't' , 't' ], 'weights' :[ 20 , 10 , 30 , 10 , 20 ], 'costs' :[ 3 , 2 , 0.7 , 1 , 7 ] }) edgelist_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } node1 node2 weights costs 0 s u 20 3.0 1 s v 10 2.0 2 u v 30 0.7 3 u t 10 1.0 4 v t 20 7.0 g = nx . DiGraph () for i , elrow in edgelist_df . iterrows (): g . add_edge ( elrow [ 0 ], elrow [ 1 ], weight = elrow [ 2 ], cost = elrow [ 3 ]) g . edges ( data = True ) OutEdgeDataView([('s', 'u', {'weight': 20, 'cost': 3.0}), ('s', 'v', {'weight': 10, 'cost': 2.0}), ('u', 'v', {'weight': 30, 'cost': 0.7}), ('u', 't', {'weight': 10, 'cost': 1.0}), ('v', 't', {'weight': 20, 'cost': 7.0})]) # for each node we are trying to fix the coordinates g . add_node ( 's' , pos = ( 0 , 1 )) g . add_node ( 'u' , pos = ( 1 , 2 )) g . add_node ( 'v' , pos = ( 1 , 0 )) g . add_node ( 't' , pos = ( 2 , 1 )) g . nodes ( data = True ) NodeDataView({'s': {'pos': (0, 1)}, 'u': {'pos': (1, 2)}, 'v': {'pos': (1, 0)}, 't': {'pos': (2, 1)}}) pos = nx . get_node_attributes ( g , 'pos' ) weight = nx . get_edge_attributes ( g , 'weight' ) cost = nx . get_edge_attributes ( g , 'cost' ) res = { key : str ( weight [ key ]) + '/$' + str ( cost . get ( key , '' )) for key in cost . keys ()} nx . draw ( g , pos , with_labels = True ) nx . draw_networkx_edge_labels ( g , pos , edge_labels = res ) plt . show () Just like maximum flow, we can use integer programming to find out the solution in this scenario also. Decision variables Integer variables \\(edge_{i,j}\\) representing the amount of flow in edge (i,j) Constraints Capacity restrictions on every edge Objective The objective is to minimise the overall cost from ortools.sat.python import cp_model min_cost_flow_model = cp_model . CpModel () # Creating one integer decision variable for each edge edge_int_vars = {} for edge in g . edges : edge_int_vars [ edge [ 0 ], edge [ 1 ]] = min_cost_flow_model . NewIntVar ( 0 , 100 , 'edge_ %s _ %s ' % edge ) print ( 'Creating the integer variable ' , edge_int_vars [ edge [ 0 ], edge [ 1 ]], 'representing the amount to flow in edge' , ( edge [ 0 ], edge [ 1 ])) Creating the integer variable edge_s_u representing the amount to flow in edge ('s', 'u') Creating the integer variable edge_s_v representing the amount to flow in edge ('s', 'v') Creating the integer variable edge_u_v representing the amount to flow in edge ('u', 'v') Creating the integer variable edge_u_t representing the amount to flow in edge ('u', 't') Creating the integer variable edge_v_t representing the amount to flow in edge ('v', 't') input_node = 's' output_node = 't' flow = 20 # Assuming a constant flow of 20 units # Adding constraints on the nodes for node in g . nodes : in_edges = g . in_edges ( node ) out_edges = g . out_edges ( node ) print ( 'Adding the constraint on node ' , node ) print ( 'This node has %i in-edges and %i out-edges' % ( len ( in_edges ), len ( out_edges ))) equation_at_this_edge = sum ( edge_int_vars [ edge [ 0 ], edge [ 1 ]] for edge in in_edges ) - \\ sum ( edge_int_vars [ edge [ 0 ], edge [ 1 ]] for edge in out_edges ) if ( node == input_node ): min_cost_flow_model . Add ( equation_at_this_edge == - flow ) print ( equation_at_this_edge == - flow ) elif ( node == output_node ): min_cost_flow_model . Add ( equation_at_this_edge == flow ) print ( equation_at_this_edge == flow ) else : min_cost_flow_model . Add ( equation_at_this_edge == 0 ) print ( equation_at_this_edge , '== 0' ) print ( '' ) Adding the constraint on node s This node has 0 in-edges and 2 out-edges (-((edge_s_u) + edge_s_v)) == -20 Adding the constraint on node u This node has 1 in-edges and 2 out-edges ((edge_s_u) + -((edge_u_v) + edge_u_t)) == 0 Adding the constraint on node v This node has 2 in-edges and 1 out-edges (((edge_s_v) + edge_u_v) + -(edge_v_t)) == 0 Adding the constraint on node t This node has 2 in-edges and 0 out-edges (((edge_u_t) + edge_v_t)) == 20 # Adding constraints on the edges for edge in g . edges : print ( 'Adding constraint on edge ' , edge ) max_flow_in_edge = g . get_edge_data ( * edge )[ 'weight' ] min_cost_flow_model . Add ( edge_int_vars [ edge [ 0 ], edge [ 1 ]] <= max_flow_in_edge ) print ( edge_int_vars [ edge [ 0 ], edge [ 1 ]] <= max_flow_in_edge ) Adding constraint on edge ('s', 'u') edge_s_u <= 20 Adding constraint on edge ('s', 'v') edge_s_v <= 10 Adding constraint on edge ('u', 'v') edge_u_v <= 30 Adding constraint on edge ('u', 't') edge_u_t <= 10 Adding constraint on edge ('v', 't') edge_v_t <= 20 # factor to make everything including costs integer factor_to_int = 10 # The objective is to maximise flow total_cost = sum ( int ( g . get_edge_data ( * edge )[ 'cost' ] * factor_to_int ) * edge_int_vars [ edge [ 0 ], edge [ 1 ]] for edge in g . edges ) print ( 'Objective is to optimise cost' ) print ( total_cost ) min_cost_flow_model . Minimize ( total_cost ) Objective is to optimise cost ((((((30 * edge_s_u)) + (20 * edge_s_v)) + (7 * edge_u_v)) + (10 * edge_u_t)) + (70 * edge_v_t)) # Solving the problem solver = cp_model . CpSolver () solution_printer = cp_model . ObjectiveSolutionPrinter () status = solver . SolveWithSolutionCallback ( min_cost_flow_model , solution_printer ) Solution 0, time = 0.24 s, objective = 1300 cp_model . OPTIMAL == status True The solution to the minimum flow problem is result_edges = {} for edge in g . edges : result_edges [ edge [ 0 ], edge [ 1 ]] = solver . Value ( edge_int_vars [ edge [ 0 ], edge [ 1 ]]) result_edges {('s', 'u'): 10, ('s', 'v'): 10, ('u', 'v'): 0, ('u', 't'): 10, ('v', 't'): 10} # This function gets the coordinates for nodes pos = nx . get_node_attributes ( g , 'pos' ) # The simplest way to draw a graph is by using nx.draw. nx . draw ( g , pos , with_labels = True ) # This adds edge lables nx . draw_networkx_edge_labels ( g , pos , edge_labels = result_edges ) plt . show ()","title":"Minimum cost flow problems"},{"location":"Python/Network%20Science/","text":"Network Science We live in a highly connected world where social networks significantly affect our lives, from getting jobs, connecting with friends, and dating and news. Network science is the study of such complex networks. There is some common phenomenon that applies to social networks: How do networks evolve \u00b6 A social network is constantly evolving; people make new friends and lose touch with other friends. Many external factors in life, like marriage, new college, school, workplace or club, can spark the creation of new friends. This phenomenon is observed in vast networks such as the internet. Is there any model that can explain the growth of networks? If so, we can simulate new networks, identify what parameters influence the growth of new networks etc. Power law explains some real-world examples of the growth and scale of networks. Small World Phenomena \u00b6 Small world phenomenon states that everyone in the world can be reached thru a short chain of acquaintances. This was first demonstrated in the Milgram experiment where random people were asked to send a letter to a stockbroker in Boston by passing it through people they know. Of the letters received, they went thru an average of 6 people. Facebook repeated this on a large scale later, which validated this with a lower number of connections. This indicates that most people are connected to every other random person on earth within six common friends. Homophily \u00b6 This can be described as \"Birds of a feather flock together\". We tend to be friends with friends who are similar to us, especially people from the same socio-economic background, social background, etc. Strength of weak ties \u00b6 If two people in a social network have a friend in common, then there is an increased likelihood that they will become friends at some point in the future. This also depends on the strength of the friendship(ties). Generally, people in the same network will have the same kind of information. Take an example of jobs; everyone in the same friend circle will have the same information about open positions for jobs. It is the weak ties, friends whom we have not interacted with within some time, who have new information which will help most people to find jobs. This is called as the strength of weak ties. Let us look at a small network of friends in Zachary karate club . Zachary's karate club is a university-based karate club consisting of a social network of 34 members. W.W. Zachary famously observed the karate club for three years (from 1970 to 1972), documenting links between pairs of members who interacted outside the everyday activities of the club. The nodes in the graph represent people, and the edges represent friendships (interactions) between them. import matplotlib.pyplot as plt % matplotlib inline import networkx as nx G = nx . karate_club_graph () nx . draw_kamada_kawai ( G , with_labels = True ) We can understand the network using some statistics about the network. Network Statistics \u00b6 Degree Distribution \u00b6 The degree of a node is the number of nodes a particular node is connected to. For example, node 11 has one degree in the karate club network as it is connected to one node only(node 0). The degree distribution is the relative frequencies of the nodes that have particular degrees. There are two types of degree distributions generally, Poisson and scale-free. For the karate club network, the degree distribution is: degree_dist = nx . degree_histogram ( G ) plt . bar ( range ( len ( degree_dist )), degree_dist ) plt . ylabel ( 'No of people' ) plt . xlabel ( 'No of friends' ); This indicates that most people have 2-5 friends while a few people are connected to >15 people in the network. Clustering coefficient \u00b6 Another statistic that we can understand is the clustering coefficient. This is the degree to which nodes cluster together. If A is friends with B and C, then B and c have a high probability of being friends if they belong to the same cluster. the clustering coefficient captures the fraction of triads (connected A, B and C) in the network. The clustering coefficient of all the nodes is calculated. The clustering coefficient of a node is the percentage of triangles that are there among the nodes that it is connected to. For example, node 11 is connected to node 6 and 5, both of which are connected. So the percentage of triads is 100%. Node 4 is connected to 0, 6 and 10. 0-6-4 and 0-10-0 make a triad, but 0-10-4 does not exist. Therefore the percentage is 66.6%. The clustering coefficient of the network is the average of the clustering coefficient across all the nodes, and it is 57% for the karate club. nx . clustering ( G ) {0: 0.15, 1: 0.3333333333333333, 2: 0.24444444444444444, 3: 0.6666666666666666, 4: 0.6666666666666666, 5: 0.5, 6: 0.5, 7: 1.0, 8: 0.5, 9: 0, 10: 0.6666666666666666, 11: 0, 12: 1.0, 13: 0.6, 14: 1.0, 15: 1.0, 16: 1.0, 17: 1.0, 18: 1.0, 19: 0.3333333333333333, 20: 1.0, 21: 1.0, 22: 1.0, 23: 0.4, 24: 0.3333333333333333, 25: 0.3333333333333333, 26: 1.0, 27: 0.16666666666666666, 28: 0.3333333333333333, 29: 0.6666666666666666, 30: 0.5, 31: 0.2, 32: 0.19696969696969696, 33: 0.11029411764705882} nx . algorithms . cluster . average_clustering ( G ) 0.5706384782076823 Community Detection \u00b6 During the study in Karate club, a conflict arose between the club president 'A' and instructor 'B' (pseudonyms), which led to a split in the club. Half of the members formed a new club around A (group one) and the remaining members under B. Can we identify the 2 communities in this network? Girvan Newman \u00b6 Community detection is close to clustering in machine learning. In clustering, we do the clustering based on the distance between the points when placed in an n-dimensional space. Similar to clustering, in community detection, we want to cluster the nodes closer to each other than others. The only difference is the concept of distance is based on the topology of the network. Like clustering, we can divide the network into two communities using top-down and bottom-up methods. This is represented by a dendrogram where we can draw a horizontal line to split the communities. This is discussed in more detail in a different blog. The two groups based on Girvan Newman are: karate_club_split = nx . algorithms . community . centrality . girvan_newman ( G ) karate_club_split_tuple = tuple ( sorted ( c ) for c in next ( karate_club_split )) karate_club_split_tuple ([0, 1, 3, 4, 5, 6, 7, 10, 11, 12, 13, 16, 17, 19, 21], [2, 8, 9, 14, 15, 18, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]) # To get colors in the graph color_list = [ 'r' if node in karate_club_split_tuple [ 0 ] else 'g' for node in G . nodes ] nx . draw_kamada_kawai ( G , node_color = color_list , with_labels = True ) Ratio cut \u00b6 Another method is the ratio cut method. A cut is a set of edges such that if we remove them, the network breaks into two components. A network will have many cuts, and to find an optimal split, we need to: 1. Find the min-balanced-cut (cut with the minimum number of edges with an almost equal number of nodes on each side) among all pairs of nodes 2. Repeat over each of the two parts This is discussed in more detail in a different blog. References 1. Zachary, W. W. (1977). An Information Flow Model for Conflict and Fission in Small Groups. Journal of Anthropological Research. 33 (4), pp. 452-473. 2. Networkx Karate club","title":"Network Science (Python)"},{"location":"Python/Network%20Science/#how-do-networks-evolve","text":"A social network is constantly evolving; people make new friends and lose touch with other friends. Many external factors in life, like marriage, new college, school, workplace or club, can spark the creation of new friends. This phenomenon is observed in vast networks such as the internet. Is there any model that can explain the growth of networks? If so, we can simulate new networks, identify what parameters influence the growth of new networks etc. Power law explains some real-world examples of the growth and scale of networks.","title":"How do networks evolve"},{"location":"Python/Network%20Science/#small-world-phenomena","text":"Small world phenomenon states that everyone in the world can be reached thru a short chain of acquaintances. This was first demonstrated in the Milgram experiment where random people were asked to send a letter to a stockbroker in Boston by passing it through people they know. Of the letters received, they went thru an average of 6 people. Facebook repeated this on a large scale later, which validated this with a lower number of connections. This indicates that most people are connected to every other random person on earth within six common friends.","title":"Small World Phenomena"},{"location":"Python/Network%20Science/#homophily","text":"This can be described as \"Birds of a feather flock together\". We tend to be friends with friends who are similar to us, especially people from the same socio-economic background, social background, etc.","title":"Homophily"},{"location":"Python/Network%20Science/#strength-of-weak-ties","text":"If two people in a social network have a friend in common, then there is an increased likelihood that they will become friends at some point in the future. This also depends on the strength of the friendship(ties). Generally, people in the same network will have the same kind of information. Take an example of jobs; everyone in the same friend circle will have the same information about open positions for jobs. It is the weak ties, friends whom we have not interacted with within some time, who have new information which will help most people to find jobs. This is called as the strength of weak ties. Let us look at a small network of friends in Zachary karate club . Zachary's karate club is a university-based karate club consisting of a social network of 34 members. W.W. Zachary famously observed the karate club for three years (from 1970 to 1972), documenting links between pairs of members who interacted outside the everyday activities of the club. The nodes in the graph represent people, and the edges represent friendships (interactions) between them. import matplotlib.pyplot as plt % matplotlib inline import networkx as nx G = nx . karate_club_graph () nx . draw_kamada_kawai ( G , with_labels = True ) We can understand the network using some statistics about the network.","title":"Strength of weak ties"},{"location":"Python/Network%20Science/#network-statistics","text":"","title":"Network Statistics"},{"location":"Python/Network%20Science/#degree-distribution","text":"The degree of a node is the number of nodes a particular node is connected to. For example, node 11 has one degree in the karate club network as it is connected to one node only(node 0). The degree distribution is the relative frequencies of the nodes that have particular degrees. There are two types of degree distributions generally, Poisson and scale-free. For the karate club network, the degree distribution is: degree_dist = nx . degree_histogram ( G ) plt . bar ( range ( len ( degree_dist )), degree_dist ) plt . ylabel ( 'No of people' ) plt . xlabel ( 'No of friends' ); This indicates that most people have 2-5 friends while a few people are connected to >15 people in the network.","title":"Degree Distribution"},{"location":"Python/Network%20Science/#clustering-coefficient","text":"Another statistic that we can understand is the clustering coefficient. This is the degree to which nodes cluster together. If A is friends with B and C, then B and c have a high probability of being friends if they belong to the same cluster. the clustering coefficient captures the fraction of triads (connected A, B and C) in the network. The clustering coefficient of all the nodes is calculated. The clustering coefficient of a node is the percentage of triangles that are there among the nodes that it is connected to. For example, node 11 is connected to node 6 and 5, both of which are connected. So the percentage of triads is 100%. Node 4 is connected to 0, 6 and 10. 0-6-4 and 0-10-0 make a triad, but 0-10-4 does not exist. Therefore the percentage is 66.6%. The clustering coefficient of the network is the average of the clustering coefficient across all the nodes, and it is 57% for the karate club. nx . clustering ( G ) {0: 0.15, 1: 0.3333333333333333, 2: 0.24444444444444444, 3: 0.6666666666666666, 4: 0.6666666666666666, 5: 0.5, 6: 0.5, 7: 1.0, 8: 0.5, 9: 0, 10: 0.6666666666666666, 11: 0, 12: 1.0, 13: 0.6, 14: 1.0, 15: 1.0, 16: 1.0, 17: 1.0, 18: 1.0, 19: 0.3333333333333333, 20: 1.0, 21: 1.0, 22: 1.0, 23: 0.4, 24: 0.3333333333333333, 25: 0.3333333333333333, 26: 1.0, 27: 0.16666666666666666, 28: 0.3333333333333333, 29: 0.6666666666666666, 30: 0.5, 31: 0.2, 32: 0.19696969696969696, 33: 0.11029411764705882} nx . algorithms . cluster . average_clustering ( G ) 0.5706384782076823","title":"Clustering coefficient"},{"location":"Python/Network%20Science/#community-detection","text":"During the study in Karate club, a conflict arose between the club president 'A' and instructor 'B' (pseudonyms), which led to a split in the club. Half of the members formed a new club around A (group one) and the remaining members under B. Can we identify the 2 communities in this network?","title":"Community Detection"},{"location":"Python/Network%20Science/#girvan-newman","text":"Community detection is close to clustering in machine learning. In clustering, we do the clustering based on the distance between the points when placed in an n-dimensional space. Similar to clustering, in community detection, we want to cluster the nodes closer to each other than others. The only difference is the concept of distance is based on the topology of the network. Like clustering, we can divide the network into two communities using top-down and bottom-up methods. This is represented by a dendrogram where we can draw a horizontal line to split the communities. This is discussed in more detail in a different blog. The two groups based on Girvan Newman are: karate_club_split = nx . algorithms . community . centrality . girvan_newman ( G ) karate_club_split_tuple = tuple ( sorted ( c ) for c in next ( karate_club_split )) karate_club_split_tuple ([0, 1, 3, 4, 5, 6, 7, 10, 11, 12, 13, 16, 17, 19, 21], [2, 8, 9, 14, 15, 18, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]) # To get colors in the graph color_list = [ 'r' if node in karate_club_split_tuple [ 0 ] else 'g' for node in G . nodes ] nx . draw_kamada_kawai ( G , node_color = color_list , with_labels = True )","title":"Girvan Newman"},{"location":"Python/Network%20Science/#ratio-cut","text":"Another method is the ratio cut method. A cut is a set of edges such that if we remove them, the network breaks into two components. A network will have many cuts, and to find an optimal split, we need to: 1. Find the min-balanced-cut (cut with the minimum number of edges with an almost equal number of nodes on each side) among all pairs of nodes 2. Repeat over each of the two parts This is discussed in more detail in a different blog. References 1. Zachary, W. W. (1977). An Information Flow Model for Conflict and Fission in Small Groups. Journal of Anthropological Research. 33 (4), pp. 452-473. 2. Networkx Karate club","title":"Ratio cut"},{"location":"Python/Network%20centrality/","text":"Centrality measures \u00b6 Centrality measures help us understand who are the most important people in the network. For this example, \u201cemail-Eu-core network,\u201d from Stanford\u2019s SNAP is used. You may find the original dataset here: https://snap.stanford.edu/data/email-Eu-core.html .. This is a dataset of various email communications between people, and we are interested about the important people, leaders and opinion makers in this network. import pandas as pd import numpy as np import matplotlib.pyplot as plt import networkx as nx % matplotlib inline For the purposes of this blog, I am filtering the data only for the first 100 people for brevity. Same analysis can be done across the complete dataset. df = pd . read_csv ( \"https://raw.githubusercontent.com/jinhangjiang/Datasets/main/Network%20Data/emailEUcore/email-Eu-core.txt\" , delimiter = \" \" , names = [ \"Source\" , \"Target\" ]) df = df . loc [( df . Source < 100 ) & ( df . Target < 100 )] df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Source Target 0 0 1 1 2 3 2 2 4 3 5 6 4 5 7 We are using a directed graph as we are interested emails being sent and received. G = nx . from_pandas_edgelist ( df , source = 'Source' , target = 'Target' , create_using = nx . DiGraph ()) plt . subplots ( figsize = ( 20 , 20 )) plt . title ( 'Email, EU-Core network' ) nx . draw_kamada_kawai ( G , with_labels = True ) We can see that there are two densily connected networks and many connections between them. Who is the most important person in the network? This is based on what is the question you are asking. Is it the person who can send the emails the fastest? Is it the person why can connect with maximum number of people? Some basic features would be: 1. Just by the number of mails sent/received, who has sent the maximum number of mails 2. By the number of people that a person has a immediate relation with (a relation is defined as sending an email to that person): degree centrality 3. We can also look at the persons who are essential for communication between different closely knit groups. (betweenness centrality) The person with the maximum number of interactions is: student_sent_mails = list ( df . groupby ( 'Source' )[ 'Target' ] . count ()) student_sent_mails . index ( max ( student_sent_mails )) + 1 86 We can say that the person with index 86 has the most number of interactions and is one of the most important person. He would be the go to person who has connections with maximum number of people. We can see this in the graph also. Degree centrality \u00b6 This way of looking at the importance of a person based on number of connections (Degrees) is called degree centrality. In ascending order, the top 10 people with maximum number of connections are: degree_centrality = nx . algorithms . centrality . degree_centrality ( G ) for i , w in enumerate ( sorted ( degree_centrality , key = degree_centrality . get , reverse = True )): if ( i < 10 ): print ( w , degree_centrality [ w ]) else : break 86 0.8282828282828284 62 0.7676767676767677 82 0.6464646464646465 96 0.6161616161616162 28 0.6060606060606061 21 0.5555555555555556 13 0.5353535353535354 23 0.5151515151515152 30 0.494949494949495 64 0.494949494949495 This network can be visualised with most size proportional to the importance according to degree centrality. plt . subplots ( figsize = ( 20 , 20 )) plt . title ( 'Email, EU-Core network: degree centrality' ) nx . draw_kamada_kawai ( G , with_labels = True , node_size = list ( np . array ( list ( degree_centrality . values ())) * 500 )) Betweenness centrality \u00b6 Although person 86 has sent mails to the maximum number of people, he has still sent mails to only 45 out of the 100 people. This means if we want to send a mail to the whole class, that there might be other important people thru whom the remaining people receive mails. We should identify people who are part of different closely knit groups and who are essential for passing information between groups. This is given by betweenness centrality. The most important people according to this metric in descending order are: btw_centrality = nx . algorithms . centrality . betweenness_centrality ( G ) for i , w in enumerate ( sorted ( btw_centrality . items (), key = lambda item : item [ 1 ], reverse = True )): if ( i < 10 ): print ( w ) else : break (86, 0.14149427475629858) (62, 0.12154539177634052) (96, 0.08160276331043798) (82, 0.0670387255089669) (64, 0.06502342115189583) (21, 0.05061277358840613) (13, 0.04673865990243992) (44, 0.04327552467861193) (5, 0.0412363399245883) (28, 0.03679924185307917) plt . subplots ( figsize = ( 20 , 20 )) plt . title ( 'Email, EU-Core network: betweenness centrality' ) nx . draw_kamada_kawai ( G , with_labels = True , node_size = list ( np . array ( list ( btw_centrality . values ())) * 5000 )) We get the same people at the top, nut this time inclusding a few additional people. Page rank (Eigenvector centrality) \u00b6 Another way to identify the important people in a network is to not only look at the number of people who are connected to the person but also the importance of the people whom they are connected to. The importance of the person can be defined as proportional to the sum of importance of the immediate people connected to him/her. This is defined by eigenvector centrality. Below is the top ten people who are important based on who they know and whom they are connected to: ev_centrality = nx . algorithms . centrality . eigenvector_centrality ( G ) for i , w in enumerate ( sorted ( ev_centrality . items (), key = lambda item : item [ 1 ], reverse = True )): if ( i < 10 ): print ( w ) else : break (28, 0.25779806388704485) (23, 0.2412215124270137) (30, 0.21579251773540695) (62, 0.2061936996975851) (29, 0.1962488726486241) (35, 0.19522994610327346) (86, 0.18934292971595598) (27, 0.1823911579409791) (96, 0.18188010730196982) (40, 0.1803640484660976) plt . subplots ( figsize = ( 20 , 20 )) plt . title ( 'Email, EU-Core network: eigenvector centrality' ) nx . draw_kamada_kawai ( G , with_labels = True , node_size = list ( np . array ( list ( ev_centrality . values ())) * 2500 )) Clustering coefficient \u00b6 Any graph in general can be densely connected or sparcely connected. The behaviour of the graph, and therefore the actions that have to be taken, are different for sparce graphs vs densely connected graphs. The betweenness measures above should be looked at along with clustering coefficient. The clustering coefficient is a way of measuring the degree to which the nodes in a graph cluster together. Networks with high number of clustering coefficient are more social. The average clustering coefficient for the graph is (assuming a undirected graph): G_undir = nx . from_pandas_edgelist ( df , source = 'Source' , target = 'Target' ) nx . algorithms . cluster . average_clustering ( G_undir ) 0.5130839320504162 From the above analysis, we can find the most important people in the network. The people who has the most connections, the people who are part of multiple groups and are important to send message across and the people who are important based on whom they know immediately. References \u00b6 https://towardsdatascience.com/visualize-high-dimensional-network-data-with-3d-360-degree-animated-scatter-plot-d583932d3693 Assignment and student notes, Business Analytics MSc, Imperial College London, Network Analytics module Hao Yin, Austin R. Benson, Jure Leskovec, and David F. Gleich. \"Local Higher-order Graph Clustering.\" In Proceedings of the 23 rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 2017 https://snap.stanford.edu/data/email-Eu-core.html Networks, Crowds, and Markets: Reasoning about a Highly Connected World by David Easley and Jon Kleinberg","title":"Network Centrality (Python)"},{"location":"Python/Network%20centrality/#centrality-measures","text":"Centrality measures help us understand who are the most important people in the network. For this example, \u201cemail-Eu-core network,\u201d from Stanford\u2019s SNAP is used. You may find the original dataset here: https://snap.stanford.edu/data/email-Eu-core.html .. This is a dataset of various email communications between people, and we are interested about the important people, leaders and opinion makers in this network. import pandas as pd import numpy as np import matplotlib.pyplot as plt import networkx as nx % matplotlib inline For the purposes of this blog, I am filtering the data only for the first 100 people for brevity. Same analysis can be done across the complete dataset. df = pd . read_csv ( \"https://raw.githubusercontent.com/jinhangjiang/Datasets/main/Network%20Data/emailEUcore/email-Eu-core.txt\" , delimiter = \" \" , names = [ \"Source\" , \"Target\" ]) df = df . loc [( df . Source < 100 ) & ( df . Target < 100 )] df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Source Target 0 0 1 1 2 3 2 2 4 3 5 6 4 5 7 We are using a directed graph as we are interested emails being sent and received. G = nx . from_pandas_edgelist ( df , source = 'Source' , target = 'Target' , create_using = nx . DiGraph ()) plt . subplots ( figsize = ( 20 , 20 )) plt . title ( 'Email, EU-Core network' ) nx . draw_kamada_kawai ( G , with_labels = True ) We can see that there are two densily connected networks and many connections between them. Who is the most important person in the network? This is based on what is the question you are asking. Is it the person who can send the emails the fastest? Is it the person why can connect with maximum number of people? Some basic features would be: 1. Just by the number of mails sent/received, who has sent the maximum number of mails 2. By the number of people that a person has a immediate relation with (a relation is defined as sending an email to that person): degree centrality 3. We can also look at the persons who are essential for communication between different closely knit groups. (betweenness centrality) The person with the maximum number of interactions is: student_sent_mails = list ( df . groupby ( 'Source' )[ 'Target' ] . count ()) student_sent_mails . index ( max ( student_sent_mails )) + 1 86 We can say that the person with index 86 has the most number of interactions and is one of the most important person. He would be the go to person who has connections with maximum number of people. We can see this in the graph also.","title":"Centrality measures"},{"location":"Python/Network%20centrality/#degree-centrality","text":"This way of looking at the importance of a person based on number of connections (Degrees) is called degree centrality. In ascending order, the top 10 people with maximum number of connections are: degree_centrality = nx . algorithms . centrality . degree_centrality ( G ) for i , w in enumerate ( sorted ( degree_centrality , key = degree_centrality . get , reverse = True )): if ( i < 10 ): print ( w , degree_centrality [ w ]) else : break 86 0.8282828282828284 62 0.7676767676767677 82 0.6464646464646465 96 0.6161616161616162 28 0.6060606060606061 21 0.5555555555555556 13 0.5353535353535354 23 0.5151515151515152 30 0.494949494949495 64 0.494949494949495 This network can be visualised with most size proportional to the importance according to degree centrality. plt . subplots ( figsize = ( 20 , 20 )) plt . title ( 'Email, EU-Core network: degree centrality' ) nx . draw_kamada_kawai ( G , with_labels = True , node_size = list ( np . array ( list ( degree_centrality . values ())) * 500 ))","title":"Degree centrality"},{"location":"Python/Network%20centrality/#betweenness-centrality","text":"Although person 86 has sent mails to the maximum number of people, he has still sent mails to only 45 out of the 100 people. This means if we want to send a mail to the whole class, that there might be other important people thru whom the remaining people receive mails. We should identify people who are part of different closely knit groups and who are essential for passing information between groups. This is given by betweenness centrality. The most important people according to this metric in descending order are: btw_centrality = nx . algorithms . centrality . betweenness_centrality ( G ) for i , w in enumerate ( sorted ( btw_centrality . items (), key = lambda item : item [ 1 ], reverse = True )): if ( i < 10 ): print ( w ) else : break (86, 0.14149427475629858) (62, 0.12154539177634052) (96, 0.08160276331043798) (82, 0.0670387255089669) (64, 0.06502342115189583) (21, 0.05061277358840613) (13, 0.04673865990243992) (44, 0.04327552467861193) (5, 0.0412363399245883) (28, 0.03679924185307917) plt . subplots ( figsize = ( 20 , 20 )) plt . title ( 'Email, EU-Core network: betweenness centrality' ) nx . draw_kamada_kawai ( G , with_labels = True , node_size = list ( np . array ( list ( btw_centrality . values ())) * 5000 )) We get the same people at the top, nut this time inclusding a few additional people.","title":"Betweenness centrality"},{"location":"Python/Network%20centrality/#page-rank-eigenvector-centrality","text":"Another way to identify the important people in a network is to not only look at the number of people who are connected to the person but also the importance of the people whom they are connected to. The importance of the person can be defined as proportional to the sum of importance of the immediate people connected to him/her. This is defined by eigenvector centrality. Below is the top ten people who are important based on who they know and whom they are connected to: ev_centrality = nx . algorithms . centrality . eigenvector_centrality ( G ) for i , w in enumerate ( sorted ( ev_centrality . items (), key = lambda item : item [ 1 ], reverse = True )): if ( i < 10 ): print ( w ) else : break (28, 0.25779806388704485) (23, 0.2412215124270137) (30, 0.21579251773540695) (62, 0.2061936996975851) (29, 0.1962488726486241) (35, 0.19522994610327346) (86, 0.18934292971595598) (27, 0.1823911579409791) (96, 0.18188010730196982) (40, 0.1803640484660976) plt . subplots ( figsize = ( 20 , 20 )) plt . title ( 'Email, EU-Core network: eigenvector centrality' ) nx . draw_kamada_kawai ( G , with_labels = True , node_size = list ( np . array ( list ( ev_centrality . values ())) * 2500 ))","title":"Page rank (Eigenvector centrality)"},{"location":"Python/Network%20centrality/#clustering-coefficient","text":"Any graph in general can be densely connected or sparcely connected. The behaviour of the graph, and therefore the actions that have to be taken, are different for sparce graphs vs densely connected graphs. The betweenness measures above should be looked at along with clustering coefficient. The clustering coefficient is a way of measuring the degree to which the nodes in a graph cluster together. Networks with high number of clustering coefficient are more social. The average clustering coefficient for the graph is (assuming a undirected graph): G_undir = nx . from_pandas_edgelist ( df , source = 'Source' , target = 'Target' ) nx . algorithms . cluster . average_clustering ( G_undir ) 0.5130839320504162 From the above analysis, we can find the most important people in the network. The people who has the most connections, the people who are part of multiple groups and are important to send message across and the people who are important based on whom they know immediately.","title":"Clustering coefficient"},{"location":"Python/Network%20centrality/#references","text":"https://towardsdatascience.com/visualize-high-dimensional-network-data-with-3d-360-degree-animated-scatter-plot-d583932d3693 Assignment and student notes, Business Analytics MSc, Imperial College London, Network Analytics module Hao Yin, Austin R. Benson, Jure Leskovec, and David F. Gleich. \"Local Higher-order Graph Clustering.\" In Proceedings of the 23 rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 2017 https://snap.stanford.edu/data/email-Eu-core.html Networks, Crowds, and Markets: Reasoning about a Highly Connected World by David Easley and Jon Kleinberg","title":"References"},{"location":"Python/Shortest%20path%20problems/","text":"MathJax.Hub.Config({ extensions: [\"tex2jax.js\",\"TeX/AMSmath.js\",\"TeX/AMSsymbols.js\"], jax: [\"input/TeX\", \"output/HTML-CSS\"], tex2jax: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ], }, \"HTML-CSS\": { availableFonts: [\"TeX\"] } }); Shortest path problems \u00b6 Shortest path is one problem in networks which appears in many forms across many industries. It tells the user how to find the shortest path between two pair of nodes. In this particular example, we will look at finding the shortest path between a pair of nodes in a directed network using an integer programming solver. import networkx as nx import numpy as np import pandas as pd import matplotlib.pyplot as plt Consider the following simple weighted directed network, with four nodes and five edges. edgelist_df = pd . DataFrame ({ 'node1' :[ 's' , 's' , 'u' , 'u' , 'v' ], 'node2' :[ 'u' , 'v' , 'v' , 't' , 't' ], 'weights' :[ 3 , 2 , 0.7 , 1 , 7 ] }) edgelist_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } node1 node2 weights 0 s u 3.0 1 s v 2.0 2 u v 0.7 3 u t 1.0 4 v t 7.0 g = nx . DiGraph () for i , elrow in edgelist_df . iterrows (): g . add_edge ( elrow [ 0 ], elrow [ 1 ], weight = elrow [ 2 ]) g . edges ( data = True ) OutEdgeDataView([('s', 'u', {'weight': 3.0}), ('s', 'v', {'weight': 2.0}), ('u', 'v', {'weight': 0.7}), ('u', 't', {'weight': 1.0}), ('v', 't', {'weight': 7.0})]) # for each node we are trying to fix the coordinates g . add_node ( 's' , pos = ( 0 , 1 )) g . add_node ( 'u' , pos = ( 1 , 2 )) g . add_node ( 'v' , pos = ( 1 , 0 )) g . add_node ( 't' , pos = ( 2 , 1 )) g . nodes ( data = True ) NodeDataView({'s': {'pos': (0, 1)}, 'u': {'pos': (1, 2)}, 'v': {'pos': (1, 0)}, 't': {'pos': (2, 1)}}) # This function gets the coordinates for nodes pos = nx . get_node_attributes ( g , 'pos' ) # This function gets the weights for the edges weight = nx . get_edge_attributes ( g , 'weight' ) nx . draw ( g , pos , with_labels = True ) nx . draw_networkx_edge_labels ( g , pos , edge_labels = weight ) plt . show () Now, we want to find the shortest path between the node 's' and 't'. In this network, edge-weights represent the costs for each edge. The weights could be distance, or time. NetworkX has an inbuilt function shortest_path which returs the shortest path. Using networkX, we get the following shortest path: path_2_0 = nx . shortest_path ( g , source = 's' , target = 't' ) path_2_0 ['s', 'u', 't'] Formulating the problem using integer programming \u00b6 We have n nodes V and m edges E (n=4, m=5 for this example). OR tools is an open source software built by Google for solving integer programming problems. Cp-Sat solver is one such model by OR Tools, which we are going to use today. We can use binary decision variables \\(edge_{i,j}\\) representing the edge that goes from node i to node j. If \\(edge_{i,j}=1\\) the shortest path belongs to the path between i and j, 0 otherwise. from ortools.sat.python import cp_model shortest_path_model = cp_model . CpModel () # Creating one integer decision variable for each edge edge_bool_vars = {} for edge in g . edges : edge_bool_vars [ edge [ 0 ], edge [ 1 ]] = shortest_path_model . NewBoolVar ( 'edge_ %s _ %s ' % edge ) print ( 'Creating the boolean variable ' , edge_bool_vars [ edge [ 0 ], edge [ 1 ]], 'representing the if we should travel through ' , ( edge [ 0 ], edge [ 1 ])) Creating the boolean variable edge_s_u representing the if we should travel through ('s', 'u') Creating the boolean variable edge_s_v representing the if we should travel through ('s', 'v') Creating the boolean variable edge_u_v representing the if we should travel through ('u', 'v') Creating the boolean variable edge_u_t representing the if we should travel through ('u', 't') Creating the boolean variable edge_v_t representing the if we should travel through ('v', 't') The shortest path (in isolation) will have the following properties: 1. Starting node has a degree -1 2. Ending node has a degree +1 3. All intermediatory nodes have degree 0 This can be written in the form of flow balance constraints as follows: input_node = 's' output_node = 't' # Adding constraints on the nodes for node in g . nodes : in_edges = g . in_edges ( node ) out_edges = g . out_edges ( node ) print ( 'Adding the constraint on node ' , node ) print ( 'This node has %i in-edges and %i out-edges' % ( len ( in_edges ), len ( out_edges ))) equation_at_this_edge = sum ( edge_bool_vars [ edge [ 0 ], edge [ 1 ]] for edge in in_edges ) - \\ sum ( edge_bool_vars [ edge [ 0 ], edge [ 1 ]] for edge in out_edges ) if ( node == input_node ): shortest_path_model . Add ( equation_at_this_edge == - 1 ) print ( equation_at_this_edge == - 1 ) elif ( node == output_node ): shortest_path_model . Add ( equation_at_this_edge == 1 ) print ( equation_at_this_edge == 1 ) else : shortest_path_model . Add ( equation_at_this_edge == 0 ) print ( equation_at_this_edge , '== 0' ) print ( '' ) Adding the constraint on node s This node has 0 in-edges and 2 out-edges (-((edge_s_u) + edge_s_v)) == -1 Adding the constraint on node u This node has 1 in-edges and 2 out-edges ((edge_s_u) + -((edge_u_v) + edge_u_t)) == 0 Adding the constraint on node v This node has 2 in-edges and 1 out-edges (((edge_s_v) + edge_u_v) + -(edge_v_t)) == 0 Adding the constraint on node t This node has 2 in-edges and 0 out-edges (((edge_u_t) + edge_v_t)) == 1 The objective of the shortest path problem is to find the path with the minimum cost. This can be written as minimising the costs as follows: # factor to make everything including costs integer factor_to_int = 10 # The objective is to maximise flow total_cost = sum ( int ( g . get_edge_data ( * edge )[ 'weight' ] * factor_to_int ) * edge_bool_vars [ edge [ 0 ], edge [ 1 ]] for edge in g . edges ) print ( 'Objective is to optimise cost' ) print ( total_cost ) shortest_path_model . Minimize ( total_cost ) Objective is to optimise cost ((((((30 * edge_s_u)) + (20 * edge_s_v)) + (7 * edge_u_v)) + (10 * edge_u_t)) + (70 * edge_v_t)) Solving the problem, we have an optimal solution with the overall cost as 40 units. # Solving the problem solver = cp_model . CpSolver () solution_printer = cp_model . ObjectiveSolutionPrinter () status = solver . SolveWithSolutionCallback ( shortest_path_model , solution_printer ) Solution 0, time = 0.02 s, objective = 40 cp_model . OPTIMAL == status True The solution is given as result_edges = {} for edge in g . edges : result_edges [ edge [ 0 ], edge [ 1 ]] = solver . Value ( edge_bool_vars [ edge [ 0 ], edge [ 1 ]]) result_edges {('s', 'u'): 1, ('s', 'v'): 0, ('u', 'v'): 0, ('u', 't'): 1, ('v', 't'): 0} Plotting the network in such a way that the green lines represent the shortest path, we get pos = nx . get_node_attributes ( g , 'pos' ) color = [ 'g' if val == 1 else 'r' for val in result_edges . values ()] nx . draw ( g , pos , with_labels = True , edge_color = color ) nx . draw_networkx_edge_labels ( g , pos , edge_labels = result_edges ) plt . show ()","title":"Shortest path using integer programming (Python)"},{"location":"Python/Shortest%20path%20problems/#shortest-path-problems","text":"Shortest path is one problem in networks which appears in many forms across many industries. It tells the user how to find the shortest path between two pair of nodes. In this particular example, we will look at finding the shortest path between a pair of nodes in a directed network using an integer programming solver. import networkx as nx import numpy as np import pandas as pd import matplotlib.pyplot as plt Consider the following simple weighted directed network, with four nodes and five edges. edgelist_df = pd . DataFrame ({ 'node1' :[ 's' , 's' , 'u' , 'u' , 'v' ], 'node2' :[ 'u' , 'v' , 'v' , 't' , 't' ], 'weights' :[ 3 , 2 , 0.7 , 1 , 7 ] }) edgelist_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } node1 node2 weights 0 s u 3.0 1 s v 2.0 2 u v 0.7 3 u t 1.0 4 v t 7.0 g = nx . DiGraph () for i , elrow in edgelist_df . iterrows (): g . add_edge ( elrow [ 0 ], elrow [ 1 ], weight = elrow [ 2 ]) g . edges ( data = True ) OutEdgeDataView([('s', 'u', {'weight': 3.0}), ('s', 'v', {'weight': 2.0}), ('u', 'v', {'weight': 0.7}), ('u', 't', {'weight': 1.0}), ('v', 't', {'weight': 7.0})]) # for each node we are trying to fix the coordinates g . add_node ( 's' , pos = ( 0 , 1 )) g . add_node ( 'u' , pos = ( 1 , 2 )) g . add_node ( 'v' , pos = ( 1 , 0 )) g . add_node ( 't' , pos = ( 2 , 1 )) g . nodes ( data = True ) NodeDataView({'s': {'pos': (0, 1)}, 'u': {'pos': (1, 2)}, 'v': {'pos': (1, 0)}, 't': {'pos': (2, 1)}}) # This function gets the coordinates for nodes pos = nx . get_node_attributes ( g , 'pos' ) # This function gets the weights for the edges weight = nx . get_edge_attributes ( g , 'weight' ) nx . draw ( g , pos , with_labels = True ) nx . draw_networkx_edge_labels ( g , pos , edge_labels = weight ) plt . show () Now, we want to find the shortest path between the node 's' and 't'. In this network, edge-weights represent the costs for each edge. The weights could be distance, or time. NetworkX has an inbuilt function shortest_path which returs the shortest path. Using networkX, we get the following shortest path: path_2_0 = nx . shortest_path ( g , source = 's' , target = 't' ) path_2_0 ['s', 'u', 't']","title":"Shortest path problems"},{"location":"Python/Shortest%20path%20problems/#formulating-the-problem-using-integer-programming","text":"We have n nodes V and m edges E (n=4, m=5 for this example). OR tools is an open source software built by Google for solving integer programming problems. Cp-Sat solver is one such model by OR Tools, which we are going to use today. We can use binary decision variables \\(edge_{i,j}\\) representing the edge that goes from node i to node j. If \\(edge_{i,j}=1\\) the shortest path belongs to the path between i and j, 0 otherwise. from ortools.sat.python import cp_model shortest_path_model = cp_model . CpModel () # Creating one integer decision variable for each edge edge_bool_vars = {} for edge in g . edges : edge_bool_vars [ edge [ 0 ], edge [ 1 ]] = shortest_path_model . NewBoolVar ( 'edge_ %s _ %s ' % edge ) print ( 'Creating the boolean variable ' , edge_bool_vars [ edge [ 0 ], edge [ 1 ]], 'representing the if we should travel through ' , ( edge [ 0 ], edge [ 1 ])) Creating the boolean variable edge_s_u representing the if we should travel through ('s', 'u') Creating the boolean variable edge_s_v representing the if we should travel through ('s', 'v') Creating the boolean variable edge_u_v representing the if we should travel through ('u', 'v') Creating the boolean variable edge_u_t representing the if we should travel through ('u', 't') Creating the boolean variable edge_v_t representing the if we should travel through ('v', 't') The shortest path (in isolation) will have the following properties: 1. Starting node has a degree -1 2. Ending node has a degree +1 3. All intermediatory nodes have degree 0 This can be written in the form of flow balance constraints as follows: input_node = 's' output_node = 't' # Adding constraints on the nodes for node in g . nodes : in_edges = g . in_edges ( node ) out_edges = g . out_edges ( node ) print ( 'Adding the constraint on node ' , node ) print ( 'This node has %i in-edges and %i out-edges' % ( len ( in_edges ), len ( out_edges ))) equation_at_this_edge = sum ( edge_bool_vars [ edge [ 0 ], edge [ 1 ]] for edge in in_edges ) - \\ sum ( edge_bool_vars [ edge [ 0 ], edge [ 1 ]] for edge in out_edges ) if ( node == input_node ): shortest_path_model . Add ( equation_at_this_edge == - 1 ) print ( equation_at_this_edge == - 1 ) elif ( node == output_node ): shortest_path_model . Add ( equation_at_this_edge == 1 ) print ( equation_at_this_edge == 1 ) else : shortest_path_model . Add ( equation_at_this_edge == 0 ) print ( equation_at_this_edge , '== 0' ) print ( '' ) Adding the constraint on node s This node has 0 in-edges and 2 out-edges (-((edge_s_u) + edge_s_v)) == -1 Adding the constraint on node u This node has 1 in-edges and 2 out-edges ((edge_s_u) + -((edge_u_v) + edge_u_t)) == 0 Adding the constraint on node v This node has 2 in-edges and 1 out-edges (((edge_s_v) + edge_u_v) + -(edge_v_t)) == 0 Adding the constraint on node t This node has 2 in-edges and 0 out-edges (((edge_u_t) + edge_v_t)) == 1 The objective of the shortest path problem is to find the path with the minimum cost. This can be written as minimising the costs as follows: # factor to make everything including costs integer factor_to_int = 10 # The objective is to maximise flow total_cost = sum ( int ( g . get_edge_data ( * edge )[ 'weight' ] * factor_to_int ) * edge_bool_vars [ edge [ 0 ], edge [ 1 ]] for edge in g . edges ) print ( 'Objective is to optimise cost' ) print ( total_cost ) shortest_path_model . Minimize ( total_cost ) Objective is to optimise cost ((((((30 * edge_s_u)) + (20 * edge_s_v)) + (7 * edge_u_v)) + (10 * edge_u_t)) + (70 * edge_v_t)) Solving the problem, we have an optimal solution with the overall cost as 40 units. # Solving the problem solver = cp_model . CpSolver () solution_printer = cp_model . ObjectiveSolutionPrinter () status = solver . SolveWithSolutionCallback ( shortest_path_model , solution_printer ) Solution 0, time = 0.02 s, objective = 40 cp_model . OPTIMAL == status True The solution is given as result_edges = {} for edge in g . edges : result_edges [ edge [ 0 ], edge [ 1 ]] = solver . Value ( edge_bool_vars [ edge [ 0 ], edge [ 1 ]]) result_edges {('s', 'u'): 1, ('s', 'v'): 0, ('u', 'v'): 0, ('u', 't'): 1, ('v', 't'): 0} Plotting the network in such a way that the green lines represent the shortest path, we get pos = nx . get_node_attributes ( g , 'pos' ) color = [ 'g' if val == 1 else 'r' for val in result_edges . values ()] nx . draw ( g , pos , with_labels = True , edge_color = color ) nx . draw_networkx_edge_labels ( g , pos , edge_labels = result_edges ) plt . show ()","title":"Formulating the problem using integer programming"},{"location":"R/Univariate-analysis/","text":"Introduction \u00b6 A univariate analysis is the simplest form of EDA. \"Uni\" means \"one\", so in other words, this analysis is on only one variable. It doesn't deal with causes or relationships, and its primary purpose is to describe; it takes data, summarizes that data and finds patterns in it. In describing or characterizing the observations of an individual variable, three basic properties are of interest: 1. The location of observations, or how large or small the values of the individual observations are 2. The dispersion (sometimes called scale or spread) of the observations 3. The distribution of the observations Uni-variate plots provide one way to find out about those properties. There are two basic kinds of univariate plots: 1. Enumeration plots, or plots that show every observation 2. Summary plots that generalize the data into a simplified representation. For the current tutorial, I will be using my office attendance data set. The data set contains the time when I swiped into the office and when I swiped out of the office. Data from 4 th October 2017 to 29 th November 2018. After some data set manipulation, I will get the difference between policy out-time and my actual out-time. I can leave 15 minutes before the policy out time. After manipulation, a sample of the data is as follows: (Actual data is not shown for security reasons. This is mock data that is very similar to the actual one.) ## Attendance.Date diff.in.time diff.out.time ## 1 2018-03-22 18 mins 226 mins ## 2 2018-08-14 -9 mins 5 mins ## 3 2017-12-04 42 mins 11 mins ## 4 2018-03-01 26 mins -6 mins ## 5 2018-01-23 35 mins -4 mins Summary Statistics \u00b6 Before further analysis, some basic summary statistics would show me the mean and standard deviation of the data. For this tutorial, I will use diff.in.time (difference between actual in-time and policy in-time) mean ( as.numeric ( attendance $ diff.out.time )) # Mean in minutes ## [1] 20.3227 sd ( as.numeric ( attendance $ diff.out.time )) # Standard Deviation in minutes ## [1] 69.06549 nrow ( attendance ) # Length of the data set ## [1] 282 Enumerative plots \u00b6 \"Enumerative plots\" are called such because they enumerate or show every individual data point. Index Plot/Univariate Scatter Diagram \u00b6 Displays the values of a single variable for each observation using symbols plotted relative to the observation number. ggplot ( attendance , aes ( x = Attendance.Date , y = as.numeric ( diff.out.time ), color = ( diff.out.time >= -15 ))) + geom_point ( show.legend = FALSE ) + labs ( x = 'Time' , y = 'Out-time difference (Minutes)' ) + theme_minimal () Just looking at this plot, I can say the following: 1. I could cluster into three parts. - One cluster would be before December 2017, where I used to leave the office way after my out-time. - The second cluster would be from December 2017 to June 2018, where I used to leave the office 15 minutes before my out time. - The third cluster was after June 2018, when I was leaving way after my out-time. 2. The red dots indicate the days when I came to the office after 15 minutes from in-time. They are anomalies, days when I took half days etc. We can exclude them from our current analysis. Y Zero High-Density Plot \u00b6 Another way to look at the same data is by using a Y Zero High-Density Plot. It displays the values of a single variable plotted as thin vertical lines. Here the magnitude of the observations iss highlighted. ggplot ( attendance , aes ( x = Attendance.Date , y = 0 , color = ( diff.out.time >= -15 ), xend = Attendance.Date , yend = as.numeric ( diff.out.time ))) + geom_segment ( show.legend = FALSE ) + labs ( x = 'Time' , y = 'Out-time difference (Minutes)' ) + theme_minimal () Removing half-days as outliers attendance <- attendance %>% filter ( diff.out.time >= -15 ) Strip Plot/Strip Chart (univariate scatter diagram) \u00b6 Displays the values of a single variable as symbols plotted along a line. This is a basic plot where we can see the spread of the data. stripchart ( x = as.numeric ( attendance $ diff.out.time ), xlab = 'Out-time difference (minutes)' ) Sometimes it is more visually apparent when the points are stacked. ggplot ( attendance , aes ( x = as.numeric ( diff.out.time ), y = ..density.. )) + geom_dotplot ( binwidth = 3 , method = 'histodot' ) + labs ( x = 'Out-time difference (minutes)' ) + theme_minimal () We can observe that the number of observations is high and slowly tends to drop as time progresses. Dot Plot/Dot Chart \u00b6 Displays the values of a single variable as symbols plotted along a line. With a separate line for each observation, it is generally constructed after sorting the rows of the data table. df = attendance %>% arrange ( as.numeric ( diff.out.time )) ggplot ( df , aes ( x = as.numeric ( row.names ( df )), y = as.numeric ( diff.out.time ))) + geom_point () + labs ( x = 'count' , y = 'Out time difference (min)' ) + theme_minimal () From the graph, I can observe that the distribution initially seems to be exponential. A sample normal distribution is plotted for reference. We can see that the distribution looks nowhere like a normal distribution. Instead, I suspect that it is close to an exponential distribution. Univariate Summary Plots \u00b6 Summary plots display an object or graph that gives a more concise expression of a variable's location, dispersion, and distribution than an enumerative plot. This comes at the expense of some loss of information: In a summary plot, it is no longer possible to retrieve the individual data value, but the gain usually matches this loss in understanding that results from the efficient representation of the data. Summary plots generally prove to be much better than the enumerative plots in revealing the data distribution. Box plot \u00b6 A simple way of representing statistical data on a plot in which a rectangle is drawn to represent the second and third quartiles, usually with a vertical line inside to indicate the median value. The lower and upper quartiles are shown as horizontal lines on either side of the rectangle. ggplot ( attendance , aes ( x = \"Box Plot\" , y = as.numeric ( diff.out.time ), group = 123 )) + geom_boxplot () + labs ( y = 'Out time difference (min)' ) + theme_minimal () Histograms \u00b6 The other summary plots are of various types: Histograms: Histograms are bar charts that display the counts or relative frequencies of values falling in different class intervals or ranges. Density Plots: A density plot plots the local relative frequency or density of points along the number line or x-axis of a plot. The local density is determined by summing the individual \"kernel\" densities for each point. Where points occur more frequently, this sum, and consequently the local density, will be greater. legendcols <- c ( \"Normal distribution\" = \"darkred\" , \"Density\" = \"darkBlue\" , \"Histogram\" = \"lightBlue\" ) ggplot ( attendance , aes ( x = as.numeric ( diff.out.time ))) + geom_histogram ( aes ( y = ..density.. , fill = \"Histogram\" ), bins = 50 ) + stat_function ( fun = dnorm , aes ( color = \"Normal distribution\" ), size = 1 , args = list ( mean = mean ( as.numeric ( attendance $ diff.out.time )), sd = sd ( as.numeric ( attendance $ diff.out.time )) )) + geom_density ( aes ( y = ..density.. , color = \"Density\" ), size = 1 ) + scale_colour_manual ( name = \"Distribution\" , values = legendcols ) + scale_fill_manual ( name = \"Bar\" , values = legendcols ) + labs ( x = 'Out-time difference (minutes)' , y = 'Density' ) + theme_minimal () + theme ( legend.position = \"bottom\" ) In the above graph, the red line is a normal distribution(with the same mean and standard deviation) while the blue line is the density plot of in-time. Q-Q plot \u00b6 In statistics, a Q-Q (quantile-quantile) plot is a probability plot, a graphical method for comparing two probability distributions by plotting their quantiles against each other. If the two distributions being compared are similar, the points in the Q-Q plot will approximately lie on the line y = x. If the distributions are linearly related, the points in the Q-Q plot will approximately lie on a line, but not necessarily on the line y = x. Q-Q plots can also be used as a graphical means of estimating parameters in a location-scale family of distributions. A Q-Q plot is used to compare the shapes of distributions, providing a graphical view of how properties such as location, scale, and skewness are similar or different in the two distributions. Below is a Q-Q plot with a normal distribution ggplot ( attendance , aes ( sample = as.numeric ( diff.out.time ))) + stat_qq () + stat_qq_line () + ggtitle ( \"Normal distribution Q-Q plot\" ) + theme_minimal () We can see that the distribution is not normal. Trying to check with exponential distribution params <- as.list ( fitdistr ( rexp ( nrow ( attendance ), rate = 2 ), \"exponential\" ) $ estimate ) ggplot ( attendance , aes ( sample = as.numeric ( diff.out.time ))) + stat_qq ( distribution = qexp , dparams = params ) + stat_qq_line ( distribution = qexp , dparams = params ) + ggtitle ( \"Exponential distribution Q-Q plot\" ) + theme_minimal () From the above graph, I am approximating my distribution to an exponential distribution. lamda <- 1 / mean ( sd ( as.numeric ( attendance $ diff.out.time )), mean ( as.numeric ( attendance $ diff.out.time ))) exp.curve <- function ( x ){ lamda * exp ( - lamda * ( x +15 )) } legendcols <- c ( \"Exponential distribution\" = \"darkred\" , \"Density\" = \"darkBlue\" , \"Histogram\" = \"lightBlue\" ) ggplot ( attendance , aes ( x = as.numeric ( diff.out.time ))) + geom_histogram ( aes ( y = ..density.. , fill = \"Histogram\" ), bins = 50 ) + stat_function ( fun = exp.curve , aes ( color = \"Exponential distribution\" ), size = 1 ) + geom_density ( aes ( y = ..density.. , color = \"Density\" ), size = 1 ) + scale_colour_manual ( name = \"Distribution\" , values = legendcols ) + scale_fill_manual ( name = \"Bar\" , values = legendcols ) + labs ( x = 'Out-time difference (minutes)' , y = 'Density' ) + theme_minimal () + theme ( legend.position = \"bottom\" ) In the above graph, the red line is exponential distribution, while the blue line is the density plot of in-time. Created using RMarkdown.","title":"Univariate Analysis (R)"},{"location":"R/Univariate-analysis/#introduction","text":"A univariate analysis is the simplest form of EDA. \"Uni\" means \"one\", so in other words, this analysis is on only one variable. It doesn't deal with causes or relationships, and its primary purpose is to describe; it takes data, summarizes that data and finds patterns in it. In describing or characterizing the observations of an individual variable, three basic properties are of interest: 1. The location of observations, or how large or small the values of the individual observations are 2. The dispersion (sometimes called scale or spread) of the observations 3. The distribution of the observations Uni-variate plots provide one way to find out about those properties. There are two basic kinds of univariate plots: 1. Enumeration plots, or plots that show every observation 2. Summary plots that generalize the data into a simplified representation. For the current tutorial, I will be using my office attendance data set. The data set contains the time when I swiped into the office and when I swiped out of the office. Data from 4 th October 2017 to 29 th November 2018. After some data set manipulation, I will get the difference between policy out-time and my actual out-time. I can leave 15 minutes before the policy out time. After manipulation, a sample of the data is as follows: (Actual data is not shown for security reasons. This is mock data that is very similar to the actual one.) ## Attendance.Date diff.in.time diff.out.time ## 1 2018-03-22 18 mins 226 mins ## 2 2018-08-14 -9 mins 5 mins ## 3 2017-12-04 42 mins 11 mins ## 4 2018-03-01 26 mins -6 mins ## 5 2018-01-23 35 mins -4 mins","title":"Introduction"},{"location":"R/Univariate-analysis/#summary-statistics","text":"Before further analysis, some basic summary statistics would show me the mean and standard deviation of the data. For this tutorial, I will use diff.in.time (difference between actual in-time and policy in-time) mean ( as.numeric ( attendance $ diff.out.time )) # Mean in minutes ## [1] 20.3227 sd ( as.numeric ( attendance $ diff.out.time )) # Standard Deviation in minutes ## [1] 69.06549 nrow ( attendance ) # Length of the data set ## [1] 282","title":"Summary Statistics"},{"location":"R/Univariate-analysis/#enumerative-plots","text":"\"Enumerative plots\" are called such because they enumerate or show every individual data point.","title":"Enumerative plots"},{"location":"R/Univariate-analysis/#index-plotunivariate-scatter-diagram","text":"Displays the values of a single variable for each observation using symbols plotted relative to the observation number. ggplot ( attendance , aes ( x = Attendance.Date , y = as.numeric ( diff.out.time ), color = ( diff.out.time >= -15 ))) + geom_point ( show.legend = FALSE ) + labs ( x = 'Time' , y = 'Out-time difference (Minutes)' ) + theme_minimal () Just looking at this plot, I can say the following: 1. I could cluster into three parts. - One cluster would be before December 2017, where I used to leave the office way after my out-time. - The second cluster would be from December 2017 to June 2018, where I used to leave the office 15 minutes before my out time. - The third cluster was after June 2018, when I was leaving way after my out-time. 2. The red dots indicate the days when I came to the office after 15 minutes from in-time. They are anomalies, days when I took half days etc. We can exclude them from our current analysis.","title":"Index Plot/Univariate Scatter Diagram"},{"location":"R/Univariate-analysis/#y-zero-high-density-plot","text":"Another way to look at the same data is by using a Y Zero High-Density Plot. It displays the values of a single variable plotted as thin vertical lines. Here the magnitude of the observations iss highlighted. ggplot ( attendance , aes ( x = Attendance.Date , y = 0 , color = ( diff.out.time >= -15 ), xend = Attendance.Date , yend = as.numeric ( diff.out.time ))) + geom_segment ( show.legend = FALSE ) + labs ( x = 'Time' , y = 'Out-time difference (Minutes)' ) + theme_minimal () Removing half-days as outliers attendance <- attendance %>% filter ( diff.out.time >= -15 )","title":"Y Zero High-Density Plot"},{"location":"R/Univariate-analysis/#strip-plotstrip-chart-univariate-scatter-diagram","text":"Displays the values of a single variable as symbols plotted along a line. This is a basic plot where we can see the spread of the data. stripchart ( x = as.numeric ( attendance $ diff.out.time ), xlab = 'Out-time difference (minutes)' ) Sometimes it is more visually apparent when the points are stacked. ggplot ( attendance , aes ( x = as.numeric ( diff.out.time ), y = ..density.. )) + geom_dotplot ( binwidth = 3 , method = 'histodot' ) + labs ( x = 'Out-time difference (minutes)' ) + theme_minimal () We can observe that the number of observations is high and slowly tends to drop as time progresses.","title":"Strip Plot/Strip Chart (univariate scatter diagram)"},{"location":"R/Univariate-analysis/#dot-plotdot-chart","text":"Displays the values of a single variable as symbols plotted along a line. With a separate line for each observation, it is generally constructed after sorting the rows of the data table. df = attendance %>% arrange ( as.numeric ( diff.out.time )) ggplot ( df , aes ( x = as.numeric ( row.names ( df )), y = as.numeric ( diff.out.time ))) + geom_point () + labs ( x = 'count' , y = 'Out time difference (min)' ) + theme_minimal () From the graph, I can observe that the distribution initially seems to be exponential. A sample normal distribution is plotted for reference. We can see that the distribution looks nowhere like a normal distribution. Instead, I suspect that it is close to an exponential distribution.","title":"Dot Plot/Dot Chart"},{"location":"R/Univariate-analysis/#univariate-summary-plots","text":"Summary plots display an object or graph that gives a more concise expression of a variable's location, dispersion, and distribution than an enumerative plot. This comes at the expense of some loss of information: In a summary plot, it is no longer possible to retrieve the individual data value, but the gain usually matches this loss in understanding that results from the efficient representation of the data. Summary plots generally prove to be much better than the enumerative plots in revealing the data distribution.","title":"Univariate Summary Plots"},{"location":"R/Univariate-analysis/#box-plot","text":"A simple way of representing statistical data on a plot in which a rectangle is drawn to represent the second and third quartiles, usually with a vertical line inside to indicate the median value. The lower and upper quartiles are shown as horizontal lines on either side of the rectangle. ggplot ( attendance , aes ( x = \"Box Plot\" , y = as.numeric ( diff.out.time ), group = 123 )) + geom_boxplot () + labs ( y = 'Out time difference (min)' ) + theme_minimal ()","title":"Box plot"},{"location":"R/Univariate-analysis/#histograms","text":"The other summary plots are of various types: Histograms: Histograms are bar charts that display the counts or relative frequencies of values falling in different class intervals or ranges. Density Plots: A density plot plots the local relative frequency or density of points along the number line or x-axis of a plot. The local density is determined by summing the individual \"kernel\" densities for each point. Where points occur more frequently, this sum, and consequently the local density, will be greater. legendcols <- c ( \"Normal distribution\" = \"darkred\" , \"Density\" = \"darkBlue\" , \"Histogram\" = \"lightBlue\" ) ggplot ( attendance , aes ( x = as.numeric ( diff.out.time ))) + geom_histogram ( aes ( y = ..density.. , fill = \"Histogram\" ), bins = 50 ) + stat_function ( fun = dnorm , aes ( color = \"Normal distribution\" ), size = 1 , args = list ( mean = mean ( as.numeric ( attendance $ diff.out.time )), sd = sd ( as.numeric ( attendance $ diff.out.time )) )) + geom_density ( aes ( y = ..density.. , color = \"Density\" ), size = 1 ) + scale_colour_manual ( name = \"Distribution\" , values = legendcols ) + scale_fill_manual ( name = \"Bar\" , values = legendcols ) + labs ( x = 'Out-time difference (minutes)' , y = 'Density' ) + theme_minimal () + theme ( legend.position = \"bottom\" ) In the above graph, the red line is a normal distribution(with the same mean and standard deviation) while the blue line is the density plot of in-time.","title":"Histograms"},{"location":"R/Univariate-analysis/#q-q-plot","text":"In statistics, a Q-Q (quantile-quantile) plot is a probability plot, a graphical method for comparing two probability distributions by plotting their quantiles against each other. If the two distributions being compared are similar, the points in the Q-Q plot will approximately lie on the line y = x. If the distributions are linearly related, the points in the Q-Q plot will approximately lie on a line, but not necessarily on the line y = x. Q-Q plots can also be used as a graphical means of estimating parameters in a location-scale family of distributions. A Q-Q plot is used to compare the shapes of distributions, providing a graphical view of how properties such as location, scale, and skewness are similar or different in the two distributions. Below is a Q-Q plot with a normal distribution ggplot ( attendance , aes ( sample = as.numeric ( diff.out.time ))) + stat_qq () + stat_qq_line () + ggtitle ( \"Normal distribution Q-Q plot\" ) + theme_minimal () We can see that the distribution is not normal. Trying to check with exponential distribution params <- as.list ( fitdistr ( rexp ( nrow ( attendance ), rate = 2 ), \"exponential\" ) $ estimate ) ggplot ( attendance , aes ( sample = as.numeric ( diff.out.time ))) + stat_qq ( distribution = qexp , dparams = params ) + stat_qq_line ( distribution = qexp , dparams = params ) + ggtitle ( \"Exponential distribution Q-Q plot\" ) + theme_minimal () From the above graph, I am approximating my distribution to an exponential distribution. lamda <- 1 / mean ( sd ( as.numeric ( attendance $ diff.out.time )), mean ( as.numeric ( attendance $ diff.out.time ))) exp.curve <- function ( x ){ lamda * exp ( - lamda * ( x +15 )) } legendcols <- c ( \"Exponential distribution\" = \"darkred\" , \"Density\" = \"darkBlue\" , \"Histogram\" = \"lightBlue\" ) ggplot ( attendance , aes ( x = as.numeric ( diff.out.time ))) + geom_histogram ( aes ( y = ..density.. , fill = \"Histogram\" ), bins = 50 ) + stat_function ( fun = exp.curve , aes ( color = \"Exponential distribution\" ), size = 1 ) + geom_density ( aes ( y = ..density.. , color = \"Density\" ), size = 1 ) + scale_colour_manual ( name = \"Distribution\" , values = legendcols ) + scale_fill_manual ( name = \"Bar\" , values = legendcols ) + labs ( x = 'Out-time difference (minutes)' , y = 'Density' ) + theme_minimal () + theme ( legend.position = \"bottom\" ) In the above graph, the red line is exponential distribution, while the blue line is the density plot of in-time. Created using RMarkdown.","title":"Q-Q plot"},{"location":"R/multivariateAnalysis/","text":"Introduction \u00b6 Multivariate EDA techniques generally show the relationship between two or more variables with the depandant variable in the form of either cross-tabulation, statistics or visually. In the current problem it will help us look at relationships between our data. This blog is a part of in-time analysis problem . I want to analyse my entry time at office and understand what factors effect it. After integrating Google maps data with attendence dataset , I currently have the factors 1. date (month / week day / season etc) 2. main_activity (means of transport) 3. hours.worked (of the previous day) 4. travelling.time (time it took to travel from house to office) 5. home.addr (the place of residence) The dependent variable is diff.in.time (difference between my actual in time vs policy in-time) A sample of the data is shown Sample Data diff.in.time date main_activity hours.worked travelling.time home.addr diff.out.time -9 2018-08-14 IN_VEHICLE 8.933333 900.719 Old House 5 17 2018-03-16 ON_FOOT 9.116667 930.126 Old House -10 -14 2018-09-10 ON_FOOT 4.583333 1179.873 Old House -251 -7 2018-10-19 ON_BICYCLE 9.583333 1501.060 New House 42 -9 2018-06-28 IN_VEHICLE 9.783333 670.700 Old House 56 Cross-tabulation \u00b6 For categorical data cross-tabulation is very useful. For two variables, cross-tabulation is performed by making a two-way table with column headings that match the levels of one variable and row headings that match the levels of the other variable, then filling in the counts of all subjects that share a pair of levels. The two variables might be both explanatory, both outcome, or one of each. I am using Kable to make cool tables. cross_table <- travel %>% group_by ( home.addr , main_activity ) %>% summarise ( avg.travel.time = mean ( travelling.time ), avg.in.time.diff = mean ( diff.in.time ), median.in.time.diff = median ( diff.in.time )) %>% arrange ( home.addr , main_activity ) library ( kableExtra ) kable ( cross_table , caption = 'Cross Tabulation' ) %>% kable_styling ( full_width = F ) %>% column_spec ( 1 , bold = T ) %>% collapse_rows ( columns = 1 : 2 , valign = \"middle\" ) %>% scroll_box () Cross Tabulation home.addr main_activity avg.travel.time avg.in.time.diff median.in.time.diff New House IN_VEHICLE 1285.0264 -1.800000 -3 New House ON_BICYCLE 1547.5557 -4.000000 -6 New House ON_FOOT 1695.7091 5.285714 5 Old House IN_VEHICLE 771.1752 2.857143 -4 Old House ON_BICYCLE 1029.6329 14.941176 18 Old House ON_FOOT 1170.4783 17.433628 17 Scatter plots \u00b6 Scatter plots show how much one variable is affected by another. To see how travelling time affects in-time ggplot ( travel , aes ( x = diff.in.time , y = travelling.time , color = main_activity )) + geom_point ( show.legend = TRUE ) + labs ( x = 'In-time difference (Minutes)' , y = 'Travelling time (seconds)' , title = \"Travelling time vs in-time\" , color = 'Mode of transport' ) + theme_minimal () + theme ( legend.position = \"bottom\" ) From the above graph, I can see that: 1. For bicycle, as travelling time decreases(low traffic) in-time difference increases(coming earlier to office) 2. There seems to be no relationship between travelling time (traffic) and in-time difference when on foot. 3. Travelling time has little affect on it-time difference when travelling on vehicle. To see how hours worked(on previous day) affects in-time From the above graph, I can observe that irrespective of mode of transport, my in-time difference increases (coming earlier to office) as hours worked on the previous day increases. Box plots \u00b6 Similarly, I want to see how mode of transport affects in-time difference. For categorical variable, box plots display this information in the most ideal manner. ggplot ( travel , aes ( x = main_activity , y = diff.in.time , group = main_activity )) + geom_boxplot () + labs ( x = 'Mode of transport' , y = 'In time difference (min)' ) + theme_minimal () From the above graph, I can observe that: 1. On vehicle, I went to office on average, ~12 minutes after the policy in-time (in-time difference is -12) 2. On cycle, I went to office almost close to the policy in-time 3. While walking, I was almost always before the policy in-time Similarly for place of residence. From this graph, I can understand that from New house I was close to ~5 minutes after the policy in-time while I used to be on-time while living in Old house. Created using R Markdown. Credits: Thinkstats Experimental Design and Analysis","title":"Multivariate Analysis (R)"},{"location":"R/multivariateAnalysis/#introduction","text":"Multivariate EDA techniques generally show the relationship between two or more variables with the depandant variable in the form of either cross-tabulation, statistics or visually. In the current problem it will help us look at relationships between our data. This blog is a part of in-time analysis problem . I want to analyse my entry time at office and understand what factors effect it. After integrating Google maps data with attendence dataset , I currently have the factors 1. date (month / week day / season etc) 2. main_activity (means of transport) 3. hours.worked (of the previous day) 4. travelling.time (time it took to travel from house to office) 5. home.addr (the place of residence) The dependent variable is diff.in.time (difference between my actual in time vs policy in-time) A sample of the data is shown Sample Data diff.in.time date main_activity hours.worked travelling.time home.addr diff.out.time -9 2018-08-14 IN_VEHICLE 8.933333 900.719 Old House 5 17 2018-03-16 ON_FOOT 9.116667 930.126 Old House -10 -14 2018-09-10 ON_FOOT 4.583333 1179.873 Old House -251 -7 2018-10-19 ON_BICYCLE 9.583333 1501.060 New House 42 -9 2018-06-28 IN_VEHICLE 9.783333 670.700 Old House 56","title":"Introduction"},{"location":"R/multivariateAnalysis/#cross-tabulation","text":"For categorical data cross-tabulation is very useful. For two variables, cross-tabulation is performed by making a two-way table with column headings that match the levels of one variable and row headings that match the levels of the other variable, then filling in the counts of all subjects that share a pair of levels. The two variables might be both explanatory, both outcome, or one of each. I am using Kable to make cool tables. cross_table <- travel %>% group_by ( home.addr , main_activity ) %>% summarise ( avg.travel.time = mean ( travelling.time ), avg.in.time.diff = mean ( diff.in.time ), median.in.time.diff = median ( diff.in.time )) %>% arrange ( home.addr , main_activity ) library ( kableExtra ) kable ( cross_table , caption = 'Cross Tabulation' ) %>% kable_styling ( full_width = F ) %>% column_spec ( 1 , bold = T ) %>% collapse_rows ( columns = 1 : 2 , valign = \"middle\" ) %>% scroll_box () Cross Tabulation home.addr main_activity avg.travel.time avg.in.time.diff median.in.time.diff New House IN_VEHICLE 1285.0264 -1.800000 -3 New House ON_BICYCLE 1547.5557 -4.000000 -6 New House ON_FOOT 1695.7091 5.285714 5 Old House IN_VEHICLE 771.1752 2.857143 -4 Old House ON_BICYCLE 1029.6329 14.941176 18 Old House ON_FOOT 1170.4783 17.433628 17","title":"Cross-tabulation"},{"location":"R/multivariateAnalysis/#scatter-plots","text":"Scatter plots show how much one variable is affected by another. To see how travelling time affects in-time ggplot ( travel , aes ( x = diff.in.time , y = travelling.time , color = main_activity )) + geom_point ( show.legend = TRUE ) + labs ( x = 'In-time difference (Minutes)' , y = 'Travelling time (seconds)' , title = \"Travelling time vs in-time\" , color = 'Mode of transport' ) + theme_minimal () + theme ( legend.position = \"bottom\" ) From the above graph, I can see that: 1. For bicycle, as travelling time decreases(low traffic) in-time difference increases(coming earlier to office) 2. There seems to be no relationship between travelling time (traffic) and in-time difference when on foot. 3. Travelling time has little affect on it-time difference when travelling on vehicle. To see how hours worked(on previous day) affects in-time From the above graph, I can observe that irrespective of mode of transport, my in-time difference increases (coming earlier to office) as hours worked on the previous day increases.","title":"Scatter plots"},{"location":"R/multivariateAnalysis/#box-plots","text":"Similarly, I want to see how mode of transport affects in-time difference. For categorical variable, box plots display this information in the most ideal manner. ggplot ( travel , aes ( x = main_activity , y = diff.in.time , group = main_activity )) + geom_boxplot () + labs ( x = 'Mode of transport' , y = 'In time difference (min)' ) + theme_minimal () From the above graph, I can observe that: 1. On vehicle, I went to office on average, ~12 minutes after the policy in-time (in-time difference is -12) 2. On cycle, I went to office almost close to the policy in-time 3. While walking, I was almost always before the policy in-time Similarly for place of residence. From this graph, I can understand that from New house I was close to ~5 minutes after the policy in-time while I used to be on-time while living in Old house. Created using R Markdown. Credits: Thinkstats Experimental Design and Analysis","title":"Box plots"},{"location":"R/time-series/","text":"MathJax.Hub.Config({ extensions: [\"tex2jax.js\",\"TeX/AMSmath.js\",\"TeX/AMSsymbols.js\"], jax: [\"input/TeX\", \"output/HTML-CSS\"], tex2jax: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ], }, \"HTML-CSS\": { availableFonts: [\"TeX\"] } }); Time Series \u00b6 A time series is a series of data points captured in time order. Most commonly, a time series is a sequence taken at successive equally spaced points in time. This post is the first in a series of blogs on time series methods and forecasting. In this blog, we will discuss about stationarity, random walk, deterministic drift and other vocabulary which form as foundation to time series: Stochastic processes \u00b6 A random or stochastic process is a collection of random variables ordered in time. It is denoted as \\(Y_t\\) . For example, in-time of an employee is a stochastic process. How is in-time a stochastic process? Consider the in-time on a particular day is 9:00 AM. In theory, the in-time could be any particular value which depends on many factors like traffic, work load, weather etc. The figure 9:00 AM is a particular realization of many such possibilities. Therefore we can say that in-time is a stochastic process where as the actual values observed are a particular realization (sample) of the process. Stationary Processes \u00b6 A stochastic process is said to be stationary if the following conditions are met: 1. Mean is constant over time 2. Variance is constant over time 3. Value of the co-variance between two time periods depends only on the distance or gap or lag between the two time periods and not the actual time at which the co variance is computed This type of process is also called weakly stationary, or co variance stationary, or second-order stationary or wide sense stationary process. Written mathematically, the conditions are: $$ Mean: E(Y_t) = \\mu $$ $$ Variance: var(Y_t) = E(Y_t-\\mu)^2 = \\sigma^2 $$ $$ Covariance: \\gamma_k = E[(Y_y - \\mu)(Y_{t+k} - \\mu)] $$ Purely random or white noise process \u00b6 A stochastic process is purely random if it has zero mean, constant variance, and is serially uncorrelated. An example of white noise is the error term in a linear regression which has zero mean, constant standard deviation and no auto-correlation. Simulation \u00b6 For simulating a stationary process, I am creating 100 realizations(samples) and comparing their mean, variance and co-variance. The data for 6 days and 5 realizations is shown: Samples of Stationary process date realization_1 realization_2 realization_25 realization_50 realization_100 1 2021-08-22 0.8567523 0.0640669 0.7249642 0.3505523 0.8072590 2 2021-08-23 0.1086189 0.3813137 0.5232923 0.4626156 0.7855022 3 2021-08-24 0.4652674 0.3546999 0.2091995 0.2395056 0.9567884 10 2021-08-31 0.8191081 0.1502863 0.1491222 0.6095235 0.3435151 15 2021-09-05 0.9950261 0.1406165 0.1177429 0.9329218 0.4191168 30 2021-09-20 0.8800055 0.9952208 0.7189119 0.7209880 0.6886932 The mean, variance and co-variance between the samples (realizations) across are as follows: For a stationary process, the mean, variance and co variance are constant. Non-stationary Processes \u00b6 If a time series is not stationary, it is called a non-stationary time series. In other words, a non-stationary time series will have a time-varying mean or a time-varying variance or both. Random walk, random walk with drift etc are examples of non-stationary processes. Random walk \u00b6 Suppose \\(\\epsilon_t\\) is a white noise error term with mean 0 and variance \\(\u03c3_2\\) . Then the series \\(Y_t\\) is said to be a random walk if $$ Y_t = Y_{t\u22121} + \\epsilon_t $$ In the random walk model, the value of Y at time t is equal to its value at time (t \u2212 1) plus a random shock. For a random walk, $$ Y_1 = Y_0 + \\epsilon_1 $$ $$ Y_2 = Y_1 + \\epsilon_2 = Y_0 + \\epsilon_1 + \\epsilon_2 $$ $$ Y_3 = Y_2 + \\epsilon_3 = Y_0 + \\epsilon_1 + \\epsilon_2 + \\epsilon_3 $$ and so on.. In general we could write $$ Y_t = Y_0 + \\sum \\epsilon_t $$ As $$ E(Y_t) = E(Y_0 + \\sum \\epsilon_t) = Y_0 $$ $$ var(Y_t) = t\\times \\sigma^2 $$ Although the mean is constant with time, the variance is proportional to time. For simulating a random walk process, I am creating 100 realizations(samples) and comparing their mean, variance and co-variance. The data for 6 days of 5 realizations (samples) is shown: Samples of Random walk process date realization_1 realization_2 realization_25 realization_50 realization_100 1 2021-08-22 4.0000000 4.000000 4.000000 4.000000 4.000000 2 2021-08-23 2.8539208 3.176672 5.446530 5.983017 4.092452 3 2021-08-24 2.9712968 2.009320 5.349939 5.785329 3.451442 10 2021-08-31 -0.7251274 2.289063 2.809076 7.623148 3.587220 15 2021-09-05 -0.5766986 2.559916 5.796322 11.124585 3.992667 30 2021-09-20 0.8613258 6.340583 7.554369 12.667196 9.039007 The mean, variance and covariances between the samples (realizations) across time would look like follows: From the above plot, the mean of Y is equal to its initial, or starting value, which is constant, but as t increases, its variance increases indefinitely, thus violating a condition of stationarity. A random walk process is also called as a unit root process . Random walk with drift \u00b6 If the random walk model predicts that the value at time t will equal the last period's value plus a constant, or drift ( \\(\\delta\\) ), and a white noise term ( \\(\u03b5_t\\) ), then the process is random walk with a drift. $$ Y_t = \\delta + Y_{t\u22121} + \\epsilon_t $$ The mean $$ E(Y_t) = E(Y_0 + \\sum \\epsilon_t + \\delta) = Y_0 + t\\times\\delta $$ so mean is dependent on time and the variance $$ var(Y_t) = t\\times \\sigma^2 $$ is also dependent on time. As random walk with drift violates the conditions of stationary process, it is a non-stationary process. Samples of Random walk with drift process date realization_1 realization_2 realization_25 realization_50 realization_100 1 2021-08-22 4.000000 4.000000 4.000000 4.000000 4.000000 2 2021-08-23 5.801028 5.137174 5.133682 4.186611 4.148583 3 2021-08-24 5.668875 5.741908 4.304969 1.706955 4.467738 10 2021-08-31 11.843748 12.207555 6.289473 5.411678 6.595657 15 2021-09-05 15.982235 15.516312 10.664175 5.937640 11.215054 30 2021-09-20 25.507838 22.875968 20.498484 11.750327 16.976809 The mean, variance and the co-variance are all dependent on time. Unit root stochastic process \u00b6 Unit root stochastic process is another name for Random walk process. A random walk process can be written as $$ Y_t = \\rho \\times Y_{t\u22121} + \\epsilon_t $$ Where \\(\\rho = 1\\) . If \\(|\\rho| < 1\\) then the process represents Markov first order auto regressive model which is stationary. Only for \\(\\rho = 1\\) we get non-stationary. The distribution of mean, variance and co-variance for \\(\\rho =0.5\\) is Deterministic trend process \u00b6 In the above random walk and random walk with drift, the trend component is stochastic in nature. If instead the trend is deterministic in nature, it will follow a deterministic trend process. $$ Y_t = \u03b2_1 + \u03b2_2\\times t + \\epsilon_t$$ In a deterministic trend process, the mean is \\(\u03b2_1 + \u03b2_2\\times t\\) which is proportional with time but the variance is constant. This type of process is also called as trend seasonality as subtracting mean of \\(Y_t\\) from \\(Y_t\\) will give us a stationary process. This procedure is called de-trending. Samples of Deterministic trend process date realization_1 realization_2 realization_25 realization_50 realization_100 1 2021-08-22 0.6408252 0.4380591 3.145050 0.5609335 1.175747 2 2021-08-23 0.7580409 1.5146209 1.939386 2.3880532 2.137324 3 2021-08-24 1.7964618 4.6772129 5.069398 4.7261802 2.939012 10 2021-08-31 7.7861035 9.3627214 8.584801 10.7855074 9.830755 15 2021-09-05 15.8649412 14.2029992 15.279319 13.5478111 14.647241 30 2021-09-20 30.9469391 30.6891297 29.446486 28.9586901 31.170632 A combination of deterministic and stochastic trend could also exist in a process. Comparison. \u00b6 A comparison of all the processes is shown below: References \u00b6 Basic Econometrics - Damodar N Gujarati (textbook for reference) Business Analytics: The Science of Data-Driven Decision Making - Dinesh Kumar (textbook for reference) Customer Analytics at Flipkart.Com - Naveen Bhansali (case study in Harvard business review)","title":"Introduction to stationarity (R)"},{"location":"R/time-series/#time-series","text":"A time series is a series of data points captured in time order. Most commonly, a time series is a sequence taken at successive equally spaced points in time. This post is the first in a series of blogs on time series methods and forecasting. In this blog, we will discuss about stationarity, random walk, deterministic drift and other vocabulary which form as foundation to time series:","title":"Time Series"},{"location":"R/time-series/#stochastic-processes","text":"A random or stochastic process is a collection of random variables ordered in time. It is denoted as \\(Y_t\\) . For example, in-time of an employee is a stochastic process. How is in-time a stochastic process? Consider the in-time on a particular day is 9:00 AM. In theory, the in-time could be any particular value which depends on many factors like traffic, work load, weather etc. The figure 9:00 AM is a particular realization of many such possibilities. Therefore we can say that in-time is a stochastic process where as the actual values observed are a particular realization (sample) of the process.","title":"Stochastic processes"},{"location":"R/time-series/#stationary-processes","text":"A stochastic process is said to be stationary if the following conditions are met: 1. Mean is constant over time 2. Variance is constant over time 3. Value of the co-variance between two time periods depends only on the distance or gap or lag between the two time periods and not the actual time at which the co variance is computed This type of process is also called weakly stationary, or co variance stationary, or second-order stationary or wide sense stationary process. Written mathematically, the conditions are: $$ Mean: E(Y_t) = \\mu $$ $$ Variance: var(Y_t) = E(Y_t-\\mu)^2 = \\sigma^2 $$ $$ Covariance: \\gamma_k = E[(Y_y - \\mu)(Y_{t+k} - \\mu)] $$","title":"Stationary Processes"},{"location":"R/time-series/#purely-random-or-white-noise-process","text":"A stochastic process is purely random if it has zero mean, constant variance, and is serially uncorrelated. An example of white noise is the error term in a linear regression which has zero mean, constant standard deviation and no auto-correlation.","title":"Purely random or white noise process"},{"location":"R/time-series/#simulation","text":"For simulating a stationary process, I am creating 100 realizations(samples) and comparing their mean, variance and co-variance. The data for 6 days and 5 realizations is shown: Samples of Stationary process date realization_1 realization_2 realization_25 realization_50 realization_100 1 2021-08-22 0.8567523 0.0640669 0.7249642 0.3505523 0.8072590 2 2021-08-23 0.1086189 0.3813137 0.5232923 0.4626156 0.7855022 3 2021-08-24 0.4652674 0.3546999 0.2091995 0.2395056 0.9567884 10 2021-08-31 0.8191081 0.1502863 0.1491222 0.6095235 0.3435151 15 2021-09-05 0.9950261 0.1406165 0.1177429 0.9329218 0.4191168 30 2021-09-20 0.8800055 0.9952208 0.7189119 0.7209880 0.6886932 The mean, variance and co-variance between the samples (realizations) across are as follows: For a stationary process, the mean, variance and co variance are constant.","title":"Simulation"},{"location":"R/time-series/#non-stationary-processes","text":"If a time series is not stationary, it is called a non-stationary time series. In other words, a non-stationary time series will have a time-varying mean or a time-varying variance or both. Random walk, random walk with drift etc are examples of non-stationary processes.","title":"Non-stationary Processes"},{"location":"R/time-series/#random-walk","text":"Suppose \\(\\epsilon_t\\) is a white noise error term with mean 0 and variance \\(\u03c3_2\\) . Then the series \\(Y_t\\) is said to be a random walk if $$ Y_t = Y_{t\u22121} + \\epsilon_t $$ In the random walk model, the value of Y at time t is equal to its value at time (t \u2212 1) plus a random shock. For a random walk, $$ Y_1 = Y_0 + \\epsilon_1 $$ $$ Y_2 = Y_1 + \\epsilon_2 = Y_0 + \\epsilon_1 + \\epsilon_2 $$ $$ Y_3 = Y_2 + \\epsilon_3 = Y_0 + \\epsilon_1 + \\epsilon_2 + \\epsilon_3 $$ and so on.. In general we could write $$ Y_t = Y_0 + \\sum \\epsilon_t $$ As $$ E(Y_t) = E(Y_0 + \\sum \\epsilon_t) = Y_0 $$ $$ var(Y_t) = t\\times \\sigma^2 $$ Although the mean is constant with time, the variance is proportional to time. For simulating a random walk process, I am creating 100 realizations(samples) and comparing their mean, variance and co-variance. The data for 6 days of 5 realizations (samples) is shown: Samples of Random walk process date realization_1 realization_2 realization_25 realization_50 realization_100 1 2021-08-22 4.0000000 4.000000 4.000000 4.000000 4.000000 2 2021-08-23 2.8539208 3.176672 5.446530 5.983017 4.092452 3 2021-08-24 2.9712968 2.009320 5.349939 5.785329 3.451442 10 2021-08-31 -0.7251274 2.289063 2.809076 7.623148 3.587220 15 2021-09-05 -0.5766986 2.559916 5.796322 11.124585 3.992667 30 2021-09-20 0.8613258 6.340583 7.554369 12.667196 9.039007 The mean, variance and covariances between the samples (realizations) across time would look like follows: From the above plot, the mean of Y is equal to its initial, or starting value, which is constant, but as t increases, its variance increases indefinitely, thus violating a condition of stationarity. A random walk process is also called as a unit root process .","title":"Random walk"},{"location":"R/time-series/#random-walk-with-drift","text":"If the random walk model predicts that the value at time t will equal the last period's value plus a constant, or drift ( \\(\\delta\\) ), and a white noise term ( \\(\u03b5_t\\) ), then the process is random walk with a drift. $$ Y_t = \\delta + Y_{t\u22121} + \\epsilon_t $$ The mean $$ E(Y_t) = E(Y_0 + \\sum \\epsilon_t + \\delta) = Y_0 + t\\times\\delta $$ so mean is dependent on time and the variance $$ var(Y_t) = t\\times \\sigma^2 $$ is also dependent on time. As random walk with drift violates the conditions of stationary process, it is a non-stationary process. Samples of Random walk with drift process date realization_1 realization_2 realization_25 realization_50 realization_100 1 2021-08-22 4.000000 4.000000 4.000000 4.000000 4.000000 2 2021-08-23 5.801028 5.137174 5.133682 4.186611 4.148583 3 2021-08-24 5.668875 5.741908 4.304969 1.706955 4.467738 10 2021-08-31 11.843748 12.207555 6.289473 5.411678 6.595657 15 2021-09-05 15.982235 15.516312 10.664175 5.937640 11.215054 30 2021-09-20 25.507838 22.875968 20.498484 11.750327 16.976809 The mean, variance and the co-variance are all dependent on time.","title":"Random walk with drift"},{"location":"R/time-series/#unit-root-stochastic-process","text":"Unit root stochastic process is another name for Random walk process. A random walk process can be written as $$ Y_t = \\rho \\times Y_{t\u22121} + \\epsilon_t $$ Where \\(\\rho = 1\\) . If \\(|\\rho| < 1\\) then the process represents Markov first order auto regressive model which is stationary. Only for \\(\\rho = 1\\) we get non-stationary. The distribution of mean, variance and co-variance for \\(\\rho =0.5\\) is","title":"Unit root stochastic process"},{"location":"R/time-series/#deterministic-trend-process","text":"In the above random walk and random walk with drift, the trend component is stochastic in nature. If instead the trend is deterministic in nature, it will follow a deterministic trend process. $$ Y_t = \u03b2_1 + \u03b2_2\\times t + \\epsilon_t$$ In a deterministic trend process, the mean is \\(\u03b2_1 + \u03b2_2\\times t\\) which is proportional with time but the variance is constant. This type of process is also called as trend seasonality as subtracting mean of \\(Y_t\\) from \\(Y_t\\) will give us a stationary process. This procedure is called de-trending. Samples of Deterministic trend process date realization_1 realization_2 realization_25 realization_50 realization_100 1 2021-08-22 0.6408252 0.4380591 3.145050 0.5609335 1.175747 2 2021-08-23 0.7580409 1.5146209 1.939386 2.3880532 2.137324 3 2021-08-24 1.7964618 4.6772129 5.069398 4.7261802 2.939012 10 2021-08-31 7.7861035 9.3627214 8.584801 10.7855074 9.830755 15 2021-09-05 15.8649412 14.2029992 15.279319 13.5478111 14.647241 30 2021-09-20 30.9469391 30.6891297 29.446486 28.9586901 31.170632 A combination of deterministic and stochastic trend could also exist in a process.","title":"Deterministic trend process"},{"location":"R/time-series/#comparison","text":"A comparison of all the processes is shown below:","title":"Comparison."},{"location":"R/time-series/#references","text":"Basic Econometrics - Damodar N Gujarati (textbook for reference) Business Analytics: The Science of Data-Driven Decision Making - Dinesh Kumar (textbook for reference) Customer Analytics at Flipkart.Com - Naveen Bhansali (case study in Harvard business review)","title":"References"}]}