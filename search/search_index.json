{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"this is a sample page","title":"Home"},{"location":"Others/Imperial%20College%20London/","text":"Imperial College London Student Blogs \u00b6 I was a student content creator at Imperial College London for 2020-21. As part of this, I have published three blogs on the Imperial College Webiste, all related to Business ANalytics Msc that I was pursuing at that time. Why you should study an online Master\u2019s in Business Analytics over a short course What to look for when choosing a part-time Master\u2019s programme Applying what I studied to my role as a Data Science Consultant","title":"Imperial college London"},{"location":"Others/Imperial%20College%20London/#imperial-college-london-student-blogs","text":"I was a student content creator at Imperial College London for 2020-21. As part of this, I have published three blogs on the Imperial College Webiste, all related to Business ANalytics Msc that I was pursuing at that time. Why you should study an online Master\u2019s in Business Analytics over a short course What to look for when choosing a part-time Master\u2019s programme Applying what I studied to my role as a Data Science Consultant","title":"Imperial College London Student Blogs"},{"location":"Others/Publications/","text":"Publications, whitepapers and presentations \u00b6 The following are the papers published in a journal or presented in a conference. Paper Publisher/Conference Author Status Date Subject Parametric Study of Cantelever Beams in Supersonic and Hypersonic flow IOC Physics Primary Jul 2017 Mechanical Engineering Personal analytics: Time management using Google Maps ICSADADS Conference Primary Feb 2020 Data Science Scarecrow - Intelligent Annotation platform for Engine Health Management AI-ML Systems / ACM Secondary Oct 2021 Machine Learning","title":"Publications/conferences"},{"location":"Others/Publications/#publications-whitepapers-and-presentations","text":"The following are the papers published in a journal or presented in a conference. Paper Publisher/Conference Author Status Date Subject Parametric Study of Cantelever Beams in Supersonic and Hypersonic flow IOC Physics Primary Jul 2017 Mechanical Engineering Personal analytics: Time management using Google Maps ICSADADS Conference Primary Feb 2020 Data Science Scarecrow - Intelligent Annotation platform for Engine Health Management AI-ML Systems / ACM Secondary Oct 2021 Machine Learning","title":"Publications, whitepapers and presentations"},{"location":"Python/Bipartite%20matching/","text":"Matching algorithms \u00b6 Author: Achyuthuni Sri Harsha Stock markets, housing and labor markets, dating and so forth are examples of matching tasks. Let us take suppliers an buyers as an example. in a matching problem, our job is to match the supliers to buyers so that both sides/ either side are satisfied. Matching problems can be considered as network problems. In network terms, a matching is a subset of edges where every node in one group goes through only one node in the other group. There should be only one edge from each node. # Import networkx library and rename it as nx. import networkx as nx # Other packages required import numpy as np import pandas as pd import matplotlib.pyplot as plt Unweighted bipartite mapping \u00b6 Let us take a simple example of mapping students and dorm rooms. In this problem, students give a list of rooms they are willing to stay at. We represent students on side as nodes of a bipartite graph and rooms on the other side as nodes, and we put an edge between students and rooms as per this list. We want to map students and rooms. We want to identify a subset where we match one student to exactly one other room (no roommates). The students give what rooms are acceptable, and many solutions are possible. Consider the below problem where we have students (A, B, C and D) and we want to match them to rooms (1,2,3,4). The list of acceptable rooms for each student is given below. edgelist_df = pd . DataFrame ({ 'node1' :[ 'A' , 'A' , 'B' , 'B' , 'C' , 'C' , 'D' , 'D' ], 'node2' :[ 1 , 2 , 2 , 3 , 3 , 4 , 4 , 1 ]}) edgelist_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } node1 node2 0 A 1 1 A 2 2 B 2 3 B 3 4 C 3 5 C 4 6 D 4 7 D 1 This can be represented as a bipartite graph as follows g = nx . Graph () for i , elrow in edgelist_df . iterrows (): g . add_edge ( elrow [ 0 ], elrow [ 1 ]) # Make two sets in bipartite and get positions for the same left , right = nx . bipartite . sets ( g ) pos = {} # Update position for node from each group for i , node in enumerate ( sorted ( list ( left ))): g . add_node ( node , pos = ( 0 , i )) for i , node in enumerate ( sorted ( list ( right ))): g . add_node ( node , pos = ( 1 , i )) nx . draw ( g , pos = nx . get_node_attributes ( g , 'pos' ), with_labels = True ) Visually, we can find a couple of solutions, for example: - A:1, B:2, C:3, D:4 - A:2, B:3, C:4, D:1 In large graphs visual analysis might be difficult, and in such situations Halls theorem is useful to identify if matching is possible. Halls theorem \u00b6 But before Halls theorem, let us look at constricted set. Constricted set : A constricted set is a subset of edges (on either side) whose neighbours are smaller than the subset. For examle, if we have two students (subset of students) who give only one room (same room) in the list, then the size of the students is 2 and the size of the rooms is one, and no matching can be done. Halls theorem states that for a mapping to exist, there should be no constricted set. Augmenting paths 1. Select any random matching of unmatched nodes. 2. Switch to the augumented paths if it exists. If it doesnt exist, then we have a constricted set and we cannot do matching. 3. Repeat until all left nodes are matched to one right node. This is implemented in NetworkX as follows: # Select random edges selected_edges = [] for left_node in left : # For a left node, select a random node in the right list_of_nodes = list ( g . edges ( left_node )) random_node = np . random . randint ( len ( list_of_nodes )) selected_edges . append ( list_of_nodes [ random_node ]) selected_edges [('D', 4), ('C', 4), ('A', 2), ('B', 3)] Selected edges are shown in green for edge in g . edges : if edge in selected_edges or ( edge [ 1 ], edge [ 0 ]) in selected_edges : g . add_edge ( edge [ 0 ], edge [ 1 ], color = 'g' , weight = 10 ) else : g . add_edge ( edge [ 0 ], edge [ 1 ], color = 'b' , weight = 0.1 ) nx . draw ( g , pos = nx . get_node_attributes ( g , 'pos' ), with_labels = True , edge_color = nx . get_edge_attributes ( g , 'color' ) . values ()) We can see that this is not matching as node C and D are mapped to 4. This can be resolved by moving through the augumented paths for the C-4 node. For the C-4 node, moving thru the augumented path selects the C-3 and D-4 node. Following the smae process with with B-3 and A-3 nodes, we get: selected_edges = {( 1 , 'A' ), ( 2 , 'B' ), ( 3 , 'C' ), ( 4 , 'D' )} for edge in g . edges : if edge in selected_edges or ( edge [ 1 ], edge [ 0 ]) in selected_edges : g . add_edge ( edge [ 0 ], edge [ 1 ], color = 'g' , weight = 10 ) else : g . add_edge ( edge [ 0 ], edge [ 1 ], color = 'b' , weight = 0.1 ) nx . draw ( g , pos = nx . get_node_attributes ( g , 'pos' ), with_labels = True , edge_color = nx . get_edge_attributes ( g , 'color' ) . values ()) The final mapping is shown in green nx . algorithms . bipartite . matching . hopcroft_karp_matching ( g , top_nodes = list ( set ( edgelist_df . node1 ))) {'D': 4, 'C': 3, 'B': 2, 'A': 1, 1: 'A', 2: 'B', 3: 'C', 4: 'D'} Weighted bipartite mapping \u00b6 In the previous problem, we tried to find a perfect maching in an unweighted graph. What if every edge in the graph has certain weight attached to it. The weights could be quality index (for student-dorm matching) or valuations in a market etc. Consider the suppliers-buyers example for a housing market as shown below. We have three suppliers, A, B and C and three buyers (x, y and z). The valuation for each of the sellers is also given. For example, buyer x values house A with 12, house B with 4 and house C with 2. sellers = [ 'A' , 'B' , 'C' ] buyers = [ 'x' , 'y' , 'z' ] valuations_for_buyers = [[ 12 , 4 , 2 ], [ 8 , 7 , 6 ], [ 7 , 5 , 2 ]] sellers_price = [ 0 , 0 , 0 ] g = nx . Graph () pos = {} # Update position for node from each group for i , node in enumerate ( sellers ): g . add_node ( node , pos = ( 0 , len ( sellers ) - i )) for i , node in enumerate ( buyers ): g . add_node ( node , pos = ( 1 , len ( buyers ) - i )) # Plot text for the buyers for i , buyer in enumerate ( buyers ): plt . text ( 1.1 , len ( buyers ) - i , s = valuations_for_buyers [ i ], horizontalalignment = 'left' ) # Plot text for the sellers for i , buyer in enumerate ( buyers ): plt . text ( - 0.1 , len ( buyers ) - i , s = sellers_price [ i ], horizontalalignment = 'right' ) nx . draw ( g , pos = nx . get_node_attributes ( g , 'pos' ), with_labels = True ) What we want to achieve is clearing of the market. Clearing happens when all houses are sold to one buyer, and every buyer bought one house. This can be done using an auction algorithm. 1. Sellers quote a price 2. Buyers calculate utility: Net valuation (payoff) = Gross Valuation - Price charged by seller 3. Buyers select the object that has highest payoff 4. If the market is not cleared, the sellers who have more than one offer (overdetermined) will increase the price by one unit, and the process is repeated. # Function to pick the supplier with the maximumm utility def match_to_maximum_utility ( sellers , buyers , valuation , price ): max_utility_sellers = {} for buyer_index in range ( len ( buyers )): max_utility = 0 for seller_index in range ( len ( sellers )): if ( max_utility < valuation [ buyer_index ][ seller_index ] - price [ seller_index ]): max_utility = valuation [ buyer_index ][ seller_index ] - price [ seller_index ] max_utility_sellers [ buyers [ buyer_index ]] = [ sellers [ seller_index ]] elif ( max_utility == valuation [ buyer_index ][ seller_index ] - price [ seller_index ]): max_utility_sellers [ buyers [ buyer_index ]] . append ( sellers [ seller_index ]) return max_utility_sellers Assuming that the initial price set by the seller is zero (scaled to zero - displayed beside the node), buyer x calculates the following utility: - For A: 12-0 = 12 - For B: 4-0 = 4 - For C: 2-0 = 2 As the utility of A is the highest, x will chose A. Similarly, B and C will also choose A. max_util = match_to_maximum_utility ( sellers , buyers , valuations_for_buyers , sellers_price ) max_util {'x': ['A'], 'y': ['A'], 'z': ['A']} Plotting the selection, we see that A is overdetermined. def plot_max_utility_graph ( sellers , buyers , valuations_for_buyers , sellers_price , max_util ): g = nx . Graph () pos = {} # Update position for node from each group for i , node in enumerate ( sellers ): g . add_node ( node , pos = ( 0 , len ( sellers ) - i )) for i , node in enumerate ( buyers ): g . add_node ( node , pos = ( 1 , len ( buyers ) - i )) # Make edges for key , values in max_util . items (): for value in values : g . add_edge ( key , value ) # Plot text for the buyers for i , buyer in enumerate ( buyers ): plt . text ( 1.1 , len ( buyers ) - i , s = valuations_for_buyers [ i ], horizontalalignment = 'left' ) # Plot text for the sellers for i , buyer in enumerate ( buyers ): plt . text ( - 0.1 , len ( buyers ) - i , s = sellers_price [ i ], horizontalalignment = 'right' ) nx . draw ( g , pos = nx . get_node_attributes ( g , 'pos' ), with_labels = True ) plt . show () plot_max_utility_graph ( sellers , buyers , valuations_for_buyers , sellers_price , max_util ) For the overdetermined edge A, we increase the price by one unit. def get_overdetermined_edge_and_increase_price ( sellers , sellers_price , max_util ): from collections import Counter counts = dict ( Counter ( sum ( max_util . values (), []))) over_determined_list = [] for key , value in counts . items (): if ( value > 1 ): over_determined_list . append ( key ) sellers_price [ sellers . index ( key )] += 1 print ( 'Nodes' , over_determined_list , 'are over determined. Added 1 to the price for the suppliers' ) return counts get_overdetermined_edge_and_increase_price ( sellers , sellers_price , max_util ) Nodes ['A'] are over determined. Added 1 to the price for the suppliers {'A': 3} We then contiue this process until the market is cleared. no_of_sellers_selected = 0 while ( no_of_sellers_selected != len ( sellers )): max_util = match_to_maximum_utility ( sellers , buyers , valuations_for_buyers , sellers_price ) plot_max_utility_graph ( sellers , buyers , valuations_for_buyers , sellers_price , max_util ) counts = get_overdetermined_edge_and_increase_price ( sellers , sellers_price , max_util ) no_of_sellers_selected = len ( counts ) # need not always be the case, check Nodes ['A'] are over determined. Added 1 to the price for the suppliers Nodes ['A', 'B'] are over determined. Added 1 to the price for the suppliers Nodes ['A', 'B'] are over determined. Added 1 to the price for the suppliers We can see that for costs (A:3, B:1, C:0), the market can be cleared with buyer x choosing A, y choosing C and z choosing B. This is the maximum weight perfect matching. Matching with preferences \u00b6 In the previous scenario, we had weights on the edges which indicated the utility. In this case we will look at matching where we have preferences in a ranked order. This is more natural way in many scenarios, like students preference to universities/universities selecting students, dating scenarios etc. This was originally implemented by Al Roth for matching hospitals and residency. Our goal is to clear the market, but also have a stable matching. So, what is a stable matching? Stable matching : Stability is an eqvilibrium when no pair on ether side has an incentive to devaite from the mapping. Let us understand this using an example. Take the dating scenario for example. On the left hand side we have men and on the right hand side we have women. All men rank women in strict order and all women rank men in strict order. In a stable matching, no pair of nodes (male-female) prefers each other to their curently assigned partners. Gale Shapley Algorithm \u00b6 Let us say that are n players on both sides with males(m) on one side and women(w) on another side. The algorithm is as follows: 1. Every unmatched male (m) proposses to their first preference available. 2. If the proposed women (w) is unmatched, w accepts. If the women is already matched and the m has higher preference for w, w switches. Else, previous mapping remains. 3. This process continues until there is stability Lets take an example with three males and three females. The preferences are mentioned at the side of the node in a list. For example, m2 has a preference w2, followed by w3 and then w1. Similarly w2 has a preference of m3, followed by m1 and then m2. males = [ 'm1' , 'm2' , 'm3' ] females = [ 'w1' , 'w2' , 'w3' ] male_preferences = [[ 'w1' , 'w2' , 'w3' ], [ 'w2' , 'w3' , 'w1' ], [ 'w2' , 'w3' , 'w1' ]] female_preferences = [[ 'm2' , 'm3' , 'm1' ], [ 'm3' , 'm1' , 'm2' ], [ 'm1' , 'm2' , 'm3' ]] def match_next_male ( male_index , males , females , male_preferences , female_preferences ): for female in male_preferences [ male_index ]: if ( female not in current_mapping . values ()): current_mapping [ males [ male_index ]] = female return current_mapping elif ( female in current_mapping . values ()): current_mapping_inverse = dict ( zip ( current_mapping . values (), current_mapping . keys ())) current_male_for_the_female = current_mapping_inverse [ female ] if ( female_preferences [ males . index ( current_male_for_the_female )] > female_preferences [ male_index ]): current_mapping [ males [ male_index ]] = female current_mapping . pop ( current_male_for_the_female ) return current_mapping def plot_max_utility_graph ( males , females , male_preferences , female_preferences , current_mapping ): g = nx . Graph () pos = {} # Update position for node from each group for i , node in enumerate ( males ): g . add_node ( node , pos = ( 0 , len ( males ) - i )) for i , node in enumerate ( females ): g . add_node ( node , pos = ( 1 , len ( females ) - i )) # Make edges for key , value in current_mapping . items (): g . add_edge ( key , value ) # Plot text for the males for i , male in enumerate ( males ): plt . text ( - 0.1 , len ( males ) - i , s = male_preferences [ i ], horizontalalignment = 'right' ) # Plot text for the females for i , female in enumerate ( females ): plt . text ( 1.1 , len ( females ) - i , s = female_preferences [ i ], horizontalalignment = 'left' ) nx . draw ( g , pos = nx . get_node_attributes ( g , 'pos' ), with_labels = True ) plt . show () print ( '_____________________________________________________________________________' ) current_mapping = {} while ( len ( current_mapping ) != len ( males )): for male_index in range ( len ( males )): if ( current_mapping . get ( males [ male_index ]) is None ): current_mapping = match_next_male ( male_index , males , females , male_preferences , female_preferences ) plot_max_utility_graph ( males , females , male_preferences , female_preferences , current_mapping ) _____________________________________________________________________________ _____________________________________________________________________________ _____________________________________________________________________________ _____________________________________________________________________________ The process goes on as follows: 1. m1 proposes to w1 as w1 has maximum rank and as w1 is unselected, w1 accepts. We create an edge between them. 2. m2 proposes to w2 as w2 has maximum rank, and as w2 is unselected, w2 accepts. We crreate a edge between them. 3. m3 also proposes to w2. As w2 is already selected, it checks the preference of the current selection (m2) to m3. As m3 has better preference, w2 switches from m2 to m3. The edge between m2 and w2 is reoved and a new edge between m3 and w2 is created. 4. m2 is currently unmapped, and selects the next best preference, which is w3. As w3 is unselected, w3 accepts. 5. This clears the market and the process stops. This is stable mapping (from the men's side). These are the common matching techniques that exist.","title":"Bipartite matching (Python)"},{"location":"Python/Bipartite%20matching/#matching-algorithms","text":"Author: Achyuthuni Sri Harsha Stock markets, housing and labor markets, dating and so forth are examples of matching tasks. Let us take suppliers an buyers as an example. in a matching problem, our job is to match the supliers to buyers so that both sides/ either side are satisfied. Matching problems can be considered as network problems. In network terms, a matching is a subset of edges where every node in one group goes through only one node in the other group. There should be only one edge from each node. # Import networkx library and rename it as nx. import networkx as nx # Other packages required import numpy as np import pandas as pd import matplotlib.pyplot as plt","title":"Matching algorithms"},{"location":"Python/Bipartite%20matching/#unweighted-bipartite-mapping","text":"Let us take a simple example of mapping students and dorm rooms. In this problem, students give a list of rooms they are willing to stay at. We represent students on side as nodes of a bipartite graph and rooms on the other side as nodes, and we put an edge between students and rooms as per this list. We want to map students and rooms. We want to identify a subset where we match one student to exactly one other room (no roommates). The students give what rooms are acceptable, and many solutions are possible. Consider the below problem where we have students (A, B, C and D) and we want to match them to rooms (1,2,3,4). The list of acceptable rooms for each student is given below. edgelist_df = pd . DataFrame ({ 'node1' :[ 'A' , 'A' , 'B' , 'B' , 'C' , 'C' , 'D' , 'D' ], 'node2' :[ 1 , 2 , 2 , 3 , 3 , 4 , 4 , 1 ]}) edgelist_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } node1 node2 0 A 1 1 A 2 2 B 2 3 B 3 4 C 3 5 C 4 6 D 4 7 D 1 This can be represented as a bipartite graph as follows g = nx . Graph () for i , elrow in edgelist_df . iterrows (): g . add_edge ( elrow [ 0 ], elrow [ 1 ]) # Make two sets in bipartite and get positions for the same left , right = nx . bipartite . sets ( g ) pos = {} # Update position for node from each group for i , node in enumerate ( sorted ( list ( left ))): g . add_node ( node , pos = ( 0 , i )) for i , node in enumerate ( sorted ( list ( right ))): g . add_node ( node , pos = ( 1 , i )) nx . draw ( g , pos = nx . get_node_attributes ( g , 'pos' ), with_labels = True ) Visually, we can find a couple of solutions, for example: - A:1, B:2, C:3, D:4 - A:2, B:3, C:4, D:1 In large graphs visual analysis might be difficult, and in such situations Halls theorem is useful to identify if matching is possible.","title":"Unweighted bipartite mapping"},{"location":"Python/Bipartite%20matching/#halls-theorem","text":"But before Halls theorem, let us look at constricted set. Constricted set : A constricted set is a subset of edges (on either side) whose neighbours are smaller than the subset. For examle, if we have two students (subset of students) who give only one room (same room) in the list, then the size of the students is 2 and the size of the rooms is one, and no matching can be done. Halls theorem states that for a mapping to exist, there should be no constricted set. Augmenting paths 1. Select any random matching of unmatched nodes. 2. Switch to the augumented paths if it exists. If it doesnt exist, then we have a constricted set and we cannot do matching. 3. Repeat until all left nodes are matched to one right node. This is implemented in NetworkX as follows: # Select random edges selected_edges = [] for left_node in left : # For a left node, select a random node in the right list_of_nodes = list ( g . edges ( left_node )) random_node = np . random . randint ( len ( list_of_nodes )) selected_edges . append ( list_of_nodes [ random_node ]) selected_edges [('D', 4), ('C', 4), ('A', 2), ('B', 3)] Selected edges are shown in green for edge in g . edges : if edge in selected_edges or ( edge [ 1 ], edge [ 0 ]) in selected_edges : g . add_edge ( edge [ 0 ], edge [ 1 ], color = 'g' , weight = 10 ) else : g . add_edge ( edge [ 0 ], edge [ 1 ], color = 'b' , weight = 0.1 ) nx . draw ( g , pos = nx . get_node_attributes ( g , 'pos' ), with_labels = True , edge_color = nx . get_edge_attributes ( g , 'color' ) . values ()) We can see that this is not matching as node C and D are mapped to 4. This can be resolved by moving through the augumented paths for the C-4 node. For the C-4 node, moving thru the augumented path selects the C-3 and D-4 node. Following the smae process with with B-3 and A-3 nodes, we get: selected_edges = {( 1 , 'A' ), ( 2 , 'B' ), ( 3 , 'C' ), ( 4 , 'D' )} for edge in g . edges : if edge in selected_edges or ( edge [ 1 ], edge [ 0 ]) in selected_edges : g . add_edge ( edge [ 0 ], edge [ 1 ], color = 'g' , weight = 10 ) else : g . add_edge ( edge [ 0 ], edge [ 1 ], color = 'b' , weight = 0.1 ) nx . draw ( g , pos = nx . get_node_attributes ( g , 'pos' ), with_labels = True , edge_color = nx . get_edge_attributes ( g , 'color' ) . values ()) The final mapping is shown in green nx . algorithms . bipartite . matching . hopcroft_karp_matching ( g , top_nodes = list ( set ( edgelist_df . node1 ))) {'D': 4, 'C': 3, 'B': 2, 'A': 1, 1: 'A', 2: 'B', 3: 'C', 4: 'D'}","title":"Halls theorem"},{"location":"Python/Bipartite%20matching/#weighted-bipartite-mapping","text":"In the previous problem, we tried to find a perfect maching in an unweighted graph. What if every edge in the graph has certain weight attached to it. The weights could be quality index (for student-dorm matching) or valuations in a market etc. Consider the suppliers-buyers example for a housing market as shown below. We have three suppliers, A, B and C and three buyers (x, y and z). The valuation for each of the sellers is also given. For example, buyer x values house A with 12, house B with 4 and house C with 2. sellers = [ 'A' , 'B' , 'C' ] buyers = [ 'x' , 'y' , 'z' ] valuations_for_buyers = [[ 12 , 4 , 2 ], [ 8 , 7 , 6 ], [ 7 , 5 , 2 ]] sellers_price = [ 0 , 0 , 0 ] g = nx . Graph () pos = {} # Update position for node from each group for i , node in enumerate ( sellers ): g . add_node ( node , pos = ( 0 , len ( sellers ) - i )) for i , node in enumerate ( buyers ): g . add_node ( node , pos = ( 1 , len ( buyers ) - i )) # Plot text for the buyers for i , buyer in enumerate ( buyers ): plt . text ( 1.1 , len ( buyers ) - i , s = valuations_for_buyers [ i ], horizontalalignment = 'left' ) # Plot text for the sellers for i , buyer in enumerate ( buyers ): plt . text ( - 0.1 , len ( buyers ) - i , s = sellers_price [ i ], horizontalalignment = 'right' ) nx . draw ( g , pos = nx . get_node_attributes ( g , 'pos' ), with_labels = True ) What we want to achieve is clearing of the market. Clearing happens when all houses are sold to one buyer, and every buyer bought one house. This can be done using an auction algorithm. 1. Sellers quote a price 2. Buyers calculate utility: Net valuation (payoff) = Gross Valuation - Price charged by seller 3. Buyers select the object that has highest payoff 4. If the market is not cleared, the sellers who have more than one offer (overdetermined) will increase the price by one unit, and the process is repeated. # Function to pick the supplier with the maximumm utility def match_to_maximum_utility ( sellers , buyers , valuation , price ): max_utility_sellers = {} for buyer_index in range ( len ( buyers )): max_utility = 0 for seller_index in range ( len ( sellers )): if ( max_utility < valuation [ buyer_index ][ seller_index ] - price [ seller_index ]): max_utility = valuation [ buyer_index ][ seller_index ] - price [ seller_index ] max_utility_sellers [ buyers [ buyer_index ]] = [ sellers [ seller_index ]] elif ( max_utility == valuation [ buyer_index ][ seller_index ] - price [ seller_index ]): max_utility_sellers [ buyers [ buyer_index ]] . append ( sellers [ seller_index ]) return max_utility_sellers Assuming that the initial price set by the seller is zero (scaled to zero - displayed beside the node), buyer x calculates the following utility: - For A: 12-0 = 12 - For B: 4-0 = 4 - For C: 2-0 = 2 As the utility of A is the highest, x will chose A. Similarly, B and C will also choose A. max_util = match_to_maximum_utility ( sellers , buyers , valuations_for_buyers , sellers_price ) max_util {'x': ['A'], 'y': ['A'], 'z': ['A']} Plotting the selection, we see that A is overdetermined. def plot_max_utility_graph ( sellers , buyers , valuations_for_buyers , sellers_price , max_util ): g = nx . Graph () pos = {} # Update position for node from each group for i , node in enumerate ( sellers ): g . add_node ( node , pos = ( 0 , len ( sellers ) - i )) for i , node in enumerate ( buyers ): g . add_node ( node , pos = ( 1 , len ( buyers ) - i )) # Make edges for key , values in max_util . items (): for value in values : g . add_edge ( key , value ) # Plot text for the buyers for i , buyer in enumerate ( buyers ): plt . text ( 1.1 , len ( buyers ) - i , s = valuations_for_buyers [ i ], horizontalalignment = 'left' ) # Plot text for the sellers for i , buyer in enumerate ( buyers ): plt . text ( - 0.1 , len ( buyers ) - i , s = sellers_price [ i ], horizontalalignment = 'right' ) nx . draw ( g , pos = nx . get_node_attributes ( g , 'pos' ), with_labels = True ) plt . show () plot_max_utility_graph ( sellers , buyers , valuations_for_buyers , sellers_price , max_util ) For the overdetermined edge A, we increase the price by one unit. def get_overdetermined_edge_and_increase_price ( sellers , sellers_price , max_util ): from collections import Counter counts = dict ( Counter ( sum ( max_util . values (), []))) over_determined_list = [] for key , value in counts . items (): if ( value > 1 ): over_determined_list . append ( key ) sellers_price [ sellers . index ( key )] += 1 print ( 'Nodes' , over_determined_list , 'are over determined. Added 1 to the price for the suppliers' ) return counts get_overdetermined_edge_and_increase_price ( sellers , sellers_price , max_util ) Nodes ['A'] are over determined. Added 1 to the price for the suppliers {'A': 3} We then contiue this process until the market is cleared. no_of_sellers_selected = 0 while ( no_of_sellers_selected != len ( sellers )): max_util = match_to_maximum_utility ( sellers , buyers , valuations_for_buyers , sellers_price ) plot_max_utility_graph ( sellers , buyers , valuations_for_buyers , sellers_price , max_util ) counts = get_overdetermined_edge_and_increase_price ( sellers , sellers_price , max_util ) no_of_sellers_selected = len ( counts ) # need not always be the case, check Nodes ['A'] are over determined. Added 1 to the price for the suppliers Nodes ['A', 'B'] are over determined. Added 1 to the price for the suppliers Nodes ['A', 'B'] are over determined. Added 1 to the price for the suppliers We can see that for costs (A:3, B:1, C:0), the market can be cleared with buyer x choosing A, y choosing C and z choosing B. This is the maximum weight perfect matching.","title":"Weighted bipartite mapping"},{"location":"Python/Bipartite%20matching/#matching-with-preferences","text":"In the previous scenario, we had weights on the edges which indicated the utility. In this case we will look at matching where we have preferences in a ranked order. This is more natural way in many scenarios, like students preference to universities/universities selecting students, dating scenarios etc. This was originally implemented by Al Roth for matching hospitals and residency. Our goal is to clear the market, but also have a stable matching. So, what is a stable matching? Stable matching : Stability is an eqvilibrium when no pair on ether side has an incentive to devaite from the mapping. Let us understand this using an example. Take the dating scenario for example. On the left hand side we have men and on the right hand side we have women. All men rank women in strict order and all women rank men in strict order. In a stable matching, no pair of nodes (male-female) prefers each other to their curently assigned partners.","title":"Matching with preferences"},{"location":"Python/Bipartite%20matching/#gale-shapley-algorithm","text":"Let us say that are n players on both sides with males(m) on one side and women(w) on another side. The algorithm is as follows: 1. Every unmatched male (m) proposses to their first preference available. 2. If the proposed women (w) is unmatched, w accepts. If the women is already matched and the m has higher preference for w, w switches. Else, previous mapping remains. 3. This process continues until there is stability Lets take an example with three males and three females. The preferences are mentioned at the side of the node in a list. For example, m2 has a preference w2, followed by w3 and then w1. Similarly w2 has a preference of m3, followed by m1 and then m2. males = [ 'm1' , 'm2' , 'm3' ] females = [ 'w1' , 'w2' , 'w3' ] male_preferences = [[ 'w1' , 'w2' , 'w3' ], [ 'w2' , 'w3' , 'w1' ], [ 'w2' , 'w3' , 'w1' ]] female_preferences = [[ 'm2' , 'm3' , 'm1' ], [ 'm3' , 'm1' , 'm2' ], [ 'm1' , 'm2' , 'm3' ]] def match_next_male ( male_index , males , females , male_preferences , female_preferences ): for female in male_preferences [ male_index ]: if ( female not in current_mapping . values ()): current_mapping [ males [ male_index ]] = female return current_mapping elif ( female in current_mapping . values ()): current_mapping_inverse = dict ( zip ( current_mapping . values (), current_mapping . keys ())) current_male_for_the_female = current_mapping_inverse [ female ] if ( female_preferences [ males . index ( current_male_for_the_female )] > female_preferences [ male_index ]): current_mapping [ males [ male_index ]] = female current_mapping . pop ( current_male_for_the_female ) return current_mapping def plot_max_utility_graph ( males , females , male_preferences , female_preferences , current_mapping ): g = nx . Graph () pos = {} # Update position for node from each group for i , node in enumerate ( males ): g . add_node ( node , pos = ( 0 , len ( males ) - i )) for i , node in enumerate ( females ): g . add_node ( node , pos = ( 1 , len ( females ) - i )) # Make edges for key , value in current_mapping . items (): g . add_edge ( key , value ) # Plot text for the males for i , male in enumerate ( males ): plt . text ( - 0.1 , len ( males ) - i , s = male_preferences [ i ], horizontalalignment = 'right' ) # Plot text for the females for i , female in enumerate ( females ): plt . text ( 1.1 , len ( females ) - i , s = female_preferences [ i ], horizontalalignment = 'left' ) nx . draw ( g , pos = nx . get_node_attributes ( g , 'pos' ), with_labels = True ) plt . show () print ( '_____________________________________________________________________________' ) current_mapping = {} while ( len ( current_mapping ) != len ( males )): for male_index in range ( len ( males )): if ( current_mapping . get ( males [ male_index ]) is None ): current_mapping = match_next_male ( male_index , males , females , male_preferences , female_preferences ) plot_max_utility_graph ( males , females , male_preferences , female_preferences , current_mapping ) _____________________________________________________________________________ _____________________________________________________________________________ _____________________________________________________________________________ _____________________________________________________________________________ The process goes on as follows: 1. m1 proposes to w1 as w1 has maximum rank and as w1 is unselected, w1 accepts. We create an edge between them. 2. m2 proposes to w2 as w2 has maximum rank, and as w2 is unselected, w2 accepts. We crreate a edge between them. 3. m3 also proposes to w2. As w2 is already selected, it checks the preference of the current selection (m2) to m3. As m3 has better preference, w2 switches from m2 to m3. The edge between m2 and w2 is reoved and a new edge between m3 and w2 is created. 4. m2 is currently unmapped, and selects the next best preference, which is w3. As w3 is unselected, w3 accepts. 5. This clears the market and the process stops. This is stable mapping (from the men's side). These are the common matching techniques that exist.","title":"Gale Shapley Algorithm"},{"location":"Python/Community%20detection/","text":"MathJax.Hub.Config({ extensions: [\"tex2jax.js\",\"TeX/AMSmath.js\",\"TeX/AMSsymbols.js\"], jax: [\"input/TeX\", \"output/HTML-CSS\"], tex2jax: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ], }, \"HTML-CSS\": { availableFonts: [\"TeX\"] } }); Community detection \u00b6 Author: Achyuthuni Sri Harsha A community is a group of people who are homogeneous in some features. Marketers want to find communities so that they can target them. It is not trivial to identify communities from a given network. Community detection deals with identifying networks based only on the network topology. nodes in the community are closer than the nodes outside the community. What is closeness? To define closeness, we need to 1. Define distance metric based on network topology 2. Quantify nodes in a community are closer wth each other than nodes outside the community There are two approaches, bottom-up and top-down. Girwan Newman Algorithm \u00b6 The most popular algorithm for network community detection is the Girvan-Newman algorithm. It is a top down approach where we take the whole network and try to break it into two communities. This can be continued till the bottom. Steps: 1. Define betweenneess mesure for each edge 2. Find the edge with maximum betweenness and remove it (The edge most in-between in the network connects up most number of pairs of nodes) 3. Recalculate and repeat The example that we are using in this blog is the Zachary Karate club . It contains 34 members, and after an internal conflict, the club split into two. Our goal is to find if we can predict out how they split based on communities. import matplotlib.pyplot as plt % matplotlib inline import networkx as nx G = nx . karate_club_graph () nx . draw_kamada_kawai ( G , with_labels = True ) # Calculating the betweenness centrality btw_centrality = nx . algorithms . centrality . edge_betweenness_centrality ( G ) # Sorting based on the betweenness centraliy and displaying the first 10 edges. sorted ( btw_centrality . items (), key = lambda item : item [ 1 ], reverse = True )[ 0 : 10 ] [((0, 31), 0.1272599949070537), ((0, 6), 0.07813428401663695), ((0, 5), 0.07813428401663694), ((0, 2), 0.0777876807288572), ((0, 8), 0.07423959482783014), ((2, 32), 0.06898678663384543), ((13, 33), 0.06782389723566191), ((19, 33), 0.05938233879410351), ((0, 11), 0.058823529411764705), ((26, 33), 0.0542908072319837)] def girwan_newman ( G , no_of_components_to_split ): while ( no_of_components_to_split > nx . algorithms . components . number_connected_components ( G )): # Calculate the betweenness centrality btw_centrality = nx . algorithms . centrality . edge_betweenness_centrality ( G ) # sort based on betweenness centrality sorted_edges = sorted ( btw_centrality . items (), key = lambda item : item [ 1 ], reverse = True )[ 0 ] print ( 'Removing the edge' , sorted_edges ) # remove edge which has highest centrality G . remove_edge ( * sorted_edges [ 0 ]) # Check if graph is split if ( no_of_components_to_split <= nx . algorithms . components . number_connected_components ( G )): # Plot the graph with both the nodes having different colors nx . draw_spring ( G , with_labels = True ) # return list of nodes in each community list_of_nodes = [ c for c in sorted ( nx . connected_components ( G ), key = len , reverse = True )] return list_of_nodes G = nx . karate_club_graph () communities = girwan_newman ( G , 2 ) communities Removing the edge ((0, 31), 0.1272599949070537) Removing the edge ((0, 2), 0.11924273983097515) Removing the edge ((0, 8), 0.13782067605597018) Removing the edge ((13, 33), 0.14617273782105492) Removing the edge ((19, 33), 0.21966651886437982) Removing the edge ((2, 32), 0.1786195286195287) Removing the edge ((1, 30), 0.25601957954899124) Removing the edge ((1, 2), 0.1947415329768271) Removing the edge ((2, 3), 0.19191919191919182) Removing the edge ((2, 7), 0.25445632798573975) Removing the edge ((2, 13), 0.5080213903743315) [{2, 8, 9, 14, 15, 18, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33}, {0, 1, 3, 4, 5, 6, 7, 10, 11, 12, 13, 16, 17, 19, 21}] We can see that after removing 11 edges based on the maximum betweenness values, the club splits into two parts. This can be seen below. These groups matches with what actually happened in the karate club. # Reinstanciste G if necessary G = nx . karate_club_graph () # Colors for edges colors_for_edges = 'rbgoy' pos = nx . spring_layout ( G ) for i , nodes in enumerate ( communities ): for node in nodes : G . add_node ( node , node_color = colors_for_edges [ i ]) nx . draw_kamada_kawai ( G , with_labels = True , node_color = list ( nx . get_node_attributes ( G , 'node_color' ) . values ())) NetworkX has a inbuilt function to calculate the same split. karate_club_split = nx . algorithms . community . centrality . girvan_newman ( G ) karate_club_split_tuple = tuple ( sorted ( c ) for c in next ( karate_club_split )) karate_club_split_tuple ([0, 1, 3, 4, 5, 6, 7, 10, 11, 12, 13, 16, 17, 19, 21], [2, 8, 9, 14, 15, 18, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]) colors = 'rgb' for i , nodes in enumerate ( karate_club_split_tuple ): nx . draw_networkx_nodes ( G , pos = pos , nodelist = nodes , node_color = colors [ i ]) nx . draw_networkx_edges ( G , pos = pos ) Ratio cut method \u00b6 A more complex and advanced method is the ratio cut method. A cut in a graph is a set of edges, such that if we remove them, the network breaks up into two components. The procedure is as follows: 1. Find the minimum cut among all the pair of nodes 2. Repeat over each of the two parts cut The minimum cut can be found using Ratio Cut parameter. Minimum cut wil have the least ratio cut among all the other cuts. $$ Ratio\\,Cut\\, (P_{Red}) = \\frac{1}{number\\, of\\, partitions}\\times (\\frac{no\\, of\\, edges\\, in\\, partition1}{no\\, of\\, nodes\\, in\\, partition1} + \\frac{no\\, of\\, edges\\, in\\, partition2}{no\\, of\\, nodes\\, in\\, partition2} + ..) $$ This can be done quickly using an inbuilt function in NetworkX from networkx.algorithms import community communities = community . kernighan_lin_bisection ( G , max_iter = 100 ) pos = nx . spring_layout ( G ) nx . draw ( G , pos , with_labels = True , node_size = 100 , node_color = 'w' , node_shape = '.' ) for i in range ( len ( communities )): nx . draw_networkx_nodes ( G , pos , nodelist = communities [ i ], node_color = colors [ i ]) Other methods \u00b6 There are many other methods that can be used to identify communities, one of which is based on modularity. Modularity is defined as the \"distance\" from a random graph. essentially we will be comparing the number of edges with a community and the expected number of edges with a random graph. In NetworkX, we have a inbuilt function to implement this. # How many ideal nodes: Louvain package from community import community_louvain # https://en.wikipedia.org/wiki/Louvain_method community_louvain . best_partition ( G ) {0: 0, 1: 0, 2: 0, 3: 0, 4: 3, 5: 3, 6: 3, 7: 0, 8: 1, 9: 0, 10: 3, 11: 0, 12: 0, 13: 0, 14: 1, 15: 1, 16: 3, 17: 0, 18: 1, 19: 0, 20: 1, 21: 0, 22: 1, 23: 1, 24: 2, 25: 2, 26: 1, 27: 1, 28: 2, 29: 1, 30: 1, 31: 2, 32: 1, 33: 1}","title":"Community detection (Python)"},{"location":"Python/Community%20detection/#community-detection","text":"Author: Achyuthuni Sri Harsha A community is a group of people who are homogeneous in some features. Marketers want to find communities so that they can target them. It is not trivial to identify communities from a given network. Community detection deals with identifying networks based only on the network topology. nodes in the community are closer than the nodes outside the community. What is closeness? To define closeness, we need to 1. Define distance metric based on network topology 2. Quantify nodes in a community are closer wth each other than nodes outside the community There are two approaches, bottom-up and top-down.","title":"Community detection"},{"location":"Python/Community%20detection/#girwan-newman-algorithm","text":"The most popular algorithm for network community detection is the Girvan-Newman algorithm. It is a top down approach where we take the whole network and try to break it into two communities. This can be continued till the bottom. Steps: 1. Define betweenneess mesure for each edge 2. Find the edge with maximum betweenness and remove it (The edge most in-between in the network connects up most number of pairs of nodes) 3. Recalculate and repeat The example that we are using in this blog is the Zachary Karate club . It contains 34 members, and after an internal conflict, the club split into two. Our goal is to find if we can predict out how they split based on communities. import matplotlib.pyplot as plt % matplotlib inline import networkx as nx G = nx . karate_club_graph () nx . draw_kamada_kawai ( G , with_labels = True ) # Calculating the betweenness centrality btw_centrality = nx . algorithms . centrality . edge_betweenness_centrality ( G ) # Sorting based on the betweenness centraliy and displaying the first 10 edges. sorted ( btw_centrality . items (), key = lambda item : item [ 1 ], reverse = True )[ 0 : 10 ] [((0, 31), 0.1272599949070537), ((0, 6), 0.07813428401663695), ((0, 5), 0.07813428401663694), ((0, 2), 0.0777876807288572), ((0, 8), 0.07423959482783014), ((2, 32), 0.06898678663384543), ((13, 33), 0.06782389723566191), ((19, 33), 0.05938233879410351), ((0, 11), 0.058823529411764705), ((26, 33), 0.0542908072319837)] def girwan_newman ( G , no_of_components_to_split ): while ( no_of_components_to_split > nx . algorithms . components . number_connected_components ( G )): # Calculate the betweenness centrality btw_centrality = nx . algorithms . centrality . edge_betweenness_centrality ( G ) # sort based on betweenness centrality sorted_edges = sorted ( btw_centrality . items (), key = lambda item : item [ 1 ], reverse = True )[ 0 ] print ( 'Removing the edge' , sorted_edges ) # remove edge which has highest centrality G . remove_edge ( * sorted_edges [ 0 ]) # Check if graph is split if ( no_of_components_to_split <= nx . algorithms . components . number_connected_components ( G )): # Plot the graph with both the nodes having different colors nx . draw_spring ( G , with_labels = True ) # return list of nodes in each community list_of_nodes = [ c for c in sorted ( nx . connected_components ( G ), key = len , reverse = True )] return list_of_nodes G = nx . karate_club_graph () communities = girwan_newman ( G , 2 ) communities Removing the edge ((0, 31), 0.1272599949070537) Removing the edge ((0, 2), 0.11924273983097515) Removing the edge ((0, 8), 0.13782067605597018) Removing the edge ((13, 33), 0.14617273782105492) Removing the edge ((19, 33), 0.21966651886437982) Removing the edge ((2, 32), 0.1786195286195287) Removing the edge ((1, 30), 0.25601957954899124) Removing the edge ((1, 2), 0.1947415329768271) Removing the edge ((2, 3), 0.19191919191919182) Removing the edge ((2, 7), 0.25445632798573975) Removing the edge ((2, 13), 0.5080213903743315) [{2, 8, 9, 14, 15, 18, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33}, {0, 1, 3, 4, 5, 6, 7, 10, 11, 12, 13, 16, 17, 19, 21}] We can see that after removing 11 edges based on the maximum betweenness values, the club splits into two parts. This can be seen below. These groups matches with what actually happened in the karate club. # Reinstanciste G if necessary G = nx . karate_club_graph () # Colors for edges colors_for_edges = 'rbgoy' pos = nx . spring_layout ( G ) for i , nodes in enumerate ( communities ): for node in nodes : G . add_node ( node , node_color = colors_for_edges [ i ]) nx . draw_kamada_kawai ( G , with_labels = True , node_color = list ( nx . get_node_attributes ( G , 'node_color' ) . values ())) NetworkX has a inbuilt function to calculate the same split. karate_club_split = nx . algorithms . community . centrality . girvan_newman ( G ) karate_club_split_tuple = tuple ( sorted ( c ) for c in next ( karate_club_split )) karate_club_split_tuple ([0, 1, 3, 4, 5, 6, 7, 10, 11, 12, 13, 16, 17, 19, 21], [2, 8, 9, 14, 15, 18, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]) colors = 'rgb' for i , nodes in enumerate ( karate_club_split_tuple ): nx . draw_networkx_nodes ( G , pos = pos , nodelist = nodes , node_color = colors [ i ]) nx . draw_networkx_edges ( G , pos = pos )","title":"Girwan Newman Algorithm"},{"location":"Python/Community%20detection/#ratio-cut-method","text":"A more complex and advanced method is the ratio cut method. A cut in a graph is a set of edges, such that if we remove them, the network breaks up into two components. The procedure is as follows: 1. Find the minimum cut among all the pair of nodes 2. Repeat over each of the two parts cut The minimum cut can be found using Ratio Cut parameter. Minimum cut wil have the least ratio cut among all the other cuts. $$ Ratio\\,Cut\\, (P_{Red}) = \\frac{1}{number\\, of\\, partitions}\\times (\\frac{no\\, of\\, edges\\, in\\, partition1}{no\\, of\\, nodes\\, in\\, partition1} + \\frac{no\\, of\\, edges\\, in\\, partition2}{no\\, of\\, nodes\\, in\\, partition2} + ..) $$ This can be done quickly using an inbuilt function in NetworkX from networkx.algorithms import community communities = community . kernighan_lin_bisection ( G , max_iter = 100 ) pos = nx . spring_layout ( G ) nx . draw ( G , pos , with_labels = True , node_size = 100 , node_color = 'w' , node_shape = '.' ) for i in range ( len ( communities )): nx . draw_networkx_nodes ( G , pos , nodelist = communities [ i ], node_color = colors [ i ])","title":"Ratio cut method"},{"location":"Python/Community%20detection/#other-methods","text":"There are many other methods that can be used to identify communities, one of which is based on modularity. Modularity is defined as the \"distance\" from a random graph. essentially we will be comparing the number of edges with a community and the expected number of edges with a random graph. In NetworkX, we have a inbuilt function to implement this. # How many ideal nodes: Louvain package from community import community_louvain # https://en.wikipedia.org/wiki/Louvain_method community_louvain . best_partition ( G ) {0: 0, 1: 0, 2: 0, 3: 0, 4: 3, 5: 3, 6: 3, 7: 0, 8: 1, 9: 0, 10: 3, 11: 0, 12: 0, 13: 0, 14: 1, 15: 1, 16: 3, 17: 0, 18: 1, 19: 0, 20: 1, 21: 0, 22: 1, 23: 1, 24: 2, 25: 2, 26: 1, 27: 1, 28: 2, 29: 1, 30: 1, 31: 2, 32: 1, 33: 1}","title":"Other methods"},{"location":"Python/Demonstrating%20online%20learning/","text":"MathJax.Hub.Config({ extensions: [\"tex2jax.js\",\"TeX/AMSmath.js\",\"TeX/AMSsymbols.js\"], jax: [\"input/TeX\", \"output/HTML-CSS\"], tex2jax: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ], }, \"HTML-CSS\": { availableFonts: [\"TeX\"] } }); Streaming machine learning \u00b6 Author: Achyuthuni Sri Harsha Batch learning : In batch machine learning, we use one dataset to train a model, and we deploy the model to predict on new data. This assumes that the dataset in which the model is trained is a proper representative sample of the population. This model is assumed as a static object. In order to learn from new data, the model has to be re-trained from scratch. This is the most common form of deploying models. Online learning : Some of the machine learning models that we know can be modified to learn on a single datapoint (row). When we can learn from a single datapoint, we can learn incrementally from new datapoints. Data is considered as a stream. Once the model is trained, we need not store the historic training set. The model is also more up to date. If the data's distribution happens over time, the model will be able to handle it (drift)[1]. Where can we use them? They are most useful in scenarios where new data and patterns are constantly arriving. eg: 1. Spam filtering 2. Recommendation engines (news feed predictions) 3. Financial transactions 4. Low compute power (only one data point exists in memory as we train using one data point only) Issues 1. New and not a lot of experience 2. Very few tools and packages 3. All algorithms do not have an online version. Kernel SVMs are impossible to fit on a streaming dataset. Likewise, CART and ID3 decision trees can\u2019t be trained online. However, lesser-known online approximations exist, such as random Fourier features for SVMs and Hoeffding trees for decision trees. 4. Slower than batch learning to reach steady state in real life (It is computationally faster by more than an order of magnitude) 5. Do not guarantee that models learnt are similar to the ones obtained in batch mode(some models). Some models do not guarentee of achieving steady state. 6. Overfitting Similarities We have the same limitations of machine learning, such as: 1. We need to do proper preprocessing 2. We need to do feature engineering as usual 3. The concepts of ensembles, feature extraction, feature selection, imbalanced classes, multiclass etc are same Preprocessing steps \u00b6 How do we preprocess data when we are streaming? How do we impute the null values by mean if we do not have complete data? How do we identify outliers when working on one row at a time? How can we do one-hot encoding when we don't know what classes are present overall? The package river is a handy package for online learning. It has a lot of pre-defined preprocessing functions. Let us look at some of them: from river import preprocessing dir ( preprocessing )[ 0 : 12 ] ['AdaptiveStandardScaler', 'Binarizer', 'FeatureHasher', 'LDA', 'MaxAbsScaler', 'MinMaxScaler', 'Normalizer', 'OneHotEncoder', 'PreviousImputer', 'RobustScaler', 'StandardScaler', 'StatImputer'] There are six functions for scaling and normalizing data. They are: 1. AdaptiveStandardScalar 2. MaxAbsScalar 3. MinMaxScalar 4. Normalizer 5. RobustScalar 6. StandardScaler For example, let us look at the documentation for Standard Scaler It scales the data to have zero mean and unit variance. Under the hood, a running mean and a running variance are maintained. The scaling is slightly different than when scaling the data in batch because the exact means and variances are not known in advance. However, this doesn't have a detrimental impact on performance in the long run. Let us look at an example: import numpy as np import matplotlib.pyplot as plt import pandas as pd import random random . seed ( 1995 ) from sklearn.datasets import load_iris iris = load_iris () data1 = pd . DataFrame ( data = np . c_ [ iris [ 'data' ], iris [ 'target' ]], columns = iris [ 'feature_names' ] + [ 'target' ]) data1 = data1 . query ( 'target < 2' ) . sample ( frac = 1 ) def data_feed ( df_datafeed ): # Generator function to give the next candidate for _ctr in range ( len ( df_datafeed )): yield df_datafeed . iloc [ _ctr ] from river import compose from river import linear_model from river import preprocessing preprocessing_model = compose . Pipeline ( preprocessing . StandardScaler (), linear_model . LogisticRegression ()) data_stream = data_feed ( data1 . loc [:, data1 . columns != 'target' ]) for n in range ( 10 ): data_point = next ( data_stream ) . to_frame () . transpose () transformed_data = preprocessing_model . transform_one ( data_point . iloc [ 0 ,:]) print ( '------------------------' ) print ( data_point ) print ( transformed_data ) ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 67 5.8 2.7 4.1 1.0 {'sepal length (cm)': 0.0, 'sepal width (cm)': 0.0, 'petal length (cm)': 0.0, 'petal width (cm)': 0.0} ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 96 5.7 2.9 4.2 1.3 {'sepal length (cm)': -1.0, 'sepal width (cm)': 1.000000000000001, 'petal length (cm)': 0.9999999999999956, 'petal width (cm)': 1.0000000000000004} ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 19 5.1 3.8 1.5 0.3 {'sepal length (cm)': -1.4018260516446994, 'sepal width (cm)': 1.3934660285832352, 'petal length (cm)': -1.4134589797160622, 'petal width (cm)': -1.352447383098741} ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 5 5.4 3.9 1.7 0.4 {'sepal length (cm)': -0.36514837167010933, 'sepal width (cm)': 1.0830277015004253, 'petal length (cm)': -0.9198021534721369, 'petal width (cm)': -0.8427009716003844} ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 64 5.6 2.9 3.6 1.3 {'sepal length (cm)': 0.32232918561015234, 'sepal width (cm)': -0.6740938478604231, 'petal length (cm)': 0.49202037860731096, 'petal width (cm)': 1.019130320146575} ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 87 6.3 2.3 4.4 1.3 {'sepal length (cm)': 1.7636409634199253, 'sepal width (cm)': -1.3539553245018423, 'petal length (cm)': 0.9642101587457326, 'petal width (cm)': 0.8589556903873334} ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 80 5.5 2.4 3.8 1.1 {'sepal length (cm)': -0.37242264987106416, 'sepal width (cm)': -0.9985160994941403, 'petal length (cm)': 0.4205955120960296, 'petal width (cm)': 0.3575992699260759} ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 98 5.1 2.5 3.0 1.1 {'sepal length (cm)': -1.259494647504126, 'sepal width (cm)': -0.743358098059264, 'petal length (cm)': -0.27274857904612027, 'petal width (cm)': 0.331861655799986} ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 15 5.7 4.4 1.5 0.4 {'sepal length (cm)': 0.3503113654141663, 'sepal width (cm)': 1.8442002991885438, 'petal length (cm)': -1.3918304919158082, 'petal width (cm)': -1.282736189026269} ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 23 5.1 3.3 1.7 0.5 {'sepal length (cm)': -1.1921469919530685, 'sepal width (cm)': 0.28047526828011227, 'petal length (cm)': -1.077226017628646, 'petal width (cm)': -0.9305415914315355} Apart from preprocessing, using river, we can perform 1. Feature extraction/selection 2. Ensembles 3. Storing running statistics 4. Building regression and classification models 5. Time series 6. Anamoly detection 7. Clustering There are 6 types of machine learning models that we can build using river. They are: 1. Linear based (linear and logistic regression) 2. Tree based (Decision trees, Hoeffding trees) 3. Nearest neighbours based 4. Bayesian models 5. Neural Networks 6. Ensemble based models 7. Others from river import linear_model , naive_bayes , tree , neural_net , neighbors , expert , ensemble print ( 'Linear models' , dir ( linear_model )[ 0 : 7 ]) print ( 'Tree based modles' , dir ( tree )[ 0 : 6 ]) print ( 'Bayesian models' , dir ( naive_bayes )[ 0 : 4 ]) print ( 'Specialised models' , dir ( expert )[ 0 : 6 ]) print ( 'Ensemble mmodels' , dir ( ensemble )[ 0 : 8 ]) Linear models ['ALMAClassifier', 'LinearRegression', 'LogisticRegression', 'PAClassifier', 'PARegressor', 'Perceptron', 'SoftmaxRegression'] Tree based modles ['ExtremelyFastDecisionTreeClassifier', 'HoeffdingAdaptiveTreeClassifier', 'HoeffdingAdaptiveTreeRegressor', 'HoeffdingTreeClassifier', 'HoeffdingTreeRegressor', 'LabelCombinationHoeffdingTreeClassifier'] Bayesian models ['BernoulliNB', 'ComplementNB', 'GaussianNB', 'MultinomialNB'] Specialised models ['EWARegressor', 'EpsilonGreedyRegressor', 'StackingClassifier', 'SuccessiveHalvingClassifier', 'SuccessiveHalvingRegressor', 'UCBRegressor'] Ensemble mmodels ['ADWINBaggingClassifier', 'AdaBoostClassifier', 'AdaptiveRandomForestClassifier', 'AdaptiveRandomForestRegressor', 'BaggingClassifier', 'BaggingRegressor', 'LeveragingBaggingClassifier', 'SRPClassifier'] Modelling (under the hood) \u00b6 There are two types of streaming models, those which are entirely streaming and pseudo-online models. Pseudo-online models use a small batch of data to build the models, while completely online models build the models using only one data point.[3] Pseudo online models \u00b6 There are many theorems in statistics which can help us to bound the error of a metric between two variables. Hoeffding bound is one such theorem. \"Consider a real-valued random variable r whose range is R (e.g., for a probability the range is one, and for an information gain the range is log c, where c is the number of classes). Suppose we have made n independent observations of this variable, and computed their mean \\(\\bar r\\) . The Hoeffding bound states that, with probability 1 \u2212 \u03b4, the true mean of the variable is at least \\(\\bar r \u2212 \\epsilon\\) , where \\(\\epsilon = \\sqrt{\\frac{R^2ln(\\frac{1}{\\lambda})}{2n}}\\) .\" This is useful while creating decision trees. Using hoeffding bound, we can identify which feature should we use to split the tree. We can find if a variable has sufficient gini index (or other metrics) greater (lesser) than other variables with a probability below a cutoff probability and split the tree based on that metric. This can be acheived with very few data points which can be deleted after splitting and creating child nodes. This algorithm is called Hoeffding trees algorithm. Completely online models \u00b6 How can we update a model using only one data point? Let us look at Gradient Descent to understand this. In gradient descent, we want to minimise a convex loss function(MSE, regret, etc). As an example, consider the function \\(h(x) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ..\\) (or any convex function). The mean squared error is \\(J(\\theta) = \\frac{1}{2} \\sum_{i=1}^{m} (y_{(i)}-h_{\\theta}(x_{(i)}))^{2}\\) . In gradient descent, we find the \\(\\beta_i\\) that minimises \\(J(\\theta)\\) . In batch model, we consider all the data points that exist to identfy the optimal solution. In streaming learning, we initialise the \\(\\beta_i\\) as 0 and keep incrementally changing the \\(\\beta_i\\) 's using \\( \\(\\beta_{i, t} = \\beta_{i, t-1} + \\alpha \\times \\frac{\\partial}{\\partial \\theta_{j}} J(\\theta)\\) \\) Steps in updating the model \u00b6 In online learning, there are 4 steps[3]. For every new datapoint, we will recursively run the following steps. For \\({\\displaystyle t=1,2,...,T}\\) Learner receives input \\({\\displaystyle x_{t}}\\) Learner outputs \\({\\displaystyle w_{t}}\\) from a fixed convex set \\({\\displaystyle S}\\) Nature sends back a convex loss function \\({\\displaystyle v_{t}:S\\rightarrow \\mathbb {R} }\\) . Learner suffers loss \\({\\displaystyle v_{t}(w_{t})}\\) and updates its model. Implimentation using river \u00b6 Every online machine learning model has the following basic 5 functions: dir ( linear_model . LogisticRegression )[ 50 : 55 ] ['learn_many', 'learn_one', 'predict_many', 'predict_one', 'predict_proba_many'] As the name mentions, learn_one and predict_one learn and predict from one data point, learn_many, predict_many and predict_prob_many learn and predict using multiple data points. River is the result of a merger between creme and scikit-multiflow, and the remaining functions in the library follow a similar pattern to the same. Building a model \u00b6 Using the same data as above, let us build a sample model using river for streaming. from river import compose river_model = compose . Pipeline ( preprocessing . StandardScaler (), tree . HoeffdingTreeClassifier () ) from sklearn.metrics import accuracy_score acc_scores = [] cols_x = [ 'sepal length (cm)' , 'sepal width (cm)' , 'petal length (cm)' , 'petal width (cm)' ] def compute_accuracy ( data , model , truth_col ): predict_all = data . apply ( lambda row : model . predict_one ( row ), axis = 1 ) acc_scores . append ( accuracy_score ( np . array ( predict_all ), data [ truth_col ])) print ( 'Accuracy is ' , acc_scores [ - 1 ]) data_stream = data_feed ( data1 ) for n in range ( 20 ): data_point = next ( data_stream ) print ( data_point ) if ( n > 1 ): predict_one = river_model . predict_one ( data_point [ cols_x ]) print ( 'Current_prediction' , predict_one , data_point [ 'target' ]) river_model . learn_one ( data_point [ cols_x ], data_point [ 'target' ]) compute_accuracy ( data1 , river_model , 'target' ) print ( '------------------------' ) sepal length (cm) 5.8 sepal width (cm) 2.7 petal length (cm) 4.1 petal width (cm) 1.0 target 1.0 Name: 67, dtype: float64 Accuracy is 0.5 ------------------------ sepal length (cm) 5.7 sepal width (cm) 2.9 petal length (cm) 4.2 petal width (cm) 1.3 target 1.0 Name: 96, dtype: float64 Accuracy is 0.5 ------------------------ sepal length (cm) 5.1 sepal width (cm) 3.8 petal length (cm) 1.5 petal width (cm) 0.3 target 0.0 Name: 19, dtype: float64 Current_prediction 1.0 0.0 Accuracy is 0.5 ------------------------ sepal length (cm) 5.4 sepal width (cm) 3.9 petal length (cm) 1.7 petal width (cm) 0.4 target 0.0 Name: 5, dtype: float64 Current_prediction 1.0 0.0 Accuracy is 0.71 ------------------------ sepal length (cm) 5.6 sepal width (cm) 2.9 petal length (cm) 3.6 petal width (cm) 1.3 target 1.0 Name: 64, dtype: float64 Current_prediction 1.0 1.0 Accuracy is 0.79 ------------------------ sepal length (cm) 6.3 sepal width (cm) 2.3 petal length (cm) 4.4 petal width (cm) 1.3 target 1.0 Name: 87, dtype: float64 Current_prediction 1.0 1.0 Accuracy is 0.69 ------------------------ sepal length (cm) 5.5 sepal width (cm) 2.4 petal length (cm) 3.8 petal width (cm) 1.1 target 1.0 Name: 80, dtype: float64 Current_prediction 1.0 1.0 Accuracy is 0.69 ------------------------ sepal length (cm) 5.1 sepal width (cm) 2.5 petal length (cm) 3.0 petal width (cm) 1.1 target 1.0 Name: 98, dtype: float64 Current_prediction 1.0 1.0 Accuracy is 0.69 ------------------------ sepal length (cm) 5.7 sepal width (cm) 4.4 petal length (cm) 1.5 petal width (cm) 0.4 target 0.0 Name: 15, dtype: float64 Current_prediction 1.0 0.0 Accuracy is 0.99 ------------------------ sepal length (cm) 5.1 sepal width (cm) 3.3 petal length (cm) 1.7 petal width (cm) 0.5 target 0.0 Name: 23, dtype: float64 Current_prediction 0.0 0.0 Accuracy is 1.0 ------------------------ sepal length (cm) 4.8 sepal width (cm) 3.1 petal length (cm) 1.6 petal width (cm) 0.2 target 0.0 Name: 30, dtype: float64 Current_prediction 0.0 0.0 Accuracy is 1.0 ------------------------ sepal length (cm) 5.5 sepal width (cm) 2.5 petal length (cm) 4.0 petal width (cm) 1.3 target 1.0 Name: 89, dtype: float64 Current_prediction 1.0 1.0 Accuracy is 1.0 ------------------------ sepal length (cm) 4.5 sepal width (cm) 2.3 petal length (cm) 1.3 petal width (cm) 0.3 target 0.0 Name: 41, dtype: float64 Current_prediction 0.0 0.0 Accuracy is 1.0 ------------------------ sepal length (cm) 6.1 sepal width (cm) 3.0 petal length (cm) 4.6 petal width (cm) 1.4 target 1.0 Name: 91, dtype: float64 Current_prediction 1.0 1.0 Accuracy is 1.0 ------------------------ sepal length (cm) 5.2 sepal width (cm) 3.5 petal length (cm) 1.5 petal width (cm) 0.2 target 0.0 Name: 27, dtype: float64 Current_prediction 0.0 0.0 Accuracy is 1.0 ------------------------ sepal length (cm) 7.0 sepal width (cm) 3.2 petal length (cm) 4.7 petal width (cm) 1.4 target 1.0 Name: 50, dtype: float64 Current_prediction 1.0 1.0 Accuracy is 1.0 ------------------------ sepal length (cm) 5.5 sepal width (cm) 4.2 petal length (cm) 1.4 petal width (cm) 0.2 target 0.0 Name: 33, dtype: float64 Current_prediction 0.0 0.0 Accuracy is 1.0 ------------------------ sepal length (cm) 5.9 sepal width (cm) 3.2 petal length (cm) 4.8 petal width (cm) 1.8 target 1.0 Name: 70, dtype: float64 Current_prediction 1.0 1.0 Accuracy is 1.0 ------------------------ sepal length (cm) 6.6 sepal width (cm) 3.0 petal length (cm) 4.4 petal width (cm) 1.4 target 1.0 Name: 75, dtype: float64 Current_prediction 1.0 1.0 Accuracy is 1.0 ------------------------ sepal length (cm) 4.4 sepal width (cm) 3.0 petal length (cm) 1.3 petal width (cm) 0.2 target 0.0 Name: 38, dtype: float64 Current_prediction 0.0 0.0 Accuracy is 1.0 ------------------------ import matplotlib.pyplot as plt % matplotlib inline plt . plot ( acc_scores ) plt . ylabel ( 'Accuracy score' ) plt . xlabel ( 'Sample #' ) <matplotlib.text.Text at 0x340c5208> river_model2 = compose . Pipeline ( preprocessing . StandardScaler (), linear_model . LogisticRegression () ) acc_scores1 = [] def compute_accuracy ( data , model , truth_col ): predict_all = data . apply ( lambda row : model . predict_one ( row ), axis = 1 ) acc_scores1 . append ( accuracy_score ( np . array ( predict_all ), data [ truth_col ])) print ( 'Accuracy is ' , acc_scores1 [ - 1 ]) data_stream = data_feed ( data1 ) for n in range ( 10 ): data_point = next ( data_stream ) print ( data_point ) if ( n > 1 ): predict_one = river_model2 . predict_one ( data_point [ cols_x ]) print ( 'Current_prediction' , predict_one , data_point [ 'target' ]) river_model2 . learn_one ( data_point [ cols_x ], data_point [ 'target' ]) compute_accuracy ( data1 , river_model2 , 'target' ) print ( '------------------------' ) sepal length (cm) 5.8 sepal width (cm) 2.7 petal length (cm) 4.1 petal width (cm) 1.0 target 1.0 Name: 67, dtype: float64 Accuracy is 0.5 ------------------------ sepal length (cm) 5.7 sepal width (cm) 2.9 petal length (cm) 4.2 petal width (cm) 1.3 target 1.0 Name: 96, dtype: float64 Accuracy is 0.94 ------------------------ sepal length (cm) 5.1 sepal width (cm) 3.8 petal length (cm) 1.5 petal width (cm) 0.3 target 0.0 Name: 19, dtype: float64 Current_prediction False 0.0 Accuracy is 1.0 ------------------------ sepal length (cm) 5.4 sepal width (cm) 3.9 petal length (cm) 1.7 petal width (cm) 0.4 target 0.0 Name: 5, dtype: float64 Current_prediction False 0.0 Accuracy is 1.0 ------------------------ sepal length (cm) 5.6 sepal width (cm) 2.9 petal length (cm) 3.6 petal width (cm) 1.3 target 1.0 Name: 64, dtype: float64 Current_prediction True 1.0 Accuracy is 1.0 ------------------------ sepal length (cm) 6.3 sepal width (cm) 2.3 petal length (cm) 4.4 petal width (cm) 1.3 target 1.0 Name: 87, dtype: float64 Current_prediction True 1.0 Accuracy is 1.0 ------------------------ sepal length (cm) 5.5 sepal width (cm) 2.4 petal length (cm) 3.8 petal width (cm) 1.1 target 1.0 Name: 80, dtype: float64 Current_prediction True 1.0 Accuracy is 0.99 ------------------------ sepal length (cm) 5.1 sepal width (cm) 2.5 petal length (cm) 3.0 petal width (cm) 1.1 target 1.0 Name: 98, dtype: float64 Current_prediction True 1.0 Accuracy is 0.99 ------------------------ sepal length (cm) 5.7 sepal width (cm) 4.4 petal length (cm) 1.5 petal width (cm) 0.4 target 0.0 Name: 15, dtype: float64 Current_prediction False 0.0 Accuracy is 0.99 ------------------------ sepal length (cm) 5.1 sepal width (cm) 3.3 petal length (cm) 1.7 petal width (cm) 0.5 target 0.0 Name: 23, dtype: float64 Current_prediction False 0.0 Accuracy is 0.99 ------------------------ import matplotlib.pyplot as plt % matplotlib inline plt . plot ( acc_scores1 ) plt . ylabel ( 'Accuracy score' ) plt . xlabel ( 'Sample #' ) <matplotlib.text.Text at 0x34d304a8> First published on Rolls Royce data sciecne blogs by Harsha Achyuthuni. References \u00b6 Introductory material: https://towardsdatascience.com/machine-learning-for-streaming-data-with-creme-dacf5fb469df Hoeffding Trees: https://homes.cs.washington.edu/~pedrod/papers/kdd00.pdf Modelling under the hood: https://en.wikipedia.org/wiki/Online_machine_learning River Git: https://github.com/online-ml/river River installation steps: https://riverml.xyz/dev/getting-started/installation/ River documentation: https://riverml.xyz/dev/api/overview/ Batch decision trees: https://www.harshaash.com/cart-classification/","title":"Streaming Machine Learning (Python)"},{"location":"Python/Demonstrating%20online%20learning/#streaming-machine-learning","text":"Author: Achyuthuni Sri Harsha Batch learning : In batch machine learning, we use one dataset to train a model, and we deploy the model to predict on new data. This assumes that the dataset in which the model is trained is a proper representative sample of the population. This model is assumed as a static object. In order to learn from new data, the model has to be re-trained from scratch. This is the most common form of deploying models. Online learning : Some of the machine learning models that we know can be modified to learn on a single datapoint (row). When we can learn from a single datapoint, we can learn incrementally from new datapoints. Data is considered as a stream. Once the model is trained, we need not store the historic training set. The model is also more up to date. If the data's distribution happens over time, the model will be able to handle it (drift)[1]. Where can we use them? They are most useful in scenarios where new data and patterns are constantly arriving. eg: 1. Spam filtering 2. Recommendation engines (news feed predictions) 3. Financial transactions 4. Low compute power (only one data point exists in memory as we train using one data point only) Issues 1. New and not a lot of experience 2. Very few tools and packages 3. All algorithms do not have an online version. Kernel SVMs are impossible to fit on a streaming dataset. Likewise, CART and ID3 decision trees can\u2019t be trained online. However, lesser-known online approximations exist, such as random Fourier features for SVMs and Hoeffding trees for decision trees. 4. Slower than batch learning to reach steady state in real life (It is computationally faster by more than an order of magnitude) 5. Do not guarantee that models learnt are similar to the ones obtained in batch mode(some models). Some models do not guarentee of achieving steady state. 6. Overfitting Similarities We have the same limitations of machine learning, such as: 1. We need to do proper preprocessing 2. We need to do feature engineering as usual 3. The concepts of ensembles, feature extraction, feature selection, imbalanced classes, multiclass etc are same","title":"Streaming machine learning"},{"location":"Python/Demonstrating%20online%20learning/#preprocessing-steps","text":"How do we preprocess data when we are streaming? How do we impute the null values by mean if we do not have complete data? How do we identify outliers when working on one row at a time? How can we do one-hot encoding when we don't know what classes are present overall? The package river is a handy package for online learning. It has a lot of pre-defined preprocessing functions. Let us look at some of them: from river import preprocessing dir ( preprocessing )[ 0 : 12 ] ['AdaptiveStandardScaler', 'Binarizer', 'FeatureHasher', 'LDA', 'MaxAbsScaler', 'MinMaxScaler', 'Normalizer', 'OneHotEncoder', 'PreviousImputer', 'RobustScaler', 'StandardScaler', 'StatImputer'] There are six functions for scaling and normalizing data. They are: 1. AdaptiveStandardScalar 2. MaxAbsScalar 3. MinMaxScalar 4. Normalizer 5. RobustScalar 6. StandardScaler For example, let us look at the documentation for Standard Scaler It scales the data to have zero mean and unit variance. Under the hood, a running mean and a running variance are maintained. The scaling is slightly different than when scaling the data in batch because the exact means and variances are not known in advance. However, this doesn't have a detrimental impact on performance in the long run. Let us look at an example: import numpy as np import matplotlib.pyplot as plt import pandas as pd import random random . seed ( 1995 ) from sklearn.datasets import load_iris iris = load_iris () data1 = pd . DataFrame ( data = np . c_ [ iris [ 'data' ], iris [ 'target' ]], columns = iris [ 'feature_names' ] + [ 'target' ]) data1 = data1 . query ( 'target < 2' ) . sample ( frac = 1 ) def data_feed ( df_datafeed ): # Generator function to give the next candidate for _ctr in range ( len ( df_datafeed )): yield df_datafeed . iloc [ _ctr ] from river import compose from river import linear_model from river import preprocessing preprocessing_model = compose . Pipeline ( preprocessing . StandardScaler (), linear_model . LogisticRegression ()) data_stream = data_feed ( data1 . loc [:, data1 . columns != 'target' ]) for n in range ( 10 ): data_point = next ( data_stream ) . to_frame () . transpose () transformed_data = preprocessing_model . transform_one ( data_point . iloc [ 0 ,:]) print ( '------------------------' ) print ( data_point ) print ( transformed_data ) ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 67 5.8 2.7 4.1 1.0 {'sepal length (cm)': 0.0, 'sepal width (cm)': 0.0, 'petal length (cm)': 0.0, 'petal width (cm)': 0.0} ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 96 5.7 2.9 4.2 1.3 {'sepal length (cm)': -1.0, 'sepal width (cm)': 1.000000000000001, 'petal length (cm)': 0.9999999999999956, 'petal width (cm)': 1.0000000000000004} ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 19 5.1 3.8 1.5 0.3 {'sepal length (cm)': -1.4018260516446994, 'sepal width (cm)': 1.3934660285832352, 'petal length (cm)': -1.4134589797160622, 'petal width (cm)': -1.352447383098741} ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 5 5.4 3.9 1.7 0.4 {'sepal length (cm)': -0.36514837167010933, 'sepal width (cm)': 1.0830277015004253, 'petal length (cm)': -0.9198021534721369, 'petal width (cm)': -0.8427009716003844} ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 64 5.6 2.9 3.6 1.3 {'sepal length (cm)': 0.32232918561015234, 'sepal width (cm)': -0.6740938478604231, 'petal length (cm)': 0.49202037860731096, 'petal width (cm)': 1.019130320146575} ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 87 6.3 2.3 4.4 1.3 {'sepal length (cm)': 1.7636409634199253, 'sepal width (cm)': -1.3539553245018423, 'petal length (cm)': 0.9642101587457326, 'petal width (cm)': 0.8589556903873334} ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 80 5.5 2.4 3.8 1.1 {'sepal length (cm)': -0.37242264987106416, 'sepal width (cm)': -0.9985160994941403, 'petal length (cm)': 0.4205955120960296, 'petal width (cm)': 0.3575992699260759} ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 98 5.1 2.5 3.0 1.1 {'sepal length (cm)': -1.259494647504126, 'sepal width (cm)': -0.743358098059264, 'petal length (cm)': -0.27274857904612027, 'petal width (cm)': 0.331861655799986} ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 15 5.7 4.4 1.5 0.4 {'sepal length (cm)': 0.3503113654141663, 'sepal width (cm)': 1.8442002991885438, 'petal length (cm)': -1.3918304919158082, 'petal width (cm)': -1.282736189026269} ------------------------ sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 23 5.1 3.3 1.7 0.5 {'sepal length (cm)': -1.1921469919530685, 'sepal width (cm)': 0.28047526828011227, 'petal length (cm)': -1.077226017628646, 'petal width (cm)': -0.9305415914315355} Apart from preprocessing, using river, we can perform 1. Feature extraction/selection 2. Ensembles 3. Storing running statistics 4. Building regression and classification models 5. Time series 6. Anamoly detection 7. Clustering There are 6 types of machine learning models that we can build using river. They are: 1. Linear based (linear and logistic regression) 2. Tree based (Decision trees, Hoeffding trees) 3. Nearest neighbours based 4. Bayesian models 5. Neural Networks 6. Ensemble based models 7. Others from river import linear_model , naive_bayes , tree , neural_net , neighbors , expert , ensemble print ( 'Linear models' , dir ( linear_model )[ 0 : 7 ]) print ( 'Tree based modles' , dir ( tree )[ 0 : 6 ]) print ( 'Bayesian models' , dir ( naive_bayes )[ 0 : 4 ]) print ( 'Specialised models' , dir ( expert )[ 0 : 6 ]) print ( 'Ensemble mmodels' , dir ( ensemble )[ 0 : 8 ]) Linear models ['ALMAClassifier', 'LinearRegression', 'LogisticRegression', 'PAClassifier', 'PARegressor', 'Perceptron', 'SoftmaxRegression'] Tree based modles ['ExtremelyFastDecisionTreeClassifier', 'HoeffdingAdaptiveTreeClassifier', 'HoeffdingAdaptiveTreeRegressor', 'HoeffdingTreeClassifier', 'HoeffdingTreeRegressor', 'LabelCombinationHoeffdingTreeClassifier'] Bayesian models ['BernoulliNB', 'ComplementNB', 'GaussianNB', 'MultinomialNB'] Specialised models ['EWARegressor', 'EpsilonGreedyRegressor', 'StackingClassifier', 'SuccessiveHalvingClassifier', 'SuccessiveHalvingRegressor', 'UCBRegressor'] Ensemble mmodels ['ADWINBaggingClassifier', 'AdaBoostClassifier', 'AdaptiveRandomForestClassifier', 'AdaptiveRandomForestRegressor', 'BaggingClassifier', 'BaggingRegressor', 'LeveragingBaggingClassifier', 'SRPClassifier']","title":"Preprocessing steps"},{"location":"Python/Demonstrating%20online%20learning/#modelling-under-the-hood","text":"There are two types of streaming models, those which are entirely streaming and pseudo-online models. Pseudo-online models use a small batch of data to build the models, while completely online models build the models using only one data point.[3]","title":"Modelling (under the hood)"},{"location":"Python/Demonstrating%20online%20learning/#pseudo-online-models","text":"There are many theorems in statistics which can help us to bound the error of a metric between two variables. Hoeffding bound is one such theorem. \"Consider a real-valued random variable r whose range is R (e.g., for a probability the range is one, and for an information gain the range is log c, where c is the number of classes). Suppose we have made n independent observations of this variable, and computed their mean \\(\\bar r\\) . The Hoeffding bound states that, with probability 1 \u2212 \u03b4, the true mean of the variable is at least \\(\\bar r \u2212 \\epsilon\\) , where \\(\\epsilon = \\sqrt{\\frac{R^2ln(\\frac{1}{\\lambda})}{2n}}\\) .\" This is useful while creating decision trees. Using hoeffding bound, we can identify which feature should we use to split the tree. We can find if a variable has sufficient gini index (or other metrics) greater (lesser) than other variables with a probability below a cutoff probability and split the tree based on that metric. This can be acheived with very few data points which can be deleted after splitting and creating child nodes. This algorithm is called Hoeffding trees algorithm.","title":"Pseudo online models"},{"location":"Python/Demonstrating%20online%20learning/#completely-online-models","text":"How can we update a model using only one data point? Let us look at Gradient Descent to understand this. In gradient descent, we want to minimise a convex loss function(MSE, regret, etc). As an example, consider the function \\(h(x) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ..\\) (or any convex function). The mean squared error is \\(J(\\theta) = \\frac{1}{2} \\sum_{i=1}^{m} (y_{(i)}-h_{\\theta}(x_{(i)}))^{2}\\) . In gradient descent, we find the \\(\\beta_i\\) that minimises \\(J(\\theta)\\) . In batch model, we consider all the data points that exist to identfy the optimal solution. In streaming learning, we initialise the \\(\\beta_i\\) as 0 and keep incrementally changing the \\(\\beta_i\\) 's using \\( \\(\\beta_{i, t} = \\beta_{i, t-1} + \\alpha \\times \\frac{\\partial}{\\partial \\theta_{j}} J(\\theta)\\) \\)","title":"Completely online models"},{"location":"Python/Demonstrating%20online%20learning/#steps-in-updating-the-model","text":"In online learning, there are 4 steps[3]. For every new datapoint, we will recursively run the following steps. For \\({\\displaystyle t=1,2,...,T}\\) Learner receives input \\({\\displaystyle x_{t}}\\) Learner outputs \\({\\displaystyle w_{t}}\\) from a fixed convex set \\({\\displaystyle S}\\) Nature sends back a convex loss function \\({\\displaystyle v_{t}:S\\rightarrow \\mathbb {R} }\\) . Learner suffers loss \\({\\displaystyle v_{t}(w_{t})}\\) and updates its model.","title":"Steps in updating the model"},{"location":"Python/Demonstrating%20online%20learning/#implimentation-using-river","text":"Every online machine learning model has the following basic 5 functions: dir ( linear_model . LogisticRegression )[ 50 : 55 ] ['learn_many', 'learn_one', 'predict_many', 'predict_one', 'predict_proba_many'] As the name mentions, learn_one and predict_one learn and predict from one data point, learn_many, predict_many and predict_prob_many learn and predict using multiple data points. River is the result of a merger between creme and scikit-multiflow, and the remaining functions in the library follow a similar pattern to the same.","title":"Implimentation using river"},{"location":"Python/Demonstrating%20online%20learning/#building-a-model","text":"Using the same data as above, let us build a sample model using river for streaming. from river import compose river_model = compose . Pipeline ( preprocessing . StandardScaler (), tree . HoeffdingTreeClassifier () ) from sklearn.metrics import accuracy_score acc_scores = [] cols_x = [ 'sepal length (cm)' , 'sepal width (cm)' , 'petal length (cm)' , 'petal width (cm)' ] def compute_accuracy ( data , model , truth_col ): predict_all = data . apply ( lambda row : model . predict_one ( row ), axis = 1 ) acc_scores . append ( accuracy_score ( np . array ( predict_all ), data [ truth_col ])) print ( 'Accuracy is ' , acc_scores [ - 1 ]) data_stream = data_feed ( data1 ) for n in range ( 20 ): data_point = next ( data_stream ) print ( data_point ) if ( n > 1 ): predict_one = river_model . predict_one ( data_point [ cols_x ]) print ( 'Current_prediction' , predict_one , data_point [ 'target' ]) river_model . learn_one ( data_point [ cols_x ], data_point [ 'target' ]) compute_accuracy ( data1 , river_model , 'target' ) print ( '------------------------' ) sepal length (cm) 5.8 sepal width (cm) 2.7 petal length (cm) 4.1 petal width (cm) 1.0 target 1.0 Name: 67, dtype: float64 Accuracy is 0.5 ------------------------ sepal length (cm) 5.7 sepal width (cm) 2.9 petal length (cm) 4.2 petal width (cm) 1.3 target 1.0 Name: 96, dtype: float64 Accuracy is 0.5 ------------------------ sepal length (cm) 5.1 sepal width (cm) 3.8 petal length (cm) 1.5 petal width (cm) 0.3 target 0.0 Name: 19, dtype: float64 Current_prediction 1.0 0.0 Accuracy is 0.5 ------------------------ sepal length (cm) 5.4 sepal width (cm) 3.9 petal length (cm) 1.7 petal width (cm) 0.4 target 0.0 Name: 5, dtype: float64 Current_prediction 1.0 0.0 Accuracy is 0.71 ------------------------ sepal length (cm) 5.6 sepal width (cm) 2.9 petal length (cm) 3.6 petal width (cm) 1.3 target 1.0 Name: 64, dtype: float64 Current_prediction 1.0 1.0 Accuracy is 0.79 ------------------------ sepal length (cm) 6.3 sepal width (cm) 2.3 petal length (cm) 4.4 petal width (cm) 1.3 target 1.0 Name: 87, dtype: float64 Current_prediction 1.0 1.0 Accuracy is 0.69 ------------------------ sepal length (cm) 5.5 sepal width (cm) 2.4 petal length (cm) 3.8 petal width (cm) 1.1 target 1.0 Name: 80, dtype: float64 Current_prediction 1.0 1.0 Accuracy is 0.69 ------------------------ sepal length (cm) 5.1 sepal width (cm) 2.5 petal length (cm) 3.0 petal width (cm) 1.1 target 1.0 Name: 98, dtype: float64 Current_prediction 1.0 1.0 Accuracy is 0.69 ------------------------ sepal length (cm) 5.7 sepal width (cm) 4.4 petal length (cm) 1.5 petal width (cm) 0.4 target 0.0 Name: 15, dtype: float64 Current_prediction 1.0 0.0 Accuracy is 0.99 ------------------------ sepal length (cm) 5.1 sepal width (cm) 3.3 petal length (cm) 1.7 petal width (cm) 0.5 target 0.0 Name: 23, dtype: float64 Current_prediction 0.0 0.0 Accuracy is 1.0 ------------------------ sepal length (cm) 4.8 sepal width (cm) 3.1 petal length (cm) 1.6 petal width (cm) 0.2 target 0.0 Name: 30, dtype: float64 Current_prediction 0.0 0.0 Accuracy is 1.0 ------------------------ sepal length (cm) 5.5 sepal width (cm) 2.5 petal length (cm) 4.0 petal width (cm) 1.3 target 1.0 Name: 89, dtype: float64 Current_prediction 1.0 1.0 Accuracy is 1.0 ------------------------ sepal length (cm) 4.5 sepal width (cm) 2.3 petal length (cm) 1.3 petal width (cm) 0.3 target 0.0 Name: 41, dtype: float64 Current_prediction 0.0 0.0 Accuracy is 1.0 ------------------------ sepal length (cm) 6.1 sepal width (cm) 3.0 petal length (cm) 4.6 petal width (cm) 1.4 target 1.0 Name: 91, dtype: float64 Current_prediction 1.0 1.0 Accuracy is 1.0 ------------------------ sepal length (cm) 5.2 sepal width (cm) 3.5 petal length (cm) 1.5 petal width (cm) 0.2 target 0.0 Name: 27, dtype: float64 Current_prediction 0.0 0.0 Accuracy is 1.0 ------------------------ sepal length (cm) 7.0 sepal width (cm) 3.2 petal length (cm) 4.7 petal width (cm) 1.4 target 1.0 Name: 50, dtype: float64 Current_prediction 1.0 1.0 Accuracy is 1.0 ------------------------ sepal length (cm) 5.5 sepal width (cm) 4.2 petal length (cm) 1.4 petal width (cm) 0.2 target 0.0 Name: 33, dtype: float64 Current_prediction 0.0 0.0 Accuracy is 1.0 ------------------------ sepal length (cm) 5.9 sepal width (cm) 3.2 petal length (cm) 4.8 petal width (cm) 1.8 target 1.0 Name: 70, dtype: float64 Current_prediction 1.0 1.0 Accuracy is 1.0 ------------------------ sepal length (cm) 6.6 sepal width (cm) 3.0 petal length (cm) 4.4 petal width (cm) 1.4 target 1.0 Name: 75, dtype: float64 Current_prediction 1.0 1.0 Accuracy is 1.0 ------------------------ sepal length (cm) 4.4 sepal width (cm) 3.0 petal length (cm) 1.3 petal width (cm) 0.2 target 0.0 Name: 38, dtype: float64 Current_prediction 0.0 0.0 Accuracy is 1.0 ------------------------ import matplotlib.pyplot as plt % matplotlib inline plt . plot ( acc_scores ) plt . ylabel ( 'Accuracy score' ) plt . xlabel ( 'Sample #' ) <matplotlib.text.Text at 0x340c5208> river_model2 = compose . Pipeline ( preprocessing . StandardScaler (), linear_model . LogisticRegression () ) acc_scores1 = [] def compute_accuracy ( data , model , truth_col ): predict_all = data . apply ( lambda row : model . predict_one ( row ), axis = 1 ) acc_scores1 . append ( accuracy_score ( np . array ( predict_all ), data [ truth_col ])) print ( 'Accuracy is ' , acc_scores1 [ - 1 ]) data_stream = data_feed ( data1 ) for n in range ( 10 ): data_point = next ( data_stream ) print ( data_point ) if ( n > 1 ): predict_one = river_model2 . predict_one ( data_point [ cols_x ]) print ( 'Current_prediction' , predict_one , data_point [ 'target' ]) river_model2 . learn_one ( data_point [ cols_x ], data_point [ 'target' ]) compute_accuracy ( data1 , river_model2 , 'target' ) print ( '------------------------' ) sepal length (cm) 5.8 sepal width (cm) 2.7 petal length (cm) 4.1 petal width (cm) 1.0 target 1.0 Name: 67, dtype: float64 Accuracy is 0.5 ------------------------ sepal length (cm) 5.7 sepal width (cm) 2.9 petal length (cm) 4.2 petal width (cm) 1.3 target 1.0 Name: 96, dtype: float64 Accuracy is 0.94 ------------------------ sepal length (cm) 5.1 sepal width (cm) 3.8 petal length (cm) 1.5 petal width (cm) 0.3 target 0.0 Name: 19, dtype: float64 Current_prediction False 0.0 Accuracy is 1.0 ------------------------ sepal length (cm) 5.4 sepal width (cm) 3.9 petal length (cm) 1.7 petal width (cm) 0.4 target 0.0 Name: 5, dtype: float64 Current_prediction False 0.0 Accuracy is 1.0 ------------------------ sepal length (cm) 5.6 sepal width (cm) 2.9 petal length (cm) 3.6 petal width (cm) 1.3 target 1.0 Name: 64, dtype: float64 Current_prediction True 1.0 Accuracy is 1.0 ------------------------ sepal length (cm) 6.3 sepal width (cm) 2.3 petal length (cm) 4.4 petal width (cm) 1.3 target 1.0 Name: 87, dtype: float64 Current_prediction True 1.0 Accuracy is 1.0 ------------------------ sepal length (cm) 5.5 sepal width (cm) 2.4 petal length (cm) 3.8 petal width (cm) 1.1 target 1.0 Name: 80, dtype: float64 Current_prediction True 1.0 Accuracy is 0.99 ------------------------ sepal length (cm) 5.1 sepal width (cm) 2.5 petal length (cm) 3.0 petal width (cm) 1.1 target 1.0 Name: 98, dtype: float64 Current_prediction True 1.0 Accuracy is 0.99 ------------------------ sepal length (cm) 5.7 sepal width (cm) 4.4 petal length (cm) 1.5 petal width (cm) 0.4 target 0.0 Name: 15, dtype: float64 Current_prediction False 0.0 Accuracy is 0.99 ------------------------ sepal length (cm) 5.1 sepal width (cm) 3.3 petal length (cm) 1.7 petal width (cm) 0.5 target 0.0 Name: 23, dtype: float64 Current_prediction False 0.0 Accuracy is 0.99 ------------------------ import matplotlib.pyplot as plt % matplotlib inline plt . plot ( acc_scores1 ) plt . ylabel ( 'Accuracy score' ) plt . xlabel ( 'Sample #' ) <matplotlib.text.Text at 0x34d304a8> First published on Rolls Royce data sciecne blogs by Harsha Achyuthuni.","title":"Building a model"},{"location":"Python/Demonstrating%20online%20learning/#references","text":"Introductory material: https://towardsdatascience.com/machine-learning-for-streaming-data-with-creme-dacf5fb469df Hoeffding Trees: https://homes.cs.washington.edu/~pedrod/papers/kdd00.pdf Modelling under the hood: https://en.wikipedia.org/wiki/Online_machine_learning River Git: https://github.com/online-ml/river River installation steps: https://riverml.xyz/dev/getting-started/installation/ River documentation: https://riverml.xyz/dev/api/overview/ Batch decision trees: https://www.harshaash.com/cart-classification/","title":"References"},{"location":"Python/Diffusion%20on%20networks/","text":"MathJax.Hub.Config({ extensions: [\"tex2jax.js\",\"TeX/AMSmath.js\",\"TeX/AMSsymbols.js\"], jax: [\"input/TeX\", \"output/HTML-CSS\"], tex2jax: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ], }, \"HTML-CSS\": { availableFonts: [\"TeX\"] } }); Forecasting adoption of a new product \u00b6 Author: Achyuthuni Sri Harsha Introduction \u00b6 Forecasting new adoptions after a product introduction is an important marketing problem. I want to use a forecasting model developed by Frank Bass that has proven to be effective in forecasting the adoption of innovative and new technologies. I am going to use Nonlinear programming to estimate the parameters of the Bass forecasting model. Bass Forecasting model \u00b6 The model has three parameters that must be estimated. parameter explanation m the number of people estimated to eventually adopt the new product q the coefficient of imitation p the coefficient of innovation The coefficient of imitation (q) is a parameter that measures the likelihood of adoption due to a potential adopter being influenced by someone who has already adopted the product. It measures the \u201cword-of-mouth\u201d effect influencing purchases. The coefficient of innovation (p) measures the likelihood of adoption, assuming no influence from someone who has already purchased (adopted) the product. It is the likelihood of someone adopting the product due to her or his own interest in the innovation. If \\(C_{t\u22121}\\) is the number of people that adopted the product by time t-1, then the number of new adopters during time t is given by Bass forecasting model and it is: $$ F_t=(p+q[\\frac{C_{t\u22121}}{m}])(m\u2212C_{t\u22121}) $$ If \\(c_{t} = C_t/m\\) , then $$ c_{t} - c_{t-1} = (p+qc_{t-1})(1-c_{t-1})$$ Doing some maths, instead of one time period, we could consider \\(\\Delta t\\) time period, we can write as: \\[ c_{t+ \\Delta t} - c_{t} = (p+qc_{t})(1-c_{t}) \\Delta t \\] \\[ \\frac{c_{t+ \\Delta t} - c_{t}}{\\Delta t} = (p+qc_{t})(1-c_{t}) \\] \\[ \\frac{d}{dt}c_{t} = (p+qc_{t})(1-c_{t}) \\] Solving we get $$ c(t) = \\frac{1-e {-(p+q)t}}{1+\\frac{q}{p}e } $$ As an example, consider the following revenues for a product. import pandas as pd data = pd . DataFrame ({ 'week' : [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 ], 'revenues' : [ 0.1 , 3 , 5.2 , 7 , 5.25 , 4.9 , 3 , 2.4 , 1.9 , 1.3 , 0.8 , 0.6 ]}) data [ 'cum_sum' ] = data [ 'revenues' ] . cumsum () data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } week revenues cum_sum 0 1 0.10 0.10 1 2 3.00 3.10 2 3 5.20 8.30 3 4 7.00 15.30 4 5 5.25 20.55 5 6 4.90 25.45 6 7 3.00 28.45 7 8 2.40 30.85 8 9 1.90 32.75 9 10 1.30 34.05 10 11 0.80 34.85 11 12 0.60 35.45 Optimising for the ideal p, q and m values, we get from scipy.optimize import curve_fit def c_t ( x , p , q , m ): return ( p + ( q / m ) * ( x )) * ( m - x ) popt , pcov = curve_fit ( c_t , data . cum_sum [ 0 : 11 ], data . revenues [ 1 : 12 ]) popt array([ 0.11467648, 0.37950562, 35.22906717]) The optimal p,d and q are:0.11467648, 0.37950562, 35.22906717. We can use these to predict the future revenues of the product.","title":"Bass Forecasting model (Python)"},{"location":"Python/Diffusion%20on%20networks/#forecasting-adoption-of-a-new-product","text":"Author: Achyuthuni Sri Harsha","title":"Forecasting adoption of a new product"},{"location":"Python/Diffusion%20on%20networks/#introduction","text":"Forecasting new adoptions after a product introduction is an important marketing problem. I want to use a forecasting model developed by Frank Bass that has proven to be effective in forecasting the adoption of innovative and new technologies. I am going to use Nonlinear programming to estimate the parameters of the Bass forecasting model.","title":"Introduction"},{"location":"Python/Diffusion%20on%20networks/#bass-forecasting-model","text":"The model has three parameters that must be estimated. parameter explanation m the number of people estimated to eventually adopt the new product q the coefficient of imitation p the coefficient of innovation The coefficient of imitation (q) is a parameter that measures the likelihood of adoption due to a potential adopter being influenced by someone who has already adopted the product. It measures the \u201cword-of-mouth\u201d effect influencing purchases. The coefficient of innovation (p) measures the likelihood of adoption, assuming no influence from someone who has already purchased (adopted) the product. It is the likelihood of someone adopting the product due to her or his own interest in the innovation. If \\(C_{t\u22121}\\) is the number of people that adopted the product by time t-1, then the number of new adopters during time t is given by Bass forecasting model and it is: $$ F_t=(p+q[\\frac{C_{t\u22121}}{m}])(m\u2212C_{t\u22121}) $$ If \\(c_{t} = C_t/m\\) , then $$ c_{t} - c_{t-1} = (p+qc_{t-1})(1-c_{t-1})$$ Doing some maths, instead of one time period, we could consider \\(\\Delta t\\) time period, we can write as: \\[ c_{t+ \\Delta t} - c_{t} = (p+qc_{t})(1-c_{t}) \\Delta t \\] \\[ \\frac{c_{t+ \\Delta t} - c_{t}}{\\Delta t} = (p+qc_{t})(1-c_{t}) \\] \\[ \\frac{d}{dt}c_{t} = (p+qc_{t})(1-c_{t}) \\] Solving we get $$ c(t) = \\frac{1-e {-(p+q)t}}{1+\\frac{q}{p}e } $$ As an example, consider the following revenues for a product. import pandas as pd data = pd . DataFrame ({ 'week' : [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 ], 'revenues' : [ 0.1 , 3 , 5.2 , 7 , 5.25 , 4.9 , 3 , 2.4 , 1.9 , 1.3 , 0.8 , 0.6 ]}) data [ 'cum_sum' ] = data [ 'revenues' ] . cumsum () data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } week revenues cum_sum 0 1 0.10 0.10 1 2 3.00 3.10 2 3 5.20 8.30 3 4 7.00 15.30 4 5 5.25 20.55 5 6 4.90 25.45 6 7 3.00 28.45 7 8 2.40 30.85 8 9 1.90 32.75 9 10 1.30 34.05 10 11 0.80 34.85 11 12 0.60 35.45 Optimising for the ideal p, q and m values, we get from scipy.optimize import curve_fit def c_t ( x , p , q , m ): return ( p + ( q / m ) * ( x )) * ( m - x ) popt , pcov = curve_fit ( c_t , data . cum_sum [ 0 : 11 ], data . revenues [ 1 : 12 ]) popt array([ 0.11467648, 0.37950562, 35.22906717]) The optimal p,d and q are:0.11467648, 0.37950562, 35.22906717. We can use these to predict the future revenues of the product.","title":"Bass Forecasting model"},{"location":"Python/Introduction%20to%20Networkx/","text":"Networks in python \u00b6 Author: Achyuthuni Sri Harsha Networks play an important role in data science, with Google (page rank), Uber (route optimisation), Amazon (supply chian optimisation) and other companies becoming technology giants using network and data related optimisations. This article is an introduction to using networks in python using networkx package. # Import networkx library and rename it as nx. import networkx as nx # Other packages required import numpy as np import pandas as pd import matplotlib.pyplot as plt Undirected graphs \u00b6 A network can be represented in many ways. Edgelist represents graphs as a list of edges. Graphs can be undirected or directed. Consider the below edgelist where there is a relation between edge 1 and edge 2, and the weight for that edge is also provided. edgelist_df = pd . DataFrame ({ 'node1' :[ 1 , 1 , 1 , 2 , 2 , 3 , 4 , 4 ], 'node2' :[ 2 , 3 , 4 , 4 , 5 , 5 , 3 , 5 ], 'weights' :[ 5 , 2 , 1 , 3 , 1 , 7 , 1 , 4 ]}) edgelist_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } node1 node2 weights 0 1 2 5 1 1 3 2 2 1 4 1 3 2 4 3 4 2 5 1 5 3 5 7 6 4 3 1 7 4 5 4 One way to create a graph is to create an empty graph and add edges (and nodes) to the graph. g = nx . Graph () for i , elrow in edgelist_df . iterrows (): g . add_edge ( elrow [ 0 ], elrow [ 1 ], weight = elrow [ 2 ]) g . add_node ( 1 , pos = ( 0 , 5 )) g . edges ( data = True ) EdgeDataView([(1, 2, {'weight': 5}), (1, 3, {'weight': 2}), (1, 4, {'weight': 1}), (2, 4, {'weight': 3}), (2, 5, {'weight': 1}), (3, 5, {'weight': 7}), (3, 4, {'weight': 1}), (4, 5, {'weight': 4})]) # for each node we are trying to fix the coordinates g . add_node ( 1 , pos = ( 0 , 5 )) g . add_node ( 2 , pos = ( 5 , 10 )) g . add_node ( 3 , pos = ( 5 , 0 )) g . add_node ( 4 , pos = ( 10 , 5 )) g . add_node ( 5 , pos = ( 15 , 5 )) g . nodes ( data = True ) NodeDataView({1: {'pos': (0, 5)}, 2: {'pos': (5, 10)}, 3: {'pos': (5, 0)}, 4: {'pos': (10, 5)}, 5: {'pos': (15, 5)}}) This graph can be visualised as follows: # This function gets the coordinates for nodes pos = nx . get_node_attributes ( g , 'pos' ) # This function gets the weights for the edges weight = nx . get_edge_attributes ( g , 'weight' ) # The simplest way to draw a graph is by using nx.draw. nx . draw ( g , pos , with_labels = True ) # This adds edge lables nx . draw_networkx_edge_labels ( g , pos , edge_labels = weight ) plt . show () Mathematically this grah can be described in many ways. The most important are the adjacency matrix and incidence matrix. The adjacency matrix shows the relationship between the nodes in a matrix format where the nodes represent rows and columns. An edge between two nodes is represented by a positve number in the adjacency matrix, and the magnitude represents the weight. nx . linalg . graphmatrix . adj_matrix ( g ) . toarray () array([[0, 5, 2, 1, 0], [5, 0, 0, 3, 1], [2, 0, 0, 1, 7], [1, 3, 1, 0, 4], [0, 1, 7, 4, 0]], dtype=int64) The concurrency matrix represents the relationship between the nodes and edges. We have 8 edges as shown, and the relationship between the 5 nodes and 8 edges is shown below. g . edges EdgeView([(1, 2), (1, 3), (1, 4), (2, 4), (2, 5), (3, 5), (3, 4), (4, 5)]) nx . linalg . graphmatrix . incidence_matrix ( g ) . toarray () array([[1., 1., 1., 0., 0., 0., 0., 0.], [1., 0., 0., 1., 1., 0., 0., 0.], [0., 1., 0., 0., 0., 1., 1., 0.], [0., 0., 1., 1., 0., 0., 1., 1.], [0., 0., 0., 0., 1., 1., 0., 1.]]) Directed graph. \u00b6 A directed graph has edges which are in a particular direction. In this example, we are creating a DiGraph and reading a file where the edgelist is defined. An edge from a to b with a weight W exists when a line \"a b W\" can be found in the file. G1 = nx . DiGraph () # Read an directed graph from a list of edges, need to specify that using the parameter \u2018create_using\u2019 G1 = nx . read_edgelist ( \"edgelist1.txt\" , nodetype = str , create_using = nx . DiGraph (), data = [( 'weight' , int )]) print ( 'Edgelist' ) import sys nx . write_weighted_edgelist ( G1 , sys . stdout ) print ( 'nodes' ) print ( G1 . nodes ( data = True )) print ( 'edges' ) print ( G1 . edges ( data = True )) Edgelist 0 1 4 0 3 4 0 4 2 1 0 4 1 2 9 1 4 1 2 3 4 3 1 8 3 4 7 nodes [('0', {}), ('1', {}), ('2', {}), ('3', {}), ('4', {})] edges [('0', '1', {'weight': 4}), ('0', '3', {'weight': 4}), ('0', '4', {'weight': 2}), ('1', '0', {'weight': 4}), ('1', '2', {'weight': 9}), ('1', '4', {'weight': 1}), ('2', '3', {'weight': 4}), ('3', '1', {'weight': 8}), ('3', '4', {'weight': 7})] This graph can be visualised in many ways. Some of the visualisation patterns are (And these visualisations are same for directed or undirected graphs) 1. Spring layout (The length of edges is proportional to the weights) 2. Circular layout (The nodes are present in a circle) 3. Random layout All the three layouts are shown below nx . draw_spring ( G1 , with_labels = True , node_color = 'skyblue' , node_size = 200 , edge_color = 'black' ) nx . draw_circular ( G1 , with_labels = True , node_color = 'red' , node_size = 300 , edge_color = 'black' ) nx . draw_random ( G1 , with_labels = True , node_color = 'red' , node_size = 300 , edge_color = 'black' ) The concurrency matrix looks differently for directed graphs. If a edge is leaving a node it is represented by -1 and if it is incoming to a node i is +1. nx . linalg . graphmatrix . incidence_matrix ( G1 , oriented = True ) . toarray () array([[-1., -1., -1., 1., 0., 0., 0., 0., 0.], [ 1., 0., 0., -1., -1., -1., 0., 1., 0.], [ 0., 0., 0., 0., 1., 0., -1., 0., 0.], [ 0., 1., 0., 0., 0., 0., 1., -1., -1.], [ 0., 0., 1., 0., 0., 1., 0., 0., 1.]]) We can read from not just edgelists but also from adjacency matrix. For instance, we can create a new graph using the adjacency matrix from the directed graph. # Adjacencey matrix of the graph G1 M1 = nx . linalg . graphmatrix . adj_matrix ( G1 ) . toarray () print ( 'Adjacency matrix of G1' ) print ( M1 ) # convert the matrix into a graph G2 = nx . from_numpy_matrix ( M1 ) # Print the graph information print ( \"Printing G2\" ) nx . write_weighted_edgelist ( G2 , sys . stdout ) Adjacency matrix of G1 [[0 4 0 4 2] [4 0 9 0 1] [0 0 0 4 0] [0 8 0 0 7] [0 0 0 0 0]] Printing G2 0 1 4 0 3 4 0 4 2 1 2 9 1 4 1 1 3 8 2 3 4 3 4 7 Visualisation of graphs \u00b6 We can display the graphs using a variety of methods. The weights among the edges can be represented as thickness (or color) of the edges, the importance of nodes can be defined, etc. This will provide additional information of the graph. # get the outward degree for each node and store them as a list of (node_number, degree) out_deg = G1 . out_degree () # Making all the required parameters out_deg = [ int ( out_deg [ node ]) * 1000 for node in G1 . nodes ()] color = [ 'g' if val > np . mean ( out_deg ) else 'r' for val in out_deg ] weight = list ( nx . get_edge_attributes ( G1 , 'weight' ) . values ()) degree = dict ( G1 . degree ) # create the plot and title plt . subplots ( figsize = ( 10 , 10 )) plt . title ( 'Graph containing everything' ) nx . draw_circular ( G1 , with_labels = True , edge_color = color , width = weight , node_size = out_deg ) Anther different type of graph is the bipartite graph, which has a visualisation of its own. In the below example, we take an inbilt biparite graph to show how it can be visualised. B = nx . bipartite . gnmk_random_graph ( 3 , 5 , 10 , seed = 123 ) top = nx . bipartite . sets ( B )[ 0 ] pos = nx . bipartite_layout ( B , top ) nx . draw_networkx ( B , pos = nx . drawing . layout . bipartite_layout ( B , top ))","title":"Introduction to NetworkX (Python)"},{"location":"Python/Introduction%20to%20Networkx/#networks-in-python","text":"Author: Achyuthuni Sri Harsha Networks play an important role in data science, with Google (page rank), Uber (route optimisation), Amazon (supply chian optimisation) and other companies becoming technology giants using network and data related optimisations. This article is an introduction to using networks in python using networkx package. # Import networkx library and rename it as nx. import networkx as nx # Other packages required import numpy as np import pandas as pd import matplotlib.pyplot as plt","title":"Networks in python"},{"location":"Python/Introduction%20to%20Networkx/#undirected-graphs","text":"A network can be represented in many ways. Edgelist represents graphs as a list of edges. Graphs can be undirected or directed. Consider the below edgelist where there is a relation between edge 1 and edge 2, and the weight for that edge is also provided. edgelist_df = pd . DataFrame ({ 'node1' :[ 1 , 1 , 1 , 2 , 2 , 3 , 4 , 4 ], 'node2' :[ 2 , 3 , 4 , 4 , 5 , 5 , 3 , 5 ], 'weights' :[ 5 , 2 , 1 , 3 , 1 , 7 , 1 , 4 ]}) edgelist_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } node1 node2 weights 0 1 2 5 1 1 3 2 2 1 4 1 3 2 4 3 4 2 5 1 5 3 5 7 6 4 3 1 7 4 5 4 One way to create a graph is to create an empty graph and add edges (and nodes) to the graph. g = nx . Graph () for i , elrow in edgelist_df . iterrows (): g . add_edge ( elrow [ 0 ], elrow [ 1 ], weight = elrow [ 2 ]) g . add_node ( 1 , pos = ( 0 , 5 )) g . edges ( data = True ) EdgeDataView([(1, 2, {'weight': 5}), (1, 3, {'weight': 2}), (1, 4, {'weight': 1}), (2, 4, {'weight': 3}), (2, 5, {'weight': 1}), (3, 5, {'weight': 7}), (3, 4, {'weight': 1}), (4, 5, {'weight': 4})]) # for each node we are trying to fix the coordinates g . add_node ( 1 , pos = ( 0 , 5 )) g . add_node ( 2 , pos = ( 5 , 10 )) g . add_node ( 3 , pos = ( 5 , 0 )) g . add_node ( 4 , pos = ( 10 , 5 )) g . add_node ( 5 , pos = ( 15 , 5 )) g . nodes ( data = True ) NodeDataView({1: {'pos': (0, 5)}, 2: {'pos': (5, 10)}, 3: {'pos': (5, 0)}, 4: {'pos': (10, 5)}, 5: {'pos': (15, 5)}}) This graph can be visualised as follows: # This function gets the coordinates for nodes pos = nx . get_node_attributes ( g , 'pos' ) # This function gets the weights for the edges weight = nx . get_edge_attributes ( g , 'weight' ) # The simplest way to draw a graph is by using nx.draw. nx . draw ( g , pos , with_labels = True ) # This adds edge lables nx . draw_networkx_edge_labels ( g , pos , edge_labels = weight ) plt . show () Mathematically this grah can be described in many ways. The most important are the adjacency matrix and incidence matrix. The adjacency matrix shows the relationship between the nodes in a matrix format where the nodes represent rows and columns. An edge between two nodes is represented by a positve number in the adjacency matrix, and the magnitude represents the weight. nx . linalg . graphmatrix . adj_matrix ( g ) . toarray () array([[0, 5, 2, 1, 0], [5, 0, 0, 3, 1], [2, 0, 0, 1, 7], [1, 3, 1, 0, 4], [0, 1, 7, 4, 0]], dtype=int64) The concurrency matrix represents the relationship between the nodes and edges. We have 8 edges as shown, and the relationship between the 5 nodes and 8 edges is shown below. g . edges EdgeView([(1, 2), (1, 3), (1, 4), (2, 4), (2, 5), (3, 5), (3, 4), (4, 5)]) nx . linalg . graphmatrix . incidence_matrix ( g ) . toarray () array([[1., 1., 1., 0., 0., 0., 0., 0.], [1., 0., 0., 1., 1., 0., 0., 0.], [0., 1., 0., 0., 0., 1., 1., 0.], [0., 0., 1., 1., 0., 0., 1., 1.], [0., 0., 0., 0., 1., 1., 0., 1.]])","title":"Undirected graphs"},{"location":"Python/Introduction%20to%20Networkx/#directed-graph","text":"A directed graph has edges which are in a particular direction. In this example, we are creating a DiGraph and reading a file where the edgelist is defined. An edge from a to b with a weight W exists when a line \"a b W\" can be found in the file. G1 = nx . DiGraph () # Read an directed graph from a list of edges, need to specify that using the parameter \u2018create_using\u2019 G1 = nx . read_edgelist ( \"edgelist1.txt\" , nodetype = str , create_using = nx . DiGraph (), data = [( 'weight' , int )]) print ( 'Edgelist' ) import sys nx . write_weighted_edgelist ( G1 , sys . stdout ) print ( 'nodes' ) print ( G1 . nodes ( data = True )) print ( 'edges' ) print ( G1 . edges ( data = True )) Edgelist 0 1 4 0 3 4 0 4 2 1 0 4 1 2 9 1 4 1 2 3 4 3 1 8 3 4 7 nodes [('0', {}), ('1', {}), ('2', {}), ('3', {}), ('4', {})] edges [('0', '1', {'weight': 4}), ('0', '3', {'weight': 4}), ('0', '4', {'weight': 2}), ('1', '0', {'weight': 4}), ('1', '2', {'weight': 9}), ('1', '4', {'weight': 1}), ('2', '3', {'weight': 4}), ('3', '1', {'weight': 8}), ('3', '4', {'weight': 7})] This graph can be visualised in many ways. Some of the visualisation patterns are (And these visualisations are same for directed or undirected graphs) 1. Spring layout (The length of edges is proportional to the weights) 2. Circular layout (The nodes are present in a circle) 3. Random layout All the three layouts are shown below nx . draw_spring ( G1 , with_labels = True , node_color = 'skyblue' , node_size = 200 , edge_color = 'black' ) nx . draw_circular ( G1 , with_labels = True , node_color = 'red' , node_size = 300 , edge_color = 'black' ) nx . draw_random ( G1 , with_labels = True , node_color = 'red' , node_size = 300 , edge_color = 'black' ) The concurrency matrix looks differently for directed graphs. If a edge is leaving a node it is represented by -1 and if it is incoming to a node i is +1. nx . linalg . graphmatrix . incidence_matrix ( G1 , oriented = True ) . toarray () array([[-1., -1., -1., 1., 0., 0., 0., 0., 0.], [ 1., 0., 0., -1., -1., -1., 0., 1., 0.], [ 0., 0., 0., 0., 1., 0., -1., 0., 0.], [ 0., 1., 0., 0., 0., 0., 1., -1., -1.], [ 0., 0., 1., 0., 0., 1., 0., 0., 1.]]) We can read from not just edgelists but also from adjacency matrix. For instance, we can create a new graph using the adjacency matrix from the directed graph. # Adjacencey matrix of the graph G1 M1 = nx . linalg . graphmatrix . adj_matrix ( G1 ) . toarray () print ( 'Adjacency matrix of G1' ) print ( M1 ) # convert the matrix into a graph G2 = nx . from_numpy_matrix ( M1 ) # Print the graph information print ( \"Printing G2\" ) nx . write_weighted_edgelist ( G2 , sys . stdout ) Adjacency matrix of G1 [[0 4 0 4 2] [4 0 9 0 1] [0 0 0 4 0] [0 8 0 0 7] [0 0 0 0 0]] Printing G2 0 1 4 0 3 4 0 4 2 1 2 9 1 4 1 1 3 8 2 3 4 3 4 7","title":"Directed graph."},{"location":"Python/Introduction%20to%20Networkx/#visualisation-of-graphs","text":"We can display the graphs using a variety of methods. The weights among the edges can be represented as thickness (or color) of the edges, the importance of nodes can be defined, etc. This will provide additional information of the graph. # get the outward degree for each node and store them as a list of (node_number, degree) out_deg = G1 . out_degree () # Making all the required parameters out_deg = [ int ( out_deg [ node ]) * 1000 for node in G1 . nodes ()] color = [ 'g' if val > np . mean ( out_deg ) else 'r' for val in out_deg ] weight = list ( nx . get_edge_attributes ( G1 , 'weight' ) . values ()) degree = dict ( G1 . degree ) # create the plot and title plt . subplots ( figsize = ( 10 , 10 )) plt . title ( 'Graph containing everything' ) nx . draw_circular ( G1 , with_labels = True , edge_color = color , width = weight , node_size = out_deg ) Anther different type of graph is the bipartite graph, which has a visualisation of its own. In the below example, we take an inbilt biparite graph to show how it can be visualised. B = nx . bipartite . gnmk_random_graph ( 3 , 5 , 10 , seed = 123 ) top = nx . bipartite . sets ( B )[ 0 ] pos = nx . bipartite_layout ( B , top ) nx . draw_networkx ( B , pos = nx . drawing . layout . bipartite_layout ( B , top ))","title":"Visualisation of graphs"},{"location":"Python/Machine%20learning%20as%20HTTP%20Request/","text":"Deploying a machine learning application \u00b6 Building a machine learning model is only half the story. Deploying this application so that the business uses it is the other half. Generally, deployment is not done by machine learning engineers, or data scientists. Therefore I see a huge lacking in these skills in my peers. Although deployment is done by python developers, it is important for data scientists to know the basics of deploying a machine learning solution. In the below example, I am using the data taken from aqicn.org on the PM25 pollutant near my house in Hyderabad, India. I am using this data to build a model that will predict the Air Quality near my home. This is the machine learing model that I want to deploy a HTTP request. I have used pythonanywhere to deploy a flask application which \"GET\"s the PM25 prediction for the date provided. First let me build a machine learning model. The training data can be found at aqicn's api import pandas as pd data = pd . read_csv ( 'hyderabad-us consulate-air-quality.csv' , parse_dates = [ 'date' ]) data . columns = [ 'date' , 'pm25' ] data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date pm25 0 2021-10-01 87 1 2021-10-02 47 2 2021-10-03 50 3 2021-09-01 66 4 2021-09-02 74 ... ... ... 2302 2014-12-24 165 2303 2014-12-25 165 2304 2014-12-26 163 2305 2014-12-27 165 2306 2014-12-28 160 2307 rows \u00d7 2 columns data . plot . scatter ( x = 'date' , y = 'pm25' ) from statsmodels.tsa.seasonal import seasonal_decompose result = seasonal_decompose ( data . pm25 , model = 'additive' , period = 365 ) print ( result . plot ()) Figure(432x288) We can see the seasonality in the data where the pollution increases during winter and is lower during the summer months. But since this is not a machine learning blog, we are going to ignore building a model. Instead our prediction will simply be an average of the past data. My intention in this post is not to build a machine learning model, but show how it can be deployed. The more complex model along with the code can be found in my github repo . data . pm25 . mean () 115.66666666666667 Deployment \u00b6 The best way to deploy the machine learning model (according to me) is to encapsulate the trainig and prediction logic behind the data science model alon with the final model. This can be done by using a class as shown below, and serialise/deserialise the class. This way we can move around the machine learning model along with the training and prediction code, and we need not re-write the prediction code in the server side everytime we do a change in the machine learnig model or code. We can only change the final model file, and the application should work seamlessly. Consider the below code which encapsulates the machine learning model in a class: import dill # dill is an alternative to pickle which is better for serialising objects along with their class definitions class predict_pm25 : def __init__ ( self ): self . model = None self . version = 1 def predict ( self , time ): return [ 115.6 ] # return a list def save_model ( self ): with open ( 'predict_hyderabad_pm25.pkl' , \"wb\" ) as pkl_file : dill . dump ( self , pkl_file ) Running the code to save the model as a serialised file. predict_pm = predict_pm25 () predict_pm . save_model () Flask \u00b6 Flask server can be used to deploy this model. First, we set up flask server over local host. First, write the following code in a file named flask_app.py (any name except flask.py) from flask import Flask , request , jsonify import pandas as pd from mc_predict import predict as machine_learning_predict # has code for the predict function app = Flask ( __name__ ) # initialising the flask app @app . route ( \"/\" ) # specifying the app route over the web def base_website (): # what should happen at this route return \"Welcome to machine learning model APIs!\" @app . route ( '/predict' , methods = [ 'GET' ]) # Get request defined def predict_request (): # what should happen at this get request json_ = request . json query_df = pd . DataFrame ( json_ ) prediction = machine_learning_predict ( query_df ) # we call the predict function for the machine learning model return jsonify ({ 'prediction' : list ( prediction )}) if __name__ == '__main__' : app . run ( debug = True ) The predict function is defined in a different file called mc_predict.py . In this function, we load (unserialise) the saved model and call the predict function in the model. import dill def predict ( df ): with open ( 'predict_hyderabad_pm25.pkl' , \"rb\" ) as pkl_file : model = dill . load ( pkl_file ) # unserialise the model return model . predict ( df ) Thats it, we have our local deployment ready. We will have to go to the folder where these files are present, and type the command python flask_app.py . We will get the app running on http://127.0.0.1:5000/ . For GET request, we can type is http://127.0.0.1:5000/predict?time=\"12/10/2021 \" to get the prediction for the time 12/10/2021. Pythonanywhere \u00b6 The next step is to deploy it on pythonanywhere. The first step is to sign up for a new account. We can then \"Add a new web app\" with Flask 3.7. This will create a default flask based webapp with your username.pythonanywhere.com. We can install any packages necessary using the \"Console\" (example pip install dill ). In the files tab, under mysite are the flask files. These should be replaced with the files that we have above. The model file should also be uploaded. (We should take care of the relative location of the model file while loading it). Under Web tab, we can Reload the model which will rebuild the application. We now have our machine learning model deployed. For the model I deployed, hit the GET request https://harshaash.pythonanywhere.com/predict?time=12/10/2021 . The complete code can be found at my Github . References \u00b6 Deployment: https://medium.com/analytics-vidhya/how-to-deploy-simple-machine-learning-models-for-free-56cdccc62b8d https://www.datacamp.com/community/tutorials/machine-learning-models-api-python Data: https://aqicn.org/json-api/doc/ https://help.pythonanywhere.com/pages/Flask/","title":"Machine Learning deployment in Flask (Python)"},{"location":"Python/Machine%20learning%20as%20HTTP%20Request/#deploying-a-machine-learning-application","text":"Building a machine learning model is only half the story. Deploying this application so that the business uses it is the other half. Generally, deployment is not done by machine learning engineers, or data scientists. Therefore I see a huge lacking in these skills in my peers. Although deployment is done by python developers, it is important for data scientists to know the basics of deploying a machine learning solution. In the below example, I am using the data taken from aqicn.org on the PM25 pollutant near my house in Hyderabad, India. I am using this data to build a model that will predict the Air Quality near my home. This is the machine learing model that I want to deploy a HTTP request. I have used pythonanywhere to deploy a flask application which \"GET\"s the PM25 prediction for the date provided. First let me build a machine learning model. The training data can be found at aqicn's api import pandas as pd data = pd . read_csv ( 'hyderabad-us consulate-air-quality.csv' , parse_dates = [ 'date' ]) data . columns = [ 'date' , 'pm25' ] data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date pm25 0 2021-10-01 87 1 2021-10-02 47 2 2021-10-03 50 3 2021-09-01 66 4 2021-09-02 74 ... ... ... 2302 2014-12-24 165 2303 2014-12-25 165 2304 2014-12-26 163 2305 2014-12-27 165 2306 2014-12-28 160 2307 rows \u00d7 2 columns data . plot . scatter ( x = 'date' , y = 'pm25' ) from statsmodels.tsa.seasonal import seasonal_decompose result = seasonal_decompose ( data . pm25 , model = 'additive' , period = 365 ) print ( result . plot ()) Figure(432x288) We can see the seasonality in the data where the pollution increases during winter and is lower during the summer months. But since this is not a machine learning blog, we are going to ignore building a model. Instead our prediction will simply be an average of the past data. My intention in this post is not to build a machine learning model, but show how it can be deployed. The more complex model along with the code can be found in my github repo . data . pm25 . mean () 115.66666666666667","title":"Deploying a machine learning application"},{"location":"Python/Machine%20learning%20as%20HTTP%20Request/#deployment","text":"The best way to deploy the machine learning model (according to me) is to encapsulate the trainig and prediction logic behind the data science model alon with the final model. This can be done by using a class as shown below, and serialise/deserialise the class. This way we can move around the machine learning model along with the training and prediction code, and we need not re-write the prediction code in the server side everytime we do a change in the machine learnig model or code. We can only change the final model file, and the application should work seamlessly. Consider the below code which encapsulates the machine learning model in a class: import dill # dill is an alternative to pickle which is better for serialising objects along with their class definitions class predict_pm25 : def __init__ ( self ): self . model = None self . version = 1 def predict ( self , time ): return [ 115.6 ] # return a list def save_model ( self ): with open ( 'predict_hyderabad_pm25.pkl' , \"wb\" ) as pkl_file : dill . dump ( self , pkl_file ) Running the code to save the model as a serialised file. predict_pm = predict_pm25 () predict_pm . save_model ()","title":"Deployment"},{"location":"Python/Machine%20learning%20as%20HTTP%20Request/#flask","text":"Flask server can be used to deploy this model. First, we set up flask server over local host. First, write the following code in a file named flask_app.py (any name except flask.py) from flask import Flask , request , jsonify import pandas as pd from mc_predict import predict as machine_learning_predict # has code for the predict function app = Flask ( __name__ ) # initialising the flask app @app . route ( \"/\" ) # specifying the app route over the web def base_website (): # what should happen at this route return \"Welcome to machine learning model APIs!\" @app . route ( '/predict' , methods = [ 'GET' ]) # Get request defined def predict_request (): # what should happen at this get request json_ = request . json query_df = pd . DataFrame ( json_ ) prediction = machine_learning_predict ( query_df ) # we call the predict function for the machine learning model return jsonify ({ 'prediction' : list ( prediction )}) if __name__ == '__main__' : app . run ( debug = True ) The predict function is defined in a different file called mc_predict.py . In this function, we load (unserialise) the saved model and call the predict function in the model. import dill def predict ( df ): with open ( 'predict_hyderabad_pm25.pkl' , \"rb\" ) as pkl_file : model = dill . load ( pkl_file ) # unserialise the model return model . predict ( df ) Thats it, we have our local deployment ready. We will have to go to the folder where these files are present, and type the command python flask_app.py . We will get the app running on http://127.0.0.1:5000/ . For GET request, we can type is http://127.0.0.1:5000/predict?time=\"12/10/2021 \" to get the prediction for the time 12/10/2021.","title":"Flask"},{"location":"Python/Machine%20learning%20as%20HTTP%20Request/#pythonanywhere","text":"The next step is to deploy it on pythonanywhere. The first step is to sign up for a new account. We can then \"Add a new web app\" with Flask 3.7. This will create a default flask based webapp with your username.pythonanywhere.com. We can install any packages necessary using the \"Console\" (example pip install dill ). In the files tab, under mysite are the flask files. These should be replaced with the files that we have above. The model file should also be uploaded. (We should take care of the relative location of the model file while loading it). Under Web tab, we can Reload the model which will rebuild the application. We now have our machine learning model deployed. For the model I deployed, hit the GET request https://harshaash.pythonanywhere.com/predict?time=12/10/2021 . The complete code can be found at my Github .","title":"Pythonanywhere"},{"location":"Python/Machine%20learning%20as%20HTTP%20Request/#references","text":"Deployment: https://medium.com/analytics-vidhya/how-to-deploy-simple-machine-learning-models-for-free-56cdccc62b8d https://www.datacamp.com/community/tutorials/machine-learning-models-api-python Data: https://aqicn.org/json-api/doc/ https://help.pythonanywhere.com/pages/Flask/","title":"References"},{"location":"Python/Network%20Flow%20problems/","text":"MathJax.Hub.Config({ extensions: [\"tex2jax.js\",\"TeX/AMSmath.js\",\"TeX/AMSsymbols.js\"], jax: [\"input/TeX\", \"output/HTML-CSS\"], tex2jax: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ], }, \"HTML-CSS\": { availableFonts: [\"TeX\"] } }); Flow maximisation problems \u00b6 Author: Achyuthuni Sri Harsha A directed graph has the capacities on all the edges and our job is to find the maximum amount of flow that can happen from one node (starting node) to another node(outflow node). There are two types of flow maximisation problems: 1. Max flow: What is the maximum flow that can be sent from source to outfow without exceeding capacity. 2. Min cost flow: We have the cost along wih capacities on each edge. We want to find the minimum cost path of sending f uits of flow from source to outflow. We can use integer programming to solve both these problems. import networkx as nx # Other packages for manupulating data import numpy as np import pandas as pd # packages for plotting import matplotlib.pyplot as plt Maximum flow problem \u00b6 Let us first consider the max flow problem. Consider the below graph. edgelist_df = pd . DataFrame ({ 'node1' :[ 's' , 's' , 'u' , 'u' , 'v' ], 'node2' :[ 'u' , 'v' , 'v' , 't' , 't' ], 'weights' :[ 20 , 10 , 30 , 10 , 20 ] }) edgelist_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } node1 node2 weights 0 s u 20 1 s v 10 2 u v 30 3 u t 10 4 v t 20 g = nx . DiGraph () for i , elrow in edgelist_df . iterrows (): g . add_edge ( elrow [ 0 ], elrow [ 1 ], weight = elrow [ 2 ]) g . edges ( data = True ) OutEdgeDataView([('s', 'u', {'weight': 20}), ('s', 'v', {'weight': 10}), ('u', 'v', {'weight': 30}), ('u', 't', {'weight': 10}), ('v', 't', {'weight': 20})]) # for each node we are trying to fix the coordinates g . add_node ( 's' , pos = ( 0 , 1 )) g . add_node ( 'u' , pos = ( 1 , 2 )) g . add_node ( 'v' , pos = ( 1 , 0 )) g . add_node ( 't' , pos = ( 2 , 1 )) g . nodes ( data = True ) NodeDataView({'s': {'pos': (0, 1)}, 'u': {'pos': (1, 2)}, 'v': {'pos': (1, 0)}, 't': {'pos': (2, 1)}}) pos = nx . get_node_attributes ( g , 'pos' ) weight = nx . get_edge_attributes ( g , 'weight' ) nx . draw ( g , pos , with_labels = True ) nx . draw_networkx_edge_labels ( g , pos , edge_labels = weight ) plt . show () We want to find the maximum flow between s and t. This can be formulated as an integer programming problem, with Decision variable 1. Integer variable \\(edge_{i,j}\\) representing the amount of flow in edge ij. 2. Integer variable f representing the maximum flow from ortools.sat.python import cp_model max_flow_model = cp_model . CpModel () # Creating one integer decision variable for each edge edge_int_vars = {} for edge in g . edges : edge_int_vars [ edge [ 0 ], edge [ 1 ]] = max_flow_model . NewIntVar ( 0 , 100 , 'edge_ %s _ %s ' % edge ) print ( 'Creating the integer variable ' , edge_int_vars [ edge [ 0 ], edge [ 1 ]], 'representing the amount to flow in edge' , ( edge [ 0 ], edge [ 1 ])) Creating the integer variable edge_s_u representing the amount to flow in edge ('s', 'u') Creating the integer variable edge_s_v representing the amount to flow in edge ('s', 'v') Creating the integer variable edge_u_v representing the amount to flow in edge ('u', 'v') Creating the integer variable edge_u_t representing the amount to flow in edge ('u', 't') Creating the integer variable edge_v_t representing the amount to flow in edge ('v', 't') Constraints The first set of constraints are flow constraints, 1. At the input node, the net flow should be -f 2. At the output node, the net flow should be +f 3. The net flow in all other nodes should be 0 input_node = 's' output_node = 't' # Adding constraints on the nodes flow = max_flow_model . NewIntVar ( 0 , 100 , 'flow' ) # Initialising flow this will be maximised later for node in g . nodes : in_edges = g . in_edges ( node ) out_edges = g . out_edges ( node ) print ( 'Adding the constraint on node ' , node ) equation_at_this_edge = sum ( edge_int_vars [ edge [ 0 ], edge [ 1 ]] for edge in in_edges ) - \\ sum ( edge_int_vars [ edge [ 0 ], edge [ 1 ]] for edge in out_edges ) if ( node == input_node ): max_flow_model . Add ( equation_at_this_edge == - flow ) print ( equation_at_this_edge == - flow ) elif ( node == output_node ): max_flow_model . Add ( equation_at_this_edge == flow ) print ( equation_at_this_edge == flow ) else : max_flow_model . Add ( equation_at_this_edge == 0 ) print ( equation_at_this_edge , '== 0' ) print ( '' ) Adding the constraint on node s ((-((edge_s_u) + edge_s_v)) + (1 * flow)) == 0 Adding the constraint on node u ((edge_s_u) + -((edge_u_v) + edge_u_t)) == 0 Adding the constraint on node v (((edge_s_v) + edge_u_v) + -(edge_v_t)) == 0 Adding the constraint on node t ((((edge_u_t) + edge_v_t)) + -flow) == 0 Another set of constraints are the capacity restrictions on every edge. # Adding constraints on the edges for edge in g . edges : print ( 'Adding constraint on edge ' , edge ) max_flow_in_edge = g . get_edge_data ( * edge )[ 'weight' ] max_flow_model . Add ( edge_int_vars [ edge [ 0 ], edge [ 1 ]] <= max_flow_in_edge ) print ( edge_int_vars [ edge [ 0 ], edge [ 1 ]] <= max_flow_in_edge ) Adding constraint on edge ('s', 'u') edge_s_u <= 20 Adding constraint on edge ('s', 'v') edge_s_v <= 10 Adding constraint on edge ('u', 'v') edge_u_v <= 30 Adding constraint on edge ('u', 't') edge_u_t <= 10 Adding constraint on edge ('v', 't') edge_v_t <= 20 Objective The objective is to maximise flow. # The objective is to maximise flow max_flow_model . Maximize ( flow ) Solving the problem, we get # Solving the problem solver = cp_model . CpSolver () solution_printer = cp_model . ObjectiveSolutionPrinter () status = solver . SolveWithSolutionCallback ( max_flow_model , solution_printer ) Solution 0, time = 0.44 s, objective = 30 cp_model . OPTIMAL == status True result_edges = {} for edge in g . edges : result_edges [ edge [ 0 ], edge [ 1 ]] = solver . Value ( edge_int_vars [ edge [ 0 ], edge [ 1 ]]) result_edges {('s', 'u'): 20, ('s', 'v'): 10, ('u', 'v'): 10, ('u', 't'): 10, ('v', 't'): 20} The flow in each of the nodes is shown in the figure pos = nx . get_node_attributes ( g , 'pos' ) nx . draw ( g , pos , with_labels = True ) nx . draw_networkx_edge_labels ( g , pos , edge_labels = result_edges ) plt . show () Minimum cost flow problems \u00b6 We want to find the minimum cost to transfer a fixed amount of flow from one edge to another. The costs are given int he table bwlow. The costs are also displayed in the network below. edgelist_df = pd . DataFrame ({ 'node1' :[ 's' , 's' , 'u' , 'u' , 'v' ], 'node2' :[ 'u' , 'v' , 'v' , 't' , 't' ], 'weights' :[ 20 , 10 , 30 , 10 , 20 ], 'costs' :[ 3 , 2 , 0.7 , 1 , 7 ] }) edgelist_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } node1 node2 weights costs 0 s u 20 3.0 1 s v 10 2.0 2 u v 30 0.7 3 u t 10 1.0 4 v t 20 7.0 g = nx . DiGraph () for i , elrow in edgelist_df . iterrows (): g . add_edge ( elrow [ 0 ], elrow [ 1 ], weight = elrow [ 2 ], cost = elrow [ 3 ]) g . edges ( data = True ) OutEdgeDataView([('s', 'u', {'weight': 20, 'cost': 3.0}), ('s', 'v', {'weight': 10, 'cost': 2.0}), ('u', 'v', {'weight': 30, 'cost': 0.7}), ('u', 't', {'weight': 10, 'cost': 1.0}), ('v', 't', {'weight': 20, 'cost': 7.0})]) # for each node we are trying to fix the coordinates g . add_node ( 's' , pos = ( 0 , 1 )) g . add_node ( 'u' , pos = ( 1 , 2 )) g . add_node ( 'v' , pos = ( 1 , 0 )) g . add_node ( 't' , pos = ( 2 , 1 )) g . nodes ( data = True ) NodeDataView({'s': {'pos': (0, 1)}, 'u': {'pos': (1, 2)}, 'v': {'pos': (1, 0)}, 't': {'pos': (2, 1)}}) pos = nx . get_node_attributes ( g , 'pos' ) weight = nx . get_edge_attributes ( g , 'weight' ) cost = nx . get_edge_attributes ( g , 'cost' ) res = { key : str ( weight [ key ]) + '/$' + str ( cost . get ( key , '' )) for key in cost . keys ()} nx . draw ( g , pos , with_labels = True ) nx . draw_networkx_edge_labels ( g , pos , edge_labels = res ) plt . show () Just like maximum flow, we can use integer programming to find out the solution in this scenario also. Decision variables Integer variables \\(edge_{i,j}\\) representing the amount of flow in edge (i,j) Constraints Capacity restrictions on every edge Objective The objective is to minimise the overall cost from ortools.sat.python import cp_model min_cost_flow_model = cp_model . CpModel () # Creating one integer decision variable for each edge edge_int_vars = {} for edge in g . edges : edge_int_vars [ edge [ 0 ], edge [ 1 ]] = min_cost_flow_model . NewIntVar ( 0 , 100 , 'edge_ %s _ %s ' % edge ) print ( 'Creating the integer variable ' , edge_int_vars [ edge [ 0 ], edge [ 1 ]], 'representing the amount to flow in edge' , ( edge [ 0 ], edge [ 1 ])) Creating the integer variable edge_s_u representing the amount to flow in edge ('s', 'u') Creating the integer variable edge_s_v representing the amount to flow in edge ('s', 'v') Creating the integer variable edge_u_v representing the amount to flow in edge ('u', 'v') Creating the integer variable edge_u_t representing the amount to flow in edge ('u', 't') Creating the integer variable edge_v_t representing the amount to flow in edge ('v', 't') input_node = 's' output_node = 't' flow = 20 # Assuming a constant flow of 20 units # Adding constraints on the nodes for node in g . nodes : in_edges = g . in_edges ( node ) out_edges = g . out_edges ( node ) print ( 'Adding the constraint on node ' , node ) print ( 'This node has %i in-edges and %i out-edges' % ( len ( in_edges ), len ( out_edges ))) equation_at_this_edge = sum ( edge_int_vars [ edge [ 0 ], edge [ 1 ]] for edge in in_edges ) - \\ sum ( edge_int_vars [ edge [ 0 ], edge [ 1 ]] for edge in out_edges ) if ( node == input_node ): min_cost_flow_model . Add ( equation_at_this_edge == - flow ) print ( equation_at_this_edge == - flow ) elif ( node == output_node ): min_cost_flow_model . Add ( equation_at_this_edge == flow ) print ( equation_at_this_edge == flow ) else : min_cost_flow_model . Add ( equation_at_this_edge == 0 ) print ( equation_at_this_edge , '== 0' ) print ( '' ) Adding the constraint on node s This node has 0 in-edges and 2 out-edges (-((edge_s_u) + edge_s_v)) == -20 Adding the constraint on node u This node has 1 in-edges and 2 out-edges ((edge_s_u) + -((edge_u_v) + edge_u_t)) == 0 Adding the constraint on node v This node has 2 in-edges and 1 out-edges (((edge_s_v) + edge_u_v) + -(edge_v_t)) == 0 Adding the constraint on node t This node has 2 in-edges and 0 out-edges (((edge_u_t) + edge_v_t)) == 20 # Adding constraints on the edges for edge in g . edges : print ( 'Adding constraint on edge ' , edge ) max_flow_in_edge = g . get_edge_data ( * edge )[ 'weight' ] min_cost_flow_model . Add ( edge_int_vars [ edge [ 0 ], edge [ 1 ]] <= max_flow_in_edge ) print ( edge_int_vars [ edge [ 0 ], edge [ 1 ]] <= max_flow_in_edge ) Adding constraint on edge ('s', 'u') edge_s_u <= 20 Adding constraint on edge ('s', 'v') edge_s_v <= 10 Adding constraint on edge ('u', 'v') edge_u_v <= 30 Adding constraint on edge ('u', 't') edge_u_t <= 10 Adding constraint on edge ('v', 't') edge_v_t <= 20 # factor to make everything including costs integer factor_to_int = 10 # The objective is to maximise flow total_cost = sum ( int ( g . get_edge_data ( * edge )[ 'cost' ] * factor_to_int ) * edge_int_vars [ edge [ 0 ], edge [ 1 ]] for edge in g . edges ) print ( 'Objective is to optimise cost' ) print ( total_cost ) min_cost_flow_model . Minimize ( total_cost ) Objective is to optimise cost ((((((30 * edge_s_u)) + (20 * edge_s_v)) + (7 * edge_u_v)) + (10 * edge_u_t)) + (70 * edge_v_t)) # Solving the problem solver = cp_model . CpSolver () solution_printer = cp_model . ObjectiveSolutionPrinter () status = solver . SolveWithSolutionCallback ( min_cost_flow_model , solution_printer ) Solution 0, time = 0.24 s, objective = 1300 cp_model . OPTIMAL == status True The solution to the minimum flow problem is result_edges = {} for edge in g . edges : result_edges [ edge [ 0 ], edge [ 1 ]] = solver . Value ( edge_int_vars [ edge [ 0 ], edge [ 1 ]]) result_edges {('s', 'u'): 10, ('s', 'v'): 10, ('u', 'v'): 0, ('u', 't'): 10, ('v', 't'): 10} # This function gets the coordinates for nodes pos = nx . get_node_attributes ( g , 'pos' ) # The simplest way to draw a graph is by using nx.draw. nx . draw ( g , pos , with_labels = True ) # This adds edge lables nx . draw_networkx_edge_labels ( g , pos , edge_labels = result_edges ) plt . show ()","title":"Network flow problems (Python)"},{"location":"Python/Network%20Flow%20problems/#flow-maximisation-problems","text":"Author: Achyuthuni Sri Harsha A directed graph has the capacities on all the edges and our job is to find the maximum amount of flow that can happen from one node (starting node) to another node(outflow node). There are two types of flow maximisation problems: 1. Max flow: What is the maximum flow that can be sent from source to outfow without exceeding capacity. 2. Min cost flow: We have the cost along wih capacities on each edge. We want to find the minimum cost path of sending f uits of flow from source to outflow. We can use integer programming to solve both these problems. import networkx as nx # Other packages for manupulating data import numpy as np import pandas as pd # packages for plotting import matplotlib.pyplot as plt","title":"Flow maximisation problems"},{"location":"Python/Network%20Flow%20problems/#maximum-flow-problem","text":"Let us first consider the max flow problem. Consider the below graph. edgelist_df = pd . DataFrame ({ 'node1' :[ 's' , 's' , 'u' , 'u' , 'v' ], 'node2' :[ 'u' , 'v' , 'v' , 't' , 't' ], 'weights' :[ 20 , 10 , 30 , 10 , 20 ] }) edgelist_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } node1 node2 weights 0 s u 20 1 s v 10 2 u v 30 3 u t 10 4 v t 20 g = nx . DiGraph () for i , elrow in edgelist_df . iterrows (): g . add_edge ( elrow [ 0 ], elrow [ 1 ], weight = elrow [ 2 ]) g . edges ( data = True ) OutEdgeDataView([('s', 'u', {'weight': 20}), ('s', 'v', {'weight': 10}), ('u', 'v', {'weight': 30}), ('u', 't', {'weight': 10}), ('v', 't', {'weight': 20})]) # for each node we are trying to fix the coordinates g . add_node ( 's' , pos = ( 0 , 1 )) g . add_node ( 'u' , pos = ( 1 , 2 )) g . add_node ( 'v' , pos = ( 1 , 0 )) g . add_node ( 't' , pos = ( 2 , 1 )) g . nodes ( data = True ) NodeDataView({'s': {'pos': (0, 1)}, 'u': {'pos': (1, 2)}, 'v': {'pos': (1, 0)}, 't': {'pos': (2, 1)}}) pos = nx . get_node_attributes ( g , 'pos' ) weight = nx . get_edge_attributes ( g , 'weight' ) nx . draw ( g , pos , with_labels = True ) nx . draw_networkx_edge_labels ( g , pos , edge_labels = weight ) plt . show () We want to find the maximum flow between s and t. This can be formulated as an integer programming problem, with Decision variable 1. Integer variable \\(edge_{i,j}\\) representing the amount of flow in edge ij. 2. Integer variable f representing the maximum flow from ortools.sat.python import cp_model max_flow_model = cp_model . CpModel () # Creating one integer decision variable for each edge edge_int_vars = {} for edge in g . edges : edge_int_vars [ edge [ 0 ], edge [ 1 ]] = max_flow_model . NewIntVar ( 0 , 100 , 'edge_ %s _ %s ' % edge ) print ( 'Creating the integer variable ' , edge_int_vars [ edge [ 0 ], edge [ 1 ]], 'representing the amount to flow in edge' , ( edge [ 0 ], edge [ 1 ])) Creating the integer variable edge_s_u representing the amount to flow in edge ('s', 'u') Creating the integer variable edge_s_v representing the amount to flow in edge ('s', 'v') Creating the integer variable edge_u_v representing the amount to flow in edge ('u', 'v') Creating the integer variable edge_u_t representing the amount to flow in edge ('u', 't') Creating the integer variable edge_v_t representing the amount to flow in edge ('v', 't') Constraints The first set of constraints are flow constraints, 1. At the input node, the net flow should be -f 2. At the output node, the net flow should be +f 3. The net flow in all other nodes should be 0 input_node = 's' output_node = 't' # Adding constraints on the nodes flow = max_flow_model . NewIntVar ( 0 , 100 , 'flow' ) # Initialising flow this will be maximised later for node in g . nodes : in_edges = g . in_edges ( node ) out_edges = g . out_edges ( node ) print ( 'Adding the constraint on node ' , node ) equation_at_this_edge = sum ( edge_int_vars [ edge [ 0 ], edge [ 1 ]] for edge in in_edges ) - \\ sum ( edge_int_vars [ edge [ 0 ], edge [ 1 ]] for edge in out_edges ) if ( node == input_node ): max_flow_model . Add ( equation_at_this_edge == - flow ) print ( equation_at_this_edge == - flow ) elif ( node == output_node ): max_flow_model . Add ( equation_at_this_edge == flow ) print ( equation_at_this_edge == flow ) else : max_flow_model . Add ( equation_at_this_edge == 0 ) print ( equation_at_this_edge , '== 0' ) print ( '' ) Adding the constraint on node s ((-((edge_s_u) + edge_s_v)) + (1 * flow)) == 0 Adding the constraint on node u ((edge_s_u) + -((edge_u_v) + edge_u_t)) == 0 Adding the constraint on node v (((edge_s_v) + edge_u_v) + -(edge_v_t)) == 0 Adding the constraint on node t ((((edge_u_t) + edge_v_t)) + -flow) == 0 Another set of constraints are the capacity restrictions on every edge. # Adding constraints on the edges for edge in g . edges : print ( 'Adding constraint on edge ' , edge ) max_flow_in_edge = g . get_edge_data ( * edge )[ 'weight' ] max_flow_model . Add ( edge_int_vars [ edge [ 0 ], edge [ 1 ]] <= max_flow_in_edge ) print ( edge_int_vars [ edge [ 0 ], edge [ 1 ]] <= max_flow_in_edge ) Adding constraint on edge ('s', 'u') edge_s_u <= 20 Adding constraint on edge ('s', 'v') edge_s_v <= 10 Adding constraint on edge ('u', 'v') edge_u_v <= 30 Adding constraint on edge ('u', 't') edge_u_t <= 10 Adding constraint on edge ('v', 't') edge_v_t <= 20 Objective The objective is to maximise flow. # The objective is to maximise flow max_flow_model . Maximize ( flow ) Solving the problem, we get # Solving the problem solver = cp_model . CpSolver () solution_printer = cp_model . ObjectiveSolutionPrinter () status = solver . SolveWithSolutionCallback ( max_flow_model , solution_printer ) Solution 0, time = 0.44 s, objective = 30 cp_model . OPTIMAL == status True result_edges = {} for edge in g . edges : result_edges [ edge [ 0 ], edge [ 1 ]] = solver . Value ( edge_int_vars [ edge [ 0 ], edge [ 1 ]]) result_edges {('s', 'u'): 20, ('s', 'v'): 10, ('u', 'v'): 10, ('u', 't'): 10, ('v', 't'): 20} The flow in each of the nodes is shown in the figure pos = nx . get_node_attributes ( g , 'pos' ) nx . draw ( g , pos , with_labels = True ) nx . draw_networkx_edge_labels ( g , pos , edge_labels = result_edges ) plt . show ()","title":"Maximum flow problem"},{"location":"Python/Network%20Flow%20problems/#minimum-cost-flow-problems","text":"We want to find the minimum cost to transfer a fixed amount of flow from one edge to another. The costs are given int he table bwlow. The costs are also displayed in the network below. edgelist_df = pd . DataFrame ({ 'node1' :[ 's' , 's' , 'u' , 'u' , 'v' ], 'node2' :[ 'u' , 'v' , 'v' , 't' , 't' ], 'weights' :[ 20 , 10 , 30 , 10 , 20 ], 'costs' :[ 3 , 2 , 0.7 , 1 , 7 ] }) edgelist_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } node1 node2 weights costs 0 s u 20 3.0 1 s v 10 2.0 2 u v 30 0.7 3 u t 10 1.0 4 v t 20 7.0 g = nx . DiGraph () for i , elrow in edgelist_df . iterrows (): g . add_edge ( elrow [ 0 ], elrow [ 1 ], weight = elrow [ 2 ], cost = elrow [ 3 ]) g . edges ( data = True ) OutEdgeDataView([('s', 'u', {'weight': 20, 'cost': 3.0}), ('s', 'v', {'weight': 10, 'cost': 2.0}), ('u', 'v', {'weight': 30, 'cost': 0.7}), ('u', 't', {'weight': 10, 'cost': 1.0}), ('v', 't', {'weight': 20, 'cost': 7.0})]) # for each node we are trying to fix the coordinates g . add_node ( 's' , pos = ( 0 , 1 )) g . add_node ( 'u' , pos = ( 1 , 2 )) g . add_node ( 'v' , pos = ( 1 , 0 )) g . add_node ( 't' , pos = ( 2 , 1 )) g . nodes ( data = True ) NodeDataView({'s': {'pos': (0, 1)}, 'u': {'pos': (1, 2)}, 'v': {'pos': (1, 0)}, 't': {'pos': (2, 1)}}) pos = nx . get_node_attributes ( g , 'pos' ) weight = nx . get_edge_attributes ( g , 'weight' ) cost = nx . get_edge_attributes ( g , 'cost' ) res = { key : str ( weight [ key ]) + '/$' + str ( cost . get ( key , '' )) for key in cost . keys ()} nx . draw ( g , pos , with_labels = True ) nx . draw_networkx_edge_labels ( g , pos , edge_labels = res ) plt . show () Just like maximum flow, we can use integer programming to find out the solution in this scenario also. Decision variables Integer variables \\(edge_{i,j}\\) representing the amount of flow in edge (i,j) Constraints Capacity restrictions on every edge Objective The objective is to minimise the overall cost from ortools.sat.python import cp_model min_cost_flow_model = cp_model . CpModel () # Creating one integer decision variable for each edge edge_int_vars = {} for edge in g . edges : edge_int_vars [ edge [ 0 ], edge [ 1 ]] = min_cost_flow_model . NewIntVar ( 0 , 100 , 'edge_ %s _ %s ' % edge ) print ( 'Creating the integer variable ' , edge_int_vars [ edge [ 0 ], edge [ 1 ]], 'representing the amount to flow in edge' , ( edge [ 0 ], edge [ 1 ])) Creating the integer variable edge_s_u representing the amount to flow in edge ('s', 'u') Creating the integer variable edge_s_v representing the amount to flow in edge ('s', 'v') Creating the integer variable edge_u_v representing the amount to flow in edge ('u', 'v') Creating the integer variable edge_u_t representing the amount to flow in edge ('u', 't') Creating the integer variable edge_v_t representing the amount to flow in edge ('v', 't') input_node = 's' output_node = 't' flow = 20 # Assuming a constant flow of 20 units # Adding constraints on the nodes for node in g . nodes : in_edges = g . in_edges ( node ) out_edges = g . out_edges ( node ) print ( 'Adding the constraint on node ' , node ) print ( 'This node has %i in-edges and %i out-edges' % ( len ( in_edges ), len ( out_edges ))) equation_at_this_edge = sum ( edge_int_vars [ edge [ 0 ], edge [ 1 ]] for edge in in_edges ) - \\ sum ( edge_int_vars [ edge [ 0 ], edge [ 1 ]] for edge in out_edges ) if ( node == input_node ): min_cost_flow_model . Add ( equation_at_this_edge == - flow ) print ( equation_at_this_edge == - flow ) elif ( node == output_node ): min_cost_flow_model . Add ( equation_at_this_edge == flow ) print ( equation_at_this_edge == flow ) else : min_cost_flow_model . Add ( equation_at_this_edge == 0 ) print ( equation_at_this_edge , '== 0' ) print ( '' ) Adding the constraint on node s This node has 0 in-edges and 2 out-edges (-((edge_s_u) + edge_s_v)) == -20 Adding the constraint on node u This node has 1 in-edges and 2 out-edges ((edge_s_u) + -((edge_u_v) + edge_u_t)) == 0 Adding the constraint on node v This node has 2 in-edges and 1 out-edges (((edge_s_v) + edge_u_v) + -(edge_v_t)) == 0 Adding the constraint on node t This node has 2 in-edges and 0 out-edges (((edge_u_t) + edge_v_t)) == 20 # Adding constraints on the edges for edge in g . edges : print ( 'Adding constraint on edge ' , edge ) max_flow_in_edge = g . get_edge_data ( * edge )[ 'weight' ] min_cost_flow_model . Add ( edge_int_vars [ edge [ 0 ], edge [ 1 ]] <= max_flow_in_edge ) print ( edge_int_vars [ edge [ 0 ], edge [ 1 ]] <= max_flow_in_edge ) Adding constraint on edge ('s', 'u') edge_s_u <= 20 Adding constraint on edge ('s', 'v') edge_s_v <= 10 Adding constraint on edge ('u', 'v') edge_u_v <= 30 Adding constraint on edge ('u', 't') edge_u_t <= 10 Adding constraint on edge ('v', 't') edge_v_t <= 20 # factor to make everything including costs integer factor_to_int = 10 # The objective is to maximise flow total_cost = sum ( int ( g . get_edge_data ( * edge )[ 'cost' ] * factor_to_int ) * edge_int_vars [ edge [ 0 ], edge [ 1 ]] for edge in g . edges ) print ( 'Objective is to optimise cost' ) print ( total_cost ) min_cost_flow_model . Minimize ( total_cost ) Objective is to optimise cost ((((((30 * edge_s_u)) + (20 * edge_s_v)) + (7 * edge_u_v)) + (10 * edge_u_t)) + (70 * edge_v_t)) # Solving the problem solver = cp_model . CpSolver () solution_printer = cp_model . ObjectiveSolutionPrinter () status = solver . SolveWithSolutionCallback ( min_cost_flow_model , solution_printer ) Solution 0, time = 0.24 s, objective = 1300 cp_model . OPTIMAL == status True The solution to the minimum flow problem is result_edges = {} for edge in g . edges : result_edges [ edge [ 0 ], edge [ 1 ]] = solver . Value ( edge_int_vars [ edge [ 0 ], edge [ 1 ]]) result_edges {('s', 'u'): 10, ('s', 'v'): 10, ('u', 'v'): 0, ('u', 't'): 10, ('v', 't'): 10} # This function gets the coordinates for nodes pos = nx . get_node_attributes ( g , 'pos' ) # The simplest way to draw a graph is by using nx.draw. nx . draw ( g , pos , with_labels = True ) # This adds edge lables nx . draw_networkx_edge_labels ( g , pos , edge_labels = result_edges ) plt . show ()","title":"Minimum cost flow problems"},{"location":"Python/Network%20Science/","text":"Network Science \u00b6 Author: Achyuthuni Sri Harsha We live in a highly connected world where social networks significantly affect our lives, from getting jobs, connecting with friends, and dating and news. Network science is the study of such complex networks. There is some common phenomenon that applies to social networks: How do networks evolve \u00b6 A social network is constantly evolving; people make new friends and lose touch with other friends. Many external factors in life, like marriage, new college, school, workplace or club, can spark the creation of new friends. This phenomenon is observed in vast networks such as the internet. Is there any model that can explain the growth of networks? If so, we can simulate new networks, identify what parameters influence the growth of new networks etc. Power law explains some real-world examples of the growth and scale of networks. Small World Phenomena \u00b6 Small world phenomenon states that everyone in the world can be reached thru a short chain of acquaintances. This was first demonstrated in the Milgram experiment where random people were asked to send a letter to a stockbroker in Boston by passing it through people they know. Of the letters received, they went thru an average of 6 people. Facebook repeated this on a large scale later, which validated this with a lower number of connections. This indicates that most people are connected to every other random person on earth within six common friends. Homophily \u00b6 This can be described as \"Birds of a feather flock together\". We tend to be friends with friends who are similar to us, especially people from the same socio-economic background, social background, etc. Strength of weak ties \u00b6 If two people in a social network have a friend in common, then there is an increased likelihood that they will become friends at some point in the future. This also depends on the strength of the friendship(ties). Generally, people in the same network will have the same kind of information. Take an example of jobs; everyone in the same friend circle will have the same information about open positions for jobs. It is the weak ties, friends whom we have not interacted with within some time, who have new information which will help most people to find jobs. This is called as the strength of weak ties. Let us look at a small network of friends in Zachary karate club . Zachary's karate club is a university-based karate club consisting of a social network of 34 members. W.W. Zachary famously observed the karate club for three years (from 1970 to 1972), documenting links between pairs of members who interacted outside the everyday activities of the club. The nodes in the graph represent people, and the edges represent friendships (interactions) between them. import matplotlib.pyplot as plt % matplotlib inline import networkx as nx G = nx . karate_club_graph () nx . draw_kamada_kawai ( G , with_labels = True ) We can understand the network using some statistics about the network. Network Statistics \u00b6 Degree Distribution \u00b6 The degree of a node is the number of nodes a particular node is connected to. For example, node 11 has one degree in the karate club network as it is connected to one node only(node 0). The degree distribution is the relative frequencies of the nodes that have particular degrees. There are two types of degree distributions generally, Poisson and scale-free. For the karate club network, the degree distribution is: degree_dist = nx . degree_histogram ( G ) plt . bar ( range ( len ( degree_dist )), degree_dist ) plt . ylabel ( 'No of people' ) plt . xlabel ( 'No of friends' ); This indicates that most people have 2-5 friends while a few people are connected to >15 people in the network. Clustering coefficient \u00b6 Another statistic that we can understand is the clustering coefficient. This is the degree to which nodes cluster together. If A is friends with B and C, then B and c have a high probability of being friends if they belong to the same cluster. the clustering coefficient captures the fraction of triads (connected A, B and C) in the network. The clustering coefficient of all the nodes is calculated. The clustering coefficient of a node is the percentage of triangles that are there among the nodes that it is connected to. For example, node 11 is connected to node 6 and 5, both of which are connected. So the percentage of triads is 100%. Node 4 is connected to 0, 6 and 10. 0-6-4 and 0-10-0 make a triad, but 0-10-4 does not exist. Therefore the percentage is 66.6%. The clustering coefficient of the network is the average of the clustering coefficient across all the nodes, and it is 57% for the karate club. nx . clustering ( G ) {0: 0.15, 1: 0.3333333333333333, 2: 0.24444444444444444, 3: 0.6666666666666666, 4: 0.6666666666666666, 5: 0.5, 6: 0.5, 7: 1.0, 8: 0.5, 9: 0, 10: 0.6666666666666666, 11: 0, 12: 1.0, 13: 0.6, 14: 1.0, 15: 1.0, 16: 1.0, 17: 1.0, 18: 1.0, 19: 0.3333333333333333, 20: 1.0, 21: 1.0, 22: 1.0, 23: 0.4, 24: 0.3333333333333333, 25: 0.3333333333333333, 26: 1.0, 27: 0.16666666666666666, 28: 0.3333333333333333, 29: 0.6666666666666666, 30: 0.5, 31: 0.2, 32: 0.19696969696969696, 33: 0.11029411764705882} nx . algorithms . cluster . average_clustering ( G ) 0.5706384782076823 Community Detection \u00b6 During the study in Karate club, a conflict arose between the club president 'A' and instructor 'B' (pseudonyms), which led to a split in the club. Half of the members formed a new club around A (group one) and the remaining members under B. Can we identify the 2 communities in this network? Girvan Newman \u00b6 Community detection is close to clustering in machine learning. In clustering, we do the clustering based on the distance between the points when placed in an n-dimensional space. Similar to clustering, in community detection, we want to cluster the nodes closer to each other than others. The only difference is the concept of distance is based on the topology of the network. Like clustering, we can divide the network into two communities using top-down and bottom-up methods. This is represented by a dendrogram where we can draw a horizontal line to split the communities. This is discussed in more detail in a different blog. The two groups based on Girvan Newman are: karate_club_split = nx . algorithms . community . centrality . girvan_newman ( G ) karate_club_split_tuple = tuple ( sorted ( c ) for c in next ( karate_club_split )) karate_club_split_tuple ([0, 1, 3, 4, 5, 6, 7, 10, 11, 12, 13, 16, 17, 19, 21], [2, 8, 9, 14, 15, 18, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]) # To get colors in the graph color_list = [ 'r' if node in karate_club_split_tuple [ 0 ] else 'g' for node in G . nodes ] nx . draw_kamada_kawai ( G , node_color = color_list , with_labels = True ) Ratio cut \u00b6 Another method is the ratio cut method. A cut is a set of edges such that if we remove them, the network breaks into two components. A network will have many cuts, and to find an optimal split, we need to: 1. Find the min-balanced-cut (cut with the minimum number of edges with an almost equal number of nodes on each side) among all pairs of nodes 2. Repeat over each of the two parts This is discussed in more detail in a different blog. References \u00b6 Zachary, W. W. (1977). An Information Flow Model for Conflict and Fission in Small Groups. Journal of Anthropological Research. 33 (4), pp. 452-473. Networkx Karate club","title":"Network Science (Python)"},{"location":"Python/Network%20Science/#network-science","text":"Author: Achyuthuni Sri Harsha We live in a highly connected world where social networks significantly affect our lives, from getting jobs, connecting with friends, and dating and news. Network science is the study of such complex networks. There is some common phenomenon that applies to social networks:","title":"Network Science"},{"location":"Python/Network%20Science/#how-do-networks-evolve","text":"A social network is constantly evolving; people make new friends and lose touch with other friends. Many external factors in life, like marriage, new college, school, workplace or club, can spark the creation of new friends. This phenomenon is observed in vast networks such as the internet. Is there any model that can explain the growth of networks? If so, we can simulate new networks, identify what parameters influence the growth of new networks etc. Power law explains some real-world examples of the growth and scale of networks.","title":"How do networks evolve"},{"location":"Python/Network%20Science/#small-world-phenomena","text":"Small world phenomenon states that everyone in the world can be reached thru a short chain of acquaintances. This was first demonstrated in the Milgram experiment where random people were asked to send a letter to a stockbroker in Boston by passing it through people they know. Of the letters received, they went thru an average of 6 people. Facebook repeated this on a large scale later, which validated this with a lower number of connections. This indicates that most people are connected to every other random person on earth within six common friends.","title":"Small World Phenomena"},{"location":"Python/Network%20Science/#homophily","text":"This can be described as \"Birds of a feather flock together\". We tend to be friends with friends who are similar to us, especially people from the same socio-economic background, social background, etc.","title":"Homophily"},{"location":"Python/Network%20Science/#strength-of-weak-ties","text":"If two people in a social network have a friend in common, then there is an increased likelihood that they will become friends at some point in the future. This also depends on the strength of the friendship(ties). Generally, people in the same network will have the same kind of information. Take an example of jobs; everyone in the same friend circle will have the same information about open positions for jobs. It is the weak ties, friends whom we have not interacted with within some time, who have new information which will help most people to find jobs. This is called as the strength of weak ties. Let us look at a small network of friends in Zachary karate club . Zachary's karate club is a university-based karate club consisting of a social network of 34 members. W.W. Zachary famously observed the karate club for three years (from 1970 to 1972), documenting links between pairs of members who interacted outside the everyday activities of the club. The nodes in the graph represent people, and the edges represent friendships (interactions) between them. import matplotlib.pyplot as plt % matplotlib inline import networkx as nx G = nx . karate_club_graph () nx . draw_kamada_kawai ( G , with_labels = True ) We can understand the network using some statistics about the network.","title":"Strength of weak ties"},{"location":"Python/Network%20Science/#network-statistics","text":"","title":"Network Statistics"},{"location":"Python/Network%20Science/#degree-distribution","text":"The degree of a node is the number of nodes a particular node is connected to. For example, node 11 has one degree in the karate club network as it is connected to one node only(node 0). The degree distribution is the relative frequencies of the nodes that have particular degrees. There are two types of degree distributions generally, Poisson and scale-free. For the karate club network, the degree distribution is: degree_dist = nx . degree_histogram ( G ) plt . bar ( range ( len ( degree_dist )), degree_dist ) plt . ylabel ( 'No of people' ) plt . xlabel ( 'No of friends' ); This indicates that most people have 2-5 friends while a few people are connected to >15 people in the network.","title":"Degree Distribution"},{"location":"Python/Network%20Science/#clustering-coefficient","text":"Another statistic that we can understand is the clustering coefficient. This is the degree to which nodes cluster together. If A is friends with B and C, then B and c have a high probability of being friends if they belong to the same cluster. the clustering coefficient captures the fraction of triads (connected A, B and C) in the network. The clustering coefficient of all the nodes is calculated. The clustering coefficient of a node is the percentage of triangles that are there among the nodes that it is connected to. For example, node 11 is connected to node 6 and 5, both of which are connected. So the percentage of triads is 100%. Node 4 is connected to 0, 6 and 10. 0-6-4 and 0-10-0 make a triad, but 0-10-4 does not exist. Therefore the percentage is 66.6%. The clustering coefficient of the network is the average of the clustering coefficient across all the nodes, and it is 57% for the karate club. nx . clustering ( G ) {0: 0.15, 1: 0.3333333333333333, 2: 0.24444444444444444, 3: 0.6666666666666666, 4: 0.6666666666666666, 5: 0.5, 6: 0.5, 7: 1.0, 8: 0.5, 9: 0, 10: 0.6666666666666666, 11: 0, 12: 1.0, 13: 0.6, 14: 1.0, 15: 1.0, 16: 1.0, 17: 1.0, 18: 1.0, 19: 0.3333333333333333, 20: 1.0, 21: 1.0, 22: 1.0, 23: 0.4, 24: 0.3333333333333333, 25: 0.3333333333333333, 26: 1.0, 27: 0.16666666666666666, 28: 0.3333333333333333, 29: 0.6666666666666666, 30: 0.5, 31: 0.2, 32: 0.19696969696969696, 33: 0.11029411764705882} nx . algorithms . cluster . average_clustering ( G ) 0.5706384782076823","title":"Clustering coefficient"},{"location":"Python/Network%20Science/#community-detection","text":"During the study in Karate club, a conflict arose between the club president 'A' and instructor 'B' (pseudonyms), which led to a split in the club. Half of the members formed a new club around A (group one) and the remaining members under B. Can we identify the 2 communities in this network?","title":"Community Detection"},{"location":"Python/Network%20Science/#girvan-newman","text":"Community detection is close to clustering in machine learning. In clustering, we do the clustering based on the distance between the points when placed in an n-dimensional space. Similar to clustering, in community detection, we want to cluster the nodes closer to each other than others. The only difference is the concept of distance is based on the topology of the network. Like clustering, we can divide the network into two communities using top-down and bottom-up methods. This is represented by a dendrogram where we can draw a horizontal line to split the communities. This is discussed in more detail in a different blog. The two groups based on Girvan Newman are: karate_club_split = nx . algorithms . community . centrality . girvan_newman ( G ) karate_club_split_tuple = tuple ( sorted ( c ) for c in next ( karate_club_split )) karate_club_split_tuple ([0, 1, 3, 4, 5, 6, 7, 10, 11, 12, 13, 16, 17, 19, 21], [2, 8, 9, 14, 15, 18, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]) # To get colors in the graph color_list = [ 'r' if node in karate_club_split_tuple [ 0 ] else 'g' for node in G . nodes ] nx . draw_kamada_kawai ( G , node_color = color_list , with_labels = True )","title":"Girvan Newman"},{"location":"Python/Network%20Science/#ratio-cut","text":"Another method is the ratio cut method. A cut is a set of edges such that if we remove them, the network breaks into two components. A network will have many cuts, and to find an optimal split, we need to: 1. Find the min-balanced-cut (cut with the minimum number of edges with an almost equal number of nodes on each side) among all pairs of nodes 2. Repeat over each of the two parts This is discussed in more detail in a different blog.","title":"Ratio cut"},{"location":"Python/Network%20Science/#references","text":"Zachary, W. W. (1977). An Information Flow Model for Conflict and Fission in Small Groups. Journal of Anthropological Research. 33 (4), pp. 452-473. Networkx Karate club","title":"References"},{"location":"Python/Network%20centrality/","text":"Centrality measures \u00b6 Author: Achyuthuni Sri Harsha Centrality measures help us understand who are the most important people in the network. For this example, \u201cemail-Eu-core network,\u201d from Stanford\u2019s SNAP is used. You may find the original dataset here: https://snap.stanford.edu/data/email-Eu-core.html .. This is a dataset of various email communications between people, and we are interested about the important people, leaders and opinion makers in this network. import pandas as pd import numpy as np import matplotlib.pyplot as plt import networkx as nx % matplotlib inline For the purposes of this blog, I am filtering the data only for the first 100 people for brevity. Same analysis can be done across the complete dataset. df = pd . read_csv ( \"https://raw.githubusercontent.com/jinhangjiang/Datasets/main/Network%20Data/emailEUcore/email-Eu-core.txt\" , delimiter = \" \" , names = [ \"Source\" , \"Target\" ]) df = df . loc [( df . Source < 100 ) & ( df . Target < 100 )] df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Source Target 0 0 1 1 2 3 2 2 4 3 5 6 4 5 7 We are using a directed graph as we are interested emails being sent and received. G = nx . from_pandas_edgelist ( df , source = 'Source' , target = 'Target' , create_using = nx . DiGraph ()) plt . subplots ( figsize = ( 20 , 20 )) plt . title ( 'Email, EU-Core network' ) nx . draw_kamada_kawai ( G , with_labels = True ) We can see that there are two densily connected networks and many connections between them. Who is the most important person in the network? This is based on what is the question you are asking. Is it the person who can send the emails the fastest? Is it the person why can connect with maximum number of people? Some basic features would be: 1. Just by the number of mails sent/received, who has sent the maximum number of mails 2. By the number of people that a person has a immediate relation with (a relation is defined as sending an email to that person): degree centrality 3. We can also look at the persons who are essential for communication between different closely knit groups. (betweenness centrality) The person with the maximum number of interactions is: student_sent_mails = list ( df . groupby ( 'Source' )[ 'Target' ] . count ()) student_sent_mails . index ( max ( student_sent_mails )) + 1 86 We can say that the person with index 86 has the most number of interactions and is one of the most important person. He would be the go to person who has connections with maximum number of people. We can see this in the graph also. Degree centrality \u00b6 This way of looking at the importance of a person based on number of connections (Degrees) is called degree centrality. In ascending order, the top 10 people with maximum number of connections are: degree_centrality = nx . algorithms . centrality . degree_centrality ( G ) for i , w in enumerate ( sorted ( degree_centrality , key = degree_centrality . get , reverse = True )): if ( i < 10 ): print ( w , degree_centrality [ w ]) else : break 86 0.8282828282828284 62 0.7676767676767677 82 0.6464646464646465 96 0.6161616161616162 28 0.6060606060606061 21 0.5555555555555556 13 0.5353535353535354 23 0.5151515151515152 30 0.494949494949495 64 0.494949494949495 This network can be visualised with most size proportional to the importance according to degree centrality. plt . subplots ( figsize = ( 20 , 20 )) plt . title ( 'Email, EU-Core network: degree centrality' ) nx . draw_kamada_kawai ( G , with_labels = True , node_size = list ( np . array ( list ( degree_centrality . values ())) * 500 )) Betweenness centrality \u00b6 Although person 86 has sent mails to the maximum number of people, he has still sent mails to only 45 out of the 100 people. This means if we want to send a mail to the whole class, that there might be other important people thru whom the remaining people receive mails. We should identify people who are part of different closely knit groups and who are essential for passing information between groups. This is given by betweenness centrality. The most important people according to this metric in descending order are: btw_centrality = nx . algorithms . centrality . betweenness_centrality ( G ) for i , w in enumerate ( sorted ( btw_centrality . items (), key = lambda item : item [ 1 ], reverse = True )): if ( i < 10 ): print ( w ) else : break (86, 0.14149427475629858) (62, 0.12154539177634052) (96, 0.08160276331043798) (82, 0.0670387255089669) (64, 0.06502342115189583) (21, 0.05061277358840613) (13, 0.04673865990243992) (44, 0.04327552467861193) (5, 0.0412363399245883) (28, 0.03679924185307917) plt . subplots ( figsize = ( 20 , 20 )) plt . title ( 'Email, EU-Core network: betweenness centrality' ) nx . draw_kamada_kawai ( G , with_labels = True , node_size = list ( np . array ( list ( btw_centrality . values ())) * 5000 )) We get the same people at the top, nut this time inclusding a few additional people. Page rank (Eigenvector centrality) \u00b6 Another way to identify the important people in a network is to not only look at the number of people who are connected to the person but also the importance of the people whom they are connected to. The importance of the person can be defined as proportional to the sum of importance of the immediate people connected to him/her. This is defined by eigenvector centrality. Below is the top ten people who are important based on who they know and whom they are connected to: ev_centrality = nx . algorithms . centrality . eigenvector_centrality ( G ) for i , w in enumerate ( sorted ( ev_centrality . items (), key = lambda item : item [ 1 ], reverse = True )): if ( i < 10 ): print ( w ) else : break (28, 0.25779806388704485) (23, 0.2412215124270137) (30, 0.21579251773540695) (62, 0.2061936996975851) (29, 0.1962488726486241) (35, 0.19522994610327346) (86, 0.18934292971595598) (27, 0.1823911579409791) (96, 0.18188010730196982) (40, 0.1803640484660976) plt . subplots ( figsize = ( 20 , 20 )) plt . title ( 'Email, EU-Core network: eigenvector centrality' ) nx . draw_kamada_kawai ( G , with_labels = True , node_size = list ( np . array ( list ( ev_centrality . values ())) * 2500 )) Clustering coefficient \u00b6 Any graph in general can be densely connected or sparcely connected. The behaviour of the graph, and therefore the actions that have to be taken, are different for sparce graphs vs densely connected graphs. The betweenness measures above should be looked at along with clustering coefficient. The clustering coefficient is a way of measuring the degree to which the nodes in a graph cluster together. Networks with high number of clustering coefficient are more social. The average clustering coefficient for the graph is (assuming a undirected graph): G_undir = nx . from_pandas_edgelist ( df , source = 'Source' , target = 'Target' ) nx . algorithms . cluster . average_clustering ( G_undir ) 0.5130839320504162 From the above analysis, we can find the most important people in the network. The people who has the most connections, the people who are part of multiple groups and are important to send message across and the people who are important based on whom they know immediately. References \u00b6 https://towardsdatascience.com/visualize-high-dimensional-network-data-with-3d-360-degree-animated-scatter-plot-d583932d3693 Assignment and student notes, Business Analytics MSc, Imperial College London, Network Analytics module Hao Yin, Austin R. Benson, Jure Leskovec, and David F. Gleich. \"Local Higher-order Graph Clustering.\" In Proceedings of the 23 rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 2017 https://snap.stanford.edu/data/email-Eu-core.html Networks, Crowds, and Markets: Reasoning about a Highly Connected World by David Easley and Jon Kleinberg","title":"Network Centrality (Python)"},{"location":"Python/Network%20centrality/#centrality-measures","text":"Author: Achyuthuni Sri Harsha Centrality measures help us understand who are the most important people in the network. For this example, \u201cemail-Eu-core network,\u201d from Stanford\u2019s SNAP is used. You may find the original dataset here: https://snap.stanford.edu/data/email-Eu-core.html .. This is a dataset of various email communications between people, and we are interested about the important people, leaders and opinion makers in this network. import pandas as pd import numpy as np import matplotlib.pyplot as plt import networkx as nx % matplotlib inline For the purposes of this blog, I am filtering the data only for the first 100 people for brevity. Same analysis can be done across the complete dataset. df = pd . read_csv ( \"https://raw.githubusercontent.com/jinhangjiang/Datasets/main/Network%20Data/emailEUcore/email-Eu-core.txt\" , delimiter = \" \" , names = [ \"Source\" , \"Target\" ]) df = df . loc [( df . Source < 100 ) & ( df . Target < 100 )] df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Source Target 0 0 1 1 2 3 2 2 4 3 5 6 4 5 7 We are using a directed graph as we are interested emails being sent and received. G = nx . from_pandas_edgelist ( df , source = 'Source' , target = 'Target' , create_using = nx . DiGraph ()) plt . subplots ( figsize = ( 20 , 20 )) plt . title ( 'Email, EU-Core network' ) nx . draw_kamada_kawai ( G , with_labels = True ) We can see that there are two densily connected networks and many connections between them. Who is the most important person in the network? This is based on what is the question you are asking. Is it the person who can send the emails the fastest? Is it the person why can connect with maximum number of people? Some basic features would be: 1. Just by the number of mails sent/received, who has sent the maximum number of mails 2. By the number of people that a person has a immediate relation with (a relation is defined as sending an email to that person): degree centrality 3. We can also look at the persons who are essential for communication between different closely knit groups. (betweenness centrality) The person with the maximum number of interactions is: student_sent_mails = list ( df . groupby ( 'Source' )[ 'Target' ] . count ()) student_sent_mails . index ( max ( student_sent_mails )) + 1 86 We can say that the person with index 86 has the most number of interactions and is one of the most important person. He would be the go to person who has connections with maximum number of people. We can see this in the graph also.","title":"Centrality measures"},{"location":"Python/Network%20centrality/#degree-centrality","text":"This way of looking at the importance of a person based on number of connections (Degrees) is called degree centrality. In ascending order, the top 10 people with maximum number of connections are: degree_centrality = nx . algorithms . centrality . degree_centrality ( G ) for i , w in enumerate ( sorted ( degree_centrality , key = degree_centrality . get , reverse = True )): if ( i < 10 ): print ( w , degree_centrality [ w ]) else : break 86 0.8282828282828284 62 0.7676767676767677 82 0.6464646464646465 96 0.6161616161616162 28 0.6060606060606061 21 0.5555555555555556 13 0.5353535353535354 23 0.5151515151515152 30 0.494949494949495 64 0.494949494949495 This network can be visualised with most size proportional to the importance according to degree centrality. plt . subplots ( figsize = ( 20 , 20 )) plt . title ( 'Email, EU-Core network: degree centrality' ) nx . draw_kamada_kawai ( G , with_labels = True , node_size = list ( np . array ( list ( degree_centrality . values ())) * 500 ))","title":"Degree centrality"},{"location":"Python/Network%20centrality/#betweenness-centrality","text":"Although person 86 has sent mails to the maximum number of people, he has still sent mails to only 45 out of the 100 people. This means if we want to send a mail to the whole class, that there might be other important people thru whom the remaining people receive mails. We should identify people who are part of different closely knit groups and who are essential for passing information between groups. This is given by betweenness centrality. The most important people according to this metric in descending order are: btw_centrality = nx . algorithms . centrality . betweenness_centrality ( G ) for i , w in enumerate ( sorted ( btw_centrality . items (), key = lambda item : item [ 1 ], reverse = True )): if ( i < 10 ): print ( w ) else : break (86, 0.14149427475629858) (62, 0.12154539177634052) (96, 0.08160276331043798) (82, 0.0670387255089669) (64, 0.06502342115189583) (21, 0.05061277358840613) (13, 0.04673865990243992) (44, 0.04327552467861193) (5, 0.0412363399245883) (28, 0.03679924185307917) plt . subplots ( figsize = ( 20 , 20 )) plt . title ( 'Email, EU-Core network: betweenness centrality' ) nx . draw_kamada_kawai ( G , with_labels = True , node_size = list ( np . array ( list ( btw_centrality . values ())) * 5000 )) We get the same people at the top, nut this time inclusding a few additional people.","title":"Betweenness centrality"},{"location":"Python/Network%20centrality/#page-rank-eigenvector-centrality","text":"Another way to identify the important people in a network is to not only look at the number of people who are connected to the person but also the importance of the people whom they are connected to. The importance of the person can be defined as proportional to the sum of importance of the immediate people connected to him/her. This is defined by eigenvector centrality. Below is the top ten people who are important based on who they know and whom they are connected to: ev_centrality = nx . algorithms . centrality . eigenvector_centrality ( G ) for i , w in enumerate ( sorted ( ev_centrality . items (), key = lambda item : item [ 1 ], reverse = True )): if ( i < 10 ): print ( w ) else : break (28, 0.25779806388704485) (23, 0.2412215124270137) (30, 0.21579251773540695) (62, 0.2061936996975851) (29, 0.1962488726486241) (35, 0.19522994610327346) (86, 0.18934292971595598) (27, 0.1823911579409791) (96, 0.18188010730196982) (40, 0.1803640484660976) plt . subplots ( figsize = ( 20 , 20 )) plt . title ( 'Email, EU-Core network: eigenvector centrality' ) nx . draw_kamada_kawai ( G , with_labels = True , node_size = list ( np . array ( list ( ev_centrality . values ())) * 2500 ))","title":"Page rank (Eigenvector centrality)"},{"location":"Python/Network%20centrality/#clustering-coefficient","text":"Any graph in general can be densely connected or sparcely connected. The behaviour of the graph, and therefore the actions that have to be taken, are different for sparce graphs vs densely connected graphs. The betweenness measures above should be looked at along with clustering coefficient. The clustering coefficient is a way of measuring the degree to which the nodes in a graph cluster together. Networks with high number of clustering coefficient are more social. The average clustering coefficient for the graph is (assuming a undirected graph): G_undir = nx . from_pandas_edgelist ( df , source = 'Source' , target = 'Target' ) nx . algorithms . cluster . average_clustering ( G_undir ) 0.5130839320504162 From the above analysis, we can find the most important people in the network. The people who has the most connections, the people who are part of multiple groups and are important to send message across and the people who are important based on whom they know immediately.","title":"Clustering coefficient"},{"location":"Python/Network%20centrality/#references","text":"https://towardsdatascience.com/visualize-high-dimensional-network-data-with-3d-360-degree-animated-scatter-plot-d583932d3693 Assignment and student notes, Business Analytics MSc, Imperial College London, Network Analytics module Hao Yin, Austin R. Benson, Jure Leskovec, and David F. Gleich. \"Local Higher-order Graph Clustering.\" In Proceedings of the 23 rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 2017 https://snap.stanford.edu/data/email-Eu-core.html Networks, Crowds, and Markets: Reasoning about a Highly Connected World by David Easley and Jon Kleinberg","title":"References"},{"location":"Python/Saving%20predictions%20in%20database/","text":"Handling databases using python \u00b6 I want to create a database for storing the predictions and actual values of the air pollution near my home in Hyderabad. In a previous blog , I have demonstrated how we can deploy a flask application using pythonanywhere which predicts the pollution using a machine learning model that I built. In this blog, we will see how we can store this information in a database to be used either for visualisation or other analytics. Free database on yugabyte \u00b6 Yugabyte cloud is one of the few companies that lets us to host a free database. The steps to create a free database is given at the documentation as: I have created a database with the following details: Cluster: hyderabad-pollution-data Username (to connect to cluster): harshaash After creating the cluster, I get the following dashboard Creating tables \u00b6 Once we have a cluster created, we can create tables in three ways. 1. Using the shell in yugabyte 2. Using psycopg2 in Python 3. Using SQLAlchemy Yugabyte shell \u00b6 First, let us create using yugabyte shell. The details of the database connection are: In the shell, we can create a database and tables using the standard SQL commands as follows: CREATE DATABASE hyderabad_aqi ; CREATE TABLE daily_historic_pollution ( date DATE PRIMARY KEY , type VARCHAR ( 20 ), value FLOAT ); Load data using python \u00b6 We can use psycopg to connect and create the same table as shown above. The details of the connection are provided under \"Run your own application\". After connecting to the database, we can create tables using SQL commands. import psycopg2 conn = psycopg2 . connect ( \"postgresql://[user[:password]@][netloc][:port][/dbname][?param1=value1&...]\" ) conn . set_session ( autocommit = True ) cur = conn . cursor () cur . execute ( \"\"\" DROP TABLE IF EXISTS daily_historic_pollution \"\"\" ) cur . execute ( \"\"\" CREATE TABLE daily_historic_pollution (date DATE PRIMARY KEY, type VARCHAR(20), value INTEGER ) \"\"\" ) print ( \"Created table\" ) cur . close () conn . close () Created table employee ) SqlAlchemy \u00b6 According to me, the best way to interface with a database is using SqlAlchemy. For example, consider the data from aqicn's api on the air pollution data for Hyderabad. import pandas as pd df = pd . read_csv ( 'hyderabad-us consulate, india-air-quality.csv' ) df [ 'type' ] = 'pm25' df . columns = [ 'date' , 'value' , 'type' ] df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date value type 0 2021/10/1 87 pm25 1 2021/10/2 47 pm25 2 2021/10/3 50 pm25 3 2021/9/1 66 pm25 4 2021/9/2 74 pm25 Using SqlAlchemy, we can upload this data to the previously created table. from sqlalchemy import create_engine import numpy from psycopg2.extensions import register_adapter , AsIs def addapt_numpy_float64 ( numpy_float64 ): return AsIs ( numpy_float64 ) def addapt_numpy_int64 ( numpy_int64 ): return AsIs ( numpy_int64 ) register_adapter ( numpy . float64 , addapt_numpy_float64 ) register_adapter ( numpy . int64 , addapt_numpy_int64 ) engine = create_engine ( 'postgresql://[user[:password]@][netloc][:port][/dbname][?param1=value1&...]' ) for i in range ( len ( df )): query = \"\"\"INSERT INTO daily_historic_pollution (date, type, value) VALUES( %s , %s , %s ) ON CONFLICT (date) DO UPDATE SET type = excluded.type, value = excluded.value; \"\"\" engine . execute ( query , ( df . date [ i ], df . type [ i ], df . value [ i ])) In this way, we can load data into a database. Now I want to create a new table that can store the predictions from the flask application into a table. conn = psycopg2 . connect ( \"postgresql://[user[:password]@][netloc][:port][/dbname][?param1=value1&...]\" ) conn . set_session ( autocommit = True ) cur = conn . cursor () cur . execute ( \"\"\" DROP TABLE IF EXISTS daily_prediction_pollution \"\"\" ) cur . execute ( \"\"\" CREATE TABLE daily_prediction_pollution (date DATE PRIMARY KEY, type VARCHAR(20), prediction FLOAT ) \"\"\" ) print ( \"Created table\" ) cur . close () conn . close () Created table We can use the GET request from the flask application in the previous blog to predict PM25 for the date. import requests from datetime import date , datetime today = str ( date . today () . year ) + '-' + str ( date . today () . month ) + '-' + str ( date . today () . day ) response = requests . get ( \"https://harshaash.pythonanywhere.com/predict\" , params = { 'date' : today }) response . json ()[ 'prediction' ][ 0 ] 115.6 The prediction is 115.6 PM25. We can use our knowledge on SqlAlchemy to store this value in a database. query2 = \"\"\"INSERT INTO daily_prediction_pollution (date, type, prediction) VALUES( %s , %s , %s ) ON CONFLICT (date) DO UPDATE SET type = excluded.type, prediction = excluded.prediction; \"\"\" engine . execute ( query2 , ( today , 'pm25' , response . json ()[ 'prediction' ][ 0 ])) We can see this result in the database We can get actual pollution data for the past from JSON API of AQICN.org . We can use this data to update the database with the previous day's value. aqi_token = { 'token' : 'hidden_token_no' } response = requests . get ( \"https://api.waqi.info/feed/hyderabad/\" , params = aqi_token ) data = pd . DataFrame ( response . json ()[ 'data' ][ 'forecast' ][ 'daily' ][ 'pm25' ]) data . day = pd . to_datetime ( data . day ) data [ 'type' ] = 'pm25' data . columns = [ 'value' , 'date' , 'max' , 'min' , 'type' ] data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } value date max min type 0 148 2021-10-14 159 138 pm25 1 158 2021-10-15 159 145 pm25 2 150 2021-10-16 159 138 pm25 3 154 2021-10-17 174 138 pm25 4 140 2021-10-18 153 138 pm25 5 150 2021-10-19 159 138 pm25 6 138 2021-10-20 138 138 pm25 7 138 2021-10-21 138 138 pm25 8 138 2021-10-22 138 89 pm25 From the API result, we can see the previous two day's data and predictions (by AQICN) for the next few days. We can use this API to update the data for previous days data in our table. for i in range ( 2 ): query = \"\"\"INSERT INTO daily_historic_pollution (date, type, value) VALUES( %s , %s , %s ) ON CONFLICT (date) DO UPDATE SET type = excluded.type, value = excluded.value; \"\"\" engine . execute ( query , ( data . date [ i ], data . type [ i ], data . value [ i ])) Scheduling using Github Actions \u00b6 I want to run these predictions every day, and the data/predictions are stored at a set point of time every day. To do this, we can combine all the codes above into one python file that gets executed every day. This file contains code that will predict the pollution for today and store this value along with the pollution for yesterday in the database. # File name: aqi_script.py import pandas as pd import requests from datetime import date , datetime from sqlalchemy import create_engine import numpy from psycopg2.extensions import register_adapter , AsIs def addapt_numpy_float64 ( numpy_float64 ): return AsIs ( numpy_float64 ) def addapt_numpy_int64 ( numpy_int64 ): return AsIs ( numpy_int64 ) register_adapter ( numpy . float64 , addapt_numpy_float64 ) register_adapter ( numpy . int64 , addapt_numpy_int64 ) engine = create_engine ( 'postgresql://[user[:password]@][netloc][:port][/dbname][?param1=value1&...]' ) today = str ( date . today () . year ) + '-' + str ( date . today () . month ) + '-' + str ( date . today () . day ) # Upload the prediction data response = requests . get ( \"https://harshaash.pythonanywhere.com/predict\" , params = { 'date' : today }) query2 = \"\"\"INSERT INTO daily_prediction_pollution (date, type, prediction) VALUES( %s , %s , %s ) ON CONFLICT (date) DO UPDATE SET type = excluded.type, prediction = excluded.prediction; \"\"\" engine . execute ( query2 , ( today , 'pm25' , response . json ()[ 'prediction' ][ 0 ])) # Upload the actual data aqi_token = { 'token' : 'dummy_token' } response = requests . get ( \"https://api.waqi.info/feed/hyderabad/\" , params = aqi_token ) data = pd . DataFrame ( response . json ()[ 'data' ][ 'forecast' ][ 'daily' ][ 'pm25' ]) data . day = pd . to_datetime ( data . day ) data [ 'type' ] = 'pm25' data . columns = [ 'value' , 'date' , 'max' , 'min' , 'type' ] for i in range ( 2 ): query = \"\"\"INSERT INTO daily_historic_pollution (date, type, value) VALUES( %s , %s , %s ) ON CONFLICT (date) DO UPDATE SET type = excluded.type, value = excluded.value; \"\"\" engine . execute ( query , ( data . date [ i ], data . type [ i ], data . value [ i ])) We can schedule this code to run every day at a particular time so that our database is updated every day with the predictions. This can be done using cron jobs. One way to implement cron jobs is using GitHub actions. To create a cron job, a yml file should be created within the .github/workfows folder in the Github repository. This yml file is converted to actions, and this is shown on the actions page. For more details, refer here . The yml file for our use case would be: # FIle name: any_name.yml # location: .github/workflows name : update_database_AQI_data on : schedule : - cron : '5 1 * * *' # to run at 1:05 AM GMT everyday jobs : build : name : Build project runs - on : ubuntu - latest steps : - name : Checkout repository uses : actions / checkout @v2 # Downloads the current git repo - name : Setup Python uses : actions / setup - python @v2 # Installs the python setup - name : Install dependancies # Installs the required packages run : | python - m pip install -- upgrade pip pip install - r requirements . txt - name : Execute python file # Run the script file run : python aqi_script . py Challenges \u00b6 Setting the \"Allow list\" of IP Addresses. This is the list of IP addresses that have access to the Yugabyte DB. Building redundancy if Github Actions fail. Created by Achyuthuni Sri Harsha","title":"Saving predictions in database using Python"},{"location":"Python/Saving%20predictions%20in%20database/#handling-databases-using-python","text":"I want to create a database for storing the predictions and actual values of the air pollution near my home in Hyderabad. In a previous blog , I have demonstrated how we can deploy a flask application using pythonanywhere which predicts the pollution using a machine learning model that I built. In this blog, we will see how we can store this information in a database to be used either for visualisation or other analytics.","title":"Handling databases using python"},{"location":"Python/Saving%20predictions%20in%20database/#free-database-on-yugabyte","text":"Yugabyte cloud is one of the few companies that lets us to host a free database. The steps to create a free database is given at the documentation as: I have created a database with the following details: Cluster: hyderabad-pollution-data Username (to connect to cluster): harshaash After creating the cluster, I get the following dashboard","title":"Free database on yugabyte"},{"location":"Python/Saving%20predictions%20in%20database/#creating-tables","text":"Once we have a cluster created, we can create tables in three ways. 1. Using the shell in yugabyte 2. Using psycopg2 in Python 3. Using SQLAlchemy","title":"Creating tables"},{"location":"Python/Saving%20predictions%20in%20database/#yugabyte-shell","text":"First, let us create using yugabyte shell. The details of the database connection are: In the shell, we can create a database and tables using the standard SQL commands as follows: CREATE DATABASE hyderabad_aqi ; CREATE TABLE daily_historic_pollution ( date DATE PRIMARY KEY , type VARCHAR ( 20 ), value FLOAT );","title":"Yugabyte shell"},{"location":"Python/Saving%20predictions%20in%20database/#load-data-using-python","text":"We can use psycopg to connect and create the same table as shown above. The details of the connection are provided under \"Run your own application\". After connecting to the database, we can create tables using SQL commands. import psycopg2 conn = psycopg2 . connect ( \"postgresql://[user[:password]@][netloc][:port][/dbname][?param1=value1&...]\" ) conn . set_session ( autocommit = True ) cur = conn . cursor () cur . execute ( \"\"\" DROP TABLE IF EXISTS daily_historic_pollution \"\"\" ) cur . execute ( \"\"\" CREATE TABLE daily_historic_pollution (date DATE PRIMARY KEY, type VARCHAR(20), value INTEGER ) \"\"\" ) print ( \"Created table\" ) cur . close () conn . close () Created table employee )","title":"Load data using python"},{"location":"Python/Saving%20predictions%20in%20database/#sqlalchemy","text":"According to me, the best way to interface with a database is using SqlAlchemy. For example, consider the data from aqicn's api on the air pollution data for Hyderabad. import pandas as pd df = pd . read_csv ( 'hyderabad-us consulate, india-air-quality.csv' ) df [ 'type' ] = 'pm25' df . columns = [ 'date' , 'value' , 'type' ] df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date value type 0 2021/10/1 87 pm25 1 2021/10/2 47 pm25 2 2021/10/3 50 pm25 3 2021/9/1 66 pm25 4 2021/9/2 74 pm25 Using SqlAlchemy, we can upload this data to the previously created table. from sqlalchemy import create_engine import numpy from psycopg2.extensions import register_adapter , AsIs def addapt_numpy_float64 ( numpy_float64 ): return AsIs ( numpy_float64 ) def addapt_numpy_int64 ( numpy_int64 ): return AsIs ( numpy_int64 ) register_adapter ( numpy . float64 , addapt_numpy_float64 ) register_adapter ( numpy . int64 , addapt_numpy_int64 ) engine = create_engine ( 'postgresql://[user[:password]@][netloc][:port][/dbname][?param1=value1&...]' ) for i in range ( len ( df )): query = \"\"\"INSERT INTO daily_historic_pollution (date, type, value) VALUES( %s , %s , %s ) ON CONFLICT (date) DO UPDATE SET type = excluded.type, value = excluded.value; \"\"\" engine . execute ( query , ( df . date [ i ], df . type [ i ], df . value [ i ])) In this way, we can load data into a database. Now I want to create a new table that can store the predictions from the flask application into a table. conn = psycopg2 . connect ( \"postgresql://[user[:password]@][netloc][:port][/dbname][?param1=value1&...]\" ) conn . set_session ( autocommit = True ) cur = conn . cursor () cur . execute ( \"\"\" DROP TABLE IF EXISTS daily_prediction_pollution \"\"\" ) cur . execute ( \"\"\" CREATE TABLE daily_prediction_pollution (date DATE PRIMARY KEY, type VARCHAR(20), prediction FLOAT ) \"\"\" ) print ( \"Created table\" ) cur . close () conn . close () Created table We can use the GET request from the flask application in the previous blog to predict PM25 for the date. import requests from datetime import date , datetime today = str ( date . today () . year ) + '-' + str ( date . today () . month ) + '-' + str ( date . today () . day ) response = requests . get ( \"https://harshaash.pythonanywhere.com/predict\" , params = { 'date' : today }) response . json ()[ 'prediction' ][ 0 ] 115.6 The prediction is 115.6 PM25. We can use our knowledge on SqlAlchemy to store this value in a database. query2 = \"\"\"INSERT INTO daily_prediction_pollution (date, type, prediction) VALUES( %s , %s , %s ) ON CONFLICT (date) DO UPDATE SET type = excluded.type, prediction = excluded.prediction; \"\"\" engine . execute ( query2 , ( today , 'pm25' , response . json ()[ 'prediction' ][ 0 ])) We can see this result in the database We can get actual pollution data for the past from JSON API of AQICN.org . We can use this data to update the database with the previous day's value. aqi_token = { 'token' : 'hidden_token_no' } response = requests . get ( \"https://api.waqi.info/feed/hyderabad/\" , params = aqi_token ) data = pd . DataFrame ( response . json ()[ 'data' ][ 'forecast' ][ 'daily' ][ 'pm25' ]) data . day = pd . to_datetime ( data . day ) data [ 'type' ] = 'pm25' data . columns = [ 'value' , 'date' , 'max' , 'min' , 'type' ] data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } value date max min type 0 148 2021-10-14 159 138 pm25 1 158 2021-10-15 159 145 pm25 2 150 2021-10-16 159 138 pm25 3 154 2021-10-17 174 138 pm25 4 140 2021-10-18 153 138 pm25 5 150 2021-10-19 159 138 pm25 6 138 2021-10-20 138 138 pm25 7 138 2021-10-21 138 138 pm25 8 138 2021-10-22 138 89 pm25 From the API result, we can see the previous two day's data and predictions (by AQICN) for the next few days. We can use this API to update the data for previous days data in our table. for i in range ( 2 ): query = \"\"\"INSERT INTO daily_historic_pollution (date, type, value) VALUES( %s , %s , %s ) ON CONFLICT (date) DO UPDATE SET type = excluded.type, value = excluded.value; \"\"\" engine . execute ( query , ( data . date [ i ], data . type [ i ], data . value [ i ]))","title":"SqlAlchemy"},{"location":"Python/Saving%20predictions%20in%20database/#scheduling-using-github-actions","text":"I want to run these predictions every day, and the data/predictions are stored at a set point of time every day. To do this, we can combine all the codes above into one python file that gets executed every day. This file contains code that will predict the pollution for today and store this value along with the pollution for yesterday in the database. # File name: aqi_script.py import pandas as pd import requests from datetime import date , datetime from sqlalchemy import create_engine import numpy from psycopg2.extensions import register_adapter , AsIs def addapt_numpy_float64 ( numpy_float64 ): return AsIs ( numpy_float64 ) def addapt_numpy_int64 ( numpy_int64 ): return AsIs ( numpy_int64 ) register_adapter ( numpy . float64 , addapt_numpy_float64 ) register_adapter ( numpy . int64 , addapt_numpy_int64 ) engine = create_engine ( 'postgresql://[user[:password]@][netloc][:port][/dbname][?param1=value1&...]' ) today = str ( date . today () . year ) + '-' + str ( date . today () . month ) + '-' + str ( date . today () . day ) # Upload the prediction data response = requests . get ( \"https://harshaash.pythonanywhere.com/predict\" , params = { 'date' : today }) query2 = \"\"\"INSERT INTO daily_prediction_pollution (date, type, prediction) VALUES( %s , %s , %s ) ON CONFLICT (date) DO UPDATE SET type = excluded.type, prediction = excluded.prediction; \"\"\" engine . execute ( query2 , ( today , 'pm25' , response . json ()[ 'prediction' ][ 0 ])) # Upload the actual data aqi_token = { 'token' : 'dummy_token' } response = requests . get ( \"https://api.waqi.info/feed/hyderabad/\" , params = aqi_token ) data = pd . DataFrame ( response . json ()[ 'data' ][ 'forecast' ][ 'daily' ][ 'pm25' ]) data . day = pd . to_datetime ( data . day ) data [ 'type' ] = 'pm25' data . columns = [ 'value' , 'date' , 'max' , 'min' , 'type' ] for i in range ( 2 ): query = \"\"\"INSERT INTO daily_historic_pollution (date, type, value) VALUES( %s , %s , %s ) ON CONFLICT (date) DO UPDATE SET type = excluded.type, value = excluded.value; \"\"\" engine . execute ( query , ( data . date [ i ], data . type [ i ], data . value [ i ])) We can schedule this code to run every day at a particular time so that our database is updated every day with the predictions. This can be done using cron jobs. One way to implement cron jobs is using GitHub actions. To create a cron job, a yml file should be created within the .github/workfows folder in the Github repository. This yml file is converted to actions, and this is shown on the actions page. For more details, refer here . The yml file for our use case would be: # FIle name: any_name.yml # location: .github/workflows name : update_database_AQI_data on : schedule : - cron : '5 1 * * *' # to run at 1:05 AM GMT everyday jobs : build : name : Build project runs - on : ubuntu - latest steps : - name : Checkout repository uses : actions / checkout @v2 # Downloads the current git repo - name : Setup Python uses : actions / setup - python @v2 # Installs the python setup - name : Install dependancies # Installs the required packages run : | python - m pip install -- upgrade pip pip install - r requirements . txt - name : Execute python file # Run the script file run : python aqi_script . py","title":"Scheduling using Github Actions"},{"location":"Python/Saving%20predictions%20in%20database/#challenges","text":"Setting the \"Allow list\" of IP Addresses. This is the list of IP addresses that have access to the Yugabyte DB. Building redundancy if Github Actions fail. Created by Achyuthuni Sri Harsha","title":"Challenges"},{"location":"Python/Shortest%20path%20problems/","text":"MathJax.Hub.Config({ extensions: [\"tex2jax.js\",\"TeX/AMSmath.js\",\"TeX/AMSsymbols.js\"], jax: [\"input/TeX\", \"output/HTML-CSS\"], tex2jax: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ], }, \"HTML-CSS\": { availableFonts: [\"TeX\"] } }); Shortest path problems \u00b6 Author: Achyuthuni Sri Harsha Shortest path is one problem in networks which appears in many forms across many industries. It tells the user how to find the shortest path between two pair of nodes. In this particular example, we will look at finding the shortest path between a pair of nodes in a directed network using an integer programming solver. import networkx as nx import numpy as np import pandas as pd import matplotlib.pyplot as plt Consider the following simple weighted directed network, with four nodes and five edges. edgelist_df = pd . DataFrame ({ 'node1' :[ 's' , 's' , 'u' , 'u' , 'v' ], 'node2' :[ 'u' , 'v' , 'v' , 't' , 't' ], 'weights' :[ 3 , 2 , 0.7 , 1 , 7 ] }) edgelist_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } node1 node2 weights 0 s u 3.0 1 s v 2.0 2 u v 0.7 3 u t 1.0 4 v t 7.0 g = nx . DiGraph () for i , elrow in edgelist_df . iterrows (): g . add_edge ( elrow [ 0 ], elrow [ 1 ], weight = elrow [ 2 ]) g . edges ( data = True ) OutEdgeDataView([('s', 'u', {'weight': 3.0}), ('s', 'v', {'weight': 2.0}), ('u', 'v', {'weight': 0.7}), ('u', 't', {'weight': 1.0}), ('v', 't', {'weight': 7.0})]) # for each node we are trying to fix the coordinates g . add_node ( 's' , pos = ( 0 , 1 )) g . add_node ( 'u' , pos = ( 1 , 2 )) g . add_node ( 'v' , pos = ( 1 , 0 )) g . add_node ( 't' , pos = ( 2 , 1 )) g . nodes ( data = True ) NodeDataView({'s': {'pos': (0, 1)}, 'u': {'pos': (1, 2)}, 'v': {'pos': (1, 0)}, 't': {'pos': (2, 1)}}) # This function gets the coordinates for nodes pos = nx . get_node_attributes ( g , 'pos' ) # This function gets the weights for the edges weight = nx . get_edge_attributes ( g , 'weight' ) nx . draw ( g , pos , with_labels = True ) nx . draw_networkx_edge_labels ( g , pos , edge_labels = weight ) plt . show () Now, we want to find the shortest path between the node 's' and 't'. In this network, edge-weights represent the costs for each edge. The weights could be distance, or time. NetworkX has an inbuilt function shortest_path which returs the shortest path. Using networkX, we get the following shortest path: path_2_0 = nx . shortest_path ( g , source = 's' , target = 't' ) path_2_0 ['s', 'u', 't'] Formulating the problem using integer programming \u00b6 We have n nodes V and m edges E (n=4, m=5 for this example). OR tools is an open source software built by Google for solving integer programming problems. Cp-Sat solver is one such model by OR Tools, which we are going to use today. We can use binary decision variables \\(edge_{i,j}\\) representing the edge that goes from node i to node j. If \\(edge_{i,j}=1\\) the shortest path belongs to the path between i and j, 0 otherwise. from ortools.sat.python import cp_model shortest_path_model = cp_model . CpModel () # Creating one integer decision variable for each edge edge_bool_vars = {} for edge in g . edges : edge_bool_vars [ edge [ 0 ], edge [ 1 ]] = shortest_path_model . NewBoolVar ( 'edge_ %s _ %s ' % edge ) print ( 'Creating the boolean variable ' , edge_bool_vars [ edge [ 0 ], edge [ 1 ]], 'representing the if we should travel through ' , ( edge [ 0 ], edge [ 1 ])) Creating the boolean variable edge_s_u representing the if we should travel through ('s', 'u') Creating the boolean variable edge_s_v representing the if we should travel through ('s', 'v') Creating the boolean variable edge_u_v representing the if we should travel through ('u', 'v') Creating the boolean variable edge_u_t representing the if we should travel through ('u', 't') Creating the boolean variable edge_v_t representing the if we should travel through ('v', 't') The shortest path (in isolation) will have the following properties: 1. Starting node has a degree -1 2. Ending node has a degree +1 3. All intermediatory nodes have degree 0 This can be written in the form of flow balance constraints as follows: input_node = 's' output_node = 't' # Adding constraints on the nodes for node in g . nodes : in_edges = g . in_edges ( node ) out_edges = g . out_edges ( node ) print ( 'Adding the constraint on node ' , node ) print ( 'This node has %i in-edges and %i out-edges' % ( len ( in_edges ), len ( out_edges ))) equation_at_this_edge = sum ( edge_bool_vars [ edge [ 0 ], edge [ 1 ]] for edge in in_edges ) - \\ sum ( edge_bool_vars [ edge [ 0 ], edge [ 1 ]] for edge in out_edges ) if ( node == input_node ): shortest_path_model . Add ( equation_at_this_edge == - 1 ) print ( equation_at_this_edge == - 1 ) elif ( node == output_node ): shortest_path_model . Add ( equation_at_this_edge == 1 ) print ( equation_at_this_edge == 1 ) else : shortest_path_model . Add ( equation_at_this_edge == 0 ) print ( equation_at_this_edge , '== 0' ) print ( '' ) Adding the constraint on node s This node has 0 in-edges and 2 out-edges (-((edge_s_u) + edge_s_v)) == -1 Adding the constraint on node u This node has 1 in-edges and 2 out-edges ((edge_s_u) + -((edge_u_v) + edge_u_t)) == 0 Adding the constraint on node v This node has 2 in-edges and 1 out-edges (((edge_s_v) + edge_u_v) + -(edge_v_t)) == 0 Adding the constraint on node t This node has 2 in-edges and 0 out-edges (((edge_u_t) + edge_v_t)) == 1 The objective of the shortest path problem is to find the path with the minimum cost. This can be written as minimising the costs as follows: # factor to make everything including costs integer factor_to_int = 10 # The objective is to maximise flow total_cost = sum ( int ( g . get_edge_data ( * edge )[ 'weight' ] * factor_to_int ) * edge_bool_vars [ edge [ 0 ], edge [ 1 ]] for edge in g . edges ) print ( 'Objective is to optimise cost' ) print ( total_cost ) shortest_path_model . Minimize ( total_cost ) Objective is to optimise cost ((((((30 * edge_s_u)) + (20 * edge_s_v)) + (7 * edge_u_v)) + (10 * edge_u_t)) + (70 * edge_v_t)) Solving the problem, we have an optimal solution with the overall cost as 40 units. # Solving the problem solver = cp_model . CpSolver () solution_printer = cp_model . ObjectiveSolutionPrinter () status = solver . SolveWithSolutionCallback ( shortest_path_model , solution_printer ) Solution 0, time = 0.02 s, objective = 40 cp_model . OPTIMAL == status True The solution is given as result_edges = {} for edge in g . edges : result_edges [ edge [ 0 ], edge [ 1 ]] = solver . Value ( edge_bool_vars [ edge [ 0 ], edge [ 1 ]]) result_edges {('s', 'u'): 1, ('s', 'v'): 0, ('u', 'v'): 0, ('u', 't'): 1, ('v', 't'): 0} Plotting the network in such a way that the green lines represent the shortest path, we get pos = nx . get_node_attributes ( g , 'pos' ) color = [ 'g' if val == 1 else 'r' for val in result_edges . values ()] nx . draw ( g , pos , with_labels = True , edge_color = color ) nx . draw_networkx_edge_labels ( g , pos , edge_labels = result_edges ) plt . show ()","title":"Shortest path using integer programming (Python)"},{"location":"Python/Shortest%20path%20problems/#shortest-path-problems","text":"Author: Achyuthuni Sri Harsha Shortest path is one problem in networks which appears in many forms across many industries. It tells the user how to find the shortest path between two pair of nodes. In this particular example, we will look at finding the shortest path between a pair of nodes in a directed network using an integer programming solver. import networkx as nx import numpy as np import pandas as pd import matplotlib.pyplot as plt Consider the following simple weighted directed network, with four nodes and five edges. edgelist_df = pd . DataFrame ({ 'node1' :[ 's' , 's' , 'u' , 'u' , 'v' ], 'node2' :[ 'u' , 'v' , 'v' , 't' , 't' ], 'weights' :[ 3 , 2 , 0.7 , 1 , 7 ] }) edgelist_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } node1 node2 weights 0 s u 3.0 1 s v 2.0 2 u v 0.7 3 u t 1.0 4 v t 7.0 g = nx . DiGraph () for i , elrow in edgelist_df . iterrows (): g . add_edge ( elrow [ 0 ], elrow [ 1 ], weight = elrow [ 2 ]) g . edges ( data = True ) OutEdgeDataView([('s', 'u', {'weight': 3.0}), ('s', 'v', {'weight': 2.0}), ('u', 'v', {'weight': 0.7}), ('u', 't', {'weight': 1.0}), ('v', 't', {'weight': 7.0})]) # for each node we are trying to fix the coordinates g . add_node ( 's' , pos = ( 0 , 1 )) g . add_node ( 'u' , pos = ( 1 , 2 )) g . add_node ( 'v' , pos = ( 1 , 0 )) g . add_node ( 't' , pos = ( 2 , 1 )) g . nodes ( data = True ) NodeDataView({'s': {'pos': (0, 1)}, 'u': {'pos': (1, 2)}, 'v': {'pos': (1, 0)}, 't': {'pos': (2, 1)}}) # This function gets the coordinates for nodes pos = nx . get_node_attributes ( g , 'pos' ) # This function gets the weights for the edges weight = nx . get_edge_attributes ( g , 'weight' ) nx . draw ( g , pos , with_labels = True ) nx . draw_networkx_edge_labels ( g , pos , edge_labels = weight ) plt . show () Now, we want to find the shortest path between the node 's' and 't'. In this network, edge-weights represent the costs for each edge. The weights could be distance, or time. NetworkX has an inbuilt function shortest_path which returs the shortest path. Using networkX, we get the following shortest path: path_2_0 = nx . shortest_path ( g , source = 's' , target = 't' ) path_2_0 ['s', 'u', 't']","title":"Shortest path problems"},{"location":"Python/Shortest%20path%20problems/#formulating-the-problem-using-integer-programming","text":"We have n nodes V and m edges E (n=4, m=5 for this example). OR tools is an open source software built by Google for solving integer programming problems. Cp-Sat solver is one such model by OR Tools, which we are going to use today. We can use binary decision variables \\(edge_{i,j}\\) representing the edge that goes from node i to node j. If \\(edge_{i,j}=1\\) the shortest path belongs to the path between i and j, 0 otherwise. from ortools.sat.python import cp_model shortest_path_model = cp_model . CpModel () # Creating one integer decision variable for each edge edge_bool_vars = {} for edge in g . edges : edge_bool_vars [ edge [ 0 ], edge [ 1 ]] = shortest_path_model . NewBoolVar ( 'edge_ %s _ %s ' % edge ) print ( 'Creating the boolean variable ' , edge_bool_vars [ edge [ 0 ], edge [ 1 ]], 'representing the if we should travel through ' , ( edge [ 0 ], edge [ 1 ])) Creating the boolean variable edge_s_u representing the if we should travel through ('s', 'u') Creating the boolean variable edge_s_v representing the if we should travel through ('s', 'v') Creating the boolean variable edge_u_v representing the if we should travel through ('u', 'v') Creating the boolean variable edge_u_t representing the if we should travel through ('u', 't') Creating the boolean variable edge_v_t representing the if we should travel through ('v', 't') The shortest path (in isolation) will have the following properties: 1. Starting node has a degree -1 2. Ending node has a degree +1 3. All intermediatory nodes have degree 0 This can be written in the form of flow balance constraints as follows: input_node = 's' output_node = 't' # Adding constraints on the nodes for node in g . nodes : in_edges = g . in_edges ( node ) out_edges = g . out_edges ( node ) print ( 'Adding the constraint on node ' , node ) print ( 'This node has %i in-edges and %i out-edges' % ( len ( in_edges ), len ( out_edges ))) equation_at_this_edge = sum ( edge_bool_vars [ edge [ 0 ], edge [ 1 ]] for edge in in_edges ) - \\ sum ( edge_bool_vars [ edge [ 0 ], edge [ 1 ]] for edge in out_edges ) if ( node == input_node ): shortest_path_model . Add ( equation_at_this_edge == - 1 ) print ( equation_at_this_edge == - 1 ) elif ( node == output_node ): shortest_path_model . Add ( equation_at_this_edge == 1 ) print ( equation_at_this_edge == 1 ) else : shortest_path_model . Add ( equation_at_this_edge == 0 ) print ( equation_at_this_edge , '== 0' ) print ( '' ) Adding the constraint on node s This node has 0 in-edges and 2 out-edges (-((edge_s_u) + edge_s_v)) == -1 Adding the constraint on node u This node has 1 in-edges and 2 out-edges ((edge_s_u) + -((edge_u_v) + edge_u_t)) == 0 Adding the constraint on node v This node has 2 in-edges and 1 out-edges (((edge_s_v) + edge_u_v) + -(edge_v_t)) == 0 Adding the constraint on node t This node has 2 in-edges and 0 out-edges (((edge_u_t) + edge_v_t)) == 1 The objective of the shortest path problem is to find the path with the minimum cost. This can be written as minimising the costs as follows: # factor to make everything including costs integer factor_to_int = 10 # The objective is to maximise flow total_cost = sum ( int ( g . get_edge_data ( * edge )[ 'weight' ] * factor_to_int ) * edge_bool_vars [ edge [ 0 ], edge [ 1 ]] for edge in g . edges ) print ( 'Objective is to optimise cost' ) print ( total_cost ) shortest_path_model . Minimize ( total_cost ) Objective is to optimise cost ((((((30 * edge_s_u)) + (20 * edge_s_v)) + (7 * edge_u_v)) + (10 * edge_u_t)) + (70 * edge_v_t)) Solving the problem, we have an optimal solution with the overall cost as 40 units. # Solving the problem solver = cp_model . CpSolver () solution_printer = cp_model . ObjectiveSolutionPrinter () status = solver . SolveWithSolutionCallback ( shortest_path_model , solution_printer ) Solution 0, time = 0.02 s, objective = 40 cp_model . OPTIMAL == status True The solution is given as result_edges = {} for edge in g . edges : result_edges [ edge [ 0 ], edge [ 1 ]] = solver . Value ( edge_bool_vars [ edge [ 0 ], edge [ 1 ]]) result_edges {('s', 'u'): 1, ('s', 'v'): 0, ('u', 'v'): 0, ('u', 't'): 1, ('v', 't'): 0} Plotting the network in such a way that the green lines represent the shortest path, we get pos = nx . get_node_attributes ( g , 'pos' ) color = [ 'g' if val == 1 else 'r' for val in result_edges . values ()] nx . draw ( g , pos , with_labels = True , edge_color = color ) nx . draw_networkx_edge_labels ( g , pos , edge_labels = result_edges ) plt . show ()","title":"Formulating the problem using integer programming"},{"location":"Python/Visualization%20for%20predictive%20analytics/","text":"Data visualisation for predictive analytics \u00b6 Author: Achyuthuni Sri Harsha Data visualisation can be performed in many ways. There are infinite ways to visualise the data, and what works is dependant on the patterns in the data. In this post, we are trying to categorise the visualisation of data for regression and classification problems. Every regression, classification and clustering problem has some or all of the following assumptions: 1. Change in independent variables changes the dependant variable. In other words, there is a relationship between the dependant variable and the independent variables. Before building a model, it is advised to visualise this relationship. 2. Assumptions on the distribution of the dependant or independent variable. For example, for Naive Bayes classifier, the independent variables should follow a normal distribution. 3. Assumptions of relationships between independent variables. For example, for linear regression, the independent variables should not be correlated. 4. Unbalanced dataset. The frequency of the smaller class should be significant when compared to the frequency of the larger class. 5. The time series of data/features are stationary. Apart from validating the assumptions and identifying trends in the data, data visualisation can also be used for gathering insights and feature engineering. The below example is from the marketing department of a consulting firm. The problem is to identify the projects that they can win. # Importing the necessary libraries import pandas as pd import numpy as np import os import matplotlib.pyplot as plt from statsmodels.graphics.mosaicplot import mosaic import seaborn as sns % matplotlib inline # loading th data path = \"data/marketing dept.csv\" df = pd . read_csv ( path ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } reporting_status product industry region strength_in_segment profit_for_customer sales_value profit_perc joint_bid_portion 0 Lost F Cap Oth 57 1.225 6.5 64 59 1 Lost L Def UK 51 1.469 9.9 56 58 2 Lost Lo Cli UK 79 0.887 7.0 59 48 3 Lost G Fin UK 55 1.316 8.9 34 41 4 Won G Sec UK 32 1.010 5.7 43 63 Univariate analysis \u00b6 The univariate analysis deals with EDA on one variable alone. In describing or characterising the observations of an individual variable, three basic properties are of interest: 1. The location of observations, or how large or small the values of the individual observations are 2. The dispersion (sometimes called scale or spread) of the observations 3. The distribution of the observations Uni-variate plots provide one way to find out about those properties. There are two basic kinds of univariate plots: 1. Enumeration plots, or plots that show every observation 2. Summary plots that generalise the data into a simplified representation 3. Enumerative plots Index Plot/Univariate Scatter Diagram \u00b6 The most common enumerative plot is the index plot. It displays the values of a single variable for each observation using symbols plotted relative to the observation number. plt . plot ( df . sales_value , 'o' , color = 'black' ) plt . title ( \"Index plot\" ) plt . xlabel ( 'Sales Value' ); From the above plot, we can infer that there are around 3000 observations for sales, and they are captured randomly along the data. Strip Plot/Strip Chart (univariate scatter diagram) \u00b6 Displays the values of a single variable as symbols plotted along a line. This is a basic plot where we can see the spread of the data. ax = sns . stripplot ( x = df . sales_value ) ax . set ( xlabel = 'Sales_value' , title = 'Strip Chart' ); Dot Plot/Dot Chart \u00b6 Dot plot displays the values plotted along a line. It is generally constructed after sorting the rows. This can help us in determining the distribution of the data. It can also help us identify the continuity of the data. plt . plot ( df . sort_values ( by = 'sales_value' ) . reset_index () . sales_value , 'o' , color = 'black' ) plt . title ( \"Dot plot\" ) plt . ylabel ( 'Sales Value' ); From looking at the plot, most of the data lies within 6-12 while the frequeny of the data decreases as we go away from the mean. The graph is also symmetric. This indicates the distribution could be Normal distribution. Univariate Summary Plots \u00b6 Summary plots display an object or a graph that gives a more concise expression of the location, dispersion, and distribution of a variable than an enumerative plot, but this comes at the expense of some loss of information: In a summary plot, it is no longer possible to retrieve the individual data value, but the gain usually matches this loss in understanding that results from the efficient representation of the data. Summary plots generally prove to be much better than the enumerative plots in revealing the distribution of the data. Box plot \u00b6 A simple way of representing statistical data on a plot in which a rectangle is drawn to represent the second and third quartiles, usually with a vertical line inside to indicate the median value. The lower and upper quartiles are shown as horizontal lines on either side of the rectangle. ax = sns . boxplot ( x = df . sales_value ) ax . set ( xlabel = 'Sales value' , title = 'Box Chart' ); Histograms \u00b6 The other summary plots are of various types: Histograms: Histograms are a type of bar chart that displays the counts or relative frequencies of values falling in different class intervals or ranges. Density Plots: A density plot is a plot of the local relative frequency or density of points along the number line or x-axis of a plot. Where points occur more frequently, this sum, and consequently the local density, will be greater. # For continuous data ax = df . sales_value . plot . hist ( alpha = 0.75 ) df . groupby ( 'sales_value' )[ 'sales_value' ] . count () . plot () ax . set ( xlabel = 'Sales value' , title = 'Histogram' ); Q-Q plot \u00b6 In statistics, a Q-Q (quantile-quantile) plot is a probability plot, which is a graphical method for comparing two probability distributions by plotting their quantiles against each other. If the two distributions being compared are similar, the points in the Q-Q plot will approximately lie on the line y = x. If the distributions are linearly related, the points in the Q-Q plot will approximately lie on a line, but not necessarily on the line y = x. Q-Q plots can also be used as a graphical means of estimating parameters in a location-scale family of distributions. A Q-Q plot is used to compare the shapes of distributions, providing a graphical view of how properties such as location, scale, and skewness are similar or different in the two distributions. Below is a Q-Q plot of the sales data with a normal distribution from scipy import stats stats . probplot ( df . sales_value , plot = sns . mpl . pyplot ); From the above plot, it is clear that the distribution is normal. Bar chart \u00b6 Whereas the above plots are applicable for continuous data, a simple bar chart can help us with categorical data. df . groupby ( 'region' )[ 'region' ] . count () . plot . bar () . set ( xlabel = 'Sales value' , title = 'Histogram' ); Combining the univariate EDA's \u00b6 The below code will do the following for all the columns in the dataset: 1. For continuous data, it will plot the scatter plot, box plot, histogram and q-q plot with normal distribution 2. For categorical data, it will plot the bar chart def univariate_analysis ( dataset ): # For catogorical data for i in ( dataset . select_dtypes ( exclude = [ 'int' , 'int64' , 'float' , 'float64' ]) . columns ): dataset . groupby ( i )[ i ] . count () . plot . bar () plt . show (); # For continuous data ## Selecting the columns that are continuous for i in ( dataset . select_dtypes ( include = [ 'int' , 'int64' , 'float' , 'float64' ]) . columns ): # Index plot plt . subplot ( 221 ) plt . plot ( dataset [ i ], 'o' , color = 'black' ) plt . xlabel ( i ) # q-q plot plt . subplot ( 222 ) stats . probplot ( dataset [ i ], plot = sns . mpl . pyplot ) #Box chart plt . title ( i ) plt . subplot ( 223 ) ax = sns . boxplot ( x = dataset [ i ]) # Histogram plt . subplot ( 224 ) ax2 = dataset [ i ] . plot . hist ( alpha = 0.75 ) dataset . groupby ( i )[ i ] . count () . plot () plt . show (); univariate_analysis ( df ) Bivariate analysis \u00b6 The bivariate analysis deals with visualisations between two variables. The bi-variate analysis is used to identify the relationship between dependant and independent variable. The dependent and independent variables can be of the following types: Problem Independent var Dependent var Classification Categorical Categorical Classification Continuous Categorical Classification Categorical Continuous Classification Continuous Continuous For all the four types, we want to identify the relation between the dependant variable and the independent variable. Classification Visualisations \u00b6 First, let us consider the classification problem. Let's say we have to predict the reporting status of the bid. We have three categorical independent variables and five continuous independent variables. Joint Histograms \u00b6 The five continuous variables are: 1. Strength in segment 2. Profit for customer 3. Sales Value 4. Profit percentage 5. joint bid portion For these variables, we can look at joint histograms. What we are trying to see is the overlap between the distributions for the two different classes. If the overlap between the two variables is small, then that variable can be a good predictor and vice versa. bi_con_cat = df . groupby ([ 'reporting_status' ])[ 'strength_in_segment' ] . plot . hist ( alpha = 0.5 ) plt . xlabel ( 'strength_in_segment' ) plt . legend ( df . groupby ([ 'reporting_status' ])[ 'strength_in_segment' ] . count () . axes [ 0 ] . tolist ()) plt . title ( 'Joint histogram' ); bi_con_cat = df . groupby ([ 'reporting_status' ])[ 'profit_for_customer' ] . plot . hist ( alpha = 0.5 ) plt . xlabel ( 'profit_for_customer' ) plt . legend ( df . groupby ([ 'reporting_status' ])[ 'profit_for_customer' ] . count () . axes [ 0 ] . tolist ()) plt . title ( 'Joint histogram' ); From the above graphs, we can see that profit for customer can explain the status of the bid when compared to strength of segment. We can also see the mean, variance and the distributions of the independent variables between the classes. In a decision tree, the tree will split with profit_for_customer > 1 as 'Lost' class and profit_for_customer < 1 as 'Won'. In logistic regression, the pseudo R-squared will be greater for profit_for_customer than for strength_in_segment. Similar thinking can be applied to SVM, Naive-Bates classifiers etc. Mosaic Plots \u00b6 The three categorical variables are: 1. Product 2. Industry 3. Region For these variables, a mosaic plot will be useful. In the mosaic plot, the area of the rectangles is proportional to the frequency of the class. In the x-axis, we have the dependant variables, and in the y-axis, we have the continuous variables. Using this, we can see the relative frequencies of the 'Won' and 'Lost' in each of the dependant variable classes. # from statsmodels.graphics.mosaicplot import mosaic mosaic ( df , [ 'product' , 'reporting_status' ]); For example, the ratio of Lost to won cases is same in products 'G', 'Li', 'P'. Product 'F' has more wins than normal, while product 'L' has more losses than normal. The products 'C' and 'Lo' are too small to be statistically significant. Intuitively, in logistic regression, the products 'G', 'Li', 'P' can be considered as base classes with 'F' having a positive slope value and 'L' having a negative slope value. In decision-trees, the products 'G', 'Li', 'P' will be part of one branch while products 'L' and 'F' will be part of different branches. Similar thinking can be applied to SVM, Naive-Bates classifiers etc. Combining the classification EDA's \u00b6 The below code will do the following for all the columns in the dataset: 1. For continuous data, it will plot the joint histograms 2. For categorical data, it will plot the mosaic plot def classification_bivariate_analysis ( dataset , dependant_variable ): # For continuous data for i in ( dataset . select_dtypes ( include = [ 'int' , 'int64' , 'float' , 'float64' ]) . columns ): bi_con_cat = dataset . groupby ([ dependant_variable ])[ i ] . plot . hist ( alpha = 0.5 ) plt . xlabel ( i ) plt . legend ( dataset . groupby ([ dependant_variable ])[ i ] . count () . axes [ 0 ] . tolist ()) plt . title ( i ) plt . show (); # For catogorical data for i in ( dataset . select_dtypes ( exclude = [ 'int' , 'int64' , 'float' , 'float64' ]) . columns ): if ( i != dependant_variable ): mosaic ( dataset , [ i , dependant_variable ]); plt . show (); classification_bivariate_analysis ( df , 'reporting_status' ) Bivariate Regression Visualisations \u00b6 Let us consider the regression problem. Let's say we have to predict sales_value of the successful bids. We have three categorical independent variables and four continuous independent variables. successful_bids = df [ df [ 'reporting_status' ] == 'Won' ] Scatter plots \u00b6 There are four continuous variables: 1. Strength in segment 2. Profit for customer 3. Profit percentage 4. Joint bid portion Scatter plots show how much and how one variable is affected by another. We can use them to identify how changing the independent variable changes the dependant variable. Using this, we can identify if we have to do any transformations to the variables. plt . scatter ( successful_bids [ 'joint_bid_portion' ], successful_bids [ 'sales_value' ]) plt . xlabel ( 'joint_bid_portion' ) plt . ylabel ( 'sales_value' ) plt . title ( 'Scatter plot' ); In the above plot, there seems to be no relation between joint_bid_portion and sales_value. We can also observe how joint bid portion behaves after 80. Box plots \u00b6 The three categorical variables are: 1. Product 2. Industry 3. Region For these variables, a box plot will be useful. While showing the relative means among the classes, we can also visualise the variations and distributions in the data. bi_variate_boxplot = sns . boxplot ( x = \"industry\" , y = \"sales_value\" , data = successful_bids ) bi_variate_boxplot . set ( title = 'Box Chart' ); From the above plot, the mean of sales for 'Sec', 'Air, 'Ban', 'Cap', 'Con', 'Oth', 'Def', 'Agr' are similar with similar distributions. The mean of 'Ins', 'OG', 'Gov', 'Hea','Whi' classes seems to be higher and the mean of 'Mob', 'Fin', 'Tel' is lower. In a linear regression, the following industries would be considered as base classes: 'Sec', 'Air, 'Ban', 'Cap', 'Con', 'Oth', 'Def', 'Agr' while 'Ins', 'OG', 'Gov', 'Hea','Whi' will have positive slope value and 'Mob', 'Fin', 'Tel' will have a negative slope. Combining the bivariate regression EDA's \u00b6 The below code will do the following for all the columns in the dataset: 1. For continuous data, it will plot the scatter plots 2. For categorical data, it will plot the bar charts def regression_bivariate_analysis ( dataset , dependant_variable ): # For continuous data for i in ( dataset . select_dtypes ( include = [ 'int' , 'int64' , 'float' , 'float64' ]) . columns ): if ( i != dependant_variable ): plt . scatter ( dataset [ i ], dataset [ dependant_variable ]) plt . show (); # For catogorical data for i in ( dataset . select_dtypes ( exclude = [ 'int' , 'int64' , 'float' , 'float64' ]) . columns ): bi_variate_boxplot = sns . boxplot ( x = i , y = dependant_variable , data = dataset ) bi_variate_boxplot . set ( title = i ) plt . show (); regression_bivariate_analysis ( successful_bids , 'sales_value' )","title":"Vizualising for predictive analytics (Python)"},{"location":"Python/Visualization%20for%20predictive%20analytics/#data-visualisation-for-predictive-analytics","text":"Author: Achyuthuni Sri Harsha Data visualisation can be performed in many ways. There are infinite ways to visualise the data, and what works is dependant on the patterns in the data. In this post, we are trying to categorise the visualisation of data for regression and classification problems. Every regression, classification and clustering problem has some or all of the following assumptions: 1. Change in independent variables changes the dependant variable. In other words, there is a relationship between the dependant variable and the independent variables. Before building a model, it is advised to visualise this relationship. 2. Assumptions on the distribution of the dependant or independent variable. For example, for Naive Bayes classifier, the independent variables should follow a normal distribution. 3. Assumptions of relationships between independent variables. For example, for linear regression, the independent variables should not be correlated. 4. Unbalanced dataset. The frequency of the smaller class should be significant when compared to the frequency of the larger class. 5. The time series of data/features are stationary. Apart from validating the assumptions and identifying trends in the data, data visualisation can also be used for gathering insights and feature engineering. The below example is from the marketing department of a consulting firm. The problem is to identify the projects that they can win. # Importing the necessary libraries import pandas as pd import numpy as np import os import matplotlib.pyplot as plt from statsmodels.graphics.mosaicplot import mosaic import seaborn as sns % matplotlib inline # loading th data path = \"data/marketing dept.csv\" df = pd . read_csv ( path ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } reporting_status product industry region strength_in_segment profit_for_customer sales_value profit_perc joint_bid_portion 0 Lost F Cap Oth 57 1.225 6.5 64 59 1 Lost L Def UK 51 1.469 9.9 56 58 2 Lost Lo Cli UK 79 0.887 7.0 59 48 3 Lost G Fin UK 55 1.316 8.9 34 41 4 Won G Sec UK 32 1.010 5.7 43 63","title":"Data visualisation for predictive analytics"},{"location":"Python/Visualization%20for%20predictive%20analytics/#univariate-analysis","text":"The univariate analysis deals with EDA on one variable alone. In describing or characterising the observations of an individual variable, three basic properties are of interest: 1. The location of observations, or how large or small the values of the individual observations are 2. The dispersion (sometimes called scale or spread) of the observations 3. The distribution of the observations Uni-variate plots provide one way to find out about those properties. There are two basic kinds of univariate plots: 1. Enumeration plots, or plots that show every observation 2. Summary plots that generalise the data into a simplified representation 3. Enumerative plots","title":"Univariate analysis"},{"location":"Python/Visualization%20for%20predictive%20analytics/#index-plotunivariate-scatter-diagram","text":"The most common enumerative plot is the index plot. It displays the values of a single variable for each observation using symbols plotted relative to the observation number. plt . plot ( df . sales_value , 'o' , color = 'black' ) plt . title ( \"Index plot\" ) plt . xlabel ( 'Sales Value' ); From the above plot, we can infer that there are around 3000 observations for sales, and they are captured randomly along the data.","title":"Index Plot/Univariate Scatter Diagram"},{"location":"Python/Visualization%20for%20predictive%20analytics/#strip-plotstrip-chart-univariate-scatter-diagram","text":"Displays the values of a single variable as symbols plotted along a line. This is a basic plot where we can see the spread of the data. ax = sns . stripplot ( x = df . sales_value ) ax . set ( xlabel = 'Sales_value' , title = 'Strip Chart' );","title":"Strip Plot/Strip Chart (univariate scatter diagram)"},{"location":"Python/Visualization%20for%20predictive%20analytics/#dot-plotdot-chart","text":"Dot plot displays the values plotted along a line. It is generally constructed after sorting the rows. This can help us in determining the distribution of the data. It can also help us identify the continuity of the data. plt . plot ( df . sort_values ( by = 'sales_value' ) . reset_index () . sales_value , 'o' , color = 'black' ) plt . title ( \"Dot plot\" ) plt . ylabel ( 'Sales Value' ); From looking at the plot, most of the data lies within 6-12 while the frequeny of the data decreases as we go away from the mean. The graph is also symmetric. This indicates the distribution could be Normal distribution.","title":"Dot Plot/Dot Chart"},{"location":"Python/Visualization%20for%20predictive%20analytics/#univariate-summary-plots","text":"Summary plots display an object or a graph that gives a more concise expression of the location, dispersion, and distribution of a variable than an enumerative plot, but this comes at the expense of some loss of information: In a summary plot, it is no longer possible to retrieve the individual data value, but the gain usually matches this loss in understanding that results from the efficient representation of the data. Summary plots generally prove to be much better than the enumerative plots in revealing the distribution of the data.","title":"Univariate Summary Plots"},{"location":"Python/Visualization%20for%20predictive%20analytics/#box-plot","text":"A simple way of representing statistical data on a plot in which a rectangle is drawn to represent the second and third quartiles, usually with a vertical line inside to indicate the median value. The lower and upper quartiles are shown as horizontal lines on either side of the rectangle. ax = sns . boxplot ( x = df . sales_value ) ax . set ( xlabel = 'Sales value' , title = 'Box Chart' );","title":"Box plot"},{"location":"Python/Visualization%20for%20predictive%20analytics/#histograms","text":"The other summary plots are of various types: Histograms: Histograms are a type of bar chart that displays the counts or relative frequencies of values falling in different class intervals or ranges. Density Plots: A density plot is a plot of the local relative frequency or density of points along the number line or x-axis of a plot. Where points occur more frequently, this sum, and consequently the local density, will be greater. # For continuous data ax = df . sales_value . plot . hist ( alpha = 0.75 ) df . groupby ( 'sales_value' )[ 'sales_value' ] . count () . plot () ax . set ( xlabel = 'Sales value' , title = 'Histogram' );","title":"Histograms"},{"location":"Python/Visualization%20for%20predictive%20analytics/#q-q-plot","text":"In statistics, a Q-Q (quantile-quantile) plot is a probability plot, which is a graphical method for comparing two probability distributions by plotting their quantiles against each other. If the two distributions being compared are similar, the points in the Q-Q plot will approximately lie on the line y = x. If the distributions are linearly related, the points in the Q-Q plot will approximately lie on a line, but not necessarily on the line y = x. Q-Q plots can also be used as a graphical means of estimating parameters in a location-scale family of distributions. A Q-Q plot is used to compare the shapes of distributions, providing a graphical view of how properties such as location, scale, and skewness are similar or different in the two distributions. Below is a Q-Q plot of the sales data with a normal distribution from scipy import stats stats . probplot ( df . sales_value , plot = sns . mpl . pyplot ); From the above plot, it is clear that the distribution is normal.","title":"Q-Q plot"},{"location":"Python/Visualization%20for%20predictive%20analytics/#bar-chart","text":"Whereas the above plots are applicable for continuous data, a simple bar chart can help us with categorical data. df . groupby ( 'region' )[ 'region' ] . count () . plot . bar () . set ( xlabel = 'Sales value' , title = 'Histogram' );","title":"Bar chart"},{"location":"Python/Visualization%20for%20predictive%20analytics/#combining-the-univariate-edas","text":"The below code will do the following for all the columns in the dataset: 1. For continuous data, it will plot the scatter plot, box plot, histogram and q-q plot with normal distribution 2. For categorical data, it will plot the bar chart def univariate_analysis ( dataset ): # For catogorical data for i in ( dataset . select_dtypes ( exclude = [ 'int' , 'int64' , 'float' , 'float64' ]) . columns ): dataset . groupby ( i )[ i ] . count () . plot . bar () plt . show (); # For continuous data ## Selecting the columns that are continuous for i in ( dataset . select_dtypes ( include = [ 'int' , 'int64' , 'float' , 'float64' ]) . columns ): # Index plot plt . subplot ( 221 ) plt . plot ( dataset [ i ], 'o' , color = 'black' ) plt . xlabel ( i ) # q-q plot plt . subplot ( 222 ) stats . probplot ( dataset [ i ], plot = sns . mpl . pyplot ) #Box chart plt . title ( i ) plt . subplot ( 223 ) ax = sns . boxplot ( x = dataset [ i ]) # Histogram plt . subplot ( 224 ) ax2 = dataset [ i ] . plot . hist ( alpha = 0.75 ) dataset . groupby ( i )[ i ] . count () . plot () plt . show (); univariate_analysis ( df )","title":"Combining the univariate EDA's"},{"location":"Python/Visualization%20for%20predictive%20analytics/#bivariate-analysis","text":"The bivariate analysis deals with visualisations between two variables. The bi-variate analysis is used to identify the relationship between dependant and independent variable. The dependent and independent variables can be of the following types: Problem Independent var Dependent var Classification Categorical Categorical Classification Continuous Categorical Classification Categorical Continuous Classification Continuous Continuous For all the four types, we want to identify the relation between the dependant variable and the independent variable.","title":"Bivariate analysis"},{"location":"Python/Visualization%20for%20predictive%20analytics/#classification-visualisations","text":"First, let us consider the classification problem. Let's say we have to predict the reporting status of the bid. We have three categorical independent variables and five continuous independent variables.","title":"Classification Visualisations"},{"location":"Python/Visualization%20for%20predictive%20analytics/#joint-histograms","text":"The five continuous variables are: 1. Strength in segment 2. Profit for customer 3. Sales Value 4. Profit percentage 5. joint bid portion For these variables, we can look at joint histograms. What we are trying to see is the overlap between the distributions for the two different classes. If the overlap between the two variables is small, then that variable can be a good predictor and vice versa. bi_con_cat = df . groupby ([ 'reporting_status' ])[ 'strength_in_segment' ] . plot . hist ( alpha = 0.5 ) plt . xlabel ( 'strength_in_segment' ) plt . legend ( df . groupby ([ 'reporting_status' ])[ 'strength_in_segment' ] . count () . axes [ 0 ] . tolist ()) plt . title ( 'Joint histogram' ); bi_con_cat = df . groupby ([ 'reporting_status' ])[ 'profit_for_customer' ] . plot . hist ( alpha = 0.5 ) plt . xlabel ( 'profit_for_customer' ) plt . legend ( df . groupby ([ 'reporting_status' ])[ 'profit_for_customer' ] . count () . axes [ 0 ] . tolist ()) plt . title ( 'Joint histogram' ); From the above graphs, we can see that profit for customer can explain the status of the bid when compared to strength of segment. We can also see the mean, variance and the distributions of the independent variables between the classes. In a decision tree, the tree will split with profit_for_customer > 1 as 'Lost' class and profit_for_customer < 1 as 'Won'. In logistic regression, the pseudo R-squared will be greater for profit_for_customer than for strength_in_segment. Similar thinking can be applied to SVM, Naive-Bates classifiers etc.","title":"Joint Histograms"},{"location":"Python/Visualization%20for%20predictive%20analytics/#mosaic-plots","text":"The three categorical variables are: 1. Product 2. Industry 3. Region For these variables, a mosaic plot will be useful. In the mosaic plot, the area of the rectangles is proportional to the frequency of the class. In the x-axis, we have the dependant variables, and in the y-axis, we have the continuous variables. Using this, we can see the relative frequencies of the 'Won' and 'Lost' in each of the dependant variable classes. # from statsmodels.graphics.mosaicplot import mosaic mosaic ( df , [ 'product' , 'reporting_status' ]); For example, the ratio of Lost to won cases is same in products 'G', 'Li', 'P'. Product 'F' has more wins than normal, while product 'L' has more losses than normal. The products 'C' and 'Lo' are too small to be statistically significant. Intuitively, in logistic regression, the products 'G', 'Li', 'P' can be considered as base classes with 'F' having a positive slope value and 'L' having a negative slope value. In decision-trees, the products 'G', 'Li', 'P' will be part of one branch while products 'L' and 'F' will be part of different branches. Similar thinking can be applied to SVM, Naive-Bates classifiers etc.","title":"Mosaic Plots"},{"location":"Python/Visualization%20for%20predictive%20analytics/#combining-the-classification-edas","text":"The below code will do the following for all the columns in the dataset: 1. For continuous data, it will plot the joint histograms 2. For categorical data, it will plot the mosaic plot def classification_bivariate_analysis ( dataset , dependant_variable ): # For continuous data for i in ( dataset . select_dtypes ( include = [ 'int' , 'int64' , 'float' , 'float64' ]) . columns ): bi_con_cat = dataset . groupby ([ dependant_variable ])[ i ] . plot . hist ( alpha = 0.5 ) plt . xlabel ( i ) plt . legend ( dataset . groupby ([ dependant_variable ])[ i ] . count () . axes [ 0 ] . tolist ()) plt . title ( i ) plt . show (); # For catogorical data for i in ( dataset . select_dtypes ( exclude = [ 'int' , 'int64' , 'float' , 'float64' ]) . columns ): if ( i != dependant_variable ): mosaic ( dataset , [ i , dependant_variable ]); plt . show (); classification_bivariate_analysis ( df , 'reporting_status' )","title":"Combining the classification EDA's"},{"location":"Python/Visualization%20for%20predictive%20analytics/#bivariate-regression-visualisations","text":"Let us consider the regression problem. Let's say we have to predict sales_value of the successful bids. We have three categorical independent variables and four continuous independent variables. successful_bids = df [ df [ 'reporting_status' ] == 'Won' ]","title":"Bivariate Regression Visualisations"},{"location":"Python/Visualization%20for%20predictive%20analytics/#scatter-plots","text":"There are four continuous variables: 1. Strength in segment 2. Profit for customer 3. Profit percentage 4. Joint bid portion Scatter plots show how much and how one variable is affected by another. We can use them to identify how changing the independent variable changes the dependant variable. Using this, we can identify if we have to do any transformations to the variables. plt . scatter ( successful_bids [ 'joint_bid_portion' ], successful_bids [ 'sales_value' ]) plt . xlabel ( 'joint_bid_portion' ) plt . ylabel ( 'sales_value' ) plt . title ( 'Scatter plot' ); In the above plot, there seems to be no relation between joint_bid_portion and sales_value. We can also observe how joint bid portion behaves after 80.","title":"Scatter plots"},{"location":"Python/Visualization%20for%20predictive%20analytics/#box-plots","text":"The three categorical variables are: 1. Product 2. Industry 3. Region For these variables, a box plot will be useful. While showing the relative means among the classes, we can also visualise the variations and distributions in the data. bi_variate_boxplot = sns . boxplot ( x = \"industry\" , y = \"sales_value\" , data = successful_bids ) bi_variate_boxplot . set ( title = 'Box Chart' ); From the above plot, the mean of sales for 'Sec', 'Air, 'Ban', 'Cap', 'Con', 'Oth', 'Def', 'Agr' are similar with similar distributions. The mean of 'Ins', 'OG', 'Gov', 'Hea','Whi' classes seems to be higher and the mean of 'Mob', 'Fin', 'Tel' is lower. In a linear regression, the following industries would be considered as base classes: 'Sec', 'Air, 'Ban', 'Cap', 'Con', 'Oth', 'Def', 'Agr' while 'Ins', 'OG', 'Gov', 'Hea','Whi' will have positive slope value and 'Mob', 'Fin', 'Tel' will have a negative slope.","title":"Box plots"},{"location":"Python/Visualization%20for%20predictive%20analytics/#combining-the-bivariate-regression-edas","text":"The below code will do the following for all the columns in the dataset: 1. For continuous data, it will plot the scatter plots 2. For categorical data, it will plot the bar charts def regression_bivariate_analysis ( dataset , dependant_variable ): # For continuous data for i in ( dataset . select_dtypes ( include = [ 'int' , 'int64' , 'float' , 'float64' ]) . columns ): if ( i != dependant_variable ): plt . scatter ( dataset [ i ], dataset [ dependant_variable ]) plt . show (); # For catogorical data for i in ( dataset . select_dtypes ( exclude = [ 'int' , 'int64' , 'float' , 'float64' ]) . columns ): bi_variate_boxplot = sns . boxplot ( x = i , y = dependant_variable , data = dataset ) bi_variate_boxplot . set ( title = i ) plt . show (); regression_bivariate_analysis ( successful_bids , 'sales_value' )","title":"Combining the bivariate regression EDA's"},{"location":"Python/Vizualisation%20using%20python%20Part%201/","text":"Visualizalising tabular data \u00b6 Author: Achyuthuni Sri Harsha In this visualistion, we look at various visualisation types on datatype tables. Matplotlib is the most popular library for viz in Python. Seaborn is built on top of it with integrated analysis, specialized plots, and pretty good integration with Pandas. Plotly express is another library for viz. Also see the full gallery of Seaborn or Matplotlib . #disable some annoying warnings import warnings warnings . filterwarnings ( 'ignore' , category = FutureWarning ) #plots the figures in place instead of a new window % matplotlib inline import matplotlib.pyplot as plt import seaborn as sns import plotly.express as px import pandas as pd import numpy as np In this blog, we are going to look into the Airbnb data for London. We will look at some trends, patterns and effect of seasonality on the data. First, let us ponder over the popularity of Airbnb over time. The popularity is proportional to the number of reviews. Importing the reviews dataset: reviews = pd . read_csv ( 'reviews.csv.gz' , parse_dates = [ 'date' ]) reviews . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } listing_id id date reviewer_id reviewer_name comments 0 11551 30672 2010-03-21 93896 Shar-Lyn The flat was bright, comfortable and clean and... 1 11551 32236 2010-03-29 97890 Zane We stayed with Adriano and Valerio for a week ... 2 11551 41044 2010-05-09 104133 Chase Adriano was a fantastic host. We felt very at ... 3 11551 48926 2010-06-01 122714 John & Sylvia We had a most wonderful stay with Adriano and ... 4 11551 58352 2010-06-28 111543 Monique I'm not sure which of us misunderstood the s... Scatterplot \u00b6 To find the number of reviews, we add the total reviews everyday and we then plot it across time as a scatterplot. fs , axs = plt . subplots ( 1 , figsize = ( 15 , 10 )) plt . title ( \"Airbnb London: Number of reviews across time\" , fontsize = 30 ) reviews . groupby ( 'date' ) . aggregate ({ 'listing_id' : 'count' }) . reset_index () . plot . scatter ( x = 'date' , y = 'listing_id' , alpha = 0.25 , ax = axs ) axs . set_ylabel ( 'Number of reviews' , fontsize = 20 ) axs . set_xlabel ( 'Time' , fontsize = 20 ) plt . show () From this plot, we can observe the following: 1. There is an exponential growth in the business pre-pandemic and this has a sudden drop after Covid related restrictions started. 2. Seasonality within every year is visible To expand n these trends, we should zoom in two sections of the plot. First we should find the seasonality and trend of the data, and then zoom into one of the pre-pandemic year to elaborate on the seasonality. Second, we can zoom into 2020-21 to identify the patterns from covid related lockdowns. # getting the total number of reviews per day reviews_time = reviews . groupby ( 'date' ) . aggregate ({ 'listing_id' : 'count' }) . reset_index () reviews_time [ 'year' ] = reviews_time . date . dt . year # Pre covid data reviews_time_proper = reviews_time [( reviews_time . year < 2020 )] We can find seasonality and trend within the pre-pandemic data by using decomposition (not explained in this blog). Splitting the data into trend, seasonal and random component, gives me the following: from statsmodels.tsa.seasonal import seasonal_decompose result = seasonal_decompose ( reviews_time_proper . listing_id , model = 'additive' , period = 365 ) print ( result . plot ()) We can see a exponential trend and a repeating constant seasonality within the data. Identifying and predicting pre-pandemic trend and predicting for 2020-21. y_values = result . trend [ 182 : 3136 ] x_values = range ( 182 , 3136 ) coeffs = np . polyfit ( x_values , y_values , 2 ) poly_eqn = np . poly1d ( coeffs ) y_hat = poly_eqn ( range ( 365 , len ( reviews_time [ reviews_time . year < 2020 ]))) y_hat1 = poly_eqn ( range ( len ( reviews_time [ reviews_time . year < 2020 ]), len ( reviews_time ))) Approximating the seasonality by using a polynomial equation. y_values = reviews_time [ reviews_time . year == 2019 ] . listing_id x_values = range ( 365 ) coeffs = np . polyfit ( x_values , y_values , 15 ) poly_eqn = np . poly1d ( coeffs ) y_hat_seasonal = poly_eqn ( range ( 365 )) Approximating the pattern in 2020-21 with a polynomial equation. reviews_covid = reviews_time [ reviews_time . year >= 2020 ] y_values = reviews_covid . listing_id x_values = range ( len ( reviews_covid . listing_id )) coeffs = np . polyfit ( x_values , y_values , 17 ) poly_eqn = np . poly1d ( coeffs ) y_hat_covid = poly_eqn ( range ( 25 , len ( reviews_covid . listing_id ) - 15 )) from mpl_toolkits.axes_grid1.inset_locator import mark_inset , inset_axes from matplotlib.patches import ConnectionPatch # making lines from top lot to below plot # Two plots, the main on the top with height 20 inches and the bottom one is 10 inches. fs , axs = plt . subplots ( 2 , figsize = ( 20 , 30 ), gridspec_kw = { 'height_ratios' : [ 2 , 1 ]}, constrained_layout = True ) # Title plt . suptitle ( \"Airbnb London: Number of reviews across time\" , fontsize = 30 ) # First plot, main scatterplot reviews . groupby ( 'date' ) . aggregate ({ 'listing_id' : 'count' }) . \\ reset_index () . plot . scatter ( x = 'date' , y = 'listing_id' , alpha = 0.25 , ax = axs [ 0 ]) # adding the trend lines axs [ 0 ] . plot ( reviews_time [ 365 : len ( reviews_time [ reviews_time . year < 2020 ])] . date , y_hat , color = 'red' ) axs [ 0 ] . plot ( reviews_time [ len ( reviews_time [ reviews_time . year < 2020 ]):] . date , y_hat1 , color = 'red' , linestyle = 'dashed' ) # Modifying the labels and title axs [ 0 ] . set_ylabel ( 'Number of reviews' , fontsize = 15 ) axs [ 0 ] . set_xlabel ( 'Time' , fontsize = 15 ) axs [ 0 ] . set_title ( 'Total reviews of all types of rooms across London. A trend line is plotted taking the exponential growth of the business before Covid 19 and projecting the same trend during Covid. \\n ' + 'Seasonality before Covid is shown by zooming for 2019 (sample year). The affect of covid related lockdowns is also shown by zooming from 2020 onwards.' , fontsize = 15 , loc = 'left' ) # Plotting the data within 2019 as a semantic zooming axins = inset_axes ( axs [ 0 ], 8 , 5 , loc = 2 , bbox_to_anchor = ( 0.15 , 0.925 ), bbox_transform = axs [ 0 ] . figure . transFigure ) # Semantic zooming plot, scatterplot reviews . groupby ( 'date' ) . aggregate ({ 'listing_id' : 'count' }) . reset_index () . \\ plot . scatter ( x = 'date' , y = 'listing_id' , alpha = 0.5 , ax = axins ) #adding the trend lines, labels and title plt . plot ( reviews_time [ reviews_time . year == 2019 ] . date , y_hat_seasonal , color = 'red' ) plt . ylabel ( 'Number of reviews' , fontsize = 15 ) plt . xlabel ( 'Date' , fontsize = 15 ) plt . title ( 'Sesonality in a year (before COVID 19)' , fontsize = 20 ) # Seasonality plot x and y limits x1 = min ( reviews_time [ reviews_time . year == 2019 ] . date ) x2 = max ( reviews_time [ reviews_time . year == 2019 ] . date ) axins . set_xlim ( x1 , x2 ) axins . set_ylim ( 0 , 2000 ) mark_inset ( axs [ 0 ], axins , loc1 = 1 , loc2 = 3 , fc = \"none\" , ec = \"0.5\" ) # Second plot x1 = min ( reviews_time [ reviews_time . year == 2020 ] . date ) x2 = max ( reviews_time [ reviews_time . year >= 2020 ] . date ) reviews_time [ reviews_time . year >= 2020 ] . plot . scatter ( x = 'date' , y = 'listing_id' , alpha = 0.75 , ax = axs [ 1 ]) axs [ 1 ] . set_ylabel ( 'Number of reviews' , fontsize = 15 ) axs [ 1 ] . set_xlabel ( 'Date' , fontsize = 15 ) axs [ 1 ] . set_ylim ( 0 , 1200 ) axs [ 1 ] . set_xlim ( x1 , x2 ) axs [ 1 ] . set_title ( 'Effect of Covid19 on number of reviews' , fontsize = 20 ) axs [ 1 ] . plot ( reviews_covid . date [ 25 : - 15 ], y_hat_covid , color = 'red' ) # Adding annotations in the plot axs [ 1 ] . annotate ( text = 'First Covid 19 advisory \\n 3-16-2020' , # the text xy = ( '3-16-2020' , 500 ), #what to annotate xytext = ( '3-16-2020' , 700 ), # where the text should be arrowprops = dict ( arrowstyle = \"->\" , connectionstyle = \"angle3,angleA=-90,angleB=0\" ), fontsize = 15 ) axs [ 1 ] . annotate ( 'First Lockdown \\n 3-23-2020' , xy = ( '3-23-2020' , 350 ), xytext = ( '5-10-2020' , 600 ), arrowprops = dict ( arrowstyle = \"->\" ), fontsize = 15 ) axs [ 1 ] . annotate ( 'Easing restrictions \\n 7-4-2020' , xy = ( '7-4-2020' , 50 ), xytext = ( '6-15-2020' , 700 ), arrowprops = dict ( arrowstyle = \"->\" ), fontsize = 15 ) axs [ 1 ] . annotate ( 'Restrictions eased further \\n 8-14-2020' , xy = ( '8-14-2020' , 250 ), xytext = ( '8-1-2020' , 600 ), arrowprops = dict ( arrowstyle = \"->\" ), fontsize = 15 ) axs [ 1 ] . annotate ( 'Second Lockdown \\n 10-31-2020' , xy = ( '10-31-2020' , 165 ), xytext = ( '10-1-2020' , 700 ), arrowprops = dict ( arrowstyle = \"->\" ), fontsize = 15 ) axs [ 1 ] . annotate ( 'Easing restrictions \\n 12-2-2020' , xy = ( '12-2-2020' , 120 ), xytext = ( '11-10-2020' , 600 ), arrowprops = dict ( arrowstyle = \"->\" ), fontsize = 15 ) axs [ 1 ] . annotate ( 'Christmas \\n 12-25-2020' , xy = ( '12-25-2020' , 160 ), xytext = ( '12-25-2020' , 700 ), arrowprops = dict ( arrowstyle = \"->\" ), fontsize = 15 ) axs [ 1 ] . annotate ( 'Third Lockdown \\n 1-6-2021' , xy = ( '1-6-2021' , 140 ), xytext = ( '1-20-2021' , 600 ), arrowprops = dict ( arrowstyle = \"->\" ), fontsize = 15 ) axs [ 1 ] . annotate ( 'Schools reopen \\n 3-8-2021' , xy = ( '3-8-2021' , 125 ), xytext = ( '2-25-2021' , 700 ), arrowprops = dict ( arrowstyle = \"->\" ), fontsize = 15 ) axs [ 1 ] . annotate ( 'All restrictions removed \\n 6-21-2021' , xy = ( '6-21-2021' , 400 ), xytext = ( '5-21-2021' , 700 ), arrowprops = dict ( arrowstyle = \"->\" ), fontsize = 15 ) axs [ 1 ] . annotate ( 'Non essentials reopen \\n 4-12-2021' , xy = ( '4-12-2021' , 270 ), xytext = ( '4-1-2021' , 600 ), arrowprops = dict ( arrowstyle = \"->\" ), fontsize = 15 ) # plotting the connections between the two plots con = ConnectionPatch ( xyA = ( x1 , - 105 ), xyB = ( x1 , 1200 ), coordsA = \"data\" , coordsB = \"data\" , axesA = axs [ 0 ], axesB = axs [ 1 ]) axs [ 1 ] . add_artist ( con ) # con = ConnectionPatch(axesA=axs[0], axesB=axs[1]) con = ConnectionPatch ( xyA = ( x2 , - 105 ), xyB = ( x2 , 1200 ), coordsA = \"data\" , coordsB = \"data\" , axesA = axs [ 0 ], axesB = axs [ 1 ]) axs [ 1 ] . add_artist ( con ) plt . show () From this plot, we can see the following: 1. The variation of the reviews across time and the trend before the pandemic are captured. The trend is extrapolated to 2020-21 to show the growth that could have happened if not for the pandemic. 2. Seasonality within the data is shown by semantic zooming into one sample year. We can see how Airbnb is more popular in July, September and January. 3. We can also see the effect of the panemic on the number of reviews. We can observe a sharp decline in the first few months of 2020, and then how lockdowns and openings have affected the total number of reviews. Sunburst and pie charts \u00b6 Let us now deep dive into the data and look at the type of listings and locations that have contributed to this growth. Importing the complete listings dataset. listing_detailed = pd . read_csv ( 'listings.csv.gz' ) pd . options . display . max_columns = None # to show all the columns listing_detailed . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id listing_url scrape_id last_scraped name description neighborhood_overview picture_url host_id host_url host_name host_since host_location host_about host_response_time host_response_rate host_acceptance_rate host_is_superhost host_thumbnail_url host_picture_url host_neighbourhood host_listings_count host_total_listings_count host_verifications host_has_profile_pic host_identity_verified neighbourhood neighbourhood_cleansed neighbourhood_group_cleansed latitude longitude property_type room_type accommodates bathrooms bathrooms_text bedrooms beds amenities price minimum_nights maximum_nights minimum_minimum_nights maximum_minimum_nights minimum_maximum_nights maximum_maximum_nights minimum_nights_avg_ntm maximum_nights_avg_ntm calendar_updated has_availability availability_30 availability_60 availability_90 availability_365 calendar_last_scraped number_of_reviews number_of_reviews_ltm number_of_reviews_l30d first_review last_review review_scores_rating review_scores_accuracy review_scores_cleanliness review_scores_checkin review_scores_communication review_scores_location review_scores_value license instant_bookable calculated_host_listings_count calculated_host_listings_count_entire_homes calculated_host_listings_count_private_rooms calculated_host_listings_count_shared_rooms reviews_per_month 0 11551 https://www.airbnb.com/rooms/11551 20210706215658 2021-07-08 Arty and Bright London Apartment in Zone 2 Unlike most rental apartments my flat gives yo... Not even 10 minutes by metro from Victoria Sta... https://a0.muscache.com/pictures/b7afccf4-18e5... 43039 https://www.airbnb.com/users/show/43039 Adriano 2009-10-03 London, England, United Kingdom Hello, I'm a friendly Italian man with a posit... within an hour 100% 85% f https://a0.muscache.com/im/pictures/user/5f182... https://a0.muscache.com/im/pictures/user/5f182... Brixton 0.0 0.0 ['email', 'phone', 'reviews', 'jumio', 'offlin... t t London, United Kingdom Lambeth NaN 51.46095 -0.11758 Entire apartment Entire home/apt 4 NaN 1 bath 1.0 3.0 [\"Hot water\", \"Hair dryer\", \"Smoke alarm\", \"Fi... $99.00 2 1125 2.0 2.0 1125.0 1125.0 2.0 1125.0 NaN t 0 30 58 290 2021-07-08 193 1 0 2011-10-11 2018-04-29 4.57 4.62 4.58 4.78 4.85 4.53 4.52 NaN f 3 3 0 0 1.63 1 13913 https://www.airbnb.com/rooms/13913 20210706215658 2021-07-08 Holiday London DB Room Let-on going My bright double bedroom with a large window h... Finsbury Park is a friendly melting pot commun... https://a0.muscache.com/pictures/miso/Hosting-... 54730 https://www.airbnb.com/users/show/54730 Alina 2009-11-16 London, England, United Kingdom I am a Multi-Media Visual Artist and Creative ... within a few hours 100% 100% f https://a0.muscache.com/im/users/54730/profile... https://a0.muscache.com/im/users/54730/profile... LB of Islington 3.0 3.0 ['email', 'phone', 'facebook', 'reviews', 'off... t t Islington, Greater London, United Kingdom Islington NaN 51.56861 -0.11270 Private room in apartment Private room 2 NaN 1 shared bath 1.0 0.0 [\"Host greets you\", \"Dryer\", \"Hot water\", \"Sha... $65.00 1 29 1.0 1.0 29.0 29.0 1.0 29.0 NaN t 30 60 90 365 2021-07-08 21 0 0 2011-07-11 2011-09-13 4.85 4.79 4.84 4.79 4.89 4.63 4.74 NaN f 2 1 1 0 0.17 2 15400 https://www.airbnb.com/rooms/15400 20210706215658 2021-07-08 Bright Chelsea Apartment. Chelsea! Lots of windows and light. St Luke's Gardens ... It is Chelsea. https://a0.muscache.com/pictures/428392/462d26... 60302 https://www.airbnb.com/users/show/60302 Philippa 2009-12-05 Kensington, England, United Kingdom English, grandmother, I have travelled quite ... NaN NaN NaN f https://a0.muscache.com/im/users/60302/profile... https://a0.muscache.com/im/users/60302/profile... Chelsea 1.0 1.0 ['email', 'phone', 'reviews', 'jumio', 'govern... t t London, United Kingdom Kensington and Chelsea NaN 51.48780 -0.16813 Entire apartment Entire home/apt 2 NaN 1 bath 1.0 1.0 [\"Dryer\", \"Hot water\", \"Shampoo\", \"Hair dryer\"... $75.00 10 50 10.0 10.0 50.0 50.0 10.0 50.0 NaN t 0 14 44 319 2021-07-08 89 0 0 2012-07-16 2019-08-10 4.79 4.84 4.88 4.87 4.82 4.93 4.73 NaN t 1 1 0 0 0.81 3 17402 https://www.airbnb.com/rooms/17402 20210706215658 2021-07-08 Superb 3-Bed/2 Bath & Wifi: Trendy W1 You'll have a wonderful stay in this superb mo... Location, location, location! You won't find b... https://a0.muscache.com/pictures/39d5309d-fba7... 67564 https://www.airbnb.com/users/show/67564 Liz 2010-01-04 Brighton and Hove, England, United Kingdom We are Liz and Jack. We manage a number of ho... within a day 70% 90% f https://a0.muscache.com/im/users/67564/profile... https://a0.muscache.com/im/users/67564/profile... Fitzrovia 18.0 18.0 ['email', 'phone', 'reviews', 'jumio', 'offlin... t t London, Fitzrovia, United Kingdom Westminster NaN 51.52195 -0.14094 Entire apartment Entire home/apt 6 NaN 2 baths 3.0 3.0 [\"Dryer\", \"Hot water\", \"Shampoo\", \"Hair dryer\"... $307.00 4 365 4.0 4.0 365.0 365.0 4.0 365.0 NaN t 6 6 17 218 2021-07-08 43 1 1 2011-09-18 2019-11-02 4.69 4.80 4.68 4.66 4.66 4.85 4.59 NaN f 15 15 0 0 0.36 4 17506 https://www.airbnb.com/rooms/17506 20210706215658 2021-07-08 Boutique Chelsea/Fulham Double bed 5-star ensuite Enjoy a chic stay in this elegant but fully mo... Fulham is 'villagey' and residential \u2013 a real ... https://a0.muscache.com/pictures/11901327/e63d... 67915 https://www.airbnb.com/users/show/67915 Charlotte 2010-01-05 London, England, United Kingdom Named best B&B by The Times. Easy going hosts,... NaN NaN NaN f https://a0.muscache.com/im/users/67915/profile... https://a0.muscache.com/im/users/67915/profile... Fulham 3.0 3.0 ['email', 'phone', 'jumio', 'selfie', 'governm... t t London, United Kingdom Hammersmith and Fulham NaN 51.47935 -0.19743 Private room in townhouse Private room 2 NaN 1 private bath 1.0 1.0 [\"Air conditioning\", \"Carbon monoxide alarm\", ... $150.00 3 21 3.0 3.0 21.0 21.0 3.0 21.0 NaN t 29 59 89 364 2021-07-08 0 0 0 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN f 2 0 2 0 NaN Looking at the number of reviews by room type and by location, we have: listing_detailed . groupby ( 'room_type' ) . \\ aggregate ({ 'number_of_reviews' : 'sum' }) . \\ plot . pie ( y = 'number_of_reviews' , figsize = ( 10 , 10 ), autopct = ' %1.1f%% ' # to add the percentages text ) plt . ylabel ( \"\" ) plt . title ( \"Airbnb London: Number of reviews by room type\" , fontsize = 20 ) plt . show () # Function to combine last few classes ito one class total_other_reviews = 0 def combine_last_n ( df ): df1 = df . copy () global total_other_reviews total_other_reviews = ( sum ( df [ df . number_of_reviews <= 5000 ] . number_of_reviews )) df1 . loc [ df . number_of_reviews <= 10000 , 'number_of_reviews' ] = 0 return df1 . number_of_reviews listing_detailed . groupby ( 'neighbourhood_cleansed' ) . \\ aggregate ({ 'number_of_reviews' : 'sum' }) . \\ sort_values ( 'number_of_reviews' , ascending = False ) . \\ assign ( no_reviews_alt = combine_last_n ) . \\ append ( pd . Series ({ 'number_of_reviews' : 0 , 'no_reviews_alt' : total_other_reviews }, name = 'Others' )) . \\ plot . pie ( y = 'no_reviews_alt' , figsize = ( 10 , 10 ), legend = None , rotatelabels = True , wedgeprops = dict ( width = .5 ) # for donut shape ) plt . ylabel ( \"\" ) We can see that private room is the most popular with the most number of reviews followed by entire home/apt. The others are insignificant. Similarly, Westminster and Camden are the top two locations in London. Using a sunburst chart, we can look at these two combined. listings_sunburst = listing_detailed . groupby ([ 'room_type' , 'neighbourhood_cleansed' ]) . \\ aggregate ({ 'number_of_reviews' : 'sum' }) . \\ sort_values ( 'number_of_reviews' , ascending = False ) . reset_index () fig = px . sunburst ( listings_sunburst , path = [ 'room_type' , 'neighbourhood_cleansed' ], values = 'number_of_reviews' , hover_data = [ 'room_type' , 'number_of_reviews' ], hover_name = 'neighbourhood_cleansed' , title = 'Airbnb London: Popularity sunburst chart' , width = 900 , height = 900 ) fig . show () Parallel Coordinates \u00b6 From this chart, we can see that Westminster is the most popular location for all the room types, the second and the third popular are different for different room types. Let us take Kensington and Chelsea for example, we can see the ranking of this area in every room type using a parallel cordinates plot. top_20_names = list ( listings_sunburst . sort_values ( 'number_of_reviews' , ascending = False ) . head ( 30 )[ 'neighbourhood_cleansed' ]) listings_sunburst_wide = listings_sunburst . \\ pivot ( index = 'neighbourhood_cleansed' , columns = 'room_type' , values = 'number_of_reviews' ) . reset_index () listings_sunburst_wide = listings_sunburst_wide . replace ( np . nan , 0 ) fig = px . parallel_coordinates ( listings_sunburst_wide ) # add the pink line to highlight Kensington fig . data [ 0 ][ 'dimensions' ][ 0 ][ 'constraintrange' ] = [ 50000 , 60000 ] fig . update_layout ( title_text = 'Kensington and Chelsea (in blue) popularity ranking across room types' , title_x = 0.7 , title_y = 0.05 ) fig . show () From this chart we can see how Kensington (highlighted in Blue) is in top 2 for Entire home Apartment while its not in the top 5 for shared room. This chart, along with the sunburst above shows what type of locations are popular for different room types. Bar chart and Steam graphs \u00b6 How does this ratio between the popularities change with time? One way to see this is using a stacked bar chart. reviews_detailed = pd . merge ( listing_detailed [[ 'id' , 'neighbourhood_cleansed' , 'room_type' ]], reviews , left_on = 'id' , right_on = 'listing_id' ) fs , axs = plt . subplots ( 1 , figsize = ( 15 , 10 )) reviews_detailed [ 'year' ] = reviews_detailed . date . dt . year reviews_detailed . groupby ([ 'year' , 'room_type' ]) . \\ aggregate ({ 'listing_id' : 'count' }) . \\ unstack () . reset_index () . \\ plot . bar ( x = 'year' , y = 'listing_id' , ax = axs , stacked = True ) plt . title ( 'Popularity of different rooms across the years' , fontsize = 25 ) plt . legend ( loc = 'upper right' , title = \"Type of room\" , fontsize = 'medium' , fancybox = True ) axs . set_ylabel ( 'Number of reviews' , fontsize = 20 ) axs . set_xlabel ( 'Years' , fontsize = 20 ) plt . show () From this bar chart we can see the same increase that we have seen in the scatter plot, that is an exponential increase till 2019, and a subsequent decrease due to the pandemic. Another cool way to look at this is by looking at streamgraphs. In streamgraph, we can see the effect of seasonality within the classes. fs , ax = plt . subplots ( 1 , figsize = ( 15 , 10 )) reviews_room = reviews_detailed . groupby ([ 'date' , 'room_type' ]) . \\ aggregate ({ 'listing_id' : 'count' }) . \\ unstack () . reset_index () reviews_room . columns = [ 'date' , 'Entire home/apt' , 'Hotel room' , 'Private room' , 'Shared room' ] ax . stackplot ( reviews_room . date , list ( reviews_room [[ 'Entire home/apt' , 'Hotel room' , 'Private room' , 'Shared room' ]] . fillna ( 0 ) . \\ to_numpy () . transpose ()), baseline = 'wiggle' ) plt . title ( 'Popularity of different rooms across the years' , fontsize = 20 , y = 1.05 , loc = 'left' ) ax . text ( \"2010\" , 1050 , 'Streamgraph of the number of reviews across time' , ha = 'left' , fontsize = 12 ) plt . legend ([ 'Entire home/apt' , 'Hotel room' , 'Private room' , 'Shared room' ], loc = 'upper left' , title = \"Type of room\" ) ax . set_xlabel ( 'Years' , fontsize = 20 ) plt . show () Heatmap \u00b6 Which locations are better, and which locations should improve on their rating? We can find the average rating across the location by averaging out the rating for each host within the location. location_rating = listing_detailed . groupby ([ 'neighbourhood_cleansed' , 'room_type' ]) . \\ aggregate ({ 'review_scores_rating' : 'mean' }) . unstack () . reset_index () location_rating . columns = [ 'neighbourhood' , 'Entire home/apt' , 'Hotel room' , 'Private room' , 'Shared room' ] location_rating . index = location_rating . neighbourhood One of the ways to visualise the average rating is using a heatmap. fig , ax = plt . subplots ( figsize = [ 20 , len ( location_rating ) / 3.3 ]) sns . heatmap ( data = location_rating [[ 'Entire home/apt' , 'Hotel room' , 'Private room' , 'Shared room' ]], annot = False , cbar_kws = { \"shrink\" : 0.5 , \"orientation\" : 'vertical' }, linewidths = 0.004 , linecolor = 'grey' , vmin = 0 , vmax = 5 , center = 0.25 ) plt . show () Although this heatmap presents us the with the average ratings per burough, we can further add the following details for clarity: 1. Location of the burough in London (eg: Central London) 2. Arranged from the best rated to the worst rated buroughs within each location 3. Proper color selection based on scale and human rating psychology : Average human ratings below 2.5 means bad rating and above 4.5 means very good rating (out of 5). It is more natural to use a diverging red-green scale for displaying negitive-positive relationship. def add_regions ( df , borough_col_name ): \"\"\" This function takes as input a dataframe with a column which includes London's borough name Then returns the same dataframw with sub regions names added for each borough \"\"\" central = [ 'Camden' , 'City of London' , 'Kensington and Chelsea' , 'Islington' , 'Lambeth' , 'Southwark' , 'Westminster' ] east = [ 'Barking and Dagenham' , 'Bexley' , 'Greenwich' , 'Hackney' , 'Havering' , 'Lewisham' , 'Newham' , 'Redbridge' , 'Tower Hamlets' , 'Waltham Forest' ] north = [ 'Barnet' , 'Enfield' , 'Haringey' ] south = [ 'Bromley' , 'Croydon' , 'Kingston upon Thames' , 'Merton' , 'Sutton' , 'Wandsworth' ] west = [ 'Brent' , 'Ealing' , 'Hammersmith and Fulham' , 'Harrow' , 'Richmond upon Thames' , 'Hillingdon' , 'Hounslow' ] df [ 'sub_regions' ] = np . where (( df [ borough_col_name ] == central [ 0 ]) | ( df [ borough_col_name ] == central [ 1 ]) | ( df [ borough_col_name ] == central [ 2 ]) | ( df [ borough_col_name ] == central [ 3 ]) | ( df [ borough_col_name ] == central [ 4 ]) | ( df [ borough_col_name ] == central [ 5 ]) | ( df [ borough_col_name ] == central [ 6 ]) , 'Central' , 'no' ) df [ 'sub_regions' ] = np . where (( df [ borough_col_name ] == east [ 0 ]) | ( df [ borough_col_name ] == east [ 1 ]) | ( df [ borough_col_name ] == east [ 2 ]) | ( df [ borough_col_name ] == east [ 3 ]) | ( df [ borough_col_name ] == east [ 4 ]) | ( df [ borough_col_name ] == east [ 5 ]) | ( df [ borough_col_name ] == east [ 6 ]) | ( df [ borough_col_name ] == east [ 7 ]) | ( df [ borough_col_name ] == east [ 8 ]) | ( df [ borough_col_name ] == east [ 9 ]) , 'East' , df [ 'sub_regions' ]) df [ 'sub_regions' ] = np . where (( df [ borough_col_name ] == north [ 0 ]) | ( df [ borough_col_name ] == north [ 1 ]) | ( df [ borough_col_name ] == north [ 2 ]) , 'North' , df [ 'sub_regions' ]) df [ 'sub_regions' ] = np . where (( df [ borough_col_name ] == south [ 0 ]) | ( df [ borough_col_name ] == south [ 1 ]) | ( df [ borough_col_name ] == south [ 2 ]) | ( df [ borough_col_name ] == south [ 3 ]) | ( df [ borough_col_name ] == south [ 4 ]) | ( df [ borough_col_name ] == south [ 5 ]) , 'South' , df [ 'sub_regions' ]) df [ 'sub_regions' ] = np . where (( df [ borough_col_name ] == west [ 0 ]) | ( df [ borough_col_name ] == west [ 1 ]) | ( df [ borough_col_name ] == west [ 2 ]) | ( df [ borough_col_name ] == west [ 3 ]) | ( df [ borough_col_name ] == west [ 4 ]) | ( df [ borough_col_name ] == west [ 5 ]) | ( df [ borough_col_name ] == west [ 6 ]) , 'West' , df [ 'sub_regions' ]) return df def sort_data ( df ): \"\"\" Groups the data by location and sorts the data based on average rating within the location. Different locations are sorted by average rating. \"\"\" df1 = df . copy () df1 [ 'average_rating' ] = ( df1 [ 'Entire home/apt' ] + df1 [ 'Hotel room' ] + df1 [ 'Private room' ] + df1 [ 'Shared room' ]) / 4 df1 [ 'location_average' ] = df1 . groupby ( 'sub_regions' )[ 'average_rating' ] . transform ( 'mean' ) df1 = df1 . sort_values ([ 'location_average' , 'average_rating' ], ascending = False ) return df1 [[ 'Private room' , 'Entire home/apt' , 'Hotel room' , 'Shared room' , 'sub_regions' ]] def prepare_reg_annotation_lists (): \"\"\" Creates the annotation in the form of groups within the data. Displays this on the right hand side of the heatmap. \"\"\" reg_sorted_list = location_rating . sub_regions . unique () reg_len = location_rating . sub_regions . value_counts () . to_dict () sorted_len = [] cum_len = [] arrow_style_str_list = [] # here we define the width of the bracket used, which is proportional to the number of boroughs within a sub region for i in range ( 5 ): if i == 0 : value = reg_len [ reg_sorted_list [ i ]] cum_value = value else : value = reg_len [ reg_sorted_list [ i ]] cum_value += value sorted_len . append ( value ) cum_len . append ( cum_value ) arrow_style_str_list . append ( '-[,widthB=' + str (( value / 1.2 ) - 0.5 ) + ',lengthB=0.7' ) # here we define ticks which represent the center location of each sub region relative to the heatmap Ticks = [] for i in range ( 5 ): if i == 0 : Ticks . append ( 1 - (( cum_len [ i ] / 2 ) / 33 )) else : Ticks . append ( 1 - (((( cum_len [ i ] - cum_len [ i - 1 ]) / 2 ) + cum_len [ i - 1 ]) / 33 )) return Ticks , arrow_style_str_list , reg_sorted_list location_rating = location_rating . replace ( np . nan , 2.5 ) location_rating = add_regions ( location_rating , 'neighbourhood' ) location_rating = sort_data ( location_rating ) Ticks_h , arrow , region = prepare_reg_annotation_lists () A diverging red-green palatte is chosen to represent good reviews and bad reviews. red_green_cmap = sns . diverging_palette ( 10 , 133 , as_cmap = True ) red_green_cmap fig , ax = plt . subplots ( figsize = [ 20 , len ( location_rating ) / 3.3 ]) sns . heatmap ( data = location_rating [[ 'Private room' , 'Entire home/apt' , 'Hotel room' , 'Shared room' ]], annot = False , cbar_kws = { \"shrink\" : 0.5 , \"orientation\" : 'vertical' }, linewidths = 0.004 , linecolor = 'grey' , vmin = 2.25 , vmax = 4.75 , cmap = red_green_cmap ) plt . title ( \"Average ratings for different locations in London\" , fontsize = 20 , y = 1.1 , loc = 'left' ) plt . text ( 0 , - 1 , 'Heatmap depicting the ratings among different locations in London. If no rating is available, minimum rating of 2.5 is assumed. \\n Good ratings are ratings above 3.5 while bad ratings are below. The data is grouped by location (right) and sorted by average rating.' , ha = 'left' , fontsize = 12 ) ax . set_ylabel ( '' ) #annotation for the borough for i in range ( 5 ): ax . annotate ( region [ i ], xy = ( 1.01 , Ticks_h [ i ]), xytext = ( 1.02 , Ticks_h [ i ]), xycoords = 'axes fraction' , ha = 'left' , va = 'center' , arrowprops = dict ( arrowstyle = arrow [ i ], lw = 1 )) We can see the ratings are good across the private rooms and entire home. The best location in each zone is: - West: Richmond upon Thames - Central: Camden - North: Enfield - East: Hackney - South: Croydon Treemap \u00b6 In this context, its not fair to compare ratings of different locations as we have seen that their popularities are different. So there could be 10 reviews in one location while 100 reviews in another. To combine them, we can use a treemap. reviews_treemap = listing_detailed . groupby ([ 'neighbourhood_cleansed' ]) . \\ aggregate ({ 'review_scores_rating' : 'mean' , 'number_of_reviews' : 'sum' }) . reset_index () reviews_treemap = add_regions ( reviews_treemap , 'neighbourhood_cleansed' ) #change col names for nice viz on hover reviews_treemap . columns = [ 'neighbourhood' , 'Average Reviews' , 'Number of reviews' , 'regions' ] fig = px . treemap ( reviews_treemap , path = [ px . Constant ( \"London\" ), 'regions' , 'neighbourhood' ], values = 'Number of reviews' , color = 'Average Reviews' , color_continuous_scale = 'RdBu' ) fig . update_layout ( margin = dict ( t = 50 , l = 25 , r = 25 , b = 25 )) fig . update_layout ( title_text = 'Airbnb London: Ratings overview' ) Wordcloud \u00b6 Now that we have classified the ratings into good ratings and bad ratings, let us look at the text in these ratings and identify if there are any patterns. reviews_detailed_text = pd . merge ( listing_detailed [[ 'id' , 'description' , 'neighborhood_overview' , 'host_about' , 'review_scores_rating' ]], reviews , left_on = 'id' , right_on = 'listing_id' ) reviews_detailed_positive_text = reviews_detailed_text [ reviews_detailed_text . review_scores_rating > 3.75 ] reviews_detailed_negative_text = reviews_detailed_text [ reviews_detailed_text . review_scores_rating <= 3.75 ] Selecting 100 random reviews each for positive and negative sets. pos_reviews_text = reviews_detailed_positive_text . sample ( n = 100 , random_state = 2 ) . comments . str . cat () neg_reviews_text = reviews_detailed_negative_text . sample ( n = 100 , random_state = 3 ) . comments . str . cat () Wordcloud for positive reviews # !pip install wordcloud from wordcloud import WordCloud , STOPWORDS , ImageColorGenerator from PIL import Image mask_pos = np . array ( Image . open ( 'thumbs-up-xxl.png' )) # word cloud, good vs bad ratings stop_words = [ \"https\" , \"co\" , \"RT\" , 'br' , '<br>' , '<br/>' , ' \\r ' , 'r' ] + list ( STOPWORDS ) wordcloud_pattern = WordCloud ( stopwords = stop_words , background_color = \"white\" , max_words = 2000 , max_font_size = 256 , random_state = 42 , mask = mask_pos , width = mask_pos . shape [ 1 ], height = mask_pos . shape [ 0 ]) wordcloud_positive = wordcloud_pattern . generate ( pos_reviews_text ) plt . imshow ( wordcloud_positive , interpolation = 'bilinear' ) plt . axis ( \"off\" ) plt . show () Wordcloud for negative reviews mask_neg = np . array ( Image . open ( 'thumbs-down-xxl.png' )) # word cloud, good vs bad ratings wordcloud_pattern = WordCloud ( stopwords = stop_words , background_color = \"white\" , max_words = 2000 , max_font_size = 256 , random_state = 42 , mask = mask_neg , width = mask_neg . shape [ 1 ], height = mask_neg . shape [ 0 ]) wordcloud_neg = wordcloud_pattern . generate ( neg_reviews_text ) plt . imshow ( wordcloud_neg , interpolation = 'bilinear' ) plt . axis ( \"off\" ) plt . show () Combining the positive and negative reviews in one plot to compare the differences: fs , axs = plt . subplots ( 1 , 2 , figsize = ( 20 , 10 )) plt . suptitle ( \"Airbnb London: Wordcloud of positive and negative reviews\" , fontsize = 20 ) plt . figtext ( 0.5 , 0.925 , 'Wordcloud derived from a random sample of 100 positive and 100 negative reviews.' , wrap = True , horizontalalignment = 'center' , fontsize = 12 ) axs [ 0 ] . imshow ( wordcloud_positive , interpolation = 'bilinear' ) axs [ 0 ] . axis ( \"off\" ) axs [ 1 ] . imshow ( wordcloud_neg , interpolation = 'bilinear' ) axs [ 1 ] . axis ( \"off\" ) plt . show () From these plots, we can see that automated postings, cancellations by hosts, and issues during arrival are the main issues that Airbnb should look into. Correlation matrix \u00b6 How are different parameters within the data related. How is ratings correlated with availability or maximum nights? This can be explained using a correlation plot. listing_detailed [ 'host_response_rate' ] = listing_detailed [ 'host_response_rate' ] . str . replace ( '%' , '' ) . astype ( float ) listing_detailed [ 'host_acceptance_rate' ] = listing_detailed [ 'host_acceptance_rate' ] . str . replace ( '%' , '' ) . astype ( float ) listing_detailed [ 'price' ] = listing_detailed [ 'price' ] . str . replace ( '$' , '' ) . str . replace ( ',' , '' ) . astype ( float ) col_for_corr = [ 'review_scores_rating' , 'review_scores_accuracy' , 'review_scores_cleanliness' , 'review_scores_checkin' , 'review_scores_communication' , 'review_scores_location' , 'review_scores_value' , 'number_of_reviews' , 'number_of_reviews_ltm' , 'number_of_reviews_l30d' , 'reviews_per_month' , 'availability_30' , 'availability_60' , 'availability_90' , 'availability_365' , 'minimum_nights' , 'maximum_nights' , 'minimum_minimum_nights' , 'maximum_minimum_nights' , 'minimum_maximum_nights' , 'maximum_maximum_nights' , 'minimum_nights_avg_ntm' , 'maximum_nights_avg_ntm' , 'bedrooms' , 'beds' , 'accommodates' , 'price' , 'host_response_rate' , 'host_acceptance_rate' , 'host_total_listings_count' ] f = plt . figure ( figsize = ( 20 , 20 )) plt . matshow ( listing_detailed [ col_for_corr ] . corr (), fignum = f , cmap = red_green_cmap , vmin =- 1 , vmax = 1 ) plt . xticks ( range ( listing_detailed [ col_for_corr ] . select_dtypes ([ 'number' ]) . shape [ 1 ]), listing_detailed [ col_for_corr ] . select_dtypes ([ 'number' ]) . columns , rotation = 90 , fontsize = 15 ) plt . yticks ( range ( listing_detailed [ col_for_corr ] . select_dtypes ([ 'number' ]) . shape [ 1 ]), listing_detailed [ col_for_corr ] . select_dtypes ([ 'number' ]) . columns , rotation = 0 , fontsize = 15 ) cb = plt . colorbar () cb . ax . tick_params ( labelsize = 14 ) plt . title ( \"Airbnb London: Correlation between different parameters\" , fontsize = 20 , loc = 'left' ) plt . text ( 0 , 32 , 'Correlation matrix displaying the different parameters within the data. DIverging green (+ve) red (-ve) scale is used to display the correlations. \\n \\ Features considered: Review scores, Number of reviews, availability, maximum and minimum days of stay, host and room parameters.' , ha = 'left' , fontsize = 12 ) plt . show () Scatterplot matrix \u00b6 While the above plot shows the correlation across various variables, I want to deep dive into change in ratings with price. I can use a scatterplot matrix to visualise this. Aditionally, I want the costliest properties, and the most popular yet cheap properties annotated. def annotate_plot ( x , y , ** kwargs ): if ( x . name == 'price' and y . name == 'review_scores_rating' ): ax = plt . gca () for index , obj in listing_detailed . nlargest ( 2 , 'price' ) . iterrows (): plt . annotate ( obj [ 'name' ], # the text xy = ( obj . price , obj . review_scores_rating ), xytext = ( 7500 , obj . review_scores_rating - 0.5 ), arrowprops = dict ( arrowstyle = \"->\" ) ) elif ( x . name == 'price' and y . name == 'number_of_reviews' ): ax = plt . gca () for index , obj in listing_detailed . nlargest ( 3 , 'number_of_reviews' ) . iterrows (): ax . text ( obj . price , obj . number_of_reviews , obj [ 'name' ]) col_for_pairplot = [ 'review_scores_rating' , 'number_of_reviews' , 'price' ] sns_plot = sns . pairplot ( listing_detailed , vars = col_for_pairplot , kind = 'scatter' , hue = 'room_type' , diag_kind = 'kde' ,) sns_plot . fig . set_size_inches ( 20 , 20 ) sns_plot . _legend . set_bbox_to_anchor (( 0.15 , 0.89 )) sns_plot . map_upper ( annotate_plot ) sns_plot . fig . suptitle ( \"Airbnb London: Scatterplot matrix\" , fontsize = 20 , y = 1 ) We can see that the two costliest properties are either historic apartments or a mansion. The three most popular yet cheap properties are small and quaint properties near popular destinations. Boxplot and Violin chart \u00b6 To look at the variation in ratings within the different room types, we could use either a boxplot or a Violin plot as shown. fs , axs = plt . subplots ( 1 , 2 , figsize = ( 20 , 20 )) listing_detailed . boxplot ( column = 'review_scores_rating' , by = 'room_type' , figsize = ( 10 , 20 ), ax = axs [ 0 ]) sns . violinplot ( 'room_type' , 'review_scores_rating' , data = listing_detailed , ax = axs [ 1 ]) plt . suptitle ( \"Airbnb London: Average rating across room types\" , fontsize = 20 , y = 0.95 ) plt . figtext ( 0.5 , 0.925 , 'Boxplot (left) and Violin plot (right) for the average review across room types' , wrap = True , horizontalalignment = 'center' , fontsize = 12 ) axs [ 0 ] . set_title ( '' ) for ax in axs : ax . set_ylim ( - 1 , 6 ) ax . set_ylabel ( 'Average Rating' , fontsize = 12 ) ax . set_xlabel ( 'Room types' , fontsize = 12 ) Cluster map \u00b6 If we wanted to cluster localities based on some features, then cluster map is teh ideal choice. In the below map, we cluster different locations based on one feature from each type. The features are also clustered to show the similarity between features. Finally we use a white-blue color palatte for displaying the variation within the data. from sklearn.preprocessing import MinMaxScaler import seaborn as sns col_for_corr = [ 'price' , 'review_scores_rating' , 'number_of_reviews' , 'availability_90' , 'minimum_nights_avg_ntm' , 'bedrooms' , 'host_response_rate' ] df_cluster = listing_detailed . groupby ( 'neighbourhood_cleansed' ) . aggregate ({ 'review_scores_rating' : 'mean' , 'number_of_reviews' : 'sum' , 'availability_90' : 'mean' , 'minimum_nights_avg_ntm' : 'mean' , 'bedrooms' : 'mean' , 'price' : 'mean' , 'host_response_rate' : 'mean' }) . reset_index () scaler = MinMaxScaler () df_cluster1 = pd . DataFrame ( scaler . fit_transform ( df_cluster [ col_for_corr ]), columns = col_for_corr ) df_cluster1 . index = df_cluster . neighbourhood_cleansed crest_cmap = sns . color_palette ( \"crest\" , as_cmap = True ) crest_cmap g = sns . clustermap ( df_cluster1 , cmap = crest_cmap , vmin = 0 , vmax = 1 ) plt . title ( \"Airbnb London: Clusters within London\" , fontsize = 20 , loc = 'left' , y = 2 , x = - 25 ) g . ax_cbar . set_position (( 1 , .2 , .03 , .4 )) g . ax_heatmap . set_ylabel ( \"\" ) plt . show () References \u00b6 Visualisation Analytics and Design, Tamara Munzner Class notes and asignments, Imperial College London Visual Analytics lab at JKU Linz","title":"Visualizalising tabular data"},{"location":"Python/Vizualisation%20using%20python%20Part%201/#visualizalising-tabular-data","text":"Author: Achyuthuni Sri Harsha In this visualistion, we look at various visualisation types on datatype tables. Matplotlib is the most popular library for viz in Python. Seaborn is built on top of it with integrated analysis, specialized plots, and pretty good integration with Pandas. Plotly express is another library for viz. Also see the full gallery of Seaborn or Matplotlib . #disable some annoying warnings import warnings warnings . filterwarnings ( 'ignore' , category = FutureWarning ) #plots the figures in place instead of a new window % matplotlib inline import matplotlib.pyplot as plt import seaborn as sns import plotly.express as px import pandas as pd import numpy as np In this blog, we are going to look into the Airbnb data for London. We will look at some trends, patterns and effect of seasonality on the data. First, let us ponder over the popularity of Airbnb over time. The popularity is proportional to the number of reviews. Importing the reviews dataset: reviews = pd . read_csv ( 'reviews.csv.gz' , parse_dates = [ 'date' ]) reviews . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } listing_id id date reviewer_id reviewer_name comments 0 11551 30672 2010-03-21 93896 Shar-Lyn The flat was bright, comfortable and clean and... 1 11551 32236 2010-03-29 97890 Zane We stayed with Adriano and Valerio for a week ... 2 11551 41044 2010-05-09 104133 Chase Adriano was a fantastic host. We felt very at ... 3 11551 48926 2010-06-01 122714 John & Sylvia We had a most wonderful stay with Adriano and ... 4 11551 58352 2010-06-28 111543 Monique I'm not sure which of us misunderstood the s...","title":"Visualizalising tabular data"},{"location":"Python/Vizualisation%20using%20python%20Part%201/#scatterplot","text":"To find the number of reviews, we add the total reviews everyday and we then plot it across time as a scatterplot. fs , axs = plt . subplots ( 1 , figsize = ( 15 , 10 )) plt . title ( \"Airbnb London: Number of reviews across time\" , fontsize = 30 ) reviews . groupby ( 'date' ) . aggregate ({ 'listing_id' : 'count' }) . reset_index () . plot . scatter ( x = 'date' , y = 'listing_id' , alpha = 0.25 , ax = axs ) axs . set_ylabel ( 'Number of reviews' , fontsize = 20 ) axs . set_xlabel ( 'Time' , fontsize = 20 ) plt . show () From this plot, we can observe the following: 1. There is an exponential growth in the business pre-pandemic and this has a sudden drop after Covid related restrictions started. 2. Seasonality within every year is visible To expand n these trends, we should zoom in two sections of the plot. First we should find the seasonality and trend of the data, and then zoom into one of the pre-pandemic year to elaborate on the seasonality. Second, we can zoom into 2020-21 to identify the patterns from covid related lockdowns. # getting the total number of reviews per day reviews_time = reviews . groupby ( 'date' ) . aggregate ({ 'listing_id' : 'count' }) . reset_index () reviews_time [ 'year' ] = reviews_time . date . dt . year # Pre covid data reviews_time_proper = reviews_time [( reviews_time . year < 2020 )] We can find seasonality and trend within the pre-pandemic data by using decomposition (not explained in this blog). Splitting the data into trend, seasonal and random component, gives me the following: from statsmodels.tsa.seasonal import seasonal_decompose result = seasonal_decompose ( reviews_time_proper . listing_id , model = 'additive' , period = 365 ) print ( result . plot ()) We can see a exponential trend and a repeating constant seasonality within the data. Identifying and predicting pre-pandemic trend and predicting for 2020-21. y_values = result . trend [ 182 : 3136 ] x_values = range ( 182 , 3136 ) coeffs = np . polyfit ( x_values , y_values , 2 ) poly_eqn = np . poly1d ( coeffs ) y_hat = poly_eqn ( range ( 365 , len ( reviews_time [ reviews_time . year < 2020 ]))) y_hat1 = poly_eqn ( range ( len ( reviews_time [ reviews_time . year < 2020 ]), len ( reviews_time ))) Approximating the seasonality by using a polynomial equation. y_values = reviews_time [ reviews_time . year == 2019 ] . listing_id x_values = range ( 365 ) coeffs = np . polyfit ( x_values , y_values , 15 ) poly_eqn = np . poly1d ( coeffs ) y_hat_seasonal = poly_eqn ( range ( 365 )) Approximating the pattern in 2020-21 with a polynomial equation. reviews_covid = reviews_time [ reviews_time . year >= 2020 ] y_values = reviews_covid . listing_id x_values = range ( len ( reviews_covid . listing_id )) coeffs = np . polyfit ( x_values , y_values , 17 ) poly_eqn = np . poly1d ( coeffs ) y_hat_covid = poly_eqn ( range ( 25 , len ( reviews_covid . listing_id ) - 15 )) from mpl_toolkits.axes_grid1.inset_locator import mark_inset , inset_axes from matplotlib.patches import ConnectionPatch # making lines from top lot to below plot # Two plots, the main on the top with height 20 inches and the bottom one is 10 inches. fs , axs = plt . subplots ( 2 , figsize = ( 20 , 30 ), gridspec_kw = { 'height_ratios' : [ 2 , 1 ]}, constrained_layout = True ) # Title plt . suptitle ( \"Airbnb London: Number of reviews across time\" , fontsize = 30 ) # First plot, main scatterplot reviews . groupby ( 'date' ) . aggregate ({ 'listing_id' : 'count' }) . \\ reset_index () . plot . scatter ( x = 'date' , y = 'listing_id' , alpha = 0.25 , ax = axs [ 0 ]) # adding the trend lines axs [ 0 ] . plot ( reviews_time [ 365 : len ( reviews_time [ reviews_time . year < 2020 ])] . date , y_hat , color = 'red' ) axs [ 0 ] . plot ( reviews_time [ len ( reviews_time [ reviews_time . year < 2020 ]):] . date , y_hat1 , color = 'red' , linestyle = 'dashed' ) # Modifying the labels and title axs [ 0 ] . set_ylabel ( 'Number of reviews' , fontsize = 15 ) axs [ 0 ] . set_xlabel ( 'Time' , fontsize = 15 ) axs [ 0 ] . set_title ( 'Total reviews of all types of rooms across London. A trend line is plotted taking the exponential growth of the business before Covid 19 and projecting the same trend during Covid. \\n ' + 'Seasonality before Covid is shown by zooming for 2019 (sample year). The affect of covid related lockdowns is also shown by zooming from 2020 onwards.' , fontsize = 15 , loc = 'left' ) # Plotting the data within 2019 as a semantic zooming axins = inset_axes ( axs [ 0 ], 8 , 5 , loc = 2 , bbox_to_anchor = ( 0.15 , 0.925 ), bbox_transform = axs [ 0 ] . figure . transFigure ) # Semantic zooming plot, scatterplot reviews . groupby ( 'date' ) . aggregate ({ 'listing_id' : 'count' }) . reset_index () . \\ plot . scatter ( x = 'date' , y = 'listing_id' , alpha = 0.5 , ax = axins ) #adding the trend lines, labels and title plt . plot ( reviews_time [ reviews_time . year == 2019 ] . date , y_hat_seasonal , color = 'red' ) plt . ylabel ( 'Number of reviews' , fontsize = 15 ) plt . xlabel ( 'Date' , fontsize = 15 ) plt . title ( 'Sesonality in a year (before COVID 19)' , fontsize = 20 ) # Seasonality plot x and y limits x1 = min ( reviews_time [ reviews_time . year == 2019 ] . date ) x2 = max ( reviews_time [ reviews_time . year == 2019 ] . date ) axins . set_xlim ( x1 , x2 ) axins . set_ylim ( 0 , 2000 ) mark_inset ( axs [ 0 ], axins , loc1 = 1 , loc2 = 3 , fc = \"none\" , ec = \"0.5\" ) # Second plot x1 = min ( reviews_time [ reviews_time . year == 2020 ] . date ) x2 = max ( reviews_time [ reviews_time . year >= 2020 ] . date ) reviews_time [ reviews_time . year >= 2020 ] . plot . scatter ( x = 'date' , y = 'listing_id' , alpha = 0.75 , ax = axs [ 1 ]) axs [ 1 ] . set_ylabel ( 'Number of reviews' , fontsize = 15 ) axs [ 1 ] . set_xlabel ( 'Date' , fontsize = 15 ) axs [ 1 ] . set_ylim ( 0 , 1200 ) axs [ 1 ] . set_xlim ( x1 , x2 ) axs [ 1 ] . set_title ( 'Effect of Covid19 on number of reviews' , fontsize = 20 ) axs [ 1 ] . plot ( reviews_covid . date [ 25 : - 15 ], y_hat_covid , color = 'red' ) # Adding annotations in the plot axs [ 1 ] . annotate ( text = 'First Covid 19 advisory \\n 3-16-2020' , # the text xy = ( '3-16-2020' , 500 ), #what to annotate xytext = ( '3-16-2020' , 700 ), # where the text should be arrowprops = dict ( arrowstyle = \"->\" , connectionstyle = \"angle3,angleA=-90,angleB=0\" ), fontsize = 15 ) axs [ 1 ] . annotate ( 'First Lockdown \\n 3-23-2020' , xy = ( '3-23-2020' , 350 ), xytext = ( '5-10-2020' , 600 ), arrowprops = dict ( arrowstyle = \"->\" ), fontsize = 15 ) axs [ 1 ] . annotate ( 'Easing restrictions \\n 7-4-2020' , xy = ( '7-4-2020' , 50 ), xytext = ( '6-15-2020' , 700 ), arrowprops = dict ( arrowstyle = \"->\" ), fontsize = 15 ) axs [ 1 ] . annotate ( 'Restrictions eased further \\n 8-14-2020' , xy = ( '8-14-2020' , 250 ), xytext = ( '8-1-2020' , 600 ), arrowprops = dict ( arrowstyle = \"->\" ), fontsize = 15 ) axs [ 1 ] . annotate ( 'Second Lockdown \\n 10-31-2020' , xy = ( '10-31-2020' , 165 ), xytext = ( '10-1-2020' , 700 ), arrowprops = dict ( arrowstyle = \"->\" ), fontsize = 15 ) axs [ 1 ] . annotate ( 'Easing restrictions \\n 12-2-2020' , xy = ( '12-2-2020' , 120 ), xytext = ( '11-10-2020' , 600 ), arrowprops = dict ( arrowstyle = \"->\" ), fontsize = 15 ) axs [ 1 ] . annotate ( 'Christmas \\n 12-25-2020' , xy = ( '12-25-2020' , 160 ), xytext = ( '12-25-2020' , 700 ), arrowprops = dict ( arrowstyle = \"->\" ), fontsize = 15 ) axs [ 1 ] . annotate ( 'Third Lockdown \\n 1-6-2021' , xy = ( '1-6-2021' , 140 ), xytext = ( '1-20-2021' , 600 ), arrowprops = dict ( arrowstyle = \"->\" ), fontsize = 15 ) axs [ 1 ] . annotate ( 'Schools reopen \\n 3-8-2021' , xy = ( '3-8-2021' , 125 ), xytext = ( '2-25-2021' , 700 ), arrowprops = dict ( arrowstyle = \"->\" ), fontsize = 15 ) axs [ 1 ] . annotate ( 'All restrictions removed \\n 6-21-2021' , xy = ( '6-21-2021' , 400 ), xytext = ( '5-21-2021' , 700 ), arrowprops = dict ( arrowstyle = \"->\" ), fontsize = 15 ) axs [ 1 ] . annotate ( 'Non essentials reopen \\n 4-12-2021' , xy = ( '4-12-2021' , 270 ), xytext = ( '4-1-2021' , 600 ), arrowprops = dict ( arrowstyle = \"->\" ), fontsize = 15 ) # plotting the connections between the two plots con = ConnectionPatch ( xyA = ( x1 , - 105 ), xyB = ( x1 , 1200 ), coordsA = \"data\" , coordsB = \"data\" , axesA = axs [ 0 ], axesB = axs [ 1 ]) axs [ 1 ] . add_artist ( con ) # con = ConnectionPatch(axesA=axs[0], axesB=axs[1]) con = ConnectionPatch ( xyA = ( x2 , - 105 ), xyB = ( x2 , 1200 ), coordsA = \"data\" , coordsB = \"data\" , axesA = axs [ 0 ], axesB = axs [ 1 ]) axs [ 1 ] . add_artist ( con ) plt . show () From this plot, we can see the following: 1. The variation of the reviews across time and the trend before the pandemic are captured. The trend is extrapolated to 2020-21 to show the growth that could have happened if not for the pandemic. 2. Seasonality within the data is shown by semantic zooming into one sample year. We can see how Airbnb is more popular in July, September and January. 3. We can also see the effect of the panemic on the number of reviews. We can observe a sharp decline in the first few months of 2020, and then how lockdowns and openings have affected the total number of reviews.","title":"Scatterplot"},{"location":"Python/Vizualisation%20using%20python%20Part%201/#sunburst-and-pie-charts","text":"Let us now deep dive into the data and look at the type of listings and locations that have contributed to this growth. Importing the complete listings dataset. listing_detailed = pd . read_csv ( 'listings.csv.gz' ) pd . options . display . max_columns = None # to show all the columns listing_detailed . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id listing_url scrape_id last_scraped name description neighborhood_overview picture_url host_id host_url host_name host_since host_location host_about host_response_time host_response_rate host_acceptance_rate host_is_superhost host_thumbnail_url host_picture_url host_neighbourhood host_listings_count host_total_listings_count host_verifications host_has_profile_pic host_identity_verified neighbourhood neighbourhood_cleansed neighbourhood_group_cleansed latitude longitude property_type room_type accommodates bathrooms bathrooms_text bedrooms beds amenities price minimum_nights maximum_nights minimum_minimum_nights maximum_minimum_nights minimum_maximum_nights maximum_maximum_nights minimum_nights_avg_ntm maximum_nights_avg_ntm calendar_updated has_availability availability_30 availability_60 availability_90 availability_365 calendar_last_scraped number_of_reviews number_of_reviews_ltm number_of_reviews_l30d first_review last_review review_scores_rating review_scores_accuracy review_scores_cleanliness review_scores_checkin review_scores_communication review_scores_location review_scores_value license instant_bookable calculated_host_listings_count calculated_host_listings_count_entire_homes calculated_host_listings_count_private_rooms calculated_host_listings_count_shared_rooms reviews_per_month 0 11551 https://www.airbnb.com/rooms/11551 20210706215658 2021-07-08 Arty and Bright London Apartment in Zone 2 Unlike most rental apartments my flat gives yo... Not even 10 minutes by metro from Victoria Sta... https://a0.muscache.com/pictures/b7afccf4-18e5... 43039 https://www.airbnb.com/users/show/43039 Adriano 2009-10-03 London, England, United Kingdom Hello, I'm a friendly Italian man with a posit... within an hour 100% 85% f https://a0.muscache.com/im/pictures/user/5f182... https://a0.muscache.com/im/pictures/user/5f182... Brixton 0.0 0.0 ['email', 'phone', 'reviews', 'jumio', 'offlin... t t London, United Kingdom Lambeth NaN 51.46095 -0.11758 Entire apartment Entire home/apt 4 NaN 1 bath 1.0 3.0 [\"Hot water\", \"Hair dryer\", \"Smoke alarm\", \"Fi... $99.00 2 1125 2.0 2.0 1125.0 1125.0 2.0 1125.0 NaN t 0 30 58 290 2021-07-08 193 1 0 2011-10-11 2018-04-29 4.57 4.62 4.58 4.78 4.85 4.53 4.52 NaN f 3 3 0 0 1.63 1 13913 https://www.airbnb.com/rooms/13913 20210706215658 2021-07-08 Holiday London DB Room Let-on going My bright double bedroom with a large window h... Finsbury Park is a friendly melting pot commun... https://a0.muscache.com/pictures/miso/Hosting-... 54730 https://www.airbnb.com/users/show/54730 Alina 2009-11-16 London, England, United Kingdom I am a Multi-Media Visual Artist and Creative ... within a few hours 100% 100% f https://a0.muscache.com/im/users/54730/profile... https://a0.muscache.com/im/users/54730/profile... LB of Islington 3.0 3.0 ['email', 'phone', 'facebook', 'reviews', 'off... t t Islington, Greater London, United Kingdom Islington NaN 51.56861 -0.11270 Private room in apartment Private room 2 NaN 1 shared bath 1.0 0.0 [\"Host greets you\", \"Dryer\", \"Hot water\", \"Sha... $65.00 1 29 1.0 1.0 29.0 29.0 1.0 29.0 NaN t 30 60 90 365 2021-07-08 21 0 0 2011-07-11 2011-09-13 4.85 4.79 4.84 4.79 4.89 4.63 4.74 NaN f 2 1 1 0 0.17 2 15400 https://www.airbnb.com/rooms/15400 20210706215658 2021-07-08 Bright Chelsea Apartment. Chelsea! Lots of windows and light. St Luke's Gardens ... It is Chelsea. https://a0.muscache.com/pictures/428392/462d26... 60302 https://www.airbnb.com/users/show/60302 Philippa 2009-12-05 Kensington, England, United Kingdom English, grandmother, I have travelled quite ... NaN NaN NaN f https://a0.muscache.com/im/users/60302/profile... https://a0.muscache.com/im/users/60302/profile... Chelsea 1.0 1.0 ['email', 'phone', 'reviews', 'jumio', 'govern... t t London, United Kingdom Kensington and Chelsea NaN 51.48780 -0.16813 Entire apartment Entire home/apt 2 NaN 1 bath 1.0 1.0 [\"Dryer\", \"Hot water\", \"Shampoo\", \"Hair dryer\"... $75.00 10 50 10.0 10.0 50.0 50.0 10.0 50.0 NaN t 0 14 44 319 2021-07-08 89 0 0 2012-07-16 2019-08-10 4.79 4.84 4.88 4.87 4.82 4.93 4.73 NaN t 1 1 0 0 0.81 3 17402 https://www.airbnb.com/rooms/17402 20210706215658 2021-07-08 Superb 3-Bed/2 Bath & Wifi: Trendy W1 You'll have a wonderful stay in this superb mo... Location, location, location! You won't find b... https://a0.muscache.com/pictures/39d5309d-fba7... 67564 https://www.airbnb.com/users/show/67564 Liz 2010-01-04 Brighton and Hove, England, United Kingdom We are Liz and Jack. We manage a number of ho... within a day 70% 90% f https://a0.muscache.com/im/users/67564/profile... https://a0.muscache.com/im/users/67564/profile... Fitzrovia 18.0 18.0 ['email', 'phone', 'reviews', 'jumio', 'offlin... t t London, Fitzrovia, United Kingdom Westminster NaN 51.52195 -0.14094 Entire apartment Entire home/apt 6 NaN 2 baths 3.0 3.0 [\"Dryer\", \"Hot water\", \"Shampoo\", \"Hair dryer\"... $307.00 4 365 4.0 4.0 365.0 365.0 4.0 365.0 NaN t 6 6 17 218 2021-07-08 43 1 1 2011-09-18 2019-11-02 4.69 4.80 4.68 4.66 4.66 4.85 4.59 NaN f 15 15 0 0 0.36 4 17506 https://www.airbnb.com/rooms/17506 20210706215658 2021-07-08 Boutique Chelsea/Fulham Double bed 5-star ensuite Enjoy a chic stay in this elegant but fully mo... Fulham is 'villagey' and residential \u2013 a real ... https://a0.muscache.com/pictures/11901327/e63d... 67915 https://www.airbnb.com/users/show/67915 Charlotte 2010-01-05 London, England, United Kingdom Named best B&B by The Times. Easy going hosts,... NaN NaN NaN f https://a0.muscache.com/im/users/67915/profile... https://a0.muscache.com/im/users/67915/profile... Fulham 3.0 3.0 ['email', 'phone', 'jumio', 'selfie', 'governm... t t London, United Kingdom Hammersmith and Fulham NaN 51.47935 -0.19743 Private room in townhouse Private room 2 NaN 1 private bath 1.0 1.0 [\"Air conditioning\", \"Carbon monoxide alarm\", ... $150.00 3 21 3.0 3.0 21.0 21.0 3.0 21.0 NaN t 29 59 89 364 2021-07-08 0 0 0 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN f 2 0 2 0 NaN Looking at the number of reviews by room type and by location, we have: listing_detailed . groupby ( 'room_type' ) . \\ aggregate ({ 'number_of_reviews' : 'sum' }) . \\ plot . pie ( y = 'number_of_reviews' , figsize = ( 10 , 10 ), autopct = ' %1.1f%% ' # to add the percentages text ) plt . ylabel ( \"\" ) plt . title ( \"Airbnb London: Number of reviews by room type\" , fontsize = 20 ) plt . show () # Function to combine last few classes ito one class total_other_reviews = 0 def combine_last_n ( df ): df1 = df . copy () global total_other_reviews total_other_reviews = ( sum ( df [ df . number_of_reviews <= 5000 ] . number_of_reviews )) df1 . loc [ df . number_of_reviews <= 10000 , 'number_of_reviews' ] = 0 return df1 . number_of_reviews listing_detailed . groupby ( 'neighbourhood_cleansed' ) . \\ aggregate ({ 'number_of_reviews' : 'sum' }) . \\ sort_values ( 'number_of_reviews' , ascending = False ) . \\ assign ( no_reviews_alt = combine_last_n ) . \\ append ( pd . Series ({ 'number_of_reviews' : 0 , 'no_reviews_alt' : total_other_reviews }, name = 'Others' )) . \\ plot . pie ( y = 'no_reviews_alt' , figsize = ( 10 , 10 ), legend = None , rotatelabels = True , wedgeprops = dict ( width = .5 ) # for donut shape ) plt . ylabel ( \"\" ) We can see that private room is the most popular with the most number of reviews followed by entire home/apt. The others are insignificant. Similarly, Westminster and Camden are the top two locations in London. Using a sunburst chart, we can look at these two combined. listings_sunburst = listing_detailed . groupby ([ 'room_type' , 'neighbourhood_cleansed' ]) . \\ aggregate ({ 'number_of_reviews' : 'sum' }) . \\ sort_values ( 'number_of_reviews' , ascending = False ) . reset_index () fig = px . sunburst ( listings_sunburst , path = [ 'room_type' , 'neighbourhood_cleansed' ], values = 'number_of_reviews' , hover_data = [ 'room_type' , 'number_of_reviews' ], hover_name = 'neighbourhood_cleansed' , title = 'Airbnb London: Popularity sunburst chart' , width = 900 , height = 900 ) fig . show ()","title":"Sunburst and pie charts"},{"location":"Python/Vizualisation%20using%20python%20Part%201/#parallel-coordinates","text":"From this chart, we can see that Westminster is the most popular location for all the room types, the second and the third popular are different for different room types. Let us take Kensington and Chelsea for example, we can see the ranking of this area in every room type using a parallel cordinates plot. top_20_names = list ( listings_sunburst . sort_values ( 'number_of_reviews' , ascending = False ) . head ( 30 )[ 'neighbourhood_cleansed' ]) listings_sunburst_wide = listings_sunburst . \\ pivot ( index = 'neighbourhood_cleansed' , columns = 'room_type' , values = 'number_of_reviews' ) . reset_index () listings_sunburst_wide = listings_sunburst_wide . replace ( np . nan , 0 ) fig = px . parallel_coordinates ( listings_sunburst_wide ) # add the pink line to highlight Kensington fig . data [ 0 ][ 'dimensions' ][ 0 ][ 'constraintrange' ] = [ 50000 , 60000 ] fig . update_layout ( title_text = 'Kensington and Chelsea (in blue) popularity ranking across room types' , title_x = 0.7 , title_y = 0.05 ) fig . show () From this chart we can see how Kensington (highlighted in Blue) is in top 2 for Entire home Apartment while its not in the top 5 for shared room. This chart, along with the sunburst above shows what type of locations are popular for different room types.","title":"Parallel Coordinates"},{"location":"Python/Vizualisation%20using%20python%20Part%201/#bar-chart-and-steam-graphs","text":"How does this ratio between the popularities change with time? One way to see this is using a stacked bar chart. reviews_detailed = pd . merge ( listing_detailed [[ 'id' , 'neighbourhood_cleansed' , 'room_type' ]], reviews , left_on = 'id' , right_on = 'listing_id' ) fs , axs = plt . subplots ( 1 , figsize = ( 15 , 10 )) reviews_detailed [ 'year' ] = reviews_detailed . date . dt . year reviews_detailed . groupby ([ 'year' , 'room_type' ]) . \\ aggregate ({ 'listing_id' : 'count' }) . \\ unstack () . reset_index () . \\ plot . bar ( x = 'year' , y = 'listing_id' , ax = axs , stacked = True ) plt . title ( 'Popularity of different rooms across the years' , fontsize = 25 ) plt . legend ( loc = 'upper right' , title = \"Type of room\" , fontsize = 'medium' , fancybox = True ) axs . set_ylabel ( 'Number of reviews' , fontsize = 20 ) axs . set_xlabel ( 'Years' , fontsize = 20 ) plt . show () From this bar chart we can see the same increase that we have seen in the scatter plot, that is an exponential increase till 2019, and a subsequent decrease due to the pandemic. Another cool way to look at this is by looking at streamgraphs. In streamgraph, we can see the effect of seasonality within the classes. fs , ax = plt . subplots ( 1 , figsize = ( 15 , 10 )) reviews_room = reviews_detailed . groupby ([ 'date' , 'room_type' ]) . \\ aggregate ({ 'listing_id' : 'count' }) . \\ unstack () . reset_index () reviews_room . columns = [ 'date' , 'Entire home/apt' , 'Hotel room' , 'Private room' , 'Shared room' ] ax . stackplot ( reviews_room . date , list ( reviews_room [[ 'Entire home/apt' , 'Hotel room' , 'Private room' , 'Shared room' ]] . fillna ( 0 ) . \\ to_numpy () . transpose ()), baseline = 'wiggle' ) plt . title ( 'Popularity of different rooms across the years' , fontsize = 20 , y = 1.05 , loc = 'left' ) ax . text ( \"2010\" , 1050 , 'Streamgraph of the number of reviews across time' , ha = 'left' , fontsize = 12 ) plt . legend ([ 'Entire home/apt' , 'Hotel room' , 'Private room' , 'Shared room' ], loc = 'upper left' , title = \"Type of room\" ) ax . set_xlabel ( 'Years' , fontsize = 20 ) plt . show ()","title":"Bar chart and Steam graphs"},{"location":"Python/Vizualisation%20using%20python%20Part%201/#heatmap","text":"Which locations are better, and which locations should improve on their rating? We can find the average rating across the location by averaging out the rating for each host within the location. location_rating = listing_detailed . groupby ([ 'neighbourhood_cleansed' , 'room_type' ]) . \\ aggregate ({ 'review_scores_rating' : 'mean' }) . unstack () . reset_index () location_rating . columns = [ 'neighbourhood' , 'Entire home/apt' , 'Hotel room' , 'Private room' , 'Shared room' ] location_rating . index = location_rating . neighbourhood One of the ways to visualise the average rating is using a heatmap. fig , ax = plt . subplots ( figsize = [ 20 , len ( location_rating ) / 3.3 ]) sns . heatmap ( data = location_rating [[ 'Entire home/apt' , 'Hotel room' , 'Private room' , 'Shared room' ]], annot = False , cbar_kws = { \"shrink\" : 0.5 , \"orientation\" : 'vertical' }, linewidths = 0.004 , linecolor = 'grey' , vmin = 0 , vmax = 5 , center = 0.25 ) plt . show () Although this heatmap presents us the with the average ratings per burough, we can further add the following details for clarity: 1. Location of the burough in London (eg: Central London) 2. Arranged from the best rated to the worst rated buroughs within each location 3. Proper color selection based on scale and human rating psychology : Average human ratings below 2.5 means bad rating and above 4.5 means very good rating (out of 5). It is more natural to use a diverging red-green scale for displaying negitive-positive relationship. def add_regions ( df , borough_col_name ): \"\"\" This function takes as input a dataframe with a column which includes London's borough name Then returns the same dataframw with sub regions names added for each borough \"\"\" central = [ 'Camden' , 'City of London' , 'Kensington and Chelsea' , 'Islington' , 'Lambeth' , 'Southwark' , 'Westminster' ] east = [ 'Barking and Dagenham' , 'Bexley' , 'Greenwich' , 'Hackney' , 'Havering' , 'Lewisham' , 'Newham' , 'Redbridge' , 'Tower Hamlets' , 'Waltham Forest' ] north = [ 'Barnet' , 'Enfield' , 'Haringey' ] south = [ 'Bromley' , 'Croydon' , 'Kingston upon Thames' , 'Merton' , 'Sutton' , 'Wandsworth' ] west = [ 'Brent' , 'Ealing' , 'Hammersmith and Fulham' , 'Harrow' , 'Richmond upon Thames' , 'Hillingdon' , 'Hounslow' ] df [ 'sub_regions' ] = np . where (( df [ borough_col_name ] == central [ 0 ]) | ( df [ borough_col_name ] == central [ 1 ]) | ( df [ borough_col_name ] == central [ 2 ]) | ( df [ borough_col_name ] == central [ 3 ]) | ( df [ borough_col_name ] == central [ 4 ]) | ( df [ borough_col_name ] == central [ 5 ]) | ( df [ borough_col_name ] == central [ 6 ]) , 'Central' , 'no' ) df [ 'sub_regions' ] = np . where (( df [ borough_col_name ] == east [ 0 ]) | ( df [ borough_col_name ] == east [ 1 ]) | ( df [ borough_col_name ] == east [ 2 ]) | ( df [ borough_col_name ] == east [ 3 ]) | ( df [ borough_col_name ] == east [ 4 ]) | ( df [ borough_col_name ] == east [ 5 ]) | ( df [ borough_col_name ] == east [ 6 ]) | ( df [ borough_col_name ] == east [ 7 ]) | ( df [ borough_col_name ] == east [ 8 ]) | ( df [ borough_col_name ] == east [ 9 ]) , 'East' , df [ 'sub_regions' ]) df [ 'sub_regions' ] = np . where (( df [ borough_col_name ] == north [ 0 ]) | ( df [ borough_col_name ] == north [ 1 ]) | ( df [ borough_col_name ] == north [ 2 ]) , 'North' , df [ 'sub_regions' ]) df [ 'sub_regions' ] = np . where (( df [ borough_col_name ] == south [ 0 ]) | ( df [ borough_col_name ] == south [ 1 ]) | ( df [ borough_col_name ] == south [ 2 ]) | ( df [ borough_col_name ] == south [ 3 ]) | ( df [ borough_col_name ] == south [ 4 ]) | ( df [ borough_col_name ] == south [ 5 ]) , 'South' , df [ 'sub_regions' ]) df [ 'sub_regions' ] = np . where (( df [ borough_col_name ] == west [ 0 ]) | ( df [ borough_col_name ] == west [ 1 ]) | ( df [ borough_col_name ] == west [ 2 ]) | ( df [ borough_col_name ] == west [ 3 ]) | ( df [ borough_col_name ] == west [ 4 ]) | ( df [ borough_col_name ] == west [ 5 ]) | ( df [ borough_col_name ] == west [ 6 ]) , 'West' , df [ 'sub_regions' ]) return df def sort_data ( df ): \"\"\" Groups the data by location and sorts the data based on average rating within the location. Different locations are sorted by average rating. \"\"\" df1 = df . copy () df1 [ 'average_rating' ] = ( df1 [ 'Entire home/apt' ] + df1 [ 'Hotel room' ] + df1 [ 'Private room' ] + df1 [ 'Shared room' ]) / 4 df1 [ 'location_average' ] = df1 . groupby ( 'sub_regions' )[ 'average_rating' ] . transform ( 'mean' ) df1 = df1 . sort_values ([ 'location_average' , 'average_rating' ], ascending = False ) return df1 [[ 'Private room' , 'Entire home/apt' , 'Hotel room' , 'Shared room' , 'sub_regions' ]] def prepare_reg_annotation_lists (): \"\"\" Creates the annotation in the form of groups within the data. Displays this on the right hand side of the heatmap. \"\"\" reg_sorted_list = location_rating . sub_regions . unique () reg_len = location_rating . sub_regions . value_counts () . to_dict () sorted_len = [] cum_len = [] arrow_style_str_list = [] # here we define the width of the bracket used, which is proportional to the number of boroughs within a sub region for i in range ( 5 ): if i == 0 : value = reg_len [ reg_sorted_list [ i ]] cum_value = value else : value = reg_len [ reg_sorted_list [ i ]] cum_value += value sorted_len . append ( value ) cum_len . append ( cum_value ) arrow_style_str_list . append ( '-[,widthB=' + str (( value / 1.2 ) - 0.5 ) + ',lengthB=0.7' ) # here we define ticks which represent the center location of each sub region relative to the heatmap Ticks = [] for i in range ( 5 ): if i == 0 : Ticks . append ( 1 - (( cum_len [ i ] / 2 ) / 33 )) else : Ticks . append ( 1 - (((( cum_len [ i ] - cum_len [ i - 1 ]) / 2 ) + cum_len [ i - 1 ]) / 33 )) return Ticks , arrow_style_str_list , reg_sorted_list location_rating = location_rating . replace ( np . nan , 2.5 ) location_rating = add_regions ( location_rating , 'neighbourhood' ) location_rating = sort_data ( location_rating ) Ticks_h , arrow , region = prepare_reg_annotation_lists () A diverging red-green palatte is chosen to represent good reviews and bad reviews. red_green_cmap = sns . diverging_palette ( 10 , 133 , as_cmap = True ) red_green_cmap fig , ax = plt . subplots ( figsize = [ 20 , len ( location_rating ) / 3.3 ]) sns . heatmap ( data = location_rating [[ 'Private room' , 'Entire home/apt' , 'Hotel room' , 'Shared room' ]], annot = False , cbar_kws = { \"shrink\" : 0.5 , \"orientation\" : 'vertical' }, linewidths = 0.004 , linecolor = 'grey' , vmin = 2.25 , vmax = 4.75 , cmap = red_green_cmap ) plt . title ( \"Average ratings for different locations in London\" , fontsize = 20 , y = 1.1 , loc = 'left' ) plt . text ( 0 , - 1 , 'Heatmap depicting the ratings among different locations in London. If no rating is available, minimum rating of 2.5 is assumed. \\n Good ratings are ratings above 3.5 while bad ratings are below. The data is grouped by location (right) and sorted by average rating.' , ha = 'left' , fontsize = 12 ) ax . set_ylabel ( '' ) #annotation for the borough for i in range ( 5 ): ax . annotate ( region [ i ], xy = ( 1.01 , Ticks_h [ i ]), xytext = ( 1.02 , Ticks_h [ i ]), xycoords = 'axes fraction' , ha = 'left' , va = 'center' , arrowprops = dict ( arrowstyle = arrow [ i ], lw = 1 )) We can see the ratings are good across the private rooms and entire home. The best location in each zone is: - West: Richmond upon Thames - Central: Camden - North: Enfield - East: Hackney - South: Croydon","title":"Heatmap"},{"location":"Python/Vizualisation%20using%20python%20Part%201/#treemap","text":"In this context, its not fair to compare ratings of different locations as we have seen that their popularities are different. So there could be 10 reviews in one location while 100 reviews in another. To combine them, we can use a treemap. reviews_treemap = listing_detailed . groupby ([ 'neighbourhood_cleansed' ]) . \\ aggregate ({ 'review_scores_rating' : 'mean' , 'number_of_reviews' : 'sum' }) . reset_index () reviews_treemap = add_regions ( reviews_treemap , 'neighbourhood_cleansed' ) #change col names for nice viz on hover reviews_treemap . columns = [ 'neighbourhood' , 'Average Reviews' , 'Number of reviews' , 'regions' ] fig = px . treemap ( reviews_treemap , path = [ px . Constant ( \"London\" ), 'regions' , 'neighbourhood' ], values = 'Number of reviews' , color = 'Average Reviews' , color_continuous_scale = 'RdBu' ) fig . update_layout ( margin = dict ( t = 50 , l = 25 , r = 25 , b = 25 )) fig . update_layout ( title_text = 'Airbnb London: Ratings overview' )","title":"Treemap"},{"location":"Python/Vizualisation%20using%20python%20Part%201/#wordcloud","text":"Now that we have classified the ratings into good ratings and bad ratings, let us look at the text in these ratings and identify if there are any patterns. reviews_detailed_text = pd . merge ( listing_detailed [[ 'id' , 'description' , 'neighborhood_overview' , 'host_about' , 'review_scores_rating' ]], reviews , left_on = 'id' , right_on = 'listing_id' ) reviews_detailed_positive_text = reviews_detailed_text [ reviews_detailed_text . review_scores_rating > 3.75 ] reviews_detailed_negative_text = reviews_detailed_text [ reviews_detailed_text . review_scores_rating <= 3.75 ] Selecting 100 random reviews each for positive and negative sets. pos_reviews_text = reviews_detailed_positive_text . sample ( n = 100 , random_state = 2 ) . comments . str . cat () neg_reviews_text = reviews_detailed_negative_text . sample ( n = 100 , random_state = 3 ) . comments . str . cat () Wordcloud for positive reviews # !pip install wordcloud from wordcloud import WordCloud , STOPWORDS , ImageColorGenerator from PIL import Image mask_pos = np . array ( Image . open ( 'thumbs-up-xxl.png' )) # word cloud, good vs bad ratings stop_words = [ \"https\" , \"co\" , \"RT\" , 'br' , '<br>' , '<br/>' , ' \\r ' , 'r' ] + list ( STOPWORDS ) wordcloud_pattern = WordCloud ( stopwords = stop_words , background_color = \"white\" , max_words = 2000 , max_font_size = 256 , random_state = 42 , mask = mask_pos , width = mask_pos . shape [ 1 ], height = mask_pos . shape [ 0 ]) wordcloud_positive = wordcloud_pattern . generate ( pos_reviews_text ) plt . imshow ( wordcloud_positive , interpolation = 'bilinear' ) plt . axis ( \"off\" ) plt . show () Wordcloud for negative reviews mask_neg = np . array ( Image . open ( 'thumbs-down-xxl.png' )) # word cloud, good vs bad ratings wordcloud_pattern = WordCloud ( stopwords = stop_words , background_color = \"white\" , max_words = 2000 , max_font_size = 256 , random_state = 42 , mask = mask_neg , width = mask_neg . shape [ 1 ], height = mask_neg . shape [ 0 ]) wordcloud_neg = wordcloud_pattern . generate ( neg_reviews_text ) plt . imshow ( wordcloud_neg , interpolation = 'bilinear' ) plt . axis ( \"off\" ) plt . show () Combining the positive and negative reviews in one plot to compare the differences: fs , axs = plt . subplots ( 1 , 2 , figsize = ( 20 , 10 )) plt . suptitle ( \"Airbnb London: Wordcloud of positive and negative reviews\" , fontsize = 20 ) plt . figtext ( 0.5 , 0.925 , 'Wordcloud derived from a random sample of 100 positive and 100 negative reviews.' , wrap = True , horizontalalignment = 'center' , fontsize = 12 ) axs [ 0 ] . imshow ( wordcloud_positive , interpolation = 'bilinear' ) axs [ 0 ] . axis ( \"off\" ) axs [ 1 ] . imshow ( wordcloud_neg , interpolation = 'bilinear' ) axs [ 1 ] . axis ( \"off\" ) plt . show () From these plots, we can see that automated postings, cancellations by hosts, and issues during arrival are the main issues that Airbnb should look into.","title":"Wordcloud"},{"location":"Python/Vizualisation%20using%20python%20Part%201/#correlation-matrix","text":"How are different parameters within the data related. How is ratings correlated with availability or maximum nights? This can be explained using a correlation plot. listing_detailed [ 'host_response_rate' ] = listing_detailed [ 'host_response_rate' ] . str . replace ( '%' , '' ) . astype ( float ) listing_detailed [ 'host_acceptance_rate' ] = listing_detailed [ 'host_acceptance_rate' ] . str . replace ( '%' , '' ) . astype ( float ) listing_detailed [ 'price' ] = listing_detailed [ 'price' ] . str . replace ( '$' , '' ) . str . replace ( ',' , '' ) . astype ( float ) col_for_corr = [ 'review_scores_rating' , 'review_scores_accuracy' , 'review_scores_cleanliness' , 'review_scores_checkin' , 'review_scores_communication' , 'review_scores_location' , 'review_scores_value' , 'number_of_reviews' , 'number_of_reviews_ltm' , 'number_of_reviews_l30d' , 'reviews_per_month' , 'availability_30' , 'availability_60' , 'availability_90' , 'availability_365' , 'minimum_nights' , 'maximum_nights' , 'minimum_minimum_nights' , 'maximum_minimum_nights' , 'minimum_maximum_nights' , 'maximum_maximum_nights' , 'minimum_nights_avg_ntm' , 'maximum_nights_avg_ntm' , 'bedrooms' , 'beds' , 'accommodates' , 'price' , 'host_response_rate' , 'host_acceptance_rate' , 'host_total_listings_count' ] f = plt . figure ( figsize = ( 20 , 20 )) plt . matshow ( listing_detailed [ col_for_corr ] . corr (), fignum = f , cmap = red_green_cmap , vmin =- 1 , vmax = 1 ) plt . xticks ( range ( listing_detailed [ col_for_corr ] . select_dtypes ([ 'number' ]) . shape [ 1 ]), listing_detailed [ col_for_corr ] . select_dtypes ([ 'number' ]) . columns , rotation = 90 , fontsize = 15 ) plt . yticks ( range ( listing_detailed [ col_for_corr ] . select_dtypes ([ 'number' ]) . shape [ 1 ]), listing_detailed [ col_for_corr ] . select_dtypes ([ 'number' ]) . columns , rotation = 0 , fontsize = 15 ) cb = plt . colorbar () cb . ax . tick_params ( labelsize = 14 ) plt . title ( \"Airbnb London: Correlation between different parameters\" , fontsize = 20 , loc = 'left' ) plt . text ( 0 , 32 , 'Correlation matrix displaying the different parameters within the data. DIverging green (+ve) red (-ve) scale is used to display the correlations. \\n \\ Features considered: Review scores, Number of reviews, availability, maximum and minimum days of stay, host and room parameters.' , ha = 'left' , fontsize = 12 ) plt . show ()","title":"Correlation matrix"},{"location":"Python/Vizualisation%20using%20python%20Part%201/#scatterplot-matrix","text":"While the above plot shows the correlation across various variables, I want to deep dive into change in ratings with price. I can use a scatterplot matrix to visualise this. Aditionally, I want the costliest properties, and the most popular yet cheap properties annotated. def annotate_plot ( x , y , ** kwargs ): if ( x . name == 'price' and y . name == 'review_scores_rating' ): ax = plt . gca () for index , obj in listing_detailed . nlargest ( 2 , 'price' ) . iterrows (): plt . annotate ( obj [ 'name' ], # the text xy = ( obj . price , obj . review_scores_rating ), xytext = ( 7500 , obj . review_scores_rating - 0.5 ), arrowprops = dict ( arrowstyle = \"->\" ) ) elif ( x . name == 'price' and y . name == 'number_of_reviews' ): ax = plt . gca () for index , obj in listing_detailed . nlargest ( 3 , 'number_of_reviews' ) . iterrows (): ax . text ( obj . price , obj . number_of_reviews , obj [ 'name' ]) col_for_pairplot = [ 'review_scores_rating' , 'number_of_reviews' , 'price' ] sns_plot = sns . pairplot ( listing_detailed , vars = col_for_pairplot , kind = 'scatter' , hue = 'room_type' , diag_kind = 'kde' ,) sns_plot . fig . set_size_inches ( 20 , 20 ) sns_plot . _legend . set_bbox_to_anchor (( 0.15 , 0.89 )) sns_plot . map_upper ( annotate_plot ) sns_plot . fig . suptitle ( \"Airbnb London: Scatterplot matrix\" , fontsize = 20 , y = 1 ) We can see that the two costliest properties are either historic apartments or a mansion. The three most popular yet cheap properties are small and quaint properties near popular destinations.","title":"Scatterplot matrix"},{"location":"Python/Vizualisation%20using%20python%20Part%201/#boxplot-and-violin-chart","text":"To look at the variation in ratings within the different room types, we could use either a boxplot or a Violin plot as shown. fs , axs = plt . subplots ( 1 , 2 , figsize = ( 20 , 20 )) listing_detailed . boxplot ( column = 'review_scores_rating' , by = 'room_type' , figsize = ( 10 , 20 ), ax = axs [ 0 ]) sns . violinplot ( 'room_type' , 'review_scores_rating' , data = listing_detailed , ax = axs [ 1 ]) plt . suptitle ( \"Airbnb London: Average rating across room types\" , fontsize = 20 , y = 0.95 ) plt . figtext ( 0.5 , 0.925 , 'Boxplot (left) and Violin plot (right) for the average review across room types' , wrap = True , horizontalalignment = 'center' , fontsize = 12 ) axs [ 0 ] . set_title ( '' ) for ax in axs : ax . set_ylim ( - 1 , 6 ) ax . set_ylabel ( 'Average Rating' , fontsize = 12 ) ax . set_xlabel ( 'Room types' , fontsize = 12 )","title":"Boxplot and Violin chart"},{"location":"Python/Vizualisation%20using%20python%20Part%201/#cluster-map","text":"If we wanted to cluster localities based on some features, then cluster map is teh ideal choice. In the below map, we cluster different locations based on one feature from each type. The features are also clustered to show the similarity between features. Finally we use a white-blue color palatte for displaying the variation within the data. from sklearn.preprocessing import MinMaxScaler import seaborn as sns col_for_corr = [ 'price' , 'review_scores_rating' , 'number_of_reviews' , 'availability_90' , 'minimum_nights_avg_ntm' , 'bedrooms' , 'host_response_rate' ] df_cluster = listing_detailed . groupby ( 'neighbourhood_cleansed' ) . aggregate ({ 'review_scores_rating' : 'mean' , 'number_of_reviews' : 'sum' , 'availability_90' : 'mean' , 'minimum_nights_avg_ntm' : 'mean' , 'bedrooms' : 'mean' , 'price' : 'mean' , 'host_response_rate' : 'mean' }) . reset_index () scaler = MinMaxScaler () df_cluster1 = pd . DataFrame ( scaler . fit_transform ( df_cluster [ col_for_corr ]), columns = col_for_corr ) df_cluster1 . index = df_cluster . neighbourhood_cleansed crest_cmap = sns . color_palette ( \"crest\" , as_cmap = True ) crest_cmap g = sns . clustermap ( df_cluster1 , cmap = crest_cmap , vmin = 0 , vmax = 1 ) plt . title ( \"Airbnb London: Clusters within London\" , fontsize = 20 , loc = 'left' , y = 2 , x = - 25 ) g . ax_cbar . set_position (( 1 , .2 , .03 , .4 )) g . ax_heatmap . set_ylabel ( \"\" ) plt . show ()","title":"Cluster map"},{"location":"Python/Vizualisation%20using%20python%20Part%201/#references","text":"Visualisation Analytics and Design, Tamara Munzner Class notes and asignments, Imperial College London Visual Analytics lab at JKU Linz","title":"References"},{"location":"R/Univariate-analysis/","text":"Introduction \u00b6 A univariate analysis is the simplest form of EDA. \"Uni\" means \"one\", so in other words, this analysis is on only one variable. It doesn't deal with causes or relationships, and its primary purpose is to describe; it takes data, summarizes that data and finds patterns in it. In describing or characterizing the observations of an individual variable, three basic properties are of interest: 1. The location of observations, or how large or small the values of the individual observations are 2. The dispersion (sometimes called scale or spread) of the observations 3. The distribution of the observations Uni-variate plots provide one way to find out about those properties. There are two basic kinds of univariate plots: 1. Enumeration plots, or plots that show every observation 2. Summary plots that generalize the data into a simplified representation. For the current tutorial, I will be using my office attendance data set. The data set contains the time when I swiped into the office and when I swiped out of the office. Data from 4 th October 2017 to 29 th November 2018. After some data set manipulation, I will get the difference between policy out-time and my actual out-time. I can leave 15 minutes before the policy out time. After manipulation, a sample of the data is as follows: (Actual data is not shown for security reasons. This is mock data that is very similar to the actual one.) ## Attendance.Date diff.in.time diff.out.time ## 1 2018-03-22 18 mins 226 mins ## 2 2018-08-14 -9 mins 5 mins ## 3 2017-12-04 42 mins 11 mins ## 4 2018-03-01 26 mins -6 mins ## 5 2018-01-23 35 mins -4 mins Summary Statistics \u00b6 Before further analysis, some basic summary statistics would show me the mean and standard deviation of the data. For this tutorial, I will use diff.in.time (difference between actual in-time and policy in-time) mean ( as.numeric ( attendance $ diff.out.time )) # Mean in minutes ## [1] 20.3227 sd ( as.numeric ( attendance $ diff.out.time )) # Standard Deviation in minutes ## [1] 69.06549 nrow ( attendance ) # Length of the data set ## [1] 282 Enumerative plots \u00b6 \"Enumerative plots\" are called such because they enumerate or show every individual data point. Index Plot/Univariate Scatter Diagram \u00b6 Displays the values of a single variable for each observation using symbols plotted relative to the observation number. ggplot ( attendance , aes ( x = Attendance.Date , y = as.numeric ( diff.out.time ), color = ( diff.out.time >= -15 ))) + geom_point ( show.legend = FALSE ) + labs ( x = 'Time' , y = 'Out-time difference (Minutes)' ) + theme_minimal () Just looking at this plot, I can say the following: 1. I could cluster into three parts. - One cluster would be before December 2017, where I used to leave the office way after my out-time. - The second cluster would be from December 2017 to June 2018, where I used to leave the office 15 minutes before my out time. - The third cluster was after June 2018, when I was leaving way after my out-time. 2. The red dots indicate the days when I came to the office after 15 minutes from in-time. They are anomalies, days when I took half days etc. We can exclude them from our current analysis. Y Zero High-Density Plot \u00b6 Another way to look at the same data is by using a Y Zero High-Density Plot. It displays the values of a single variable plotted as thin vertical lines. Here the magnitude of the observations iss highlighted. ggplot ( attendance , aes ( x = Attendance.Date , y = 0 , color = ( diff.out.time >= -15 ), xend = Attendance.Date , yend = as.numeric ( diff.out.time ))) + geom_segment ( show.legend = FALSE ) + labs ( x = 'Time' , y = 'Out-time difference (Minutes)' ) + theme_minimal () Removing half-days as outliers attendance <- attendance %>% filter ( diff.out.time >= -15 ) Strip Plot/Strip Chart (univariate scatter diagram) \u00b6 Displays the values of a single variable as symbols plotted along a line. This is a basic plot where we can see the spread of the data. stripchart ( x = as.numeric ( attendance $ diff.out.time ), xlab = 'Out-time difference (minutes)' ) Sometimes it is more visually apparent when the points are stacked. ggplot ( attendance , aes ( x = as.numeric ( diff.out.time ), y = ..density.. )) + geom_dotplot ( binwidth = 3 , method = 'histodot' ) + labs ( x = 'Out-time difference (minutes)' ) + theme_minimal () We can observe that the number of observations is high and slowly tends to drop as time progresses. Dot Plot/Dot Chart \u00b6 Displays the values of a single variable as symbols plotted along a line. With a separate line for each observation, it is generally constructed after sorting the rows of the data table. df = attendance %>% arrange ( as.numeric ( diff.out.time )) ggplot ( df , aes ( x = as.numeric ( row.names ( df )), y = as.numeric ( diff.out.time ))) + geom_point () + labs ( x = 'count' , y = 'Out time difference (min)' ) + theme_minimal () From the graph, I can observe that the distribution initially seems to be exponential. A sample normal distribution is plotted for reference. We can see that the distribution looks nowhere like a normal distribution. Instead, I suspect that it is close to an exponential distribution. Univariate Summary Plots \u00b6 Summary plots display an object or graph that gives a more concise expression of a variable's location, dispersion, and distribution than an enumerative plot. This comes at the expense of some loss of information: In a summary plot, it is no longer possible to retrieve the individual data value, but the gain usually matches this loss in understanding that results from the efficient representation of the data. Summary plots generally prove to be much better than the enumerative plots in revealing the data distribution. Box plot \u00b6 A simple way of representing statistical data on a plot in which a rectangle is drawn to represent the second and third quartiles, usually with a vertical line inside to indicate the median value. The lower and upper quartiles are shown as horizontal lines on either side of the rectangle. ggplot ( attendance , aes ( x = \"Box Plot\" , y = as.numeric ( diff.out.time ), group = 123 )) + geom_boxplot () + labs ( y = 'Out time difference (min)' ) + theme_minimal () Histograms \u00b6 The other summary plots are of various types: Histograms: Histograms are bar charts that display the counts or relative frequencies of values falling in different class intervals or ranges. Density Plots: A density plot plots the local relative frequency or density of points along the number line or x-axis of a plot. The local density is determined by summing the individual \"kernel\" densities for each point. Where points occur more frequently, this sum, and consequently the local density, will be greater. legendcols <- c ( \"Normal distribution\" = \"darkred\" , \"Density\" = \"darkBlue\" , \"Histogram\" = \"lightBlue\" ) ggplot ( attendance , aes ( x = as.numeric ( diff.out.time ))) + geom_histogram ( aes ( y = ..density.. , fill = \"Histogram\" ), bins = 50 ) + stat_function ( fun = dnorm , aes ( color = \"Normal distribution\" ), size = 1 , args = list ( mean = mean ( as.numeric ( attendance $ diff.out.time )), sd = sd ( as.numeric ( attendance $ diff.out.time )) )) + geom_density ( aes ( y = ..density.. , color = \"Density\" ), size = 1 ) + scale_colour_manual ( name = \"Distribution\" , values = legendcols ) + scale_fill_manual ( name = \"Bar\" , values = legendcols ) + labs ( x = 'Out-time difference (minutes)' , y = 'Density' ) + theme_minimal () + theme ( legend.position = \"bottom\" ) In the above graph, the red line is a normal distribution(with the same mean and standard deviation) while the blue line is the density plot of in-time. Q-Q plot \u00b6 In statistics, a Q-Q (quantile-quantile) plot is a probability plot, a graphical method for comparing two probability distributions by plotting their quantiles against each other. If the two distributions being compared are similar, the points in the Q-Q plot will approximately lie on the line y = x. If the distributions are linearly related, the points in the Q-Q plot will approximately lie on a line, but not necessarily on the line y = x. Q-Q plots can also be used as a graphical means of estimating parameters in a location-scale family of distributions. A Q-Q plot is used to compare the shapes of distributions, providing a graphical view of how properties such as location, scale, and skewness are similar or different in the two distributions. Below is a Q-Q plot with a normal distribution ggplot ( attendance , aes ( sample = as.numeric ( diff.out.time ))) + stat_qq () + stat_qq_line () + ggtitle ( \"Normal distribution Q-Q plot\" ) + theme_minimal () We can see that the distribution is not normal. Trying to check with exponential distribution params <- as.list ( fitdistr ( rexp ( nrow ( attendance ), rate = 2 ), \"exponential\" ) $ estimate ) ggplot ( attendance , aes ( sample = as.numeric ( diff.out.time ))) + stat_qq ( distribution = qexp , dparams = params ) + stat_qq_line ( distribution = qexp , dparams = params ) + ggtitle ( \"Exponential distribution Q-Q plot\" ) + theme_minimal () From the above graph, I am approximating my distribution to an exponential distribution. lamda <- 1 / mean ( sd ( as.numeric ( attendance $ diff.out.time )), mean ( as.numeric ( attendance $ diff.out.time ))) exp.curve <- function ( x ){ lamda * exp ( - lamda * ( x +15 )) } legendcols <- c ( \"Exponential distribution\" = \"darkred\" , \"Density\" = \"darkBlue\" , \"Histogram\" = \"lightBlue\" ) ggplot ( attendance , aes ( x = as.numeric ( diff.out.time ))) + geom_histogram ( aes ( y = ..density.. , fill = \"Histogram\" ), bins = 50 ) + stat_function ( fun = exp.curve , aes ( color = \"Exponential distribution\" ), size = 1 ) + geom_density ( aes ( y = ..density.. , color = \"Density\" ), size = 1 ) + scale_colour_manual ( name = \"Distribution\" , values = legendcols ) + scale_fill_manual ( name = \"Bar\" , values = legendcols ) + labs ( x = 'Out-time difference (minutes)' , y = 'Density' ) + theme_minimal () + theme ( legend.position = \"bottom\" ) In the above graph, the red line is exponential distribution, while the blue line is the density plot of in-time. Created using RMarkdown.","title":"Univariate Analysis (R)"},{"location":"R/Univariate-analysis/#introduction","text":"A univariate analysis is the simplest form of EDA. \"Uni\" means \"one\", so in other words, this analysis is on only one variable. It doesn't deal with causes or relationships, and its primary purpose is to describe; it takes data, summarizes that data and finds patterns in it. In describing or characterizing the observations of an individual variable, three basic properties are of interest: 1. The location of observations, or how large or small the values of the individual observations are 2. The dispersion (sometimes called scale or spread) of the observations 3. The distribution of the observations Uni-variate plots provide one way to find out about those properties. There are two basic kinds of univariate plots: 1. Enumeration plots, or plots that show every observation 2. Summary plots that generalize the data into a simplified representation. For the current tutorial, I will be using my office attendance data set. The data set contains the time when I swiped into the office and when I swiped out of the office. Data from 4 th October 2017 to 29 th November 2018. After some data set manipulation, I will get the difference between policy out-time and my actual out-time. I can leave 15 minutes before the policy out time. After manipulation, a sample of the data is as follows: (Actual data is not shown for security reasons. This is mock data that is very similar to the actual one.) ## Attendance.Date diff.in.time diff.out.time ## 1 2018-03-22 18 mins 226 mins ## 2 2018-08-14 -9 mins 5 mins ## 3 2017-12-04 42 mins 11 mins ## 4 2018-03-01 26 mins -6 mins ## 5 2018-01-23 35 mins -4 mins","title":"Introduction"},{"location":"R/Univariate-analysis/#summary-statistics","text":"Before further analysis, some basic summary statistics would show me the mean and standard deviation of the data. For this tutorial, I will use diff.in.time (difference between actual in-time and policy in-time) mean ( as.numeric ( attendance $ diff.out.time )) # Mean in minutes ## [1] 20.3227 sd ( as.numeric ( attendance $ diff.out.time )) # Standard Deviation in minutes ## [1] 69.06549 nrow ( attendance ) # Length of the data set ## [1] 282","title":"Summary Statistics"},{"location":"R/Univariate-analysis/#enumerative-plots","text":"\"Enumerative plots\" are called such because they enumerate or show every individual data point.","title":"Enumerative plots"},{"location":"R/Univariate-analysis/#index-plotunivariate-scatter-diagram","text":"Displays the values of a single variable for each observation using symbols plotted relative to the observation number. ggplot ( attendance , aes ( x = Attendance.Date , y = as.numeric ( diff.out.time ), color = ( diff.out.time >= -15 ))) + geom_point ( show.legend = FALSE ) + labs ( x = 'Time' , y = 'Out-time difference (Minutes)' ) + theme_minimal () Just looking at this plot, I can say the following: 1. I could cluster into three parts. - One cluster would be before December 2017, where I used to leave the office way after my out-time. - The second cluster would be from December 2017 to June 2018, where I used to leave the office 15 minutes before my out time. - The third cluster was after June 2018, when I was leaving way after my out-time. 2. The red dots indicate the days when I came to the office after 15 minutes from in-time. They are anomalies, days when I took half days etc. We can exclude them from our current analysis.","title":"Index Plot/Univariate Scatter Diagram"},{"location":"R/Univariate-analysis/#y-zero-high-density-plot","text":"Another way to look at the same data is by using a Y Zero High-Density Plot. It displays the values of a single variable plotted as thin vertical lines. Here the magnitude of the observations iss highlighted. ggplot ( attendance , aes ( x = Attendance.Date , y = 0 , color = ( diff.out.time >= -15 ), xend = Attendance.Date , yend = as.numeric ( diff.out.time ))) + geom_segment ( show.legend = FALSE ) + labs ( x = 'Time' , y = 'Out-time difference (Minutes)' ) + theme_minimal () Removing half-days as outliers attendance <- attendance %>% filter ( diff.out.time >= -15 )","title":"Y Zero High-Density Plot"},{"location":"R/Univariate-analysis/#strip-plotstrip-chart-univariate-scatter-diagram","text":"Displays the values of a single variable as symbols plotted along a line. This is a basic plot where we can see the spread of the data. stripchart ( x = as.numeric ( attendance $ diff.out.time ), xlab = 'Out-time difference (minutes)' ) Sometimes it is more visually apparent when the points are stacked. ggplot ( attendance , aes ( x = as.numeric ( diff.out.time ), y = ..density.. )) + geom_dotplot ( binwidth = 3 , method = 'histodot' ) + labs ( x = 'Out-time difference (minutes)' ) + theme_minimal () We can observe that the number of observations is high and slowly tends to drop as time progresses.","title":"Strip Plot/Strip Chart (univariate scatter diagram)"},{"location":"R/Univariate-analysis/#dot-plotdot-chart","text":"Displays the values of a single variable as symbols plotted along a line. With a separate line for each observation, it is generally constructed after sorting the rows of the data table. df = attendance %>% arrange ( as.numeric ( diff.out.time )) ggplot ( df , aes ( x = as.numeric ( row.names ( df )), y = as.numeric ( diff.out.time ))) + geom_point () + labs ( x = 'count' , y = 'Out time difference (min)' ) + theme_minimal () From the graph, I can observe that the distribution initially seems to be exponential. A sample normal distribution is plotted for reference. We can see that the distribution looks nowhere like a normal distribution. Instead, I suspect that it is close to an exponential distribution.","title":"Dot Plot/Dot Chart"},{"location":"R/Univariate-analysis/#univariate-summary-plots","text":"Summary plots display an object or graph that gives a more concise expression of a variable's location, dispersion, and distribution than an enumerative plot. This comes at the expense of some loss of information: In a summary plot, it is no longer possible to retrieve the individual data value, but the gain usually matches this loss in understanding that results from the efficient representation of the data. Summary plots generally prove to be much better than the enumerative plots in revealing the data distribution.","title":"Univariate Summary Plots"},{"location":"R/Univariate-analysis/#box-plot","text":"A simple way of representing statistical data on a plot in which a rectangle is drawn to represent the second and third quartiles, usually with a vertical line inside to indicate the median value. The lower and upper quartiles are shown as horizontal lines on either side of the rectangle. ggplot ( attendance , aes ( x = \"Box Plot\" , y = as.numeric ( diff.out.time ), group = 123 )) + geom_boxplot () + labs ( y = 'Out time difference (min)' ) + theme_minimal ()","title":"Box plot"},{"location":"R/Univariate-analysis/#histograms","text":"The other summary plots are of various types: Histograms: Histograms are bar charts that display the counts or relative frequencies of values falling in different class intervals or ranges. Density Plots: A density plot plots the local relative frequency or density of points along the number line or x-axis of a plot. The local density is determined by summing the individual \"kernel\" densities for each point. Where points occur more frequently, this sum, and consequently the local density, will be greater. legendcols <- c ( \"Normal distribution\" = \"darkred\" , \"Density\" = \"darkBlue\" , \"Histogram\" = \"lightBlue\" ) ggplot ( attendance , aes ( x = as.numeric ( diff.out.time ))) + geom_histogram ( aes ( y = ..density.. , fill = \"Histogram\" ), bins = 50 ) + stat_function ( fun = dnorm , aes ( color = \"Normal distribution\" ), size = 1 , args = list ( mean = mean ( as.numeric ( attendance $ diff.out.time )), sd = sd ( as.numeric ( attendance $ diff.out.time )) )) + geom_density ( aes ( y = ..density.. , color = \"Density\" ), size = 1 ) + scale_colour_manual ( name = \"Distribution\" , values = legendcols ) + scale_fill_manual ( name = \"Bar\" , values = legendcols ) + labs ( x = 'Out-time difference (minutes)' , y = 'Density' ) + theme_minimal () + theme ( legend.position = \"bottom\" ) In the above graph, the red line is a normal distribution(with the same mean and standard deviation) while the blue line is the density plot of in-time.","title":"Histograms"},{"location":"R/Univariate-analysis/#q-q-plot","text":"In statistics, a Q-Q (quantile-quantile) plot is a probability plot, a graphical method for comparing two probability distributions by plotting their quantiles against each other. If the two distributions being compared are similar, the points in the Q-Q plot will approximately lie on the line y = x. If the distributions are linearly related, the points in the Q-Q plot will approximately lie on a line, but not necessarily on the line y = x. Q-Q plots can also be used as a graphical means of estimating parameters in a location-scale family of distributions. A Q-Q plot is used to compare the shapes of distributions, providing a graphical view of how properties such as location, scale, and skewness are similar or different in the two distributions. Below is a Q-Q plot with a normal distribution ggplot ( attendance , aes ( sample = as.numeric ( diff.out.time ))) + stat_qq () + stat_qq_line () + ggtitle ( \"Normal distribution Q-Q plot\" ) + theme_minimal () We can see that the distribution is not normal. Trying to check with exponential distribution params <- as.list ( fitdistr ( rexp ( nrow ( attendance ), rate = 2 ), \"exponential\" ) $ estimate ) ggplot ( attendance , aes ( sample = as.numeric ( diff.out.time ))) + stat_qq ( distribution = qexp , dparams = params ) + stat_qq_line ( distribution = qexp , dparams = params ) + ggtitle ( \"Exponential distribution Q-Q plot\" ) + theme_minimal () From the above graph, I am approximating my distribution to an exponential distribution. lamda <- 1 / mean ( sd ( as.numeric ( attendance $ diff.out.time )), mean ( as.numeric ( attendance $ diff.out.time ))) exp.curve <- function ( x ){ lamda * exp ( - lamda * ( x +15 )) } legendcols <- c ( \"Exponential distribution\" = \"darkred\" , \"Density\" = \"darkBlue\" , \"Histogram\" = \"lightBlue\" ) ggplot ( attendance , aes ( x = as.numeric ( diff.out.time ))) + geom_histogram ( aes ( y = ..density.. , fill = \"Histogram\" ), bins = 50 ) + stat_function ( fun = exp.curve , aes ( color = \"Exponential distribution\" ), size = 1 ) + geom_density ( aes ( y = ..density.. , color = \"Density\" ), size = 1 ) + scale_colour_manual ( name = \"Distribution\" , values = legendcols ) + scale_fill_manual ( name = \"Bar\" , values = legendcols ) + labs ( x = 'Out-time difference (minutes)' , y = 'Density' ) + theme_minimal () + theme ( legend.position = \"bottom\" ) In the above graph, the red line is exponential distribution, while the blue line is the density plot of in-time. Created using RMarkdown.","title":"Q-Q plot"},{"location":"R/multivariateAnalysis/","text":"Introduction \u00b6 Multivariate EDA techniques generally show the relationship between two or more variables with the depandant variable in the form of either cross-tabulation, statistics or visually. In the current problem it will help us look at relationships between our data. This blog is a part of in-time analysis problem . I want to analyse my entry time at office and understand what factors effect it. After integrating Google maps data with attendence dataset , I currently have the factors 1. date (month / week day / season etc) 2. main_activity (means of transport) 3. hours.worked (of the previous day) 4. travelling.time (time it took to travel from house to office) 5. home.addr (the place of residence) The dependent variable is diff.in.time (difference between my actual in time vs policy in-time) A sample of the data is shown Sample Data diff.in.time date main_activity hours.worked travelling.time home.addr diff.out.time -9 2018-08-14 IN_VEHICLE 8.933333 900.719 Old House 5 17 2018-03-16 ON_FOOT 9.116667 930.126 Old House -10 -14 2018-09-10 ON_FOOT 4.583333 1179.873 Old House -251 -7 2018-10-19 ON_BICYCLE 9.583333 1501.060 New House 42 -9 2018-06-28 IN_VEHICLE 9.783333 670.700 Old House 56 Cross-tabulation \u00b6 For categorical data cross-tabulation is very useful. For two variables, cross-tabulation is performed by making a two-way table with column headings that match the levels of one variable and row headings that match the levels of the other variable, then filling in the counts of all subjects that share a pair of levels. The two variables might be both explanatory, both outcome, or one of each. I am using Kable to make cool tables. cross_table <- travel %>% group_by ( home.addr , main_activity ) %>% summarise ( avg.travel.time = mean ( travelling.time ), avg.in.time.diff = mean ( diff.in.time ), median.in.time.diff = median ( diff.in.time )) %>% arrange ( home.addr , main_activity ) library ( kableExtra ) kable ( cross_table , caption = 'Cross Tabulation' ) %>% kable_styling ( full_width = F ) %>% column_spec ( 1 , bold = T ) %>% collapse_rows ( columns = 1 : 2 , valign = \"middle\" ) %>% scroll_box () Cross Tabulation home.addr main_activity avg.travel.time avg.in.time.diff median.in.time.diff New House IN_VEHICLE 1285.0264 -1.800000 -3 New House ON_BICYCLE 1547.5557 -4.000000 -6 New House ON_FOOT 1695.7091 5.285714 5 Old House IN_VEHICLE 771.1752 2.857143 -4 Old House ON_BICYCLE 1029.6329 14.941176 18 Old House ON_FOOT 1170.4783 17.433628 17 Scatter plots \u00b6 Scatter plots show how much one variable is affected by another. To see how travelling time affects in-time ggplot ( travel , aes ( x = diff.in.time , y = travelling.time , color = main_activity )) + geom_point ( show.legend = TRUE ) + labs ( x = 'In-time difference (Minutes)' , y = 'Travelling time (seconds)' , title = \"Travelling time vs in-time\" , color = 'Mode of transport' ) + theme_minimal () + theme ( legend.position = \"bottom\" ) From the above graph, I can see that: 1. For bicycle, as travelling time decreases(low traffic) in-time difference increases(coming earlier to office) 2. There seems to be no relationship between travelling time (traffic) and in-time difference when on foot. 3. Travelling time has little affect on it-time difference when travelling on vehicle. To see how hours worked(on previous day) affects in-time From the above graph, I can observe that irrespective of mode of transport, my in-time difference increases (coming earlier to office) as hours worked on the previous day increases. Box plots \u00b6 Similarly, I want to see how mode of transport affects in-time difference. For categorical variable, box plots display this information in the most ideal manner. ggplot ( travel , aes ( x = main_activity , y = diff.in.time , group = main_activity )) + geom_boxplot () + labs ( x = 'Mode of transport' , y = 'In time difference (min)' ) + theme_minimal () From the above graph, I can observe that: 1. On vehicle, I went to office on average, ~12 minutes after the policy in-time (in-time difference is -12) 2. On cycle, I went to office almost close to the policy in-time 3. While walking, I was almost always before the policy in-time Similarly for place of residence. From this graph, I can understand that from New house I was close to ~5 minutes after the policy in-time while I used to be on-time while living in Old house. Created using R Markdown. Credits: Thinkstats Experimental Design and Analysis","title":"Multivariate Analysis (R)"},{"location":"R/multivariateAnalysis/#introduction","text":"Multivariate EDA techniques generally show the relationship between two or more variables with the depandant variable in the form of either cross-tabulation, statistics or visually. In the current problem it will help us look at relationships between our data. This blog is a part of in-time analysis problem . I want to analyse my entry time at office and understand what factors effect it. After integrating Google maps data with attendence dataset , I currently have the factors 1. date (month / week day / season etc) 2. main_activity (means of transport) 3. hours.worked (of the previous day) 4. travelling.time (time it took to travel from house to office) 5. home.addr (the place of residence) The dependent variable is diff.in.time (difference between my actual in time vs policy in-time) A sample of the data is shown Sample Data diff.in.time date main_activity hours.worked travelling.time home.addr diff.out.time -9 2018-08-14 IN_VEHICLE 8.933333 900.719 Old House 5 17 2018-03-16 ON_FOOT 9.116667 930.126 Old House -10 -14 2018-09-10 ON_FOOT 4.583333 1179.873 Old House -251 -7 2018-10-19 ON_BICYCLE 9.583333 1501.060 New House 42 -9 2018-06-28 IN_VEHICLE 9.783333 670.700 Old House 56","title":"Introduction"},{"location":"R/multivariateAnalysis/#cross-tabulation","text":"For categorical data cross-tabulation is very useful. For two variables, cross-tabulation is performed by making a two-way table with column headings that match the levels of one variable and row headings that match the levels of the other variable, then filling in the counts of all subjects that share a pair of levels. The two variables might be both explanatory, both outcome, or one of each. I am using Kable to make cool tables. cross_table <- travel %>% group_by ( home.addr , main_activity ) %>% summarise ( avg.travel.time = mean ( travelling.time ), avg.in.time.diff = mean ( diff.in.time ), median.in.time.diff = median ( diff.in.time )) %>% arrange ( home.addr , main_activity ) library ( kableExtra ) kable ( cross_table , caption = 'Cross Tabulation' ) %>% kable_styling ( full_width = F ) %>% column_spec ( 1 , bold = T ) %>% collapse_rows ( columns = 1 : 2 , valign = \"middle\" ) %>% scroll_box () Cross Tabulation home.addr main_activity avg.travel.time avg.in.time.diff median.in.time.diff New House IN_VEHICLE 1285.0264 -1.800000 -3 New House ON_BICYCLE 1547.5557 -4.000000 -6 New House ON_FOOT 1695.7091 5.285714 5 Old House IN_VEHICLE 771.1752 2.857143 -4 Old House ON_BICYCLE 1029.6329 14.941176 18 Old House ON_FOOT 1170.4783 17.433628 17","title":"Cross-tabulation"},{"location":"R/multivariateAnalysis/#scatter-plots","text":"Scatter plots show how much one variable is affected by another. To see how travelling time affects in-time ggplot ( travel , aes ( x = diff.in.time , y = travelling.time , color = main_activity )) + geom_point ( show.legend = TRUE ) + labs ( x = 'In-time difference (Minutes)' , y = 'Travelling time (seconds)' , title = \"Travelling time vs in-time\" , color = 'Mode of transport' ) + theme_minimal () + theme ( legend.position = \"bottom\" ) From the above graph, I can see that: 1. For bicycle, as travelling time decreases(low traffic) in-time difference increases(coming earlier to office) 2. There seems to be no relationship between travelling time (traffic) and in-time difference when on foot. 3. Travelling time has little affect on it-time difference when travelling on vehicle. To see how hours worked(on previous day) affects in-time From the above graph, I can observe that irrespective of mode of transport, my in-time difference increases (coming earlier to office) as hours worked on the previous day increases.","title":"Scatter plots"},{"location":"R/multivariateAnalysis/#box-plots","text":"Similarly, I want to see how mode of transport affects in-time difference. For categorical variable, box plots display this information in the most ideal manner. ggplot ( travel , aes ( x = main_activity , y = diff.in.time , group = main_activity )) + geom_boxplot () + labs ( x = 'Mode of transport' , y = 'In time difference (min)' ) + theme_minimal () From the above graph, I can observe that: 1. On vehicle, I went to office on average, ~12 minutes after the policy in-time (in-time difference is -12) 2. On cycle, I went to office almost close to the policy in-time 3. While walking, I was almost always before the policy in-time Similarly for place of residence. From this graph, I can understand that from New house I was close to ~5 minutes after the policy in-time while I used to be on-time while living in Old house. Created using R Markdown. Credits: Thinkstats Experimental Design and Analysis","title":"Box plots"},{"location":"R/time-series/","text":"MathJax.Hub.Config({ extensions: [\"tex2jax.js\",\"TeX/AMSmath.js\",\"TeX/AMSsymbols.js\"], jax: [\"input/TeX\", \"output/HTML-CSS\"], tex2jax: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ], }, \"HTML-CSS\": { availableFonts: [\"TeX\"] } }); Time Series and stationarity \u00b6 A time series is a series of data points captured in time order. Most commonly, a time series is a sequence taken at successive equally spaced points in time. This post is the first in a series of blogs on time series methods and forecasting. In this blog, we will discuss about stationarity, random walk, deterministic drift and other vocabulary which form as foundation to time series: Stochastic processes \u00b6 A random or stochastic process is a collection of random variables ordered in time. It is denoted as \\(Y_t\\) . For example, in-time of an employee is a stochastic process. How is in-time a stochastic process? Consider the in-time on a particular day is 9:00 AM. In theory, the in-time could be any particular value which depends on many factors like traffic, work load, weather etc. The figure 9:00 AM is a particular realization of many such possibilities. Therefore we can say that in-time is a stochastic process where as the actual values observed are a particular realization (sample) of the process. Stationary Processes \u00b6 A stochastic process is said to be stationary if the following conditions are met: 1. Mean is constant over time 2. Variance is constant over time 3. Value of the co-variance between two time periods depends only on the distance or gap or lag between the two time periods and not the actual time at which the co variance is computed This type of process is also called weakly stationary, or co variance stationary, or second-order stationary or wide sense stationary process. Written mathematically, the conditions are: $$ Mean: E(Y_t) = \\mu $$ $$ Variance: var(Y_t) = E(Y_t-\\mu)^2 = \\sigma^2 $$ $$ Covariance: \\gamma_k = E[(Y_y - \\mu)(Y_{t+k} - \\mu)] $$ Purely random or white noise process \u00b6 A stochastic process is purely random if it has zero mean, constant variance, and is serially uncorrelated. An example of white noise is the error term in a linear regression which has zero mean, constant standard deviation and no auto-correlation. Simulation \u00b6 For simulating a stationary process, I am creating 100 realizations(samples) and comparing their mean, variance and co-variance. The data for 6 days and 5 realizations is shown: Samples of Stationary process date realization_1 realization_2 realization_25 realization_50 realization_100 1 2021-08-22 0.8567523 0.0640669 0.7249642 0.3505523 0.8072590 2 2021-08-23 0.1086189 0.3813137 0.5232923 0.4626156 0.7855022 3 2021-08-24 0.4652674 0.3546999 0.2091995 0.2395056 0.9567884 10 2021-08-31 0.8191081 0.1502863 0.1491222 0.6095235 0.3435151 15 2021-09-05 0.9950261 0.1406165 0.1177429 0.9329218 0.4191168 30 2021-09-20 0.8800055 0.9952208 0.7189119 0.7209880 0.6886932 The mean, variance and co-variance between the samples (realizations) across are as follows: For a stationary process, the mean, variance and co variance are constant. Non-stationary Processes \u00b6 If a time series is not stationary, it is called a non-stationary time series. In other words, a non-stationary time series will have a time-varying mean or a time-varying variance or both. Random walk, random walk with drift etc are examples of non-stationary processes. Random walk \u00b6 Suppose \\(\\epsilon_t\\) is a white noise error term with mean 0 and variance \\(\u03c3_2\\) . Then the series \\(Y_t\\) is said to be a random walk if $$ Y_t = Y_{t\u22121} + \\epsilon_t $$ In the random walk model, the value of Y at time t is equal to its value at time (t \u2212 1) plus a random shock. For a random walk, $$ Y_1 = Y_0 + \\epsilon_1 $$ $$ Y_2 = Y_1 + \\epsilon_2 = Y_0 + \\epsilon_1 + \\epsilon_2 $$ $$ Y_3 = Y_2 + \\epsilon_3 = Y_0 + \\epsilon_1 + \\epsilon_2 + \\epsilon_3 $$ and so on.. In general we could write $$ Y_t = Y_0 + \\sum \\epsilon_t $$ As $$ E(Y_t) = E(Y_0 + \\sum \\epsilon_t) = Y_0 $$ $$ var(Y_t) = t\\times \\sigma^2 $$ Although the mean is constant with time, the variance is proportional to time. For simulating a random walk process, I am creating 100 realizations(samples) and comparing their mean, variance and co-variance. The data for 6 days of 5 realizations (samples) is shown: Samples of Random walk process date realization_1 realization_2 realization_25 realization_50 realization_100 1 2021-08-22 4.0000000 4.000000 4.000000 4.000000 4.000000 2 2021-08-23 2.8539208 3.176672 5.446530 5.983017 4.092452 3 2021-08-24 2.9712968 2.009320 5.349939 5.785329 3.451442 10 2021-08-31 -0.7251274 2.289063 2.809076 7.623148 3.587220 15 2021-09-05 -0.5766986 2.559916 5.796322 11.124585 3.992667 30 2021-09-20 0.8613258 6.340583 7.554369 12.667196 9.039007 The mean, variance and covariances between the samples (realizations) across time would look like follows: From the above plot, the mean of Y is equal to its initial, or starting value, which is constant, but as t increases, its variance increases indefinitely, thus violating a condition of stationarity. A random walk process is also called as a unit root process . Random walk with drift \u00b6 If the random walk model predicts that the value at time t will equal the last period's value plus a constant, or drift ( \\(\\delta\\) ), and a white noise term ( \\(\u03b5_t\\) ), then the process is random walk with a drift. $$ Y_t = \\delta + Y_{t\u22121} + \\epsilon_t $$ The mean $$ E(Y_t) = E(Y_0 + \\sum \\epsilon_t + \\delta) = Y_0 + t\\times\\delta $$ so mean is dependent on time and the variance $$ var(Y_t) = t\\times \\sigma^2 $$ is also dependent on time. As random walk with drift violates the conditions of stationary process, it is a non-stationary process. Samples of Random walk with drift process date realization_1 realization_2 realization_25 realization_50 realization_100 1 2021-08-22 4.000000 4.000000 4.000000 4.000000 4.000000 2 2021-08-23 5.801028 5.137174 5.133682 4.186611 4.148583 3 2021-08-24 5.668875 5.741908 4.304969 1.706955 4.467738 10 2021-08-31 11.843748 12.207555 6.289473 5.411678 6.595657 15 2021-09-05 15.982235 15.516312 10.664175 5.937640 11.215054 30 2021-09-20 25.507838 22.875968 20.498484 11.750327 16.976809 The mean, variance and the co-variance are all dependent on time. Unit root stochastic process \u00b6 Unit root stochastic process is another name for Random walk process. A random walk process can be written as $$ Y_t = \\rho \\times Y_{t\u22121} + \\epsilon_t $$ Where \\(\\rho = 1\\) . If \\(|\\rho| < 1\\) then the process represents Markov first order auto regressive model which is stationary. Only for \\(\\rho = 1\\) we get non-stationary. The distribution of mean, variance and co-variance for \\(\\rho =0.5\\) is Deterministic trend process \u00b6 In the above random walk and random walk with drift, the trend component is stochastic in nature. If instead the trend is deterministic in nature, it will follow a deterministic trend process. $$ Y_t = \u03b2_1 + \u03b2_2\\times t + \\epsilon_t$$ In a deterministic trend process, the mean is \\(\u03b2_1 + \u03b2_2\\times t\\) which is proportional with time but the variance is constant. This type of process is also called as trend seasonality as subtracting mean of \\(Y_t\\) from \\(Y_t\\) will give us a stationary process. This procedure is called de-trending. Samples of Deterministic trend process date realization_1 realization_2 realization_25 realization_50 realization_100 1 2021-08-22 0.6408252 0.4380591 3.145050 0.5609335 1.175747 2 2021-08-23 0.7580409 1.5146209 1.939386 2.3880532 2.137324 3 2021-08-24 1.7964618 4.6772129 5.069398 4.7261802 2.939012 10 2021-08-31 7.7861035 9.3627214 8.584801 10.7855074 9.830755 15 2021-09-05 15.8649412 14.2029992 15.279319 13.5478111 14.647241 30 2021-09-20 30.9469391 30.6891297 29.446486 28.9586901 31.170632 A combination of deterministic and stochastic trend could also exist in a process. Comparison. \u00b6 A comparison of all the processes is shown below: References \u00b6 Basic Econometrics - Damodar N Gujarati (textbook for reference) Business Analytics: The Science of Data-Driven Decision Making - Dinesh Kumar (textbook for reference) Customer Analytics at Flipkart.Com - Naveen Bhansali (case study in Harvard business review)","title":"Introduction to stationarity (R)"},{"location":"R/time-series/#time-series-and-stationarity","text":"A time series is a series of data points captured in time order. Most commonly, a time series is a sequence taken at successive equally spaced points in time. This post is the first in a series of blogs on time series methods and forecasting. In this blog, we will discuss about stationarity, random walk, deterministic drift and other vocabulary which form as foundation to time series:","title":"Time Series and stationarity"},{"location":"R/time-series/#stochastic-processes","text":"A random or stochastic process is a collection of random variables ordered in time. It is denoted as \\(Y_t\\) . For example, in-time of an employee is a stochastic process. How is in-time a stochastic process? Consider the in-time on a particular day is 9:00 AM. In theory, the in-time could be any particular value which depends on many factors like traffic, work load, weather etc. The figure 9:00 AM is a particular realization of many such possibilities. Therefore we can say that in-time is a stochastic process where as the actual values observed are a particular realization (sample) of the process.","title":"Stochastic processes"},{"location":"R/time-series/#stationary-processes","text":"A stochastic process is said to be stationary if the following conditions are met: 1. Mean is constant over time 2. Variance is constant over time 3. Value of the co-variance between two time periods depends only on the distance or gap or lag between the two time periods and not the actual time at which the co variance is computed This type of process is also called weakly stationary, or co variance stationary, or second-order stationary or wide sense stationary process. Written mathematically, the conditions are: $$ Mean: E(Y_t) = \\mu $$ $$ Variance: var(Y_t) = E(Y_t-\\mu)^2 = \\sigma^2 $$ $$ Covariance: \\gamma_k = E[(Y_y - \\mu)(Y_{t+k} - \\mu)] $$","title":"Stationary Processes"},{"location":"R/time-series/#purely-random-or-white-noise-process","text":"A stochastic process is purely random if it has zero mean, constant variance, and is serially uncorrelated. An example of white noise is the error term in a linear regression which has zero mean, constant standard deviation and no auto-correlation.","title":"Purely random or white noise process"},{"location":"R/time-series/#simulation","text":"For simulating a stationary process, I am creating 100 realizations(samples) and comparing their mean, variance and co-variance. The data for 6 days and 5 realizations is shown: Samples of Stationary process date realization_1 realization_2 realization_25 realization_50 realization_100 1 2021-08-22 0.8567523 0.0640669 0.7249642 0.3505523 0.8072590 2 2021-08-23 0.1086189 0.3813137 0.5232923 0.4626156 0.7855022 3 2021-08-24 0.4652674 0.3546999 0.2091995 0.2395056 0.9567884 10 2021-08-31 0.8191081 0.1502863 0.1491222 0.6095235 0.3435151 15 2021-09-05 0.9950261 0.1406165 0.1177429 0.9329218 0.4191168 30 2021-09-20 0.8800055 0.9952208 0.7189119 0.7209880 0.6886932 The mean, variance and co-variance between the samples (realizations) across are as follows: For a stationary process, the mean, variance and co variance are constant.","title":"Simulation"},{"location":"R/time-series/#non-stationary-processes","text":"If a time series is not stationary, it is called a non-stationary time series. In other words, a non-stationary time series will have a time-varying mean or a time-varying variance or both. Random walk, random walk with drift etc are examples of non-stationary processes.","title":"Non-stationary Processes"},{"location":"R/time-series/#random-walk","text":"Suppose \\(\\epsilon_t\\) is a white noise error term with mean 0 and variance \\(\u03c3_2\\) . Then the series \\(Y_t\\) is said to be a random walk if $$ Y_t = Y_{t\u22121} + \\epsilon_t $$ In the random walk model, the value of Y at time t is equal to its value at time (t \u2212 1) plus a random shock. For a random walk, $$ Y_1 = Y_0 + \\epsilon_1 $$ $$ Y_2 = Y_1 + \\epsilon_2 = Y_0 + \\epsilon_1 + \\epsilon_2 $$ $$ Y_3 = Y_2 + \\epsilon_3 = Y_0 + \\epsilon_1 + \\epsilon_2 + \\epsilon_3 $$ and so on.. In general we could write $$ Y_t = Y_0 + \\sum \\epsilon_t $$ As $$ E(Y_t) = E(Y_0 + \\sum \\epsilon_t) = Y_0 $$ $$ var(Y_t) = t\\times \\sigma^2 $$ Although the mean is constant with time, the variance is proportional to time. For simulating a random walk process, I am creating 100 realizations(samples) and comparing their mean, variance and co-variance. The data for 6 days of 5 realizations (samples) is shown: Samples of Random walk process date realization_1 realization_2 realization_25 realization_50 realization_100 1 2021-08-22 4.0000000 4.000000 4.000000 4.000000 4.000000 2 2021-08-23 2.8539208 3.176672 5.446530 5.983017 4.092452 3 2021-08-24 2.9712968 2.009320 5.349939 5.785329 3.451442 10 2021-08-31 -0.7251274 2.289063 2.809076 7.623148 3.587220 15 2021-09-05 -0.5766986 2.559916 5.796322 11.124585 3.992667 30 2021-09-20 0.8613258 6.340583 7.554369 12.667196 9.039007 The mean, variance and covariances between the samples (realizations) across time would look like follows: From the above plot, the mean of Y is equal to its initial, or starting value, which is constant, but as t increases, its variance increases indefinitely, thus violating a condition of stationarity. A random walk process is also called as a unit root process .","title":"Random walk"},{"location":"R/time-series/#random-walk-with-drift","text":"If the random walk model predicts that the value at time t will equal the last period's value plus a constant, or drift ( \\(\\delta\\) ), and a white noise term ( \\(\u03b5_t\\) ), then the process is random walk with a drift. $$ Y_t = \\delta + Y_{t\u22121} + \\epsilon_t $$ The mean $$ E(Y_t) = E(Y_0 + \\sum \\epsilon_t + \\delta) = Y_0 + t\\times\\delta $$ so mean is dependent on time and the variance $$ var(Y_t) = t\\times \\sigma^2 $$ is also dependent on time. As random walk with drift violates the conditions of stationary process, it is a non-stationary process. Samples of Random walk with drift process date realization_1 realization_2 realization_25 realization_50 realization_100 1 2021-08-22 4.000000 4.000000 4.000000 4.000000 4.000000 2 2021-08-23 5.801028 5.137174 5.133682 4.186611 4.148583 3 2021-08-24 5.668875 5.741908 4.304969 1.706955 4.467738 10 2021-08-31 11.843748 12.207555 6.289473 5.411678 6.595657 15 2021-09-05 15.982235 15.516312 10.664175 5.937640 11.215054 30 2021-09-20 25.507838 22.875968 20.498484 11.750327 16.976809 The mean, variance and the co-variance are all dependent on time.","title":"Random walk with drift"},{"location":"R/time-series/#unit-root-stochastic-process","text":"Unit root stochastic process is another name for Random walk process. A random walk process can be written as $$ Y_t = \\rho \\times Y_{t\u22121} + \\epsilon_t $$ Where \\(\\rho = 1\\) . If \\(|\\rho| < 1\\) then the process represents Markov first order auto regressive model which is stationary. Only for \\(\\rho = 1\\) we get non-stationary. The distribution of mean, variance and co-variance for \\(\\rho =0.5\\) is","title":"Unit root stochastic process"},{"location":"R/time-series/#deterministic-trend-process","text":"In the above random walk and random walk with drift, the trend component is stochastic in nature. If instead the trend is deterministic in nature, it will follow a deterministic trend process. $$ Y_t = \u03b2_1 + \u03b2_2\\times t + \\epsilon_t$$ In a deterministic trend process, the mean is \\(\u03b2_1 + \u03b2_2\\times t\\) which is proportional with time but the variance is constant. This type of process is also called as trend seasonality as subtracting mean of \\(Y_t\\) from \\(Y_t\\) will give us a stationary process. This procedure is called de-trending. Samples of Deterministic trend process date realization_1 realization_2 realization_25 realization_50 realization_100 1 2021-08-22 0.6408252 0.4380591 3.145050 0.5609335 1.175747 2 2021-08-23 0.7580409 1.5146209 1.939386 2.3880532 2.137324 3 2021-08-24 1.7964618 4.6772129 5.069398 4.7261802 2.939012 10 2021-08-31 7.7861035 9.3627214 8.584801 10.7855074 9.830755 15 2021-09-05 15.8649412 14.2029992 15.279319 13.5478111 14.647241 30 2021-09-20 30.9469391 30.6891297 29.446486 28.9586901 31.170632 A combination of deterministic and stochastic trend could also exist in a process.","title":"Deterministic trend process"},{"location":"R/time-series/#comparison","text":"A comparison of all the processes is shown below:","title":"Comparison."},{"location":"R/time-series/#references","text":"Basic Econometrics - Damodar N Gujarati (textbook for reference) Business Analytics: The Science of Data-Driven Decision Making - Dinesh Kumar (textbook for reference) Customer Analytics at Flipkart.Com - Naveen Bhansali (case study in Harvard business review)","title":"References"}]}